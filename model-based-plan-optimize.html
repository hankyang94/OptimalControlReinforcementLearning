<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Model-based Planning and Optimization | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Model-based Planning and Optimization | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Model-based Planning and Optimization | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-10-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="policy-gradient.html"/>
<link rel="next" href="advanced-materials.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.1.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.1.4" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.1.4</b> Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.2</b> Infinite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mdp.html"><a href="mdp.html#value-functions"><i class="fa fa-check"></i><b>1.2.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation-1"><i class="fa fa-check"></i><b>1.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.2.3" data-path="mdp.html"><a href="mdp.html#principle-of-optimality"><i class="fa fa-check"></i><b>1.2.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.2.4" data-path="mdp.html"><a href="mdp.html#policy-improvement"><i class="fa fa-check"></i><b>1.2.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="1.2.5" data-path="mdp.html"><a href="mdp.html#policy-iteration"><i class="fa fa-check"></i><b>1.2.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="1.2.6" data-path="mdp.html"><a href="mdp.html#value-iteration"><i class="fa fa-check"></i><b>1.2.6</b> Value Iteration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="value-rl.html"><a href="value-rl.html"><i class="fa fa-check"></i><b>2</b> Value-based Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="value-rl.html"><a href="value-rl.html#tabular-methods"><i class="fa fa-check"></i><b>2.1</b> Tabular Methods</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="value-rl.html"><a href="value-rl.html#tabular-PE"><i class="fa fa-check"></i><b>2.1.1</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.1.2" data-path="value-rl.html"><a href="value-rl.html#value-rl-convergence-td"><i class="fa fa-check"></i><b>2.1.2</b> Convergence Proof of TD Learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control"><i class="fa fa-check"></i><b>2.1.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.1.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control"><i class="fa fa-check"></i><b>2.1.4</b> Off-Policy Control</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="value-rl.html"><a href="value-rl.html#function-approximation"><i class="fa fa-check"></i><b>2.2</b> Function Approximation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="value-rl.html"><a href="value-rl.html#basics-of-continuous-mdp"><i class="fa fa-check"></i><b>2.2.1</b> Basics of Continuous MDP</a></li>
<li class="chapter" data-level="2.2.2" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-2"><i class="fa fa-check"></i><b>2.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.2.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control-1"><i class="fa fa-check"></i><b>2.2.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.2.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control-1"><i class="fa fa-check"></i><b>2.2.4</b> Off-Policy Control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="policy-gradient.html"><a href="policy-gradient.html"><i class="fa fa-check"></i><b>3</b> Policy Gradient Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-optimization"><i class="fa fa-check"></i><b>3.1</b> Gradient-based Optimization</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="policy-gradient.html"><a href="policy-gradient.html#basic-setup"><i class="fa fa-check"></i><b>3.1.1</b> Basic Setup</a></li>
<li class="chapter" data-level="3.1.2" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-ascent-and-descent"><i class="fa fa-check"></i><b>3.1.2</b> Gradient Ascent and Descent</a></li>
<li class="chapter" data-level="3.1.3" data-path="policy-gradient.html"><a href="policy-gradient.html#stochastic-gradients"><i class="fa fa-check"></i><b>3.1.3</b> Stochastic Gradients</a></li>
<li class="chapter" data-level="3.1.4" data-path="policy-gradient.html"><a href="policy-gradient.html#beyond-vanilla-gradient-methods"><i class="fa fa-check"></i><b>3.1.4</b> Beyond Vanilla Gradient Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#policy-gradients"><i class="fa fa-check"></i><b>3.2</b> Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="policy-gradient.html"><a href="policy-gradient.html#setup"><i class="fa fa-check"></i><b>3.2.1</b> Setup</a></li>
<li class="chapter" data-level="3.2.2" data-path="policy-gradient.html"><a href="policy-gradient.html#the-policy-gradient-lemma"><i class="fa fa-check"></i><b>3.2.2</b> The Policy Gradient Lemma</a></li>
<li class="chapter" data-level="3.2.3" data-path="policy-gradient.html"><a href="policy-gradient.html#reinforce"><i class="fa fa-check"></i><b>3.2.3</b> REINFORCE</a></li>
<li class="chapter" data-level="3.2.4" data-path="policy-gradient.html"><a href="policy-gradient.html#baselines-and-variance-reduction"><i class="fa fa-check"></i><b>3.2.4</b> Baselines and Variance Reduction</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#actorcritic-methods"><i class="fa fa-check"></i><b>3.3</b> Actor–Critic Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#anatomy-of-an-actorcritic"><i class="fa fa-check"></i><b>3.3.1</b> Anatomy of an Actor–Critic</a></li>
<li class="chapter" data-level="3.3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#ActorCriticTD"><i class="fa fa-check"></i><b>3.3.2</b> On-Policy Actor–Critic with TD(0)</a></li>
<li class="chapter" data-level="3.3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#PG-GAE"><i class="fa fa-check"></i><b>3.3.3</b> Generalized Advantage Estimation (GAE)</a></li>
<li class="chapter" data-level="3.3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#off-policy-actorcritic"><i class="fa fa-check"></i><b>3.3.4</b> Off-Policy Actor–Critic</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#advanced-policy-gradients"><i class="fa fa-check"></i><b>3.4</b> Advanced Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="policy-gradient.html"><a href="policy-gradient.html#revisiting-generalized-policy-iteration"><i class="fa fa-check"></i><b>3.4.1</b> Revisiting Generalized Policy Iteration</a></li>
<li class="chapter" data-level="3.4.2" data-path="policy-gradient.html"><a href="policy-gradient.html#performance-difference-lemma"><i class="fa fa-check"></i><b>3.4.2</b> Performance Difference Lemma</a></li>
<li class="chapter" data-level="3.4.3" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-constraint"><i class="fa fa-check"></i><b>3.4.3</b> Trust Region Constraint</a></li>
<li class="chapter" data-level="3.4.4" data-path="policy-gradient.html"><a href="policy-gradient.html#natural-policy-gradient"><i class="fa fa-check"></i><b>3.4.4</b> Natural Policy Gradient</a></li>
<li class="chapter" data-level="3.4.5" data-path="policy-gradient.html"><a href="policy-gradient.html#proof-fisher"><i class="fa fa-check"></i><b>3.4.5</b> Proof of Fisher Information</a></li>
<li class="chapter" data-level="3.4.6" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-policy-optimization"><i class="fa fa-check"></i><b>3.4.6</b> Trust Region Policy Optimization</a></li>
<li class="chapter" data-level="3.4.7" data-path="policy-gradient.html"><a href="policy-gradient.html#proximal-policy-optimization"><i class="fa fa-check"></i><b>3.4.7</b> Proximal Policy Optimization</a></li>
<li class="chapter" data-level="3.4.8" data-path="policy-gradient.html"><a href="policy-gradient.html#soft-actorcritic"><i class="fa fa-check"></i><b>3.4.8</b> Soft Actor–Critic</a></li>
<li class="chapter" data-level="3.4.9" data-path="policy-gradient.html"><a href="policy-gradient.html#deterministic-policy-gradient"><i class="fa fa-check"></i><b>3.4.9</b> Deterministic Policy Gradient</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="policy-gradient.html"><a href="policy-gradient.html#model-based-policy-optimization"><i class="fa fa-check"></i><b>3.5</b> Model-based Policy Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html"><i class="fa fa-check"></i><b>4</b> Model-based Planning and Optimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr"><i class="fa fa-check"></i><b>4.1</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#finite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.1</b> Finite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#infinite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.2</b> Infinite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#linear-system-basics"><i class="fa fa-check"></i><b>4.1.3</b> Linear System Basics</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr-tracking"><i class="fa fa-check"></i><b>4.2</b> LQR Trajectory Tracking</a></li>
<li class="chapter" data-level="4.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#traj-opt"><i class="fa fa-check"></i><b>4.3</b> Trajectory Optimization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#traj-opt-ilqr"><i class="fa fa-check"></i><b>4.3.1</b> Iterative LQR</a></li>
<li class="chapter" data-level="4.3.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#ddp"><i class="fa fa-check"></i><b>4.3.2</b> Differential Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#mpc"><i class="fa fa-check"></i><b>4.4</b> Model Predictive Control</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-materials.html"><a href="advanced-materials.html"><i class="fa fa-check"></i><b>5</b> Advanced Materials</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-based-plan-optimize" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Model-based Planning and Optimization<a href="model-based-plan-optimize.html#model-based-plan-optimize" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="mdp.html#mdp">1</a>, we studied tabular MDPs with known dynamics where both the state and action spaces are finite, and we saw how dynamic-programming methods—Policy Iteration (PI) and Value Iteration (VI)—recover optimal policies.</p>
<p>Chapters <a href="value-rl.html#value-rl">2</a> and <a href="policy-gradient.html#policy-gradient">3</a> generalized these ideas to unknown dynamics and continuous spaces by introducing function approximation for value functions and policies. Those methods are <em>model-free</em>: they assume no access to the transition model and rely solely on data collected from interaction.</p>
<p>This chapter turns to the complementary regime: <strong>known dynamics</strong> with <strong>continuous state and action spaces</strong>. Our goal is to develop model-based planning and optimization methods that exploit the known dynamics to compute high-quality decisions efficiently.</p>
<p>We proceed in three steps:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Linear-quadratic systems.</strong> For linear dynamics and quadratic stage/terminal rewards (or costs), dynamic programming yields a linear optimal policy that can be computed efficiently via Riccati recursions. This setting serves as a tractable, illuminating baseline and a recurring building block in more general algorithms.</p></li>
<li><p><strong>Trajectory optimization (TO) for nonlinear systems.</strong> When dynamics are nonlinear, we focus on planning an optimal state–action trajectory from a given initial condition, maximizing the cumulative reward (or minimizing cumulative cost) subject to dynamics and constraints. Unlike RL, which seeks an optimal <em>feedback policy</em> valid for all states, TO computes an <em>open-loop plan</em> (often with a time-varying local feedback around a nominal trajectory). Although less ambitious, TO naturally accommodates state/control constraints—common in motion planning under safety, actuation, and environmental limits—and is widely used in robotics and control.</p></li>
<li><p><strong>Model predictive control (MPC).</strong> MPC converts open-loop plans into a feedback controller by repeatedly solving a short-horizon TO problem at each time step, applying only the first action, and receding the horizon. This receding-horizon strategy brings robustness to disturbances and model mismatch while retaining the constraint-handling benefits of TO.</p></li>
</ol>
<p>We adopt the standard discrete-time dynamical system notation
<span class="math display">\[
x_{t+1} = f_t(x_t, u_t, w_t),
\]</span>
where <span class="math inline">\(x_t \in \mathbb{R}^n\)</span> is the state, <span class="math inline">\(u_t \in \mathbb{R}^m\)</span> is the control/action, <span class="math inline">\(w_t \in \mathbb{R}^d\)</span> is a (possibly stochastic) disturbance, and <span class="math inline">\(f_t\)</span> is a known transition function, potentially nonlinear or nonsmooth. The goal is to find a policy that maximizes a sum of stage rewards <span class="math inline">\(r(x_t,u_t)\)</span> and optional terminal reward <span class="math inline">\(r_T(x_T)\)</span>. We will often use the cost-minimization form <span class="math inline">\(c = -r\)</span>.
State and action constraints are written as
<span class="math display">\[
x_t \in \mathcal{X}, \qquad u_t \in \mathcal{U}.
\]</span></p>
<div id="lqr" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Linear Quadratic Regulator<a href="model-based-plan-optimize.html#lqr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we focus on the case when <span class="math inline">\(f_t\)</span> is a linear function, and the rewards/costs are quadratic in <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span>. This family of problems is known as linear quadratic regulator (LQR).</p>
<div id="finite-horizon-lqr" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Finite-Horizon LQR<a href="model-based-plan-optimize.html#finite-horizon-lqr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a linear discrete-time dynamical system
<span class="math display" id="eq:lqr-linear-system">\[\begin{equation}
x_{k+1} = A_k x_k + B_k u_k + w_k, \quad k=0,1,\dots,N-1,
\tag{4.1}
\end{equation}\]</span>
where <span class="math inline">\(x_k \in \mathbb{R}^n\)</span> the state, <span class="math inline">\(u_k \in \mathbb{R}^m\)</span> the control, <span class="math inline">\(w_k \in \mathbb{R}^n\)</span> the independent, zero-mean disturbance with given probability distribution that does not depend on <span class="math inline">\(x_k,u_k\)</span>, and <span class="math inline">\(A_k \in \mathbb{R}^{n \times n}, B_k \in \mathbb{R}^{n \times m}\)</span> are known matrices determining the transition dynamics.</p>
<p>We want to solve the following optimal control problem
<span class="math display" id="eq:lqr-formulation">\[\begin{equation}
\min_{\mu_0,\dots,\mu_{N-1}} \mathbb{E} \left\{ x_N^\top Q_N x_N + \sum_{k=0}^{N-1} \left( x_k^\top Q_k x_k + u_k^\top R_k u_k \right) \right\},
\tag{4.2}
\end{equation}\]</span>
where <span class="math inline">\(\mu_0,\dots,\mu_{N-1}\)</span> are feedback policies/controllers that map states to actions and the expectation is taken over the randomness in <span class="math inline">\(w_0,\dots,w_{N-1}\)</span>. In <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.2)</a>, <span class="math inline">\(\{Q_k \}_{k=0}^N\)</span> are positive semidefinite matrices, and <span class="math inline">\(\{ R_k \}_{k=0}^{N-1}\)</span> are positive definite matrices. The formulation <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.2)</a> is known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to “regulate” the system around the origin <span class="math inline">\(x=0\)</span>.</p>
<p>The Bellman Optimality condition introduced in Theorem <a href="mdp.html#thm:FiniteHorizonMDPBellmanOptimality">1.1</a> still holds for continuous state and action spaces. Therefore, we will try to follow the dynamic programming (DP) algorithm in Section <a href="mdp.html#dp">1.1.4</a> to solve for the optimal policy.</p>
<p>The DP algorithm computes the optimal cost-to-go backwards in time.
The terminal cost is
<span class="math display">\[
J_N(x_N) = x_N^\top Q_N x_N
\]</span>
by definition.</p>
<p>The optimal cost-to-go at time <span class="math inline">\(N-1\)</span> is equal to
<span class="math display" id="eq:lqr-cost-N-1">\[\begin{equation}
\begin{split}
J_{N-1}(x_{N-1}) = \min_{u_{N-1}} \mathbb{E}_{w_{N-1}} \{ x_{N-1}^\top Q_{N-1} x_{N-1} + u_{N-1}^\top R_{N-1} u_{N-1} + \\ \Vert \underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \Vert^2_{Q_N} \}
\end{split}
\tag{4.3}
\end{equation}\]</span>
where <span class="math inline">\(\Vert v \Vert_Q^2 = v^\top Q v\)</span> for <span class="math inline">\(Q \succeq 0\)</span>. Now observe that the objective in <a href="model-based-plan-optimize.html#eq:lqr-cost-N-1">(4.3)</a> is
<span class="math display">\[\begin{equation}
\begin{split}
x_{N-1}^\top Q_{N-1} x_{N-1} + u_{N-1}^\top R_{N-1} u_{N-1} + \Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \Vert_{Q_N}^2 + \\
\mathbb{E}_{w_{N-1}} \left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^\top Q_{N-1} w_{N-1} \right] + \\
\mathbb{E}_{w_{N-1}} \left[ w_{N-1}^\top Q_N w_{N-1} \right]
\end{split}
\end{equation}\]</span>
where the second line is zero due to <span class="math inline">\(\mathbb{E}[w_{N-1}] = 0\)</span> and the third line is a constant with respect to <span class="math inline">\(u_{N-1}\)</span>. Consequently, the optimal control <span class="math inline">\(u_{N-1}^\star\)</span> can be computed by setting the derivative of the objective with respect to <span class="math inline">\(u_{N-1}\)</span> equal to zero
<span class="math display" id="eq:optimal-u-N-1">\[\begin{equation}
u_{N-1}^\star = - \left[ \left( R_{N-1} + B_{N-1}^\top Q_N B_{N-1} \right)^{-1} B_{N-1}^\top Q_N A_{N-1} \right] x_{N-1}.
\tag{4.4}
\end{equation}\]</span>
Plugging the optimal controller <span class="math inline">\(u^\star_{N-1}\)</span> back to the objective of <a href="model-based-plan-optimize.html#eq:lqr-cost-N-1">(4.3)</a> leads to
<span class="math display" id="eq:optimal-cost-N-1">\[\begin{equation}
J_{N-1}(x_{N-1}) = x_{N-1}^\top S_{N-1} x_{N-1} + \mathbb{E} \left[ w_{N-1}^\top Q_N w_{N-1} \right],
\tag{4.5}
\end{equation}\]</span>
with
<span class="math display">\[
S_{N-1} = Q_{N-1} + A_{N-1}^\top \left[ Q_N - Q_N B_{N-1} \left( R_{N-1} + B_{N-1}^\top Q_N B_{N-1} \right)^{-1} B_{N-1}^\top Q_N \right] A_{N-1}.
\]</span>
We note that <span class="math inline">\(S_{N-1}\)</span> is positive semidefinite (this is an exercise for you to convince yourself).</p>
<p>Now we realize that something surprising and nice has happened.</p>
<ol style="list-style-type: decimal">
<li><p>The optimal controller <span class="math inline">\(u^{\star}_{N-1}\)</span> in <a href="model-based-plan-optimize.html#eq:optimal-u-N-1">(4.4)</a> is a linear feedback policy of the state <span class="math inline">\(x_{N-1}\)</span>, and</p></li>
<li><p>The optimal cost-to-go <span class="math inline">\(J_{N-1}(x_{N-1})\)</span> in <a href="model-based-plan-optimize.html#eq:optimal-cost-N-1">(4.5)</a> is quadratic in <span class="math inline">\(x_{N-1}\)</span>, just the same as <span class="math inline">\(J_{N}(x_N)\)</span>.</p></li>
</ol>
<p>This implies that, if we continue to compute the optimal cost-to-go at time <span class="math inline">\(N-2\)</span>, we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is,</p>
<blockquote>
<p>The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time.</p>
</blockquote>
<p>We summarize the solution for the LQR problem <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.2)</a> as follows.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:discretetimefinitehorizonlqrsolution" class="proposition"><strong>Proposition 4.1  (Solution of Discrete-Time Finite-Horizon LQR) </strong></span>The optimal controller for the LQR problem <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.2)</a> is a linear state-feedback policy
<span class="math display" id="eq:lqr-solution-control">\[\begin{equation}
\mu_k^\star(x_k) = - K_k x_k, \quad k=0,\dots,N-1.
\tag{4.6}
\end{equation}\]</span>
The gain matrix <span class="math inline">\(K_k\)</span> can be computed as
<span class="math display">\[
K_k = \left( R_k + B_k^\top S_{k+1} B_k  \right)^{-1} B_k^\top S_{k+1} A_k,
\]</span>
where the matrix <span class="math inline">\(S_k\)</span> satisfies the following backwards recursion
<span class="math display" id="eq:finite-discrete-lqr-riccati">\[\begin{equation}
\hspace{-6mm}
\begin{split}
S_N &amp;= Q_N \\
S_k &amp;= Q_k + A_k^\top \left[ S_{k+1} - S_{k+1}B_k \left( R_k + B_k^\top S_{k+1} B_k  \right)^{-1}  B_k^\top S_{k+1}  \right] A_k, k=N-1,\dots,0.
\end{split}
\tag{4.7}
\end{equation}\]</span>
The optimal cost-to-go is given by
<span class="math display">\[
J_0(x_0) = x_0^\top S_0 x_0 + \sum_{k=0}^{N-1} \mathbb{E} \left[ w_k^\top S_{k+1} w_k\right].
\]</span>
The recursion <a href="model-based-plan-optimize.html#eq:finite-discrete-lqr-riccati">(4.7)</a> is called the <em>discrete-time Riccati equation</em>.</p>
</div>
</div>
<p>Proposition <a href="model-based-plan-optimize.html#prp:discretetimefinitehorizonlqrsolution">4.1</a> states that, to evaluate the optimal policy <a href="model-based-plan-optimize.html#eq:lqr-solution-control">(4.6)</a>, one can first run the backwards Riccati equation <a href="model-based-plan-optimize.html#eq:finite-discrete-lqr-riccati">(4.7)</a> to compute all the positive definite matrices <span class="math inline">\(S_k\)</span>, and then compute the gain matrices <span class="math inline">\(K_k\)</span>. For systems of reasonable dimensions, evalutating the matrix inversion in <a href="model-based-plan-optimize.html#eq:finite-discrete-lqr-riccati">(4.7)</a> should be fairly efficient.</p>
</div>
<div id="infinite-horizon-lqr" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Infinite-Horizon LQR<a href="model-based-plan-optimize.html#infinite-horizon-lqr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now switch to the infinite-horizon LQR problem
<span class="math display" id="eq:infinite-horizon-lqr-system" id="eq:infinite-horizon-lqr-cost">\[\begin{align}
\min_{\mu} &amp; \quad  \sum_{k=0}^{\infty} \left( x_k^\top Q x_k + u_k^\top R u_k \right) \tag{4.8} \\
\text{subject to} &amp; \quad x_{k+1} = A x_k + B u_k, \quad k=0,\dots,\infty, \tag{4.9}
\end{align}\]</span>
where <span class="math inline">\(Q \succeq 0\)</span>, <span class="math inline">\(R \succ 0\)</span>, <span class="math inline">\(A,B\)</span> are constant matrices, and we seek a stationary policy <span class="math inline">\(\mu\)</span> that maps states to actions. Note that here we remove the disturbance <span class="math inline">\(w_k\)</span> because in general adding <span class="math inline">\(w_k\)</span> will make the objective function unbounded. To handle <span class="math inline">\(w_k\)</span>, we will have to either add a discount factor <span class="math inline">\(\gamma\)</span>, or switch to an average cost objective function.</p>
<p>For infinite-horizon problems, the Bellman Optimality condition changes from a recursion to an equation. Specifically, according to Theorem <a href="mdp.html#thm:BellmanOptimalityInfiniteHorizon">1.2</a> and equation <a href="mdp.html#eq:BellmanOptimalityInfiniteHorizonStateValue">(1.28)</a>, the optimal value function should satisfy the following Bellman optimality equation, restated for the case of cost minimization instead of reward maximization:
<span class="math display" id="eq:BellmanOptimalityInfiniteHorizonRestateMin">\[\begin{equation}
J^\star (x) = \min_{u} \left[ c(x,u) + \sum_{x&#39;} P(x&#39; \mid x, u) J^\star (x&#39;) \right], \quad \forall x,
\tag{4.10}
\end{equation}\]</span>
where <span class="math inline">\(c(x,u)\)</span> is the cost function.</p>
<p><strong>Guess A Solution.</strong> Based on our derivation in the finite-horizon case, we might as well guess that the optimal value function is a quadratic function:
<span class="math display">\[
J(x) = x^\top S x, \quad \forall x,
\]</span>
for some positive definite matrix <span class="math inline">\(S\)</span>. Then, our guessed solution must satisfy the Bellman optimality stated in <a href="model-based-plan-optimize.html#eq:BellmanOptimalityInfiniteHorizonRestateMin">(4.10)</a>:
<span class="math display" id="eq:infinite-horizon-lqr-invoke-dp">\[\begin{equation}
x^\top S x = J(x) = \min_{u} \left\{ x^\top Q x + u^\top R u + \Vert \underbrace{A x + B u}_{x&#39;} \Vert_S^2  \right\}.
\tag{4.11}
\end{equation}\]</span>
The minimization over <span class="math inline">\(u\)</span> in <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-invoke-dp">(4.11)</a> can again be solved in closed-form by setting the gradient of the objective with respect to <span class="math inline">\(u\)</span> to be zero
<span class="math display" id="eq:infinite-horizon-lqr-control">\[\begin{equation}
u^\star = - \underbrace{\left[ \left( R + B^\top S B \right)^{-1} B^\top S A \right]}_{K} x.
\tag{4.12}
\end{equation}\]</span>
Plugging the optimal <span class="math inline">\(u^\star\)</span> back into <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-invoke-dp">(4.11)</a>, we see that the matrix <span class="math inline">\(S\)</span> has to satisfy the following equation
<span class="math display" id="eq:algebraic-riccati">\[\begin{equation}
S = Q + A^\top \left[  S - SB \left( R + B^\top S B  \right)^{-1} B^\top S \right] A.
\tag{4.13}
\end{equation}\]</span>
Equation <a href="model-based-plan-optimize.html#eq:algebraic-riccati">(4.13)</a> is known as the <em>discrete algebraic Riccati equation</em> (DARE).</p>
<p>So the question boils down to if the DARE has a solution <span class="math inline">\(S\)</span> that is positive definite?</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:infinitehorizonlqrsolution" class="proposition"><strong>Proposition 4.2  (Solution of Discrete-Time Infinite-Horizon LQR) </strong></span>Consider a linear system
<span class="math display">\[
x_{k+1} = A x_k + B u_k,
\]</span>
with <span class="math inline">\((A,B)\)</span> controllable (see Section <a href="model-based-plan-optimize.html#linear-system-basics">4.1.3</a>). Let <span class="math inline">\(Q \succeq 0\)</span> in <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-cost">(4.8)</a> be such that <span class="math inline">\(Q\)</span> can be written as <span class="math inline">\(Q = C^\top C\)</span> with <span class="math inline">\((A,C)\)</span> observable.</p>
<p>Then the optimal controller for the infinite-horizon LQR problem <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-cost">(4.8)</a> is a stationary linear policy
<span class="math display">\[
\mu^\star (x) = - K x,
\]</span>
with
<span class="math display">\[
K = \left( R + B^\top S B \right)^{-1} B^\top S A.
\]</span>
The matrix <span class="math inline">\(S\)</span> is the unique positive definite matrix that satisfies the discrete algebraic Riccati equation
<span class="math display" id="eq:discrete-algebraic-riccati-equation">\[\begin{equation}
S = Q + A^\top \left[  S - SB \left( R + B^\top S B  \right)^{-1} B^\top S \right] A.
\tag{4.14}
\end{equation}\]</span></p>
<p>Moreover, the closed-loop system
<span class="math display">\[
x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k
\]</span>
is stable, i.e., the eigenvalues of the matrix <span class="math inline">\(A - BK\)</span> are strictly within the unit circle (see Appendix <a href="app-lti-system-theory.html#app-lti-stability-dt">B.1.2</a>).</p>
</div>
</div>
<p><em>Remark.</em> The assumptions of <span class="math inline">\((A,B)\)</span> being controllable and <span class="math inline">\((A,C)\)</span> being observable can be relaxted to <span class="math inline">\((A,B)\)</span> being stabilizable and <span class="math inline">\((A,C)\)</span> being detectable (for definitions of stabilizability and detectability, see Appendix <a href="app-lti-system-theory.html#app-lti-system-theory">B</a>).</p>
<p>We have not discussed how to solve the algebraic Riccati equation <a href="model-based-plan-optimize.html#eq:discrete-algebraic-riccati-equation">(4.14)</a>. It is clear that <a href="model-based-plan-optimize.html#eq:discrete-algebraic-riccati-equation">(4.14)</a> is not a linear system of equations in <span class="math inline">\(S\)</span>. In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see <span class="citation">(<a href="#ref-arnold84ieee-generalized">Arnold and Laub 1984</a>)</span>. Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab <a href="https://www.mathworks.com/help/control/ref/dlqr.html"><code>dlqr</code></a> and the Python <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_discrete_are.html"><code>scipy.linalg.solve_discrete_are</code></a> function computes the <span class="math inline">\(K\)</span> and <span class="math inline">\(S\)</span> matrices from <span class="math inline">\(A,B,Q,R\)</span>.</p>
<p>Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:lqr-pendulum-stabilization" class="example"><strong>Example 4.1  (Pendulum Stabilization by LQR) </strong></span>Consider the simple pendulum in Fig. <a href="model-based-plan-optimize.html#fig:pendulum-drawing">4.1</a> with dynamics
<span class="math display" id="eq:lqr-pendulum-dynamics">\[\begin{equation}
x = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}, \quad
\dot{x} = f(x,u) = \begin{bmatrix}
\dot{\theta} \\
-\frac{1}{ml^2}(b \dot{\theta} + mgl \sin \theta) + \frac{1}{ml^2} u
\end{bmatrix}
\tag{4.15}
\end{equation}\]</span>
where <span class="math inline">\(m\)</span> is the mass of the pendulum, <span class="math inline">\(l\)</span> is the length of the pole, <span class="math inline">\(g\)</span> is the gravitational constant, <span class="math inline">\(b\)</span> is the damping ratio, and <span class="math inline">\(u\)</span> is the torque applied to the pendulum.</p>
<p>We are interested in applying the LQR controller to balance the pendulum in the upright position <span class="math inline">\(x_d = [\pi,0]^\top\)</span> with a zero velocity.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-drawing"></span>
<img src="images/pendulum-drawing.png" alt="A Simple Pendulum." width="40%" />
<p class="caption">
Figure 4.1: A Simple Pendulum.
</p>
</div>
<p>Let us first shift the dynamics so that “<span class="math inline">\(0\)</span>” is the upright position. This can be done by defining a new variable <span class="math inline">\(z = x - x_d = [\theta - \pi, \dot{\theta}]^\top\)</span>, which leads to
<span class="math display" id="eq:pendulum-dynamics-z-coordinate">\[\begin{equation}
\dot{z} = \dot{x} = f(x,u) = f(z + x_d,u) = \begin{bmatrix}
z_2 \\
\frac{1}{ml^2} \left( u - b z_2 + mgl \sin z_1  \right)
\end{bmatrix} = f&#39;(z,u).
\tag{4.16}
\end{equation}\]</span>
We then linearize the nonlinear dynamics <span class="math inline">\(\dot{z} = f&#39;(z,u)\)</span> at the point <span class="math inline">\(z^\star = 0, u^\star = 0\)</span>:
<span class="math display">\[\begin{align}
\dot{z} &amp; \approx f&#39;(z^\star,u^\star) + \left( \frac{\partial f&#39;}{\partial z} \right)_{z^\star,u^\star} (z - z^\star) + \left( \frac{\partial f&#39;}{\partial u} \right)_{z^\star,u^\star} (u - u^\star) \\
&amp; = \begin{bmatrix}
0 &amp; 1 \\
\frac{g}{l} \cos z_1 &amp; - \frac{b}{ml^2}
\end{bmatrix}_{z^\star, u^\star} z +
\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix} u \\
&amp; = \underbrace{\begin{bmatrix}
0 &amp; 1 \\
\frac{g}{l} &amp; - \frac{b}{ml^2}
\end{bmatrix}}_{A_c} z  +
\underbrace{\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix}}_{B_c} u.
\end{align}\]</span>
Finally, we convert the continuous-time dynamics to discrete time with a fixed discretization <span class="math inline">\(h\)</span>
<span class="math display">\[
z_{k+1} = \dot{z}_k \cdot h + z_k = \underbrace{(h \cdot A_c + I )}_{A} z_k + \underbrace{(h \cdot B_c)}_{B} u_k.
\]</span></p>
<p>We are now ready to implement the LQR controller. In the formulation <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-cost">(4.8)</a>, we choose <span class="math inline">\(Q = I\)</span>, <span class="math inline">\(R = I\)</span>, and compute the gain matrix <span class="math inline">\(K\)</span> by solving the DARE.</p>
<p>Fig. <a href="model-based-plan-optimize.html#fig:pendulum-stabilization-sim">4.2</a> shows the simulation result for <span class="math inline">\(m=1,l=1,b=0.1\)</span>, <span class="math inline">\(g = 9.8\)</span>, and <span class="math inline">\(h = 0.01\)</span>, with an initial condition <span class="math inline">\(z^0 = [0.1,0.1]^\top\)</span>. We can see that the LQR controller successfully stabilizes the pendulum at <span class="math inline">\(z^\star\)</span>, the upright position.</p>
<p>You can play with the Python code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_lqr_stabilization.py">here</a>.</p>
<p>Alternatively, the Matlab code can be found <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/pendulum_stabilization_lqr.m">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-stabilization-sim"></span>
<img src="images/Model-based-optimization/pendulum_lqr_stabilization.png" alt="LQR stabilization of a simple pendulum." width="60%" />
<p class="caption">
Figure 4.2: LQR stabilization of a simple pendulum.
</p>
</div>
</div>
</div>
</div>
<div id="linear-system-basics" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Linear System Basics<a href="model-based-plan-optimize.html#linear-system-basics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the discrete-time linear time-invariant (LTI) system
<span class="math display">\[
x_{k+1}=Ax_k+Bu_k,\qquad y_k=Cx_k+Du_k,
\]</span>
with <span class="math inline">\(x_k\in\mathbb{R}^n,\;u_k\in\mathbb{R}^m,\;y_k\in\mathbb{R}^p\)</span>.</p>
<p>We provide a very brief review of linear system theory to understand Proposition <a href="model-based-plan-optimize.html#prp:infinitehorizonlqrsolution">4.2</a>. More details can be found in Appendix <a href="app-lti-system-theory.html#app-lti-system-theory">B</a>.</p>
<p><strong>Stability.</strong> The autonomous system <span class="math inline">\(x_{k+1}=Ax_k\)</span> is (asymptotically) stable if for every <span class="math inline">\(x_0\)</span> we have <span class="math inline">\(x_k\to 0\)</span> as <span class="math inline">\(k\to\infty\)</span>.</p>
<p><strong>Equivalent characterizations.</strong></p>
<ul>
<li><p><span class="math inline">\(A\)</span> is <strong>Schur</strong>: all eigenvalues satisfy <span class="math inline">\(|\lambda_i(A)|&lt;1\)</span>.</p></li>
<li><p>Lyapunov: <span class="math inline">\(\exists P\succ 0\)</span> s.t. <span class="math inline">\(A^\top P A - P \prec 0\)</span>.</p></li>
</ul>
<p><strong>Controllability (reachability).</strong> The pair <span class="math inline">\((A,B)\)</span> is controllable if for any <span class="math inline">\(x_0,x_f\)</span> there exists a <em>finite</em> input sequence <span class="math inline">\(\{u_0,\dots,u_{N-1}\}\)</span> that drives the state from <span class="math inline">\(x_0\)</span> to <span class="math inline">\(x_N=x_f\)</span>.</p>
<p><strong>Kalman controllability matrix.</strong>
<span class="math display">\[
  \mathcal C \;=\; [\,B\; AB\; A^2B\;\cdots\; A^{n-1}B\,],\quad
  \text{\((A,B)\) controllable} \iff \operatorname{rank}(\mathcal C)=n.
  \]</span></p>
<p><strong>Popov-Belevitch-Hautus (PBH) test.</strong>
<span class="math display">\[
  \text{\((A,B)\) controllable} \iff
  \operatorname{rank}\!\begin{bmatrix}\lambda I - A &amp; B\end{bmatrix} = n
  \ \text{for all}\ \lambda\in\mathbb{C}.
  \]</span>
It suffices to check <span class="math inline">\(\lambda\)</span> equal to the eigenvalues of <span class="math inline">\(A\)</span>.</p>
<p><strong>Observability.</strong> The pair <span class="math inline">\((A,C)\)</span> is observable if the initial state <span class="math inline">\(x_0\)</span> can be uniquely determined from a finite sequence of outputs (and known inputs), e.g., from <span class="math inline">\(\{y_0,\dots,y_{n-1}\}\)</span>.</p>
<p><strong>Observability matrix.</strong>
<span class="math display">\[
  \mathcal O \;=\; \begin{bmatrix} C \\ CA \\ \vdots \\ CA^{n-1}\end{bmatrix},\quad
  \text{\((A,C)\) observable} \iff \operatorname{rank}(\mathcal O)=n.
  \]</span></p>
<p><strong>PBH test.</strong>
<span class="math display">\[
  \text{\((A,C)\) observable} \iff
  \operatorname{rank}\!\begin{bmatrix}\lambda I - A^\top &amp; C^\top\end{bmatrix} = n
  \ \text{for all}\ \lambda\in\mathbb{C}.
  \]</span>
Dual to controllability: <span class="math inline">\((A,C)\)</span> observable <span class="math inline">\(\Leftrightarrow\)</span> <span class="math inline">\((A^\top,C^\top)\)</span> controllable.</p>
</div>
</div>
<div id="lqr-tracking" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> LQR Trajectory Tracking<a href="model-based-plan-optimize.html#lqr-tracking" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Classical LQR delivers an optimal linear state-feedback when dynamics are linear and the objective is quadratic. In many planning problems, however, we do not seek a single stationary feedback for <em>all</em> states but rather a <em>local stabilizer around a given (possibly time-varying) trajectory</em>—for instance, a motion plan from a trajectory optimizer or MPC’s rolling nominal (see Section <a href="model-based-plan-optimize.html#traj-opt">4.3</a>). LQR Tracking (also called time-varying LQR, TVLQR) provides exactly this: a <em>time-varying</em> linear feedback that stabilizes the system near a nominal state–control sequence and rejects small disturbances.</p>
<p><strong>Problem Setup.</strong> Let the nominal (i.e., ignoring the disturbance) discrete-time system be
<span class="math display">\[
x_{t+1} \;=\; f_t(x_t,u_t), \qquad t=0,\dots,N-1,
\]</span>
and suppose we have a <em>nominal trajectory</em> <span class="math inline">\(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\)</span> satisfying
<span class="math display">\[
\bar x_{t+1} \;=\; f_t(\bar x_t,\bar u_t).
\]</span></p>
<p>Our goal is to design a controller that can stabilize the system with disturbance, i.e., <span class="math inline">\(x_{t+1} = f_t(x_t, u_t, w_t)\)</span>, around the nominal trajectory.</p>
<p>Towards this, we define <em>deviations</em> from the nominal trajectory as
<span class="math display">\[
\delta x_t := x_t-\bar x_t, \qquad \delta u_t := u_t-\bar u_t.
\]</span></p>
<p>If the true system is linear time-varying (or we linearize a nonlinear system along the nominal), we obtain the <em>deviation dynamics</em>
<span class="math display" id="eq:tvlqr-deviation-dynamics">\[
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,
\quad
A_t := \left.\frac{\partial f_t}{\partial x}\right|_{(\bar x_t,\bar u_t)},\quad
B_t := \left.\frac{\partial f_t}{\partial u}\right|_{(\bar x_t,\bar u_t)}.
\tag{4.17}
\]</span></p>
<p>We penalize deviations with a quadratic cost
<span class="math display" id="eq:tvlqr-deviation-cost">\[
J \;=\; \delta x_N^\top Q_N \delta x_N
\;+\;\sum_{t=0}^{N-1} \Big(\delta x_t^\top Q_t \delta x_t \;+\; \delta u_t^\top R_t \delta u_t\Big),
\quad Q_t\succeq 0,\; R_t\succ 0.
\tag{4.18}
\]</span></p>
<p><strong>LQR Tracking Algorithm.</strong> The tracking controller takes the <em>affine</em> form
<span class="math display">\[
u_t \;=\; \bar u_t \;-\; K_t\,(x_t-\bar x_t),
\]</span>
where <span class="math inline">\(\{K_t\}_{t=0}^{N-1}\)</span> are time-varying gains computed by a backward Riccati recursion on the deviation system <a href="model-based-plan-optimize.html#eq:tvlqr-deviation-dynamics">(4.17)</a> with cost <a href="model-based-plan-optimize.html#eq:tvlqr-deviation-cost">(4.18)</a>.</p>
<p>From Proposition <a href="model-based-plan-optimize.html#prp:discretetimefinitehorizonlqrsolution">4.1</a>, we know the gains can be computed as follows.</p>
<p>Initialize at terminal time:
<span class="math display">\[
S_N \;=\; Q_N.
\]</span>
For <span class="math inline">\(t = N-1,\,N-2,\,\dots,\,0\)</span>:
<span class="math display" id="eq:tvlqr-riccati">\[\begin{equation}
\begin{split}
K_t &amp;= \Big(R_t + B_t^\top S_{t+1} B_t\Big)^{-1} B_t^\top S_{t+1} A_t, \\[2mm]
S_t &amp;= Q_t \;+\; A_t^\top \!\Big(S_{t+1} - S_{t+1} B_t \big(R_t + B_t^\top S_{t+1} B_t\big)^{-1} B_t^\top S_{t+1}\Big) A_t.
\end{split}
\tag{4.19}
\end{equation}\]</span></p>
<p>Given the gains <span class="math inline">\(\{K_t\}\)</span>, apply at runtime:
<span class="math display" id="eq:tvlqr-control-law">\[\begin{equation}
u_t \;=\; \bar u_t - K_t\,(x_t-\bar x_t), \qquad t=0,\dots,N-1.
\tag{4.20}
\end{equation}\]</span></p>
<p>The following pseudocode summarizes the algorithm.</p>
<div class="highlightbox">
<p><strong>Algorithm: LQR Trajectory Tracking (TVLQR)</strong></p>
<p><strong>Inputs:</strong> nominal <span class="math inline">\(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\)</span>, weights <span class="math inline">\(\{Q_t,R_t\}\)</span>, terminal <span class="math inline">\(Q_N\)</span>.</p>
<ol style="list-style-type: decimal">
<li><strong>Linearize</strong> along the nominal to get <span class="math inline">\(A_t,B_t\)</span> via <a href="model-based-plan-optimize.html#eq:tvlqr-deviation-dynamics">(4.17)</a>.<br />
</li>
<li><strong>Backward pass:</strong> compute <span class="math inline">\(K_t\)</span> and <span class="math inline">\(S_t\)</span> via <a href="model-based-plan-optimize.html#eq:tvlqr-riccati">(4.19)</a>.<br />
</li>
<li><strong>Apply feedback:</strong> <span class="math inline">\(u_t=\bar u_t - K_t(x_t-\bar x_t)\)</span> as in <a href="model-based-plan-optimize.html#eq:tvlqr-control-law">(4.20)</a>.</li>
</ol>
<p><strong>Output:</strong> time-varying gains <span class="math inline">\(\{K_t\}\)</span> giving a local stabilizer around the trajectory.</p>
</div>
<p>We now apply TVLQR to a vehicle trajectory tracking problem.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:tvlqr-unicyle" class="example"><strong>Example 4.2  (LQR Trajectory Tracking for Unicyle) </strong></span>We (i) define the dynamics in continuous and discrete time, (ii) specify a circular <em>nominal trajectory</em>, (iii) linearize the nonlinear dynamics <em>along the nominal</em>, (iv) state the deviation-cost weights <span class="math inline">\(Q,R\)</span> (and terminal <span class="math inline">\(Q_N\)</span>), and (v) list the <em>experiment setup</em> (discretization and horizon length).</p>
<p><strong>Unicycle Dynamics (Continuous and Discrete).</strong></p>
<p><strong>State and input.</strong><br />
<span class="math display">\[
x=\begin{bmatrix}p_x\\ p_y\\ \theta\end{bmatrix}\in\mathbb{R}^3,
\qquad
u=\begin{bmatrix}v\\ \omega\end{bmatrix}\in\mathbb{R}^2,
\]</span>
where <span class="math inline">\((p_x,p_y)\)</span> is planar position, <span class="math inline">\(\theta\)</span> is heading, <span class="math inline">\(v\)</span> is forward speed, and <span class="math inline">\(\omega\)</span> is yaw rate.</p>
<p><strong>Continuous time:</strong>
<span class="math display" id="eq:unicycle-ct">\[\begin{equation}
\dot p_x = v\cos\theta,\qquad
\dot p_y = v\sin\theta,\qquad
\dot\theta = \omega.
\tag{4.21}
\end{equation}\]</span></p>
<p><strong>Discrete time (forward Euler with step <span class="math inline">\(h&gt;0\)</span>):</strong>
<span class="math display" id="eq:unicycle-dt">\[\begin{equation}
x_{t+1} \;=\; f(x_t,u_t)
:= \begin{bmatrix}
p_{x,t} + h\, v_t\cos\theta_t\\[2pt]
p_{y,t} + h\, v_t\sin\theta_t\\[2pt]
\theta_t + h\,\omega_t
\end{bmatrix}.
\tag{4.22}
\end{equation}\]</span></p>
<p><strong>Nominal Trajectory (Circular Motion).</strong> We track a circle of radius <span class="math inline">\(R=\dfrac{\bar v}{\bar\omega}\)</span> using <em>constant nominal inputs</em>
<span class="math display" id="eq:unicycle-nominal-control">\[\begin{equation}
\bar u_t \equiv \begin{bmatrix}\bar v\\ \bar\omega\end{bmatrix},\qquad t=0,\dots,N-1,
\tag{4.23}
\end{equation}\]</span>
and generate the <em>nominal state</em> recursively under the discrete dynamics <a href="model-based-plan-optimize.html#eq:unicycle-dt">(4.22)</a>:
<span class="math display" id="eq:unicycle-discrete-nominal">\[\begin{equation}
\bar x_{t+1} \;=\; f(\bar x_t,\bar u_t),\qquad \bar x_0 = \begin{bmatrix}R\\ 0\\ \tfrac{\pi}{2}\end{bmatrix}.
\tag{4.24}
\end{equation}\]</span></p>
<p>We define <em>deviations</em> from the nominal:
<span class="math display">\[
\delta x_t := x_t-\bar x_t,\qquad \delta u_t := u_t-\bar u_t.
\]</span></p>
<p><strong>Linearization Along the Nominal.</strong> Linearize <a href="model-based-plan-optimize.html#eq:unicycle-dt">(4.22)</a> at <span class="math inline">\((\bar x_t,\bar u_t)\)</span> to obtain the deviation dynamics
<span class="math display" id="eq:unicycle-dev-dyn">\[\begin{equation}
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,
\tag{4.25}
\end{equation}\]</span>
with Jacobians (using <span class="math inline">\(c_t:=\cos\bar\theta_t,\ s_t:=\sin\bar\theta_t\)</span>)
<span class="math display" id="eq:unicycle-linearization">\[\begin{equation}
A_t \;=\; \frac{\partial f}{\partial x}\Big|_{(\bar x_t,\bar u_t)}
= \begin{bmatrix}
1 &amp; 0 &amp; -h\,\bar v\,s_t\\
0 &amp; 1 &amp; \ \ h\,\bar v\,c_t\\
0 &amp; 0 &amp; 1
\end{bmatrix},
\qquad
B_t \;=\; \frac{\partial f}{\partial u}\Big|_{(\bar x_t,\bar u_t)}
= \begin{bmatrix}
h\,c_t &amp; 0\\
h\,s_t &amp; 0\\
0 &amp; h
\end{bmatrix}.
\tag{4.26}
\end{equation}\]</span></p>
<p><strong>Deviation Cost (Weights <span class="math inline">\(Q,R,Q_N\)</span>).</strong> We penalize deviations with a quadratic stage/terminal cost
<span class="math display">\[
J \;=\; \delta x_N^\top Q_N\,\delta x_N
\;+\;\sum_{t=0}^{N-1}\Big(\delta x_t^\top Q\,\delta x_t+\delta u_t^\top R\,\delta u_t\Big),
\]</span>
using the weights:
<span class="math display" id="eq:unicycle-weights">\[\begin{equation}
Q=\mathrm{diag}(30,\;30,\;5),\qquad
Q_N=\mathrm{diag}(60,\;60,\;8),\qquad
R=\mathrm{diag}(0.2,\;0.2).
\tag{4.27}
\end{equation}\]</span>
These reflect a stronger emphasis on position tracking, a moderate penalty on heading error, and a mild penalty on control <em>deviations</em> from the nominal.</p>
<p><strong>Experiment Setup.</strong></p>
<ul>
<li><strong>Discretization step:</strong> <span class="math inline">\(h = 0.02\ \mathrm{s}\)</span>.<br />
</li>
<li><strong>Horizon length:</strong> <span class="math inline">\(T_{\mathrm{final}} = 12\ \mathrm{s}\)</span>.<br />
</li>
<li><strong>Number of steps:</strong> <span class="math inline">\(N = T_{\mathrm{final}}/h = \mathbf{600}\)</span>.<br />
</li>
<li><strong>Nominal inputs:</strong> <span class="math inline">\(\bar v = 1.2\ \mathrm{m/s},\ \bar\omega = 0.4\ \mathrm{rad/s}\)</span> (radius <span class="math inline">\(R=\bar v/\bar\omega\)</span>).<br />
</li>
<li><strong>Initialization:</strong> the nominal starts at <span class="math inline">\(\bar x_0 = [\,R,\,0,\,\pi/2\,]^\top\)</span>; the actual system may start with a small offset (see code).</li>
</ul>
<p>With <span class="math inline">\((A_t,B_t)\)</span> from <a href="model-based-plan-optimize.html#eq:unicycle-linearization">(4.26)</a> and weights <a href="model-based-plan-optimize.html#eq:unicycle-weights">(4.27)</a>, the TVLQR backward Riccati recursion (Section <a href="model-based-plan-optimize.html#lqr-tracking">4.2</a>) yields gains <span class="math inline">\(K_t\)</span>. We then apply the <strong>local feedback</strong>
<span class="math display">\[
u_t \;=\; \bar u_t - K_t\,(x_t-\bar x_t),
\]</span>
to robustly track the circular nominal under small disturbances.</p>
<p><strong>Disturbances.</strong> To test robustness, we inject additive process disturbances into the discrete dynamics:
<span class="math display">\[
x_{t+1} \;=\; f(x_t,u_t)\;+\; w_t,\qquad t=0,\dots,N-1,
\]</span>
where
<span class="math display">\[
w_t \;=\; \underbrace{\eta_t}_{\text{i.i.d. Gaussian noise}} \;+\; \underbrace{g_t}_{\text{deterministic gust}}.
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Small i.i.d. Gaussian process noise. We draw <span class="math inline">\(\eta_t \sim \mathcal N(0,W)\)</span> independently at each step with
<span class="math display">\[
W \;=\; \mathrm{diag}\!\big(\sigma_x^2,\ \sigma_y^2,\ \sigma_\theta^2\big),
\qquad
\sigma_x = \sigma_y = 0.01\ \text{m},\quad
\sigma_\theta = \mathrm{deg2rad}(0.2).
\]</span>
This noise perturbs the post-update state components <span class="math inline">\((p_x,p_y,\theta)\)</span>.</p></li>
<li><p>Finite-duration “gust” impulse.
In addition to <span class="math inline">\(\eta_t\)</span>, we apply a brief deterministic bias over a window
<span class="math display">\[
t \in [t_g,\ t_g+\Delta] \;=\; [\,4.0\,\mathrm{s},\ 4.8\,\mathrm{s}\,),
\]</span>
implemented at the discrete indices <span class="math inline">\(\{t_g,\dots,t_g+\Delta\}\)</span>. During this window we set
<span class="math display">\[
g_t \;=\; \begin{bmatrix}
\delta p_x \\[1mm] 0 \\[1mm] \delta \theta
\end{bmatrix},
\qquad
\delta p_x = 0.01\ \text{m per step},\quad
\delta \theta = \mathrm{deg2rad}(1.8)\ \text{per step},
\]</span>
and <span class="math inline">\(g_t=\mathbf{0}\)</span> otherwise. This models a short-lived lateral drift and a heading kick.</p></li>
</ol>
<p><strong>Results.</strong> Fig. <a href="model-based-plan-optimize.html#fig:unicycle-lqr-tracking-trajectory">4.3</a> visualizes the nominal trajectory (the dotted circle) and the TVLQR-stabilized trajectory in blue. To clearly see the impact of closed-loop LQR tracking, we also plotted the open-loop trajectory, i.e., the system’s trajectory if no feedback is applied. We can observe that the TVLQR controller effectively rejects the disturbances and stabilizes the closed-loop trajectory along the nominal path.</p>
<p>Fig. <a href="model-based-plan-optimize.html#fig:unicycle-lqr-tracking-error">4.4</a> visualizes the state tracking error (position and heading error) as well as compares the closed-loop control with open-loop control.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/lqr_tracking.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-lqr-tracking-trajectory"></span>
<img src="images/Model-based-optimization/tvlqr_trajectory.png" alt="LQR Trajectory Tracking for Unicyle: comparison between nominal trajectory, open-loop trajectory, and closed-loop trajectory with feedback." width="60%" />
<p class="caption">
Figure 4.3: LQR Trajectory Tracking for Unicyle: comparison between nominal trajectory, open-loop trajectory, and closed-loop trajectory with feedback.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-lqr-tracking-error"></span>
<img src="images/Model-based-optimization/tvlqr_state_error.png" alt="LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom)." width="90%" /><img src="images/Model-based-optimization/tvlqr_control.png" alt="LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom)." width="90%" />
<p class="caption">
Figure 4.4: LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom).
</p>
</div>
</div>
</div>
</div>
<div id="traj-opt" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Trajectory Optimization<a href="model-based-plan-optimize.html#traj-opt" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Section <a href="model-based-plan-optimize.html#lqr-tracking">4.2</a> we saw that TVLQR gives a powerful <em>local stabilizer</em> around a nominal state–control sequence <span class="math inline">\((\bar x_t,\bar u_t)\)</span>. This raises a natural question:</p>
<blockquote>
<p>Where do nominal trajectories come from?</p>
</blockquote>
<p>In many robotics tasks (maneuvering a car, landing a rocket, walking with a robot), we must compute a <em>feasible, high-quality open-loop plan</em> that respects the dynamics and constraints. <strong>Trajectory Optimization (TO)</strong> does exactly this: it searches over sequences <span class="math inline">\(\{x_t,u_t\}\)</span> to minimize a cumulative cost while satisfying the system dynamics and constraints.</p>
<p>Moreover, if we can solve TO <strong>quickly</strong> (or approximately, but reliably), then by re-solving over a short horizon at each time step and applying only the first control, we obtain <strong>Model Predictive Control (MPC)</strong>—a feedback controller that blends optimization with robustness (see Section <a href="model-based-plan-optimize.html#mpc">4.4</a> later). Thus, TO is both a <strong>planner</strong> and the engine behind <strong>feedback via MPC</strong>.</p>
<p><strong>General Nonlinear Trajectory Optimization Problem.</strong> We adopt the standard discrete-time nonlinear system
<span class="math display">\[
x_{t+1} = f_t(x_t,u_t),\qquad t=0,\dots,N-1,
\]</span>
with state <span class="math inline">\(x_t\in\mathbb{R}^n\)</span> and control <span class="math inline">\(u_t\in\mathbb{R}^m\)</span>. A generic finite-horizon TO problem is
<span class="math display" id="eq:to-general">\[\begin{equation}
\begin{split}
\min_{\{x_t,u_t\}} \quad &amp;
\Phi(x_N) + \sum_{t=0}^{N-1} \ell_t(x_t,u_t) \\[2mm]
\text{s.t.}\quad &amp;
x_{t+1} = f_t(x_t,u_t), \qquad t=0,\dots,N-1,\\
&amp; x_0 = \hat x_0 \ \ \text{(given)},\\
&amp; x_t \in \mathcal X_t,\quad u_t \in \mathcal U_t \quad \text{(bounds)},\\
&amp; g_t(x_t,u_t) \le 0,\quad h_t(x_t,u_t)=0 \quad \text{(path/terminal constraints).}
\end{split}
\tag{4.28}
\end{equation}\]</span>
Here <span class="math inline">\(\ell_t\)</span> and <span class="math inline">\(\Phi\)</span> encode performance (e.g., energy, time, tracking error), <span class="math inline">\(\mathcal X_t,\mathcal U_t\)</span> capture box limits and safety sets, and <span class="math inline">\(g_t,h_t\)</span> represent additional nonlinear constraints (obstacles, terminal goals, etc.).</p>
<p>Solving <a href="model-based-plan-optimize.html#eq:to-general">(4.28)</a> directly is difficult in general. A widely used strategy is to iteratively approximate it by <em>quadratic</em> subproblems that can be solved efficiently. This leads to <strong>iLQR</strong> and its second-order cousin <strong>DDP</strong> (see Section <a href="model-based-plan-optimize.html#ddp">4.3.2</a>).</p>
<div id="traj-opt-ilqr" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Iterative LQR<a href="model-based-plan-optimize.html#traj-opt-ilqr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>High-level intuition.</strong> iLQR (iterative LQR) alternates between:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Local modeling:</strong> around a <em>current</em> nominal trajectory <span class="math inline">\(\{(\bar x_t,\bar u_t)\}\)</span>,</p>
<ul>
<li><strong>linearize</strong> the dynamics,<br />
</li>
<li><strong>quadratically approximate</strong> the cost.</li>
</ul></li>
<li><p><strong>LQR step:</strong> solve the resulting <em>time-varying LQR</em> subproblem to obtain a <em>time-varying affine policy</em>
<span class="math display">\[
\delta u_t = k_t + K_t\,\delta x_t,\quad \delta x_t:=x_t-\bar x_t,\ \delta u_t:=u_t-\bar u_t,
\]</span>
which gives both a feedforward step <span class="math inline">\(k_t\)</span> (to change the nominal control) and a feedback gain <span class="math inline">\(K_t\)</span> (to stabilize the rollout).</p></li>
<li><p><strong>Forward rollout + line search:</strong> apply <span class="math inline">\(u_t^{\text{new}}=\bar u_t+\alpha k_t + K_t(x_t^{\text{new}}-\bar x_t)\)</span> to the true nonlinear dynamics, producing a new nominal trajectory <span class="math inline">\(\{(\bar x_t,\bar u_t)\}\)</span>. Here we choose <span class="math inline">\(\alpha\in(0,1]\)</span> to reduce the cost and respect constraints.</p></li>
<li><p><strong>Repeat</strong> until convergence (cost decrease and dynamics residuals are small).</p></li>
</ol>
<p>iLQR can be viewed as a <a href="https://en.wikipedia.org/wiki/Gauss–Newton_algorithm">Gauss–Newton method</a> on trajectories: it uses first-order dynamics and second-order cost, capturing the dominant curvature while remaining numerically robust and fast.</p>
<div id="lqr-subproblem-one-ilqr-outer-iteration" class="section level4 hasAnchor" number="4.3.1.1">
<h4><span class="header-section-number">4.3.1.1</span> LQR Subproblem (one iLQR outer iteration)<a href="model-based-plan-optimize.html#lqr-subproblem-one-ilqr-outer-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Given a nominal trajectory <span class="math inline">\(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\)</span> with <span class="math inline">\(\bar x_{t+1}=f_t(\bar x_t,\bar u_t)\)</span>, define deviations
<span class="math display">\[
\delta x_t := x_t-\bar x_t,\qquad \delta u_t := u_t-\bar u_t,\qquad \delta x_0\ \text{given.}
\]</span></p>
<p><strong>Linearized Dynamics.</strong> We linearize the dynamics along the nominal trajectory
<span class="math display" id="eq:ilqr-linearize">\[
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,\quad
A_t:=\left.\frac{\partial f_t}{\partial x}\right|_{(\bar x_t,\bar u_t)},\ \
B_t:=\left.\frac{\partial f_t}{\partial u}\right|_{(\bar x_t,\bar u_t)}.
\tag{4.29}
\]</span></p>
<p><strong>Quadratic Cost Approximation.</strong> We perform a quadratic approximation of the objective function about <span class="math inline">\((\bar x_t,\bar u_t)\)</span>
<span class="math display" id="eq:ilqr-quadratic-cost">\[
\begin{aligned}
\ell_t(x_t,u_t) &amp;\approx \ell_t
+ \ell_{x,t}^\top \delta x_t + \ell_{u,t}^\top \delta u_t
+ \frac{1}{2}
\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}^{\!\top}
\!\begin{bmatrix}\ell_{xx,t} &amp; \ell_{xu,t}\\ \ell_{ux,t} &amp; \ell_{uu,t}\end{bmatrix}
\!\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix},\\
\Phi(x_N) &amp;\approx \Phi + \Phi_x^\top \delta x_N + \frac{1}{2}\,\delta x_N^\top \Phi_{xx}\,\delta x_N.
\end{aligned}
\tag{4.30}
\]</span></p>
<p><strong>The LQR Subproblem.</strong> With <a href="model-based-plan-optimize.html#eq:ilqr-linearize">(4.29)</a>–<a href="model-based-plan-optimize.html#eq:ilqr-quadratic-cost">(4.30)</a>, the iLQR subproblem at this outer iteration is the <strong>finite-horizon linear–quadratic program in deviations</strong>:
<span class="math display" id="eq:ilqr-lqr-subproblem">\[
\begin{aligned}
\min_{\{\delta x_t,\delta u_t\}} \quad
&amp; \underbrace{\frac{1}{2}\,\delta x_N^\top \Phi_{xx}\,\delta x_N + \Phi_x^\top \delta x_N}_{\text{terminal}}
\;+\; \\
&amp; \sum_{t=0}^{N-1}
\underbrace{\Big(
\frac{1}{2}
\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}^{\!\top}
\!\begin{bmatrix}\ell_{xx,t} &amp; \ell_{xu,t}\\ \ell_{ux,t} &amp; \ell_{uu,t}\end{bmatrix}
\!\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}
+ \ell_{x,t}^\top\delta x_t + \ell_{u,t}^\top\delta u_t
\Big)}_{\text{stage}} \\[1mm]
\text{s.t.}\quad &amp;
\delta x_{t+1} = A_t\,\delta x_t + B_t\,\delta u_t,\qquad t=0,\dots,N-1,\\
&amp; \delta x_0\ \text{given.}
\end{aligned}
\tag{4.31}
\]</span></p>
<blockquote>
<p><strong>Notes.</strong> The iLQR subproblem <a href="model-based-plan-optimize.html#eq:ilqr-lqr-subproblem">(4.31)</a> is slightly different from the previous finite-horizon LQR formulation <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.2)</a> in the sense that the objective function of <a href="model-based-plan-optimize.html#eq:ilqr-lqr-subproblem">(4.31)</a> also contains <strong>linear terms</strong> in <span class="math inline">\(\delta x_t,\delta u_t\)</span>, and those linear terms come from the Taylor expansion of the original nonlinear objective fuctions. In this case, we will see in the following that the optimal policy is <strong>affine</strong> (feedforward <span class="math inline">\(k_t\)</span> + feedback <span class="math inline">\(K_t\)</span>).</p>
</blockquote>
</div>
<div id="solving-the-subproblem-by-dynamic-programming" class="section level4 hasAnchor" number="4.3.1.2">
<h4><span class="header-section-number">4.3.1.2</span> Solving the Subproblem by Dynamic Programming<a href="model-based-plan-optimize.html#solving-the-subproblem-by-dynamic-programming" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We posit a <strong>quadratic value approximator</strong> at each time:
<span class="math display" id="eq:ilqr-value">\[
V_{t}(\delta x_{t})
\;\approx\;
V_{t}
+ V_{x,t}^\top \delta x_t
+ \frac{1}{2}\,\delta x_t^\top V_{xx,t}\,\delta x_t,
\qquad
V_{x,N}=\Phi_x,\; V_{xx,N}=\Phi_{xx}.
\tag{4.32}
\]</span>
Note that this quadratic value approximator also contains linear and constant terms because the objective function contains linear terms.</p>
<p>Define the local Q-function at stage <span class="math inline">\(t\)</span> by substituting the linear dynamics into the next-step value (this is our familiar Q-value in RL):
<span class="math display">\[
Q_t(\delta x_t,\delta u_t)
\;=\;
\ell_t(x_t,u_t) + V_{t+1} \big(A_t\delta x_t + B_t\delta u_t\big),
\]</span>
which, after collecting terms, yields the iLQR blocks
<span class="math display" id="eq:ilqr-Q">\[
\begin{aligned}
Q_{x,t}&amp;=\ell_{x,t}+A_t^\top V_{x,t+1},\qquad
Q_{u,t}=\ell_{u,t}+B_t^\top V_{x,t+1},\\
Q_{xx,t}&amp;=\ell_{xx,t}+A_t^\top V_{xx,t+1}A_t,\quad
Q_{ux,t}=\ell_{ux,t}+B_t^\top V_{xx,t+1}A_t,\\
Q_{uu,t}&amp;=\ell_{uu,t}+B_t^\top V_{xx,t+1}B_t.
\end{aligned}
\tag{4.33}
\]</span>
The iLQR blocks assemble into a big matrix such that
<span class="math display" id="eq:ilqr-Q-aug">\[
Q_t(\delta x_t,\delta u_t)
\;=\;
\frac{1}{2}\, \begin{bmatrix} 1 \\ \delta x_t \\ \delta u_t \end{bmatrix}^\top
\begin{bmatrix}
2c_t &amp; Q_{x,t}^\top &amp; Q_{u,t}^\top\\[2pt]
Q_{x,t} &amp; Q_{xx,t} &amp; Q_{xu,t}\\[2pt]
Q_{u,t} &amp; Q_{ux,t} &amp; Q_{uu,t}
\end{bmatrix}
\begin{bmatrix} 1 \\ \delta x_t \\ \delta u_t \end{bmatrix}.
\tag{4.34}
\]</span>
where <span class="math inline">\(c_t\)</span> collects all stage/terminal constants e.g., <span class="math inline">\(\ell_t+\!V_{t+1}\)</span>.</p>
<p><strong>Solving the local Q (backward pass).</strong> Set the first-order condition w.r.t. <span class="math inline">\(\delta u\)</span>:
<span class="math display">\[
0 \;=\; \partial_{\delta u} Q_t \;=\; Q_{u,t} + Q_{ux,t}\delta x + Q_{uu,t}\delta u.
\]</span>
Solve for the affine control law
<span class="math display" id="eq:ilqr-kK">\[
\delta u_t^\star \;=\; k_t + K_t\,\delta x,\qquad
k_t = -Q_{uu,t}^{-1} Q_{u,t},\quad
K_t = -Q_{uu,t}^{-1} Q_{ux,t}\!,
\tag{4.35}
\]</span>
which is exactly the LQR solution for the quadratic <span class="math inline">\(Q_t\)</span>.</p>
<p>Substitute <span class="math inline">\(\delta u_t^\star\)</span> back into <a href="model-based-plan-optimize.html#eq:ilqr-Q-aug">(4.34)</a>. The minimized Q becomes a quadratic in <span class="math inline">\(\delta x\)</span> with coefficients given by
<span class="math display" id="eq:ilqr-V">\[
\begin{aligned}
V_{x,t}  &amp;= Q_{x,t} + Q_{xu,t}k_t + K_t^\top Q_{uu,t}k_t + K_t^\top Q_{u,t},\\
V_{xx,t} &amp;= Q_{xx,t} + Q_{xu,t}K_t + K_t^\top Q_{ux,t} + K_t^\top Q_{uu,t}K_t,
\end{aligned}
\tag{4.36}
\]</span>
with terminal
<span class="math display">\[
V_{x,N} = \Phi_x, V_{xx,N} = \Phi_{xx}.
\]</span></p>
<p><strong>Forward Pass (apply the computed policy).</strong> Given <span class="math inline">\(\{k_t,K_t\}\)</span>, produce a candidate trajectory on the <em>true</em> nonlinear dynamics using a line search <span class="math inline">\(\alpha\in(0,1]\)</span>:
<span class="math display" id="eq:ilqr-forward">\[
\begin{aligned}
u_t^{\text{cand}} &amp;= \bar u_t + \alpha k_t + K_t\big(x_t^{\text{cand}}-\bar x_t\big) ,\\
x_{t+1}^{\text{cand}} &amp;= f_t\big(x_t^{\text{cand}},u_t^{\text{cand}}\big),\qquad x_0^{\text{cand}}=\hat x_0.
\end{aligned}
\tag{4.37}
\]</span>
Choose <span class="math inline">\(\alpha\)</span> (e.g., <span class="math inline">\(\{1,\frac{1}{2},\tfrac14,\dots\}\)</span>) to reduce the <strong>true</strong> cost and respect constraints, then update the nominal:
<span class="math display">\[
(\bar x_t,\bar u_t)\ \leftarrow\ (x_t^{\text{cand}},u_t^{\text{cand}}).
\]</span></p>
<p>The following pseudocode summarizes iLQR.</p>
<div class="highlightbox">
<p><strong>Algorithm: iLQR (Trajectory Generation)</strong></p>
<p><strong>Inputs:</strong> dynamics <span class="math inline">\(f_t\)</span>, initial state <span class="math inline">\(\hat x_0\)</span>, horizon <span class="math inline">\(N\)</span>, stage/terminal costs <span class="math inline">\(\ell_t,\Phi\)</span>, initial guess <span class="math inline">\(\{\bar u_t\}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><strong>Initialize</strong> nominal rollout <span class="math inline">\(\{\bar x_t,\bar u_t\}\)</span> from <span class="math inline">\(\hat x_0\)</span>.</li>
<li><strong>Linearize &amp; quadratize</strong> at <span class="math inline">\(\{(\bar x_t,\bar u_t)\}\)</span>: build <span class="math inline">\(A_t,B_t\)</span> and cost derivatives.</li>
<li><strong>Backward pass (TVLQR):</strong> compute <span class="math inline">\(\{k_t,K_t\}\)</span> using <a href="model-based-plan-optimize.html#eq:ilqr-kK">(4.35)</a> and update <span class="math inline">\(V_{x,t},V_{xx,t}\)</span> via <a href="model-based-plan-optimize.html#eq:ilqr-V">(4.36)</a>.</li>
<li><strong>Forward rollout:</strong> apply <span class="math inline">\(u_t^{\text{new}}=\bar u_t+\alpha k_t+K_t(x_t^{\text{new}}-\bar x_t)\)</span> on the <strong>true</strong> dynamics, pick <span class="math inline">\(\alpha\)</span> by line search.</li>
<li><strong>Convergence check:</strong> stop if the cost decrease and dynamics residuals fall below thresholds; otherwise, set the new nominal and <strong>repeat</strong> from Step 2.</li>
</ol>
</div>
<p>The next example applies iLQR to trajectory generation for rocket landing.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:iLQR-rocket-landing" class="example"><strong>Example 4.3  (iLQR for Rocket Landing) </strong></span>We model a planar (2D) rocket with state and control
<span class="math display">\[
x=\begin{bmatrix}p_x &amp; p_y &amp; v_x &amp; v_y &amp; \theta &amp; \omega\end{bmatrix}^\top,\qquad
u=\begin{bmatrix}T &amp; \tau\end{bmatrix}^\top,
\]</span>
where <span class="math inline">\((p_x,p_y)\)</span> is position, <span class="math inline">\((v_x,v_y)\)</span> is velocity, <span class="math inline">\(\theta\)</span> is attitude (pitch) and <span class="math inline">\(\omega\)</span> its angular rate. The thrust <span class="math inline">\(T\ge 0\)</span> acts <strong>along the body axis</strong> (pointing out of the engine), and <span class="math inline">\(\tau\)</span> is a planar torque about the center of mass. Continuous-time dynamics are
<span class="math display" id="eq:rocket-ct">\[
\begin{aligned}
\dot p_x &amp;= v_x, &amp;
\dot p_y &amp;= v_y, \\
\dot v_x &amp;= \frac{T}{m}\sin\theta, &amp;
\dot v_y &amp;= \frac{T}{m}\cos\theta - g, \\
\dot\theta &amp;= \omega, &amp;
\dot\omega &amp;= \frac{\tau}{I_{zz}}.
\end{aligned}
\tag{4.38}
\]</span>
In simulation we use RK4 with stepsize <span class="math inline">\(h\)</span> to propagate the true dynamics <a href="model-based-plan-optimize.html#eq:rocket-ct">(4.38)</a>. For iLQR’s local subproblems we form the continuous Jacobians <span class="math inline">\((A_c,B_c)=\big(\tfrac{\partial f}{\partial x},\tfrac{\partial f}{\partial u}\big)\)</span> at the current nominal and use the standard first-order discrete map
<span class="math display" id="eq:rocket-disc-lin">\[
A_t \;\approx\; I + h\,A_c(\bar x_t,\bar u_t),\qquad
B_t \;\approx\; h\,B_c(\bar x_t,\bar u_t).
\tag{4.39}
\]</span></p>
<p><strong>Soft-Landing Objective.</strong> The goal is a <strong>soft, upright landing</strong> at the origin:
<span class="math display">\[
x_{\mathrm{goal}} = \mathbf{0}
\quad\Longleftrightarrow\quad
p_x=p_y=0,\; v_x=v_y=0,\; \theta=0,\; \omega=0.
\]</span>
We penalize deviations from this goal along the entire trajectory and especially at the terminal state to encourage low touchdown velocities and an upright attitude.</p>
<p><strong>Cost Function.</strong> With horizon <span class="math inline">\(N\)</span> and step <span class="math inline">\(h\)</span>, the discrete objective is
<span class="math display" id="eq:rocket-cost">\[
J \;=\; \tfrac12\,(x_N-x_g)^\top Q_f (x_N-x_g)
\;+\;\sum_{t=0}^{N-1}\Big[
\tfrac12\,(x_t-x_g)^\top Q (x_t-x_g) \;+\; \tfrac12\,u_t^\top R u_t
\Big],
\tag{4.40}
\]</span>
where <span class="math inline">\(x_g=\mathbf{0}\)</span>. In the example:
<span class="math display" id="eq:rocket-weights">\[
\begin{aligned}
Q&amp;=\mathrm{diag}(1,\ 2,\ 0.5,\ 0.5,\ 2,\ 0.5),\\
Q_f&amp;=\mathrm{diag}(200,\ 300,\ 50,\ 50,\ 300,\ 50),\\
R&amp;=\mathrm{diag}(10^{-3},\ 10^{-3}).
\end{aligned}
\tag{4.41}
\]</span>
These weights place strong emphasis on terminal altitude and attitude (<span class="math inline">\(p_y,\theta\)</span>), moderate emphasis on velocities and lateral position, and a light regularization on the controls.</p>
<p><strong>Experiment Setup.</strong></p>
<ul>
<li><p><strong>Physical parameters.</strong> Gravity <span class="math inline">\(g=9.81\,\mathrm{m/s^2}\)</span>, mass <span class="math inline">\(m=1.0\,\mathrm{kg}\)</span>, planar inertia <span class="math inline">\(I_{zz}=0.2\,\mathrm{kg\,m^2}\)</span>.</p></li>
<li><p><strong>Discretization.</strong> Stepsize <span class="math inline">\(h=0.05\,\mathrm{s}\)</span>; horizon <span class="math inline">\(T=6.0\,\mathrm{s}\)</span>; number of steps <span class="math inline">\(N=T/h=120\)</span>.</p></li>
<li><p><strong>Initial state.</strong>
<span class="math display">\[
x_0=\big[\,5.0,\ 10.0,\ -0.5,\ -1.0,\ \mathrm{deg2rad}(10),\ 0\,\big]^\top,
\]</span>
i.e., 10 m altitude, lateral offset, small descent and slight pitch.</p></li>
<li><p><strong>Initial nominal controls.</strong> Constant hover thrust and zero torque:
<span class="math display">\[
\bar u_t = [\,m g,\ 0\,]^\top,\qquad t=0,\dots,N-1.
\]</span></p></li>
<li><p><strong>iLQR procedure.</strong> Each outer iteration:</p>
<ol style="list-style-type: decimal">
<li>Linearize dynamics and quadratize the cost along the current nominal (<a href="model-based-plan-optimize.html#eq:rocket-disc-lin">(4.39)</a>, <a href="model-based-plan-optimize.html#eq:rocket-cost">(4.40)</a>);<br />
</li>
<li>Solve the <strong>time-varying LQR</strong> subproblem to get affine updates <span class="math inline">\(\delta u_t = k_t + K_t\,\delta x_t\)</span>;<br />
</li>
<li><strong>Forward rollout</strong> on the nonlinear RK4 dynamics with
<span class="math display">\[
u_t^{\text{new}} = \bar u_t + \alpha\,k_t + K_t\big(x_t^{\text{new}}-\bar x_t\big),
\]</span>
using a backtracking line search over <span class="math inline">\(\alpha\in\{1,\tfrac12,\tfrac14,\dots\}\)</span> (note: <span class="math inline">\(\alpha\)</span> scales only the <strong>feedforward</strong> <span class="math inline">\(k_t\)</span>, not the feedback <span class="math inline">\(K_t\)</span>);<br />
</li>
<li>Update the nominal and repeat until cost reduction is small.</li>
</ol></li>
</ul>
<p>Fig. <a href="model-based-plan-optimize.html#fig:ilqr-rocket">4.5</a> plots the <strong>initial</strong>, <strong>intermediate</strong>, and <strong>final</strong> trajectories, and render the rocket as oriented rectangles (boxes) using <span class="math inline">\((p_x,p_y,\theta)\)</span> to visualize attitude along the descent. We can see iLQR successfully generated a soft landing trajectory.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/ilqr_rocket_landing.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ilqr-rocket"></span>
<img src="images/Model-based-optimization/ilqr_rocket_landing.png" alt="iLQR Trajectory Generation for Rocket Landing." width="100%" />
<p class="caption">
Figure 4.5: iLQR Trajectory Generation for Rocket Landing.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="ddp" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Differential Dynamic Programming<a href="model-based-plan-optimize.html#ddp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="mpc" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Model Predictive Control<a href="model-based-plan-optimize.html#mpc" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-arnold84ieee-generalized" class="csl-entry">
Arnold, William F, and Alan J Laub. 1984. <span>“Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.”</span> <em>Proceedings of the IEEE</em> 72 (12): 1746–54.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="policy-gradient.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advanced-materials.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/04-model-based-control.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
