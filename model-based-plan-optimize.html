<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Model-based Planning and Optimization | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Model-based Planning and Optimization | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Model-based Planning and Optimization | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-11-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="policy-gradient.html"/>
<link rel="next" href="advanced-materials.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.1.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.1.4" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.1.4</b> Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.2</b> Infinite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mdp.html"><a href="mdp.html#value-functions"><i class="fa fa-check"></i><b>1.2.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation-1"><i class="fa fa-check"></i><b>1.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.2.3" data-path="mdp.html"><a href="mdp.html#principle-of-optimality"><i class="fa fa-check"></i><b>1.2.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.2.4" data-path="mdp.html"><a href="mdp.html#policy-improvement"><i class="fa fa-check"></i><b>1.2.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="1.2.5" data-path="mdp.html"><a href="mdp.html#policy-iteration"><i class="fa fa-check"></i><b>1.2.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="1.2.6" data-path="mdp.html"><a href="mdp.html#value-iteration"><i class="fa fa-check"></i><b>1.2.6</b> Value Iteration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="value-rl.html"><a href="value-rl.html"><i class="fa fa-check"></i><b>2</b> Value-based Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="value-rl.html"><a href="value-rl.html#tabular-methods"><i class="fa fa-check"></i><b>2.1</b> Tabular Methods</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="value-rl.html"><a href="value-rl.html#tabular-PE"><i class="fa fa-check"></i><b>2.1.1</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.1.2" data-path="value-rl.html"><a href="value-rl.html#value-rl-convergence-td"><i class="fa fa-check"></i><b>2.1.2</b> Convergence Proof of TD Learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control"><i class="fa fa-check"></i><b>2.1.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.1.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control"><i class="fa fa-check"></i><b>2.1.4</b> Off-Policy Control</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="value-rl.html"><a href="value-rl.html#function-approximation"><i class="fa fa-check"></i><b>2.2</b> Function Approximation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="value-rl.html"><a href="value-rl.html#basics-of-continuous-mdp"><i class="fa fa-check"></i><b>2.2.1</b> Basics of Continuous MDP</a></li>
<li class="chapter" data-level="2.2.2" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-2"><i class="fa fa-check"></i><b>2.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.2.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control-1"><i class="fa fa-check"></i><b>2.2.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.2.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control-1"><i class="fa fa-check"></i><b>2.2.4</b> Off-Policy Control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="policy-gradient.html"><a href="policy-gradient.html"><i class="fa fa-check"></i><b>3</b> Policy Gradient Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-optimization"><i class="fa fa-check"></i><b>3.1</b> Gradient-based Optimization</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="policy-gradient.html"><a href="policy-gradient.html#basic-setup"><i class="fa fa-check"></i><b>3.1.1</b> Basic Setup</a></li>
<li class="chapter" data-level="3.1.2" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-ascent-and-descent"><i class="fa fa-check"></i><b>3.1.2</b> Gradient Ascent and Descent</a></li>
<li class="chapter" data-level="3.1.3" data-path="policy-gradient.html"><a href="policy-gradient.html#stochastic-gradients"><i class="fa fa-check"></i><b>3.1.3</b> Stochastic Gradients</a></li>
<li class="chapter" data-level="3.1.4" data-path="policy-gradient.html"><a href="policy-gradient.html#beyond-vanilla-gradient-methods"><i class="fa fa-check"></i><b>3.1.4</b> Beyond Vanilla Gradient Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#policy-gradients"><i class="fa fa-check"></i><b>3.2</b> Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="policy-gradient.html"><a href="policy-gradient.html#setup"><i class="fa fa-check"></i><b>3.2.1</b> Setup</a></li>
<li class="chapter" data-level="3.2.2" data-path="policy-gradient.html"><a href="policy-gradient.html#the-policy-gradient-lemma"><i class="fa fa-check"></i><b>3.2.2</b> The Policy Gradient Lemma</a></li>
<li class="chapter" data-level="3.2.3" data-path="policy-gradient.html"><a href="policy-gradient.html#reinforce"><i class="fa fa-check"></i><b>3.2.3</b> REINFORCE</a></li>
<li class="chapter" data-level="3.2.4" data-path="policy-gradient.html"><a href="policy-gradient.html#baselines-and-variance-reduction"><i class="fa fa-check"></i><b>3.2.4</b> Baselines and Variance Reduction</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#actorcritic-methods"><i class="fa fa-check"></i><b>3.3</b> Actor–Critic Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#anatomy-of-an-actorcritic"><i class="fa fa-check"></i><b>3.3.1</b> Anatomy of an Actor–Critic</a></li>
<li class="chapter" data-level="3.3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#ActorCriticTD"><i class="fa fa-check"></i><b>3.3.2</b> On-Policy Actor–Critic with TD(0)</a></li>
<li class="chapter" data-level="3.3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#PG-GAE"><i class="fa fa-check"></i><b>3.3.3</b> Generalized Advantage Estimation (GAE)</a></li>
<li class="chapter" data-level="3.3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#off-policy-actorcritic"><i class="fa fa-check"></i><b>3.3.4</b> Off-Policy Actor–Critic</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#advanced-policy-gradients"><i class="fa fa-check"></i><b>3.4</b> Advanced Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="policy-gradient.html"><a href="policy-gradient.html#revisiting-generalized-policy-iteration"><i class="fa fa-check"></i><b>3.4.1</b> Revisiting Generalized Policy Iteration</a></li>
<li class="chapter" data-level="3.4.2" data-path="policy-gradient.html"><a href="policy-gradient.html#performance-difference-lemma"><i class="fa fa-check"></i><b>3.4.2</b> Performance Difference Lemma</a></li>
<li class="chapter" data-level="3.4.3" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-constraint"><i class="fa fa-check"></i><b>3.4.3</b> Trust Region Constraint</a></li>
<li class="chapter" data-level="3.4.4" data-path="policy-gradient.html"><a href="policy-gradient.html#natural-policy-gradient"><i class="fa fa-check"></i><b>3.4.4</b> Natural Policy Gradient</a></li>
<li class="chapter" data-level="3.4.5" data-path="policy-gradient.html"><a href="policy-gradient.html#proof-fisher"><i class="fa fa-check"></i><b>3.4.5</b> Proof of Fisher Information</a></li>
<li class="chapter" data-level="3.4.6" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-policy-optimization"><i class="fa fa-check"></i><b>3.4.6</b> Trust Region Policy Optimization</a></li>
<li class="chapter" data-level="3.4.7" data-path="policy-gradient.html"><a href="policy-gradient.html#proximal-policy-optimization"><i class="fa fa-check"></i><b>3.4.7</b> Proximal Policy Optimization</a></li>
<li class="chapter" data-level="3.4.8" data-path="policy-gradient.html"><a href="policy-gradient.html#soft-actorcritic"><i class="fa fa-check"></i><b>3.4.8</b> Soft Actor–Critic</a></li>
<li class="chapter" data-level="3.4.9" data-path="policy-gradient.html"><a href="policy-gradient.html#deterministic-policy-gradient"><i class="fa fa-check"></i><b>3.4.9</b> Deterministic Policy Gradient</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="policy-gradient.html"><a href="policy-gradient.html#model-based-policy-optimization"><i class="fa fa-check"></i><b>3.5</b> Model-based Policy Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html"><i class="fa fa-check"></i><b>4</b> Model-based Planning and Optimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr"><i class="fa fa-check"></i><b>4.1</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#finite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.1</b> Finite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#infinite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.2</b> Infinite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#linear-system-basics"><i class="fa fa-check"></i><b>4.1.3</b> Linear System Basics</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr-tracking"><i class="fa fa-check"></i><b>4.2</b> LQR Trajectory Tracking</a></li>
<li class="chapter" data-level="4.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#traj-opt"><i class="fa fa-check"></i><b>4.3</b> Trajectory Optimization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#traj-opt-ilqr"><i class="fa fa-check"></i><b>4.3.1</b> Iterative LQR</a></li>
<li class="chapter" data-level="4.3.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#ddp"><i class="fa fa-check"></i><b>4.3.2</b> Differential Dynamic Programming</a></li>
<li class="chapter" data-level="4.3.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#qp"><i class="fa fa-check"></i><b>4.3.3</b> Quadratic Programming</a></li>
<li class="chapter" data-level="4.3.4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#sqp"><i class="fa fa-check"></i><b>4.3.4</b> Sequential Quadratic Programming</a></li>
<li class="chapter" data-level="4.3.5" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#ipm-nlp"><i class="fa fa-check"></i><b>4.3.5</b> Interior-Point Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#mpc"><i class="fa fa-check"></i><b>4.4</b> Model Predictive Control</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-materials.html"><a href="advanced-materials.html"><i class="fa fa-check"></i><b>5</b> Advanced Materials</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrangian-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-based-plan-optimize" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Model-based Planning and Optimization<a href="model-based-plan-optimize.html#model-based-plan-optimize" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="mdp.html#mdp">1</a>, we studied tabular MDPs with known dynamics where both the state and action spaces are finite, and we saw how dynamic-programming methods—Policy Iteration (PI) and Value Iteration (VI)—recover optimal policies.</p>
<p>Chapters <a href="value-rl.html#value-rl">2</a> and <a href="policy-gradient.html#policy-gradient">3</a> generalized these ideas to unknown dynamics and continuous spaces by introducing function approximation for value functions and policies. Those methods are <em>model-free</em>: they assume no access to the transition model and rely solely on data collected from interaction.</p>
<p>This chapter turns to the complementary regime: <strong>known dynamics</strong> with <strong>continuous state and action spaces</strong>. Our goal is to develop model-based planning and optimization methods that exploit the known dynamics to compute high-quality decisions efficiently.</p>
<p>We proceed in three steps:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Linear-quadratic systems.</strong> For linear dynamics and quadratic stage/terminal rewards (or costs), dynamic programming yields a linear optimal policy that can be computed efficiently via Riccati recursions. This setting serves as a tractable, illuminating baseline and a recurring building block in more general algorithms.</p></li>
<li><p><strong>Trajectory optimization (TO) for nonlinear systems.</strong> When dynamics are nonlinear, we focus on planning an optimal state–action trajectory from a given initial condition, maximizing the cumulative reward (or minimizing cumulative cost) subject to dynamics and constraints. Unlike RL, which seeks an optimal <em>feedback policy</em> valid for all states, TO computes an <em>open-loop plan</em> (often with a time-varying local feedback around a nominal trajectory). Although less ambitious, TO naturally accommodates state/control constraints—common in motion planning under safety, actuation, and environmental limits—and is widely used in robotics and control.</p></li>
<li><p><strong>Model predictive control (MPC).</strong> MPC converts open-loop plans into a feedback controller by repeatedly solving a short-horizon TO problem at each time step, applying only the first action, and receding the horizon. This receding-horizon strategy brings robustness to disturbances and model mismatch while retaining the constraint-handling benefits of TO.</p></li>
</ol>
<p>We adopt the standard discrete-time dynamical system notation
<span class="math display" id="eq:planning-discrete-dynamics">\[\begin{equation}
x_{t+1} = f_t(x_t, u_t, w_t),
\tag{4.1}
\end{equation}\]</span>
where <span class="math inline">\(x_t \in \mathbb{R}^n\)</span> is the state, <span class="math inline">\(u_t \in \mathbb{R}^m\)</span> is the control/action, <span class="math inline">\(w_t \in \mathbb{R}^d\)</span> is a (possibly stochastic) disturbance, and <span class="math inline">\(f_t\)</span> is a known transition function, potentially nonlinear or nonsmooth. The goal is to find a policy that maximizes a sum of stage rewards <span class="math inline">\(r(x_t,u_t)\)</span> and optional terminal reward <span class="math inline">\(r_T(x_T)\)</span>. We will often use the cost-minimization form <span class="math inline">\(c = -r\)</span>.
State and action constraints are written as
<span class="math display">\[
x_t \in \mathcal{X}, \qquad u_t \in \mathcal{U}.
\]</span></p>
<div id="lqr" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Linear Quadratic Regulator<a href="model-based-plan-optimize.html#lqr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we focus on the case when <span class="math inline">\(f_t\)</span> is a linear function, and the rewards/costs are quadratic in <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span>. This family of problems is known as linear quadratic regulator (LQR).</p>
<div id="finite-horizon-lqr" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Finite-Horizon LQR<a href="model-based-plan-optimize.html#finite-horizon-lqr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a linear discrete-time dynamical system
<span class="math display" id="eq:lqr-linear-system">\[\begin{equation}
x_{k+1} = A_k x_k + B_k u_k + w_k, \quad k=0,1,\dots,N-1,
\tag{4.2}
\end{equation}\]</span>
where <span class="math inline">\(x_k \in \mathbb{R}^n\)</span> the state, <span class="math inline">\(u_k \in \mathbb{R}^m\)</span> the control, <span class="math inline">\(w_k \in \mathbb{R}^n\)</span> the independent, zero-mean disturbance with given probability distribution that does not depend on <span class="math inline">\(x_k,u_k\)</span>, and <span class="math inline">\(A_k \in \mathbb{R}^{n \times n}, B_k \in \mathbb{R}^{n \times m}\)</span> are known matrices determining the transition dynamics.</p>
<p>We want to solve the following optimal control problem
<span class="math display" id="eq:lqr-formulation">\[\begin{equation}
\min_{\mu_0,\dots,\mu_{N-1}} \mathbb{E} \left\{ x_N^\top Q_N x_N + \sum_{k=0}^{N-1} \left( x_k^\top Q_k x_k + u_k^\top R_k u_k \right) \right\},
\tag{4.3}
\end{equation}\]</span>
where <span class="math inline">\(\mu_0,\dots,\mu_{N-1}\)</span> are feedback policies/controllers that map states to actions and the expectation is taken over the randomness in <span class="math inline">\(w_0,\dots,w_{N-1}\)</span>. In <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.3)</a>, <span class="math inline">\(\{Q_k \}_{k=0}^N\)</span> are positive semidefinite matrices, and <span class="math inline">\(\{ R_k \}_{k=0}^{N-1}\)</span> are positive definite matrices. The formulation <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.3)</a> is known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to “regulate” the system around the origin <span class="math inline">\(x=0\)</span>.</p>
<p>The Bellman Optimality condition introduced in Theorem <a href="mdp.html#thm:FiniteHorizonMDPBellmanOptimality">1.1</a> still holds for continuous state and action spaces. Therefore, we will try to follow the dynamic programming (DP) algorithm in Section <a href="mdp.html#dp">1.1.4</a> to solve for the optimal policy.</p>
<p>The DP algorithm computes the optimal cost-to-go backwards in time.
The terminal cost is
<span class="math display">\[
J_N(x_N) = x_N^\top Q_N x_N
\]</span>
by definition.</p>
<p>The optimal cost-to-go at time <span class="math inline">\(N-1\)</span> is equal to
<span class="math display" id="eq:lqr-cost-N-1">\[\begin{equation}
\begin{split}
J_{N-1}(x_{N-1}) = \min_{u_{N-1}} \mathbb{E}_{w_{N-1}} \{ x_{N-1}^\top Q_{N-1} x_{N-1} + u_{N-1}^\top R_{N-1} u_{N-1} + \\ \Vert \underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \Vert^2_{Q_N} \}
\end{split}
\tag{4.4}
\end{equation}\]</span>
where <span class="math inline">\(\Vert v \Vert_Q^2 = v^\top Q v\)</span> for <span class="math inline">\(Q \succeq 0\)</span>. Now observe that the objective in <a href="model-based-plan-optimize.html#eq:lqr-cost-N-1">(4.4)</a> is
<span class="math display">\[\begin{equation}
\begin{split}
x_{N-1}^\top Q_{N-1} x_{N-1} + u_{N-1}^\top R_{N-1} u_{N-1} + \Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \Vert_{Q_N}^2 + \\
\mathbb{E}_{w_{N-1}} \left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^\top Q_{N-1} w_{N-1} \right] + \\
\mathbb{E}_{w_{N-1}} \left[ w_{N-1}^\top Q_N w_{N-1} \right]
\end{split}
\end{equation}\]</span>
where the second line is zero due to <span class="math inline">\(\mathbb{E}[w_{N-1}] = 0\)</span> and the third line is a constant with respect to <span class="math inline">\(u_{N-1}\)</span>. Consequently, the optimal control <span class="math inline">\(u_{N-1}^\star\)</span> can be computed by setting the derivative of the objective with respect to <span class="math inline">\(u_{N-1}\)</span> equal to zero
<span class="math display" id="eq:optimal-u-N-1">\[\begin{equation}
u_{N-1}^\star = - \left[ \left( R_{N-1} + B_{N-1}^\top Q_N B_{N-1} \right)^{-1} B_{N-1}^\top Q_N A_{N-1} \right] x_{N-1}.
\tag{4.5}
\end{equation}\]</span>
Plugging the optimal controller <span class="math inline">\(u^\star_{N-1}\)</span> back to the objective of <a href="model-based-plan-optimize.html#eq:lqr-cost-N-1">(4.4)</a> leads to
<span class="math display" id="eq:optimal-cost-N-1">\[\begin{equation}
J_{N-1}(x_{N-1}) = x_{N-1}^\top S_{N-1} x_{N-1} + \mathbb{E} \left[ w_{N-1}^\top Q_N w_{N-1} \right],
\tag{4.6}
\end{equation}\]</span>
with
<span class="math display">\[
S_{N-1} = Q_{N-1} + A_{N-1}^\top \left[ Q_N - Q_N B_{N-1} \left( R_{N-1} + B_{N-1}^\top Q_N B_{N-1} \right)^{-1} B_{N-1}^\top Q_N \right] A_{N-1}.
\]</span>
We note that <span class="math inline">\(S_{N-1}\)</span> is positive semidefinite (this is an exercise for you to convince yourself).</p>
<p>Now we realize that something surprising and nice has happened.</p>
<ol style="list-style-type: decimal">
<li><p>The optimal controller <span class="math inline">\(u^{\star}_{N-1}\)</span> in <a href="model-based-plan-optimize.html#eq:optimal-u-N-1">(4.5)</a> is a linear feedback policy of the state <span class="math inline">\(x_{N-1}\)</span>, and</p></li>
<li><p>The optimal cost-to-go <span class="math inline">\(J_{N-1}(x_{N-1})\)</span> in <a href="model-based-plan-optimize.html#eq:optimal-cost-N-1">(4.6)</a> is quadratic in <span class="math inline">\(x_{N-1}\)</span>, just the same as <span class="math inline">\(J_{N}(x_N)\)</span>.</p></li>
</ol>
<p>This implies that, if we continue to compute the optimal cost-to-go at time <span class="math inline">\(N-2\)</span>, we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is,</p>
<blockquote>
<p>The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time.</p>
</blockquote>
<p>We summarize the solution for the LQR problem <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.3)</a> as follows.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:discretetimefinitehorizonlqrsolution" class="proposition"><strong>Proposition 4.1  (Solution of Discrete-Time Finite-Horizon LQR) </strong></span>The optimal controller for the LQR problem <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.3)</a> is a linear state-feedback policy
<span class="math display" id="eq:lqr-solution-control">\[\begin{equation}
\mu_k^\star(x_k) = - K_k x_k, \quad k=0,\dots,N-1.
\tag{4.7}
\end{equation}\]</span>
The gain matrix <span class="math inline">\(K_k\)</span> can be computed as
<span class="math display">\[
K_k = \left( R_k + B_k^\top S_{k+1} B_k  \right)^{-1} B_k^\top S_{k+1} A_k,
\]</span>
where the matrix <span class="math inline">\(S_k\)</span> satisfies the following backwards recursion
<span class="math display" id="eq:finite-discrete-lqr-riccati">\[\begin{equation}
\hspace{-6mm}
\begin{split}
S_N &amp;= Q_N \\
S_k &amp;= Q_k + A_k^\top \left[ S_{k+1} - S_{k+1}B_k \left( R_k + B_k^\top S_{k+1} B_k  \right)^{-1}  B_k^\top S_{k+1}  \right] A_k, k=N-1,\dots,0.
\end{split}
\tag{4.8}
\end{equation}\]</span>
The optimal cost-to-go is given by
<span class="math display">\[
J_0(x_0) = x_0^\top S_0 x_0 + \sum_{k=0}^{N-1} \mathbb{E} \left[ w_k^\top S_{k+1} w_k\right].
\]</span>
The recursion <a href="model-based-plan-optimize.html#eq:finite-discrete-lqr-riccati">(4.8)</a> is called the <em>discrete-time Riccati equation</em>.</p>
</div>
</div>
<p>Proposition <a href="model-based-plan-optimize.html#prp:discretetimefinitehorizonlqrsolution">4.1</a> states that, to evaluate the optimal policy <a href="model-based-plan-optimize.html#eq:lqr-solution-control">(4.7)</a>, one can first run the backwards Riccati equation <a href="model-based-plan-optimize.html#eq:finite-discrete-lqr-riccati">(4.8)</a> to compute all the positive definite matrices <span class="math inline">\(S_k\)</span>, and then compute the gain matrices <span class="math inline">\(K_k\)</span>. For systems of reasonable dimensions, evalutating the matrix inversion in <a href="model-based-plan-optimize.html#eq:finite-discrete-lqr-riccati">(4.8)</a> should be fairly efficient.</p>
</div>
<div id="infinite-horizon-lqr" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Infinite-Horizon LQR<a href="model-based-plan-optimize.html#infinite-horizon-lqr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now switch to the infinite-horizon LQR problem
<span class="math display" id="eq:infinite-horizon-lqr-system" id="eq:infinite-horizon-lqr-cost">\[\begin{align}
\min_{\mu} &amp; \quad  \sum_{k=0}^{\infty} \left( x_k^\top Q x_k + u_k^\top R u_k \right) \tag{4.9} \\
\text{subject to} &amp; \quad x_{k+1} = A x_k + B u_k, \quad k=0,\dots,\infty, \tag{4.10}
\end{align}\]</span>
where <span class="math inline">\(Q \succeq 0\)</span>, <span class="math inline">\(R \succ 0\)</span>, <span class="math inline">\(A,B\)</span> are constant matrices, and we seek a stationary policy <span class="math inline">\(\mu\)</span> that maps states to actions. Note that here we remove the disturbance <span class="math inline">\(w_k\)</span> because in general adding <span class="math inline">\(w_k\)</span> will make the objective function unbounded. To handle <span class="math inline">\(w_k\)</span>, we will have to either add a discount factor <span class="math inline">\(\gamma\)</span>, or switch to an average cost objective function.</p>
<p>For infinite-horizon problems, the Bellman Optimality condition changes from a recursion to an equation. Specifically, according to Theorem <a href="mdp.html#thm:BellmanOptimalityInfiniteHorizon">1.2</a> and equation <a href="mdp.html#eq:BellmanOptimalityInfiniteHorizonStateValue">(1.28)</a>, the optimal value function should satisfy the following Bellman optimality equation, restated for the case of cost minimization instead of reward maximization:
<span class="math display" id="eq:BellmanOptimalityInfiniteHorizonRestateMin">\[\begin{equation}
J^\star (x) = \min_{u} \left[ c(x,u) + \sum_{x&#39;} P(x&#39; \mid x, u) J^\star (x&#39;) \right], \quad \forall x,
\tag{4.11}
\end{equation}\]</span>
where <span class="math inline">\(c(x,u)\)</span> is the cost function.</p>
<p><strong>Guess A Solution.</strong> Based on our derivation in the finite-horizon case, we might as well guess that the optimal value function is a quadratic function:
<span class="math display">\[
J(x) = x^\top S x, \quad \forall x,
\]</span>
for some positive definite matrix <span class="math inline">\(S\)</span>. Then, our guessed solution must satisfy the Bellman optimality stated in <a href="model-based-plan-optimize.html#eq:BellmanOptimalityInfiniteHorizonRestateMin">(4.11)</a>:
<span class="math display" id="eq:infinite-horizon-lqr-invoke-dp">\[\begin{equation}
x^\top S x = J(x) = \min_{u} \left\{ x^\top Q x + u^\top R u + \Vert \underbrace{A x + B u}_{x&#39;} \Vert_S^2  \right\}.
\tag{4.12}
\end{equation}\]</span>
The minimization over <span class="math inline">\(u\)</span> in <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-invoke-dp">(4.12)</a> can again be solved in closed-form by setting the gradient of the objective with respect to <span class="math inline">\(u\)</span> to be zero
<span class="math display" id="eq:infinite-horizon-lqr-control">\[\begin{equation}
u^\star = - \underbrace{\left[ \left( R + B^\top S B \right)^{-1} B^\top S A \right]}_{K} x.
\tag{4.13}
\end{equation}\]</span>
Plugging the optimal <span class="math inline">\(u^\star\)</span> back into <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-invoke-dp">(4.12)</a>, we see that the matrix <span class="math inline">\(S\)</span> has to satisfy the following equation
<span class="math display" id="eq:algebraic-riccati">\[\begin{equation}
S = Q + A^\top \left[  S - SB \left( R + B^\top S B  \right)^{-1} B^\top S \right] A.
\tag{4.14}
\end{equation}\]</span>
Equation <a href="model-based-plan-optimize.html#eq:algebraic-riccati">(4.14)</a> is known as the <em>discrete algebraic Riccati equation</em> (DARE).</p>
<p>So the question boils down to if the DARE has a solution <span class="math inline">\(S\)</span> that is positive definite?</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:infinitehorizonlqrsolution" class="proposition"><strong>Proposition 4.2  (Solution of Discrete-Time Infinite-Horizon LQR) </strong></span>Consider a linear system
<span class="math display">\[
x_{k+1} = A x_k + B u_k,
\]</span>
with <span class="math inline">\((A,B)\)</span> controllable (see Section <a href="model-based-plan-optimize.html#linear-system-basics">4.1.3</a>). Let <span class="math inline">\(Q \succeq 0\)</span> in <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-cost">(4.9)</a> be such that <span class="math inline">\(Q\)</span> can be written as <span class="math inline">\(Q = C^\top C\)</span> with <span class="math inline">\((A,C)\)</span> observable.</p>
<p>Then the optimal controller for the infinite-horizon LQR problem <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-cost">(4.9)</a> is a stationary linear policy
<span class="math display">\[
\mu^\star (x) = - K x,
\]</span>
with
<span class="math display">\[
K = \left( R + B^\top S B \right)^{-1} B^\top S A.
\]</span>
The matrix <span class="math inline">\(S\)</span> is the unique positive definite matrix that satisfies the discrete algebraic Riccati equation
<span class="math display" id="eq:discrete-algebraic-riccati-equation">\[\begin{equation}
S = Q + A^\top \left[  S - SB \left( R + B^\top S B  \right)^{-1} B^\top S \right] A.
\tag{4.15}
\end{equation}\]</span></p>
<p>Moreover, the closed-loop system
<span class="math display">\[
x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k
\]</span>
is stable, i.e., the eigenvalues of the matrix <span class="math inline">\(A - BK\)</span> are strictly within the unit circle (see Appendix <a href="app-lti-system-theory.html#app-lti-stability-dt">B.1.2</a>).</p>
</div>
</div>
<p><em>Remark.</em> The assumptions of <span class="math inline">\((A,B)\)</span> being controllable and <span class="math inline">\((A,C)\)</span> being observable can be relaxted to <span class="math inline">\((A,B)\)</span> being stabilizable and <span class="math inline">\((A,C)\)</span> being detectable (for definitions of stabilizability and detectability, see Appendix <a href="app-lti-system-theory.html#app-lti-system-theory">B</a>).</p>
<p>We have not discussed how to solve the algebraic Riccati equation <a href="model-based-plan-optimize.html#eq:discrete-algebraic-riccati-equation">(4.15)</a>. It is clear that <a href="model-based-plan-optimize.html#eq:discrete-algebraic-riccati-equation">(4.15)</a> is not a linear system of equations in <span class="math inline">\(S\)</span>. In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see <span class="citation">(<a href="#ref-arnold84ieee-generalized">Arnold and Laub 1984</a>)</span>. Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab <a href="https://www.mathworks.com/help/control/ref/dlqr.html"><code>dlqr</code></a> and the Python <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_discrete_are.html"><code>scipy.linalg.solve_discrete_are</code></a> function computes the <span class="math inline">\(K\)</span> and <span class="math inline">\(S\)</span> matrices from <span class="math inline">\(A,B,Q,R\)</span>.</p>
<p>Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:lqr-pendulum-stabilization" class="example"><strong>Example 4.1  (Pendulum Stabilization by LQR) </strong></span>Consider the simple pendulum in Fig. <a href="model-based-plan-optimize.html#fig:pendulum-drawing">4.1</a> with dynamics
<span class="math display" id="eq:lqr-pendulum-dynamics">\[\begin{equation}
x = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}, \quad
\dot{x} = f(x,u) = \begin{bmatrix}
\dot{\theta} \\
-\frac{1}{ml^2}(b \dot{\theta} + mgl \sin \theta) + \frac{1}{ml^2} u
\end{bmatrix}
\tag{4.16}
\end{equation}\]</span>
where <span class="math inline">\(m\)</span> is the mass of the pendulum, <span class="math inline">\(l\)</span> is the length of the pole, <span class="math inline">\(g\)</span> is the gravitational constant, <span class="math inline">\(b\)</span> is the damping ratio, and <span class="math inline">\(u\)</span> is the torque applied to the pendulum.</p>
<p>We are interested in applying the LQR controller to balance the pendulum in the upright position <span class="math inline">\(x_d = [\pi,0]^\top\)</span> with a zero velocity.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-drawing"></span>
<img src="images/pendulum-drawing.png" alt="A Simple Pendulum." width="40%" />
<p class="caption">
Figure 4.1: A Simple Pendulum.
</p>
</div>
<p>Let us first shift the dynamics so that “<span class="math inline">\(0\)</span>” is the upright position. This can be done by defining a new variable <span class="math inline">\(z = x - x_d = [\theta - \pi, \dot{\theta}]^\top\)</span>, which leads to
<span class="math display" id="eq:pendulum-dynamics-z-coordinate">\[\begin{equation}
\dot{z} = \dot{x} = f(x,u) = f(z + x_d,u) = \begin{bmatrix}
z_2 \\
\frac{1}{ml^2} \left( u - b z_2 + mgl \sin z_1  \right)
\end{bmatrix} = f&#39;(z,u).
\tag{4.17}
\end{equation}\]</span>
We then linearize the nonlinear dynamics <span class="math inline">\(\dot{z} = f&#39;(z,u)\)</span> at the point <span class="math inline">\(z^\star = 0, u^\star = 0\)</span>:
<span class="math display">\[\begin{align}
\dot{z} &amp; \approx f&#39;(z^\star,u^\star) + \left( \frac{\partial f&#39;}{\partial z} \right)_{z^\star,u^\star} (z - z^\star) + \left( \frac{\partial f&#39;}{\partial u} \right)_{z^\star,u^\star} (u - u^\star) \\
&amp; = \begin{bmatrix}
0 &amp; 1 \\
\frac{g}{l} \cos z_1 &amp; - \frac{b}{ml^2}
\end{bmatrix}_{z^\star, u^\star} z +
\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix} u \\
&amp; = \underbrace{\begin{bmatrix}
0 &amp; 1 \\
\frac{g}{l} &amp; - \frac{b}{ml^2}
\end{bmatrix}}_{A_c} z  +
\underbrace{\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix}}_{B_c} u.
\end{align}\]</span>
Finally, we convert the continuous-time dynamics to discrete time with a fixed discretization <span class="math inline">\(h\)</span>
<span class="math display">\[
z_{k+1} = \dot{z}_k \cdot h + z_k = \underbrace{(h \cdot A_c + I )}_{A} z_k + \underbrace{(h \cdot B_c)}_{B} u_k.
\]</span></p>
<p>We are now ready to implement the LQR controller. In the formulation <a href="model-based-plan-optimize.html#eq:infinite-horizon-lqr-cost">(4.9)</a>, we choose <span class="math inline">\(Q = I\)</span>, <span class="math inline">\(R = I\)</span>, and compute the gain matrix <span class="math inline">\(K\)</span> by solving the DARE.</p>
<p>Fig. <a href="model-based-plan-optimize.html#fig:pendulum-stabilization-sim">4.2</a> shows the simulation result for <span class="math inline">\(m=1,l=1,b=0.1\)</span>, <span class="math inline">\(g = 9.8\)</span>, and <span class="math inline">\(h = 0.01\)</span>, with an initial condition <span class="math inline">\(z^0 = [0.1,0.1]^\top\)</span>. We can see that the LQR controller successfully stabilizes the pendulum at <span class="math inline">\(z^\star\)</span>, the upright position.</p>
<p>You can play with the Python code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_lqr_stabilization.py">here</a>.</p>
<p>Alternatively, the Matlab code can be found <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/pendulum_stabilization_lqr.m">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-stabilization-sim"></span>
<img src="images/Model-based-optimization/pendulum_lqr_stabilization.png" alt="LQR stabilization of a simple pendulum." width="60%" />
<p class="caption">
Figure 4.2: LQR stabilization of a simple pendulum.
</p>
</div>
</div>
</div>
</div>
<div id="linear-system-basics" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Linear System Basics<a href="model-based-plan-optimize.html#linear-system-basics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the discrete-time linear time-invariant (LTI) system
<span class="math display">\[
x_{k+1}=Ax_k+Bu_k,\qquad y_k=Cx_k+Du_k,
\]</span>
with <span class="math inline">\(x_k\in\mathbb{R}^n,\;u_k\in\mathbb{R}^m,\;y_k\in\mathbb{R}^p\)</span>.</p>
<p>We provide a very brief review of linear system theory to understand Proposition <a href="model-based-plan-optimize.html#prp:infinitehorizonlqrsolution">4.2</a>. More details can be found in Appendix <a href="app-lti-system-theory.html#app-lti-system-theory">B</a>.</p>
<p><strong>Stability.</strong> The autonomous system <span class="math inline">\(x_{k+1}=Ax_k\)</span> is (asymptotically) stable if for every <span class="math inline">\(x_0\)</span> we have <span class="math inline">\(x_k\to 0\)</span> as <span class="math inline">\(k\to\infty\)</span>.</p>
<p><strong>Equivalent characterizations.</strong></p>
<ul>
<li><p><span class="math inline">\(A\)</span> is <strong>Schur</strong>: all eigenvalues satisfy <span class="math inline">\(|\lambda_i(A)|&lt;1\)</span>.</p></li>
<li><p>Lyapunov: <span class="math inline">\(\exists P\succ 0\)</span> s.t. <span class="math inline">\(A^\top P A - P \prec 0\)</span>.</p></li>
</ul>
<p><strong>Controllability (reachability).</strong> The pair <span class="math inline">\((A,B)\)</span> is controllable if for any <span class="math inline">\(x_0,x_f\)</span> there exists a <em>finite</em> input sequence <span class="math inline">\(\{u_0,\dots,u_{N-1}\}\)</span> that drives the state from <span class="math inline">\(x_0\)</span> to <span class="math inline">\(x_N=x_f\)</span>.</p>
<p><strong>Kalman controllability matrix.</strong>
<span class="math display">\[
  \mathcal C \;=\; [\,B\; AB\; A^2B\;\cdots\; A^{n-1}B\,],\quad
  \text{\((A,B)\) controllable} \iff \operatorname{rank}(\mathcal C)=n.
  \]</span></p>
<p><strong>Popov-Belevitch-Hautus (PBH) test.</strong>
<span class="math display">\[
  \text{\((A,B)\) controllable} \iff
  \operatorname{rank}\!\begin{bmatrix}\lambda I - A &amp; B\end{bmatrix} = n
  \ \text{for all}\ \lambda\in\mathbb{C}.
  \]</span>
It suffices to check <span class="math inline">\(\lambda\)</span> equal to the eigenvalues of <span class="math inline">\(A\)</span>.</p>
<p><strong>Observability.</strong> The pair <span class="math inline">\((A,C)\)</span> is observable if the initial state <span class="math inline">\(x_0\)</span> can be uniquely determined from a finite sequence of outputs (and known inputs), e.g., from <span class="math inline">\(\{y_0,\dots,y_{n-1}\}\)</span>.</p>
<p><strong>Observability matrix.</strong>
<span class="math display">\[
  \mathcal O \;=\; \begin{bmatrix} C \\ CA \\ \vdots \\ CA^{n-1}\end{bmatrix},\quad
  \text{\((A,C)\) observable} \iff \operatorname{rank}(\mathcal O)=n.
  \]</span></p>
<p><strong>PBH test.</strong>
<span class="math display">\[
  \text{\((A,C)\) observable} \iff
  \operatorname{rank}\!\begin{bmatrix}\lambda I - A^\top &amp; C^\top\end{bmatrix} = n
  \ \text{for all}\ \lambda\in\mathbb{C}.
  \]</span>
Dual to controllability: <span class="math inline">\((A,C)\)</span> observable <span class="math inline">\(\Leftrightarrow\)</span> <span class="math inline">\((A^\top,C^\top)\)</span> controllable.</p>
</div>
</div>
<div id="lqr-tracking" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> LQR Trajectory Tracking<a href="model-based-plan-optimize.html#lqr-tracking" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Classical LQR delivers an optimal linear state-feedback when dynamics are linear and the objective is quadratic. In many planning problems, however, we do not seek a single stationary feedback for <em>all</em> states but rather a <em>local stabilizer around a given (possibly time-varying) trajectory</em>—for instance, a motion plan from a trajectory optimizer or MPC’s rolling nominal (see Section <a href="model-based-plan-optimize.html#traj-opt">4.3</a>). LQR Tracking (also called time-varying LQR, TVLQR) provides exactly this: a <em>time-varying</em> linear feedback that stabilizes the system near a nominal state–control sequence and rejects small disturbances.</p>
<p><strong>Problem Setup.</strong> Let the nominal (i.e., ignoring the disturbance) discrete-time system be
<span class="math display">\[
x_{t+1} \;=\; f_t(x_t,u_t), \qquad t=0,\dots,N-1,
\]</span>
and suppose we have a <em>nominal trajectory</em> <span class="math inline">\(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\)</span> satisfying
<span class="math display">\[
\bar x_{t+1} \;=\; f_t(\bar x_t,\bar u_t).
\]</span></p>
<p>Our goal is to design a controller that can stabilize the system with disturbance, i.e., <span class="math inline">\(x_{t+1} = f_t(x_t, u_t, w_t)\)</span>, around the nominal trajectory.</p>
<p>Towards this, we define <em>deviations</em> from the nominal trajectory as
<span class="math display">\[
\delta x_t := x_t-\bar x_t, \qquad \delta u_t := u_t-\bar u_t.
\]</span></p>
<p>If the true system is linear time-varying (or we linearize a nonlinear system along the nominal), we obtain the <em>deviation dynamics</em>
<span class="math display" id="eq:tvlqr-deviation-dynamics">\[
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,
\quad
A_t := \left.\frac{\partial f_t}{\partial x}\right|_{(\bar x_t,\bar u_t)},\quad
B_t := \left.\frac{\partial f_t}{\partial u}\right|_{(\bar x_t,\bar u_t)}.
\tag{4.18}
\]</span></p>
<p>We penalize deviations with a quadratic cost
<span class="math display" id="eq:tvlqr-deviation-cost">\[
J \;=\; \delta x_N^\top Q_N \delta x_N
\;+\;\sum_{t=0}^{N-1} \Big(\delta x_t^\top Q_t \delta x_t \;+\; \delta u_t^\top R_t \delta u_t\Big),
\quad Q_t\succeq 0,\; R_t\succ 0.
\tag{4.19}
\]</span></p>
<p><strong>LQR Tracking Algorithm.</strong> The tracking controller takes the <em>affine</em> form
<span class="math display">\[
u_t \;=\; \bar u_t \;-\; K_t\,(x_t-\bar x_t),
\]</span>
where <span class="math inline">\(\{K_t\}_{t=0}^{N-1}\)</span> are time-varying gains computed by a backward Riccati recursion on the deviation system <a href="model-based-plan-optimize.html#eq:tvlqr-deviation-dynamics">(4.18)</a> with cost <a href="model-based-plan-optimize.html#eq:tvlqr-deviation-cost">(4.19)</a>.</p>
<p>From Proposition <a href="model-based-plan-optimize.html#prp:discretetimefinitehorizonlqrsolution">4.1</a>, we know the gains can be computed as follows.</p>
<p>Initialize at terminal time:
<span class="math display">\[
S_N \;=\; Q_N.
\]</span>
For <span class="math inline">\(t = N-1,\,N-2,\,\dots,\,0\)</span>:
<span class="math display" id="eq:tvlqr-riccati">\[\begin{equation}
\begin{split}
K_t &amp;= \Big(R_t + B_t^\top S_{t+1} B_t\Big)^{-1} B_t^\top S_{t+1} A_t, \\[2mm]
S_t &amp;= Q_t \;+\; A_t^\top \!\Big(S_{t+1} - S_{t+1} B_t \big(R_t + B_t^\top S_{t+1} B_t\big)^{-1} B_t^\top S_{t+1}\Big) A_t.
\end{split}
\tag{4.20}
\end{equation}\]</span></p>
<p>Given the gains <span class="math inline">\(\{K_t\}\)</span>, apply at runtime:
<span class="math display" id="eq:tvlqr-control-law">\[\begin{equation}
u_t \;=\; \bar u_t - K_t\,(x_t-\bar x_t), \qquad t=0,\dots,N-1.
\tag{4.21}
\end{equation}\]</span></p>
<p>The following pseudocode summarizes the algorithm.</p>
<div class="highlightbox">
<p><strong>Algorithm: LQR Trajectory Tracking (TVLQR)</strong></p>
<p><strong>Inputs:</strong> nominal <span class="math inline">\(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\)</span>, weights <span class="math inline">\(\{Q_t,R_t\}\)</span>, terminal <span class="math inline">\(Q_N\)</span>.</p>
<ol style="list-style-type: decimal">
<li><strong>Linearize</strong> along the nominal to get <span class="math inline">\(A_t,B_t\)</span> via <a href="model-based-plan-optimize.html#eq:tvlqr-deviation-dynamics">(4.18)</a>.<br />
</li>
<li><strong>Backward pass:</strong> compute <span class="math inline">\(K_t\)</span> and <span class="math inline">\(S_t\)</span> via <a href="model-based-plan-optimize.html#eq:tvlqr-riccati">(4.20)</a>.<br />
</li>
<li><strong>Apply feedback:</strong> <span class="math inline">\(u_t=\bar u_t - K_t(x_t-\bar x_t)\)</span> as in <a href="model-based-plan-optimize.html#eq:tvlqr-control-law">(4.21)</a>.</li>
</ol>
<p><strong>Output:</strong> time-varying gains <span class="math inline">\(\{K_t\}\)</span> giving a local stabilizer around the trajectory.</p>
</div>
<p>We now apply TVLQR to a vehicle trajectory tracking problem.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:tvlqr-unicyle" class="example"><strong>Example 4.2  (LQR Trajectory Tracking for Unicyle) </strong></span>We (i) define the dynamics in continuous and discrete time, (ii) specify a circular <em>nominal trajectory</em>, (iii) linearize the nonlinear dynamics <em>along the nominal</em>, (iv) state the deviation-cost weights <span class="math inline">\(Q,R\)</span> (and terminal <span class="math inline">\(Q_N\)</span>), and (v) list the <em>experiment setup</em> (discretization and horizon length).</p>
<p><strong>Unicycle Dynamics (Continuous and Discrete).</strong></p>
<p><strong>State and input.</strong><br />
<span class="math display">\[
x=\begin{bmatrix}p_x\\ p_y\\ \theta\end{bmatrix}\in\mathbb{R}^3,
\qquad
u=\begin{bmatrix}v\\ \omega\end{bmatrix}\in\mathbb{R}^2,
\]</span>
where <span class="math inline">\((p_x,p_y)\)</span> is planar position, <span class="math inline">\(\theta\)</span> is heading, <span class="math inline">\(v\)</span> is forward speed, and <span class="math inline">\(\omega\)</span> is yaw rate.</p>
<p><strong>Continuous time:</strong>
<span class="math display" id="eq:unicycle-ct">\[\begin{equation}
\dot p_x = v\cos\theta,\qquad
\dot p_y = v\sin\theta,\qquad
\dot\theta = \omega.
\tag{4.22}
\end{equation}\]</span></p>
<p><strong>Discrete time (forward Euler with step <span class="math inline">\(h&gt;0\)</span>):</strong>
<span class="math display" id="eq:unicycle-dt">\[\begin{equation}
x_{t+1} \;=\; f(x_t,u_t)
:= \begin{bmatrix}
p_{x,t} + h\, v_t\cos\theta_t\\[2pt]
p_{y,t} + h\, v_t\sin\theta_t\\[2pt]
\theta_t + h\,\omega_t
\end{bmatrix}.
\tag{4.23}
\end{equation}\]</span></p>
<p><strong>Nominal Trajectory (Circular Motion).</strong> We track a circle of radius <span class="math inline">\(R=\dfrac{\bar v}{\bar\omega}\)</span> using <em>constant nominal inputs</em>
<span class="math display" id="eq:unicycle-nominal-control">\[\begin{equation}
\bar u_t \equiv \begin{bmatrix}\bar v\\ \bar\omega\end{bmatrix},\qquad t=0,\dots,N-1,
\tag{4.24}
\end{equation}\]</span>
and generate the <em>nominal state</em> recursively under the discrete dynamics <a href="model-based-plan-optimize.html#eq:unicycle-dt">(4.23)</a>:
<span class="math display" id="eq:unicycle-discrete-nominal">\[\begin{equation}
\bar x_{t+1} \;=\; f(\bar x_t,\bar u_t),\qquad \bar x_0 = \begin{bmatrix}R\\ 0\\ \frac{\pi}{2}\end{bmatrix}.
\tag{4.25}
\end{equation}\]</span></p>
<p>We define <em>deviations</em> from the nominal:
<span class="math display">\[
\delta x_t := x_t-\bar x_t,\qquad \delta u_t := u_t-\bar u_t.
\]</span></p>
<p><strong>Linearization Along the Nominal.</strong> Linearize <a href="model-based-plan-optimize.html#eq:unicycle-dt">(4.23)</a> at <span class="math inline">\((\bar x_t,\bar u_t)\)</span> to obtain the deviation dynamics
<span class="math display" id="eq:unicycle-dev-dyn">\[\begin{equation}
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,
\tag{4.26}
\end{equation}\]</span>
with Jacobians (using <span class="math inline">\(c_t:=\cos\bar\theta_t,\ s_t:=\sin\bar\theta_t\)</span>)
<span class="math display" id="eq:unicycle-linearization">\[\begin{equation}
A_t \;=\; \frac{\partial f}{\partial x}\Big|_{(\bar x_t,\bar u_t)}
= \begin{bmatrix}
1 &amp; 0 &amp; -h\,\bar v\,s_t\\
0 &amp; 1 &amp; \ \ h\,\bar v\,c_t\\
0 &amp; 0 &amp; 1
\end{bmatrix},
\qquad
B_t \;=\; \frac{\partial f}{\partial u}\Big|_{(\bar x_t,\bar u_t)}
= \begin{bmatrix}
h\,c_t &amp; 0\\
h\,s_t &amp; 0\\
0 &amp; h
\end{bmatrix}.
\tag{4.27}
\end{equation}\]</span></p>
<p><strong>Deviation Cost (Weights <span class="math inline">\(Q,R,Q_N\)</span>).</strong> We penalize deviations with a quadratic stage/terminal cost
<span class="math display">\[
J \;=\; \delta x_N^\top Q_N\,\delta x_N
\;+\;\sum_{t=0}^{N-1}\Big(\delta x_t^\top Q\,\delta x_t+\delta u_t^\top R\,\delta u_t\Big),
\]</span>
using the weights:
<span class="math display" id="eq:unicycle-weights">\[\begin{equation}
Q=\mathrm{diag}(30,\;30,\;5),\qquad
Q_N=\mathrm{diag}(60,\;60,\;8),\qquad
R=\mathrm{diag}(0.2,\;0.2).
\tag{4.28}
\end{equation}\]</span>
These reflect a stronger emphasis on position tracking, a moderate penalty on heading error, and a mild penalty on control <em>deviations</em> from the nominal.</p>
<p><strong>Experiment Setup.</strong></p>
<ul>
<li><strong>Discretization step:</strong> <span class="math inline">\(h = 0.02\ \mathrm{s}\)</span>.<br />
</li>
<li><strong>Horizon length:</strong> <span class="math inline">\(T_{\mathrm{final}} = 12\ \mathrm{s}\)</span>.<br />
</li>
<li><strong>Number of steps:</strong> <span class="math inline">\(N = T_{\mathrm{final}}/h = \mathbf{600}\)</span>.<br />
</li>
<li><strong>Nominal inputs:</strong> <span class="math inline">\(\bar v = 1.2\ \mathrm{m/s},\ \bar\omega = 0.4\ \mathrm{rad/s}\)</span> (radius <span class="math inline">\(R=\bar v/\bar\omega\)</span>).<br />
</li>
<li><strong>Initialization:</strong> the nominal starts at <span class="math inline">\(\bar x_0 = [\,R,\,0,\,\pi/2\,]^\top\)</span>; the actual system may start with a small offset (see code).</li>
</ul>
<p>With <span class="math inline">\((A_t,B_t)\)</span> from <a href="model-based-plan-optimize.html#eq:unicycle-linearization">(4.27)</a> and weights <a href="model-based-plan-optimize.html#eq:unicycle-weights">(4.28)</a>, the TVLQR backward Riccati recursion (Section <a href="model-based-plan-optimize.html#lqr-tracking">4.2</a>) yields gains <span class="math inline">\(K_t\)</span>. We then apply the <strong>local feedback</strong>
<span class="math display">\[
u_t \;=\; \bar u_t - K_t\,(x_t-\bar x_t),
\]</span>
to robustly track the circular nominal under small disturbances.</p>
<p><strong>Disturbances.</strong> To test robustness, we inject additive process disturbances into the discrete dynamics:
<span class="math display">\[
x_{t+1} \;=\; f(x_t,u_t)\;+\; w_t,\qquad t=0,\dots,N-1,
\]</span>
where
<span class="math display">\[
w_t \;=\; \underbrace{\eta_t}_{\text{i.i.d. Gaussian noise}} \;+\; \underbrace{g_t}_{\text{deterministic gust}}.
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Small i.i.d. Gaussian process noise. We draw <span class="math inline">\(\eta_t \sim \mathcal N(0,W)\)</span> independently at each step with
<span class="math display">\[
W \;=\; \mathrm{diag}\!\big(\sigma_x^2,\ \sigma_y^2,\ \sigma_\theta^2\big),
\qquad
\sigma_x = \sigma_y = 0.01\ \text{m},\quad
\sigma_\theta = \mathrm{deg2rad}(0.2).
\]</span>
This noise perturbs the post-update state components <span class="math inline">\((p_x,p_y,\theta)\)</span>.</p></li>
<li><p>Finite-duration “gust” impulse.
In addition to <span class="math inline">\(\eta_t\)</span>, we apply a brief deterministic bias over a window
<span class="math display">\[
t \in [t_g,\ t_g+\Delta] \;=\; [\,4.0\,\mathrm{s},\ 4.8\,\mathrm{s}\,),
\]</span>
implemented at the discrete indices <span class="math inline">\(\{t_g,\dots,t_g+\Delta\}\)</span>. During this window we set
<span class="math display">\[
g_t \;=\; \begin{bmatrix}
\delta p_x \\[1mm] 0 \\[1mm] \delta \theta
\end{bmatrix},
\qquad
\delta p_x = 0.01\ \text{m per step},\quad
\delta \theta = \mathrm{deg2rad}(1.8)\ \text{per step},
\]</span>
and <span class="math inline">\(g_t=\mathbf{0}\)</span> otherwise. This models a short-lived lateral drift and a heading kick.</p></li>
</ol>
<p><strong>Results.</strong> Fig. <a href="model-based-plan-optimize.html#fig:unicycle-lqr-tracking-trajectory">4.3</a> visualizes the nominal trajectory (the dotted circle) and the TVLQR-stabilized trajectory in blue. To clearly see the impact of closed-loop LQR tracking, we also plotted the open-loop trajectory, i.e., the system’s trajectory if no feedback is applied. We can observe that the TVLQR controller effectively rejects the disturbances and stabilizes the closed-loop trajectory along the nominal path.</p>
<p>Fig. <a href="model-based-plan-optimize.html#fig:unicycle-lqr-tracking-error">4.4</a> visualizes the state tracking error (position and heading error) as well as compares the closed-loop control with open-loop control.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/lqr_tracking.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-lqr-tracking-trajectory"></span>
<img src="images/Model-based-optimization/tvlqr_trajectory.png" alt="LQR Trajectory Tracking for Unicyle: comparison between nominal trajectory, open-loop trajectory, and closed-loop trajectory with feedback." width="60%" />
<p class="caption">
Figure 4.3: LQR Trajectory Tracking for Unicyle: comparison between nominal trajectory, open-loop trajectory, and closed-loop trajectory with feedback.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-lqr-tracking-error"></span>
<img src="images/Model-based-optimization/tvlqr_state_error.png" alt="LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom)." width="90%" /><img src="images/Model-based-optimization/tvlqr_control.png" alt="LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom)." width="90%" />
<p class="caption">
Figure 4.4: LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom).
</p>
</div>
</div>
</div>
</div>
<div id="traj-opt" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Trajectory Optimization<a href="model-based-plan-optimize.html#traj-opt" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Section <a href="model-based-plan-optimize.html#lqr-tracking">4.2</a> we saw that TVLQR gives a powerful <em>local stabilizer</em> around a nominal state–control sequence <span class="math inline">\((\bar x_t,\bar u_t)\)</span>. This raises a natural question:</p>
<blockquote>
<p>Where do nominal trajectories come from?</p>
</blockquote>
<p>In many robotics tasks (maneuvering a car, landing a rocket, walking with a robot), we must compute a <em>feasible, high-quality open-loop plan</em> that respects the dynamics and constraints. <strong>Trajectory Optimization (TO)</strong> does exactly this: it searches over sequences <span class="math inline">\(\{x_t,u_t\}\)</span> to minimize a cumulative cost while satisfying the system dynamics and constraints.</p>
<p>Moreover, if we can solve TO <strong>quickly</strong> (or approximately, but reliably), then by re-solving over a short horizon at each time step and applying only the first control, we obtain <strong>Model Predictive Control (MPC)</strong>—a feedback controller that blends optimization with robustness (see Section <a href="model-based-plan-optimize.html#mpc">4.4</a> later). Thus, TO is both a <strong>planner</strong> and the engine behind <strong>feedback via MPC</strong>.</p>
<p><strong>General Nonlinear Trajectory Optimization Problem.</strong> We adopt the standard discrete-time nonlinear system
<span class="math display">\[
x_{t+1} = f_t(x_t,u_t),\qquad t=0,\dots,N-1,
\]</span>
with state <span class="math inline">\(x_t\in\mathbb{R}^n\)</span> and control <span class="math inline">\(u_t\in\mathbb{R}^m\)</span>. A generic finite-horizon TO problem is
<span class="math display" id="eq:to-general">\[\begin{equation}
\begin{split}
\min_{\{x_t,u_t\}} \quad &amp;
\Phi(x_N) + \sum_{t=0}^{N-1} \ell_t(x_t,u_t) \\[2mm]
\text{s.t.}\quad &amp;
x_{t+1} = f_t(x_t,u_t), \qquad t=0,\dots,N-1,\\
&amp; x_0 = \hat x_0 \ \ \text{(given)},\\
&amp; x_t \in \mathcal X_t,\quad u_t \in \mathcal U_t \quad \text{(bounds)},\\
&amp; g_t(x_t,u_t) \le 0,\quad h_t(x_t,u_t)=0 \quad \text{(path/terminal constraints).}
\end{split}
\tag{4.29}
\end{equation}\]</span>
Here <span class="math inline">\(\ell_t\)</span> and <span class="math inline">\(\Phi\)</span> encode performance (e.g., energy, time, tracking error), <span class="math inline">\(\mathcal X_t,\mathcal U_t\)</span> capture box limits and safety sets, and <span class="math inline">\(g_t,h_t\)</span> represent additional nonlinear constraints (obstacles, terminal goals, etc.).</p>
<p>Solving <a href="model-based-plan-optimize.html#eq:to-general">(4.29)</a> directly is difficult in general. A widely used strategy is to iteratively approximate it by <em>quadratic</em> subproblems that can be solved efficiently. This leads to <strong>iLQR</strong> and its second-order cousin <strong>DDP</strong> (see Section <a href="model-based-plan-optimize.html#ddp">4.3.2</a>).</p>
<div id="traj-opt-ilqr" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Iterative LQR<a href="model-based-plan-optimize.html#traj-opt-ilqr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>High-level intuition.</strong> iLQR (iterative LQR) alternates between:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Local modeling:</strong> around a <em>current</em> nominal trajectory <span class="math inline">\(\{(\bar x_t,\bar u_t)\}\)</span>,</p>
<ul>
<li><strong>linearize</strong> the dynamics,<br />
</li>
<li><strong>quadratically approximate</strong> the cost.</li>
</ul></li>
<li><p><strong>LQR step:</strong> solve the resulting <em>time-varying LQR</em> subproblem to obtain a <em>time-varying affine policy</em>
<span class="math display">\[
\delta u_t = k_t + K_t\,\delta x_t,\quad \delta x_t:=x_t-\bar x_t,\ \delta u_t:=u_t-\bar u_t,
\]</span>
which gives both a feedforward step <span class="math inline">\(k_t\)</span> (to change the nominal control) and a feedback gain <span class="math inline">\(K_t\)</span> (to stabilize the rollout).</p></li>
<li><p><strong>Forward rollout + line search:</strong> apply <span class="math inline">\(u_t^{\text{new}}=\bar u_t+\alpha k_t + K_t(x_t^{\text{new}}-\bar x_t)\)</span> to the true nonlinear dynamics, producing a new nominal trajectory <span class="math inline">\(\{(\bar x_t,\bar u_t)\}\)</span>. Here we choose <span class="math inline">\(\alpha\in(0,1]\)</span> to reduce the cost and respect constraints.</p></li>
<li><p><strong>Repeat</strong> until convergence (cost decrease and dynamics residuals are small).</p></li>
</ol>
<p>iLQR can be viewed as a <a href="https://en.wikipedia.org/wiki/Gauss–Newton_algorithm">Gauss–Newton method</a> on trajectories: it uses first-order dynamics and second-order cost, capturing the dominant curvature while remaining numerically robust and fast.</p>
<div id="lqr-subproblem-one-ilqr-outer-iteration" class="section level4 hasAnchor" number="4.3.1.1">
<h4><span class="header-section-number">4.3.1.1</span> LQR Subproblem (one iLQR outer iteration)<a href="model-based-plan-optimize.html#lqr-subproblem-one-ilqr-outer-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Given a nominal trajectory <span class="math inline">\(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\)</span> with <span class="math inline">\(\bar x_{t+1}=f_t(\bar x_t,\bar u_t)\)</span>, define deviations
<span class="math display">\[
\delta x_t := x_t-\bar x_t,\qquad \delta u_t := u_t-\bar u_t,\qquad \delta x_0\ \text{given.}
\]</span></p>
<p><strong>Linearized Dynamics.</strong> We linearize the dynamics along the nominal trajectory
<span class="math display" id="eq:ilqr-linearize">\[
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,\quad
A_t:=\left.\frac{\partial f_t}{\partial x}\right|_{(\bar x_t,\bar u_t)},\ \
B_t:=\left.\frac{\partial f_t}{\partial u}\right|_{(\bar x_t,\bar u_t)}.
\tag{4.30}
\]</span></p>
<p><strong>Quadratic Cost Approximation.</strong> We perform a quadratic approximation of the objective function about <span class="math inline">\((\bar x_t,\bar u_t)\)</span>
<span class="math display" id="eq:ilqr-quadratic-cost">\[
\begin{aligned}
\ell_t(x_t,u_t) &amp;\approx \ell_t
+ \ell_{x,t}^\top \delta x_t + \ell_{u,t}^\top \delta u_t
+ \frac{1}{2}
\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}^{\!\top}
\!\begin{bmatrix}\ell_{xx,t} &amp; \ell_{xu,t}\\ \ell_{ux,t} &amp; \ell_{uu,t}\end{bmatrix}
\!\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix},\\
\Phi(x_N) &amp;\approx \Phi + \Phi_x^\top \delta x_N + \frac{1}{2}\,\delta x_N^\top \Phi_{xx}\,\delta x_N.
\end{aligned}
\tag{4.31}
\]</span></p>
<p><strong>The LQR Subproblem.</strong> With <a href="model-based-plan-optimize.html#eq:ilqr-linearize">(4.30)</a>–<a href="model-based-plan-optimize.html#eq:ilqr-quadratic-cost">(4.31)</a>, the iLQR subproblem at this outer iteration is the <strong>finite-horizon linear–quadratic program in deviations</strong>:
<span class="math display" id="eq:ilqr-lqr-subproblem">\[
\begin{aligned}
\min_{\{\delta x_t,\delta u_t\}} \quad
&amp; \underbrace{\frac{1}{2}\,\delta x_N^\top \Phi_{xx}\,\delta x_N + \Phi_x^\top \delta x_N}_{\text{terminal}}
\;+\; \\
&amp; \sum_{t=0}^{N-1}
\underbrace{\Big(
\frac{1}{2}
\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}^{\!\top}
\!\begin{bmatrix}\ell_{xx,t} &amp; \ell_{xu,t}\\ \ell_{ux,t} &amp; \ell_{uu,t}\end{bmatrix}
\!\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}
+ \ell_{x,t}^\top\delta x_t + \ell_{u,t}^\top\delta u_t
\Big)}_{\text{stage}} \\[1mm]
\text{s.t.}\quad &amp;
\delta x_{t+1} = A_t\,\delta x_t + B_t\,\delta u_t,\qquad t=0,\dots,N-1,\\
&amp; \delta x_0\ \text{given.}
\end{aligned}
\tag{4.32}
\]</span></p>
<blockquote>
<p><strong>Notes.</strong> The iLQR subproblem <a href="model-based-plan-optimize.html#eq:ilqr-lqr-subproblem">(4.32)</a> is slightly different from the previous finite-horizon LQR formulation <a href="model-based-plan-optimize.html#eq:lqr-formulation">(4.3)</a> in the sense that the objective function of <a href="model-based-plan-optimize.html#eq:ilqr-lqr-subproblem">(4.32)</a> also contains <strong>linear terms</strong> in <span class="math inline">\(\delta x_t,\delta u_t\)</span>, and those linear terms come from the Taylor expansion of the original nonlinear objective fuctions. In this case, we will see in the following that the optimal policy is <strong>affine</strong> (feedforward <span class="math inline">\(k_t\)</span> + feedback <span class="math inline">\(K_t\)</span>).</p>
</blockquote>
</div>
<div id="solving-the-subproblem-by-dynamic-programming" class="section level4 hasAnchor" number="4.3.1.2">
<h4><span class="header-section-number">4.3.1.2</span> Solving the Subproblem by Dynamic Programming<a href="model-based-plan-optimize.html#solving-the-subproblem-by-dynamic-programming" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We posit a <strong>quadratic value approximator</strong> at each time:
<span class="math display" id="eq:ilqr-value">\[
V_{t}(\delta x_{t})
\;\approx\;
V_{t}
+ V_{x,t}^\top \delta x_t
+ \frac{1}{2}\,\delta x_t^\top V_{xx,t}\,\delta x_t,
\qquad
V_{x,N}=\Phi_x,\; V_{xx,N}=\Phi_{xx}.
\tag{4.33}
\]</span>
Note that this quadratic value approximator also contains linear and constant terms because the objective function contains linear terms.</p>
<p>Define the local Q-function at stage <span class="math inline">\(t\)</span> by substituting the linear dynamics into the next-step value (this is our familiar Q-value in RL):
<span class="math display">\[
Q_t(\delta x_t,\delta u_t)
\;=\;
\ell_t(x_t,u_t) + V_{t+1} \big(A_t\delta x_t + B_t\delta u_t\big),
\]</span>
which, after collecting terms, yields the iLQR blocks
<span class="math display" id="eq:ilqr-Q">\[
\begin{aligned}
Q_{x,t}&amp;=\ell_{x,t}+A_t^\top V_{x,t+1},\qquad
Q_{u,t}=\ell_{u,t}+B_t^\top V_{x,t+1},\\
Q_{xx,t}&amp;=\ell_{xx,t}+A_t^\top V_{xx,t+1}A_t,\quad
Q_{ux,t}=\ell_{ux,t}+B_t^\top V_{xx,t+1}A_t,\\
Q_{uu,t}&amp;=\ell_{uu,t}+B_t^\top V_{xx,t+1}B_t.
\end{aligned}
\tag{4.34}
\]</span>
The iLQR blocks assemble into a big matrix such that
<span class="math display" id="eq:ilqr-Q-aug">\[
Q_t(\delta x_t,\delta u_t)
\;=\;
\frac{1}{2}\, \begin{bmatrix} 1 \\ \delta x_t \\ \delta u_t \end{bmatrix}^\top
\begin{bmatrix}
2c_t &amp; Q_{x,t}^\top &amp; Q_{u,t}^\top\\[2pt]
Q_{x,t} &amp; Q_{xx,t} &amp; Q_{xu,t}\\[2pt]
Q_{u,t} &amp; Q_{ux,t} &amp; Q_{uu,t}
\end{bmatrix}
\begin{bmatrix} 1 \\ \delta x_t \\ \delta u_t \end{bmatrix}.
\tag{4.35}
\]</span>
where <span class="math inline">\(c_t\)</span> collects all stage/terminal constants e.g., <span class="math inline">\(\ell_t+\!V_{t+1}\)</span>.</p>
<p><strong>Solving the local Q (backward pass).</strong> Set the first-order condition w.r.t. <span class="math inline">\(\delta u\)</span>:
<span class="math display">\[
0 \;=\; \partial_{\delta u} Q_t \;=\; Q_{u,t} + Q_{ux,t}\delta x + Q_{uu,t}\delta u.
\]</span>
Solve for the affine control law
<span class="math display" id="eq:ilqr-kK">\[
\delta u_t^\star \;=\; k_t + K_t\,\delta x,\qquad
k_t = -Q_{uu,t}^{-1} Q_{u,t},\quad
K_t = -Q_{uu,t}^{-1} Q_{ux,t}\!,
\tag{4.36}
\]</span>
which is exactly the LQR solution for the quadratic <span class="math inline">\(Q_t\)</span>.</p>
<p>Substitute <span class="math inline">\(\delta u_t^\star\)</span> back into <a href="model-based-plan-optimize.html#eq:ilqr-Q-aug">(4.35)</a>. The minimized Q becomes a quadratic in <span class="math inline">\(\delta x\)</span> with coefficients given by
<span class="math display" id="eq:ilqr-V">\[
\begin{aligned}
V_{x,t}  &amp;= Q_{x,t} + Q_{xu,t}k_t + K_t^\top Q_{uu,t}k_t + K_t^\top Q_{u,t},\\
V_{xx,t} &amp;= Q_{xx,t} + Q_{xu,t}K_t + K_t^\top Q_{ux,t} + K_t^\top Q_{uu,t}K_t,
\end{aligned}
\tag{4.37}
\]</span>
with terminal
<span class="math display">\[
V_{x,N} = \Phi_x, V_{xx,N} = \Phi_{xx}.
\]</span></p>
<p><strong>Forward Pass (apply the computed policy).</strong> Given <span class="math inline">\(\{k_t,K_t\}\)</span>, produce a candidate trajectory on the <em>true</em> nonlinear dynamics using a line search <span class="math inline">\(\alpha\in(0,1]\)</span>:
<span class="math display" id="eq:ilqr-forward">\[
\begin{aligned}
u_t^{\text{cand}} &amp;= \bar u_t + \alpha k_t + K_t\big(x_t^{\text{cand}}-\bar x_t\big) ,\\
x_{t+1}^{\text{cand}} &amp;= f_t\big(x_t^{\text{cand}},u_t^{\text{cand}}\big),\qquad x_0^{\text{cand}}=\hat x_0.
\end{aligned}
\tag{4.38}
\]</span>
Choose <span class="math inline">\(\alpha\)</span> (e.g., <span class="math inline">\(\{1,\frac{1}{2},\frac14,\dots\}\)</span>) to reduce the <strong>true</strong> cost and respect constraints, then update the nominal:
<span class="math display">\[
(\bar x_t,\bar u_t)\ \leftarrow\ (x_t^{\text{cand}},u_t^{\text{cand}}).
\]</span></p>
<p>The following pseudocode summarizes iLQR.</p>
<div class="highlightbox">
<p><strong>Algorithm: iLQR (Trajectory Generation)</strong></p>
<p><strong>Inputs:</strong> dynamics <span class="math inline">\(f_t\)</span>, initial state <span class="math inline">\(\hat x_0\)</span>, horizon <span class="math inline">\(N\)</span>, stage/terminal costs <span class="math inline">\(\ell_t,\Phi\)</span>, initial guess <span class="math inline">\(\{\bar u_t\}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><strong>Initialize</strong> nominal rollout <span class="math inline">\(\{\bar x_t,\bar u_t\}\)</span> from <span class="math inline">\(\hat x_0\)</span>.</li>
<li><strong>Linearize &amp; quadratize</strong> at <span class="math inline">\(\{(\bar x_t,\bar u_t)\}\)</span>: build <span class="math inline">\(A_t,B_t\)</span> and cost derivatives.</li>
<li><strong>Backward pass (TVLQR):</strong> compute <span class="math inline">\(\{k_t,K_t\}\)</span> using <a href="model-based-plan-optimize.html#eq:ilqr-kK">(4.36)</a> and update <span class="math inline">\(V_{x,t},V_{xx,t}\)</span> via <a href="model-based-plan-optimize.html#eq:ilqr-V">(4.37)</a>.</li>
<li><strong>Forward rollout:</strong> apply <span class="math inline">\(u_t^{\text{new}}=\bar u_t+\alpha k_t+K_t(x_t^{\text{new}}-\bar x_t)\)</span> on the <strong>true</strong> dynamics, pick <span class="math inline">\(\alpha\)</span> by line search.</li>
<li><strong>Convergence check:</strong> stop if the cost decrease and dynamics residuals fall below thresholds; otherwise, set the new nominal and <strong>repeat</strong> from Step 2.</li>
</ol>
</div>
<p>The next example applies iLQR to trajectory generation for rocket landing.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:iLQR-rocket-landing" class="example"><strong>Example 4.3  (iLQR for Rocket Landing) </strong></span>We model a planar (2D) rocket with state and control
<span class="math display">\[
x=\begin{bmatrix}p_x &amp; p_y &amp; v_x &amp; v_y &amp; \theta &amp; \omega\end{bmatrix}^\top,\qquad
u=\begin{bmatrix}T &amp; \tau\end{bmatrix}^\top,
\]</span>
where <span class="math inline">\((p_x,p_y)\)</span> is position, <span class="math inline">\((v_x,v_y)\)</span> is velocity, <span class="math inline">\(\theta\)</span> is attitude (pitch) and <span class="math inline">\(\omega\)</span> its angular rate. The thrust <span class="math inline">\(T\ge 0\)</span> acts <strong>along the body axis</strong> (pointing out of the engine), and <span class="math inline">\(\tau\)</span> is a planar torque about the center of mass. Continuous-time dynamics are
<span class="math display" id="eq:rocket-ct">\[
\begin{aligned}
\dot p_x &amp;= v_x, &amp;
\dot p_y &amp;= v_y, \\
\dot v_x &amp;= \frac{T}{m}\sin\theta, &amp;
\dot v_y &amp;= \frac{T}{m}\cos\theta - g, \\
\dot\theta &amp;= \omega, &amp;
\dot\omega &amp;= \frac{\tau}{I_{zz}}.
\end{aligned}
\tag{4.39}
\]</span>
In simulation we use RK4 with stepsize <span class="math inline">\(h\)</span> to propagate the true dynamics <a href="model-based-plan-optimize.html#eq:rocket-ct">(4.39)</a>. For iLQR’s local subproblems we form the continuous Jacobians <span class="math inline">\((A_c,B_c)=\big(\frac{\partial f}{\partial x},\frac{\partial f}{\partial u}\big)\)</span> at the current nominal and use the standard first-order discrete map
<span class="math display" id="eq:rocket-disc-lin">\[
A_t \;\approx\; I + h\,A_c(\bar x_t,\bar u_t),\qquad
B_t \;\approx\; h\,B_c(\bar x_t,\bar u_t).
\tag{4.40}
\]</span></p>
<p><strong>Soft-Landing Objective.</strong> The goal is a <strong>soft, upright landing</strong> at the origin:
<span class="math display">\[
x_{\mathrm{goal}} = \mathbf{0}
\quad\Longleftrightarrow\quad
p_x=p_y=0,\; v_x=v_y=0,\; \theta=0,\; \omega=0.
\]</span>
We penalize deviations from this goal along the entire trajectory and especially at the terminal state to encourage low touchdown velocities and an upright attitude.</p>
<p><strong>Cost Function.</strong> With horizon <span class="math inline">\(N\)</span> and step <span class="math inline">\(h\)</span>, the discrete objective is
<span class="math display" id="eq:rocket-cost">\[
J \;=\; \frac{1}{2}\,(x_N-x_g)^\top Q_f (x_N-x_g)
\;+\;\sum_{t=0}^{N-1}\Big[
\frac{1}{2}\,(x_t-x_g)^\top Q (x_t-x_g) \;+\; \frac{1}{2}\,u_t^\top R u_t
\Big],
\tag{4.41}
\]</span>
where <span class="math inline">\(x_g=\mathbf{0}\)</span>. In the example:
<span class="math display" id="eq:rocket-weights">\[
\begin{aligned}
Q&amp;=\mathrm{diag}(1,\ 2,\ 0.5,\ 0.5,\ 2,\ 0.5),\\
Q_f&amp;=\mathrm{diag}(200,\ 300,\ 50,\ 50,\ 300,\ 50),\\
R&amp;=\mathrm{diag}(10^{-3},\ 10^{-3}).
\end{aligned}
\tag{4.42}
\]</span>
These weights place strong emphasis on terminal altitude and attitude (<span class="math inline">\(p_y,\theta\)</span>), moderate emphasis on velocities and lateral position, and a light regularization on the controls.</p>
<p><strong>Experiment Setup.</strong></p>
<ul>
<li><p><strong>Physical parameters.</strong> Gravity <span class="math inline">\(g=9.81\,\mathrm{m/s^2}\)</span>, mass <span class="math inline">\(m=1.0\,\mathrm{kg}\)</span>, planar inertia <span class="math inline">\(I_{zz}=0.2\,\mathrm{kg\,m^2}\)</span>.</p></li>
<li><p><strong>Discretization.</strong> Stepsize <span class="math inline">\(h=0.05\,\mathrm{s}\)</span>; horizon <span class="math inline">\(T=6.0\,\mathrm{s}\)</span>; number of steps <span class="math inline">\(N=T/h=120\)</span>.</p></li>
<li><p><strong>Initial state.</strong>
<span class="math display">\[
x_0=\big[\,5.0,\ 10.0,\ -0.5,\ -1.0,\ \mathrm{deg2rad}(10),\ 0\,\big]^\top,
\]</span>
i.e., 10 m altitude, lateral offset, small descent and slight pitch.</p></li>
<li><p><strong>Initial nominal controls.</strong> Constant hover thrust and zero torque:
<span class="math display">\[
\bar u_t = [\,m g,\ 0\,]^\top,\qquad t=0,\dots,N-1.
\]</span></p></li>
<li><p><strong>iLQR procedure.</strong> Each outer iteration:</p>
<ol style="list-style-type: decimal">
<li>Linearize dynamics and quadratize the cost along the current nominal (<a href="model-based-plan-optimize.html#eq:rocket-disc-lin">(4.40)</a>, <a href="model-based-plan-optimize.html#eq:rocket-cost">(4.41)</a>);<br />
</li>
<li>Solve the <strong>time-varying LQR</strong> subproblem to get affine updates <span class="math inline">\(\delta u_t = k_t + K_t\,\delta x_t\)</span>;<br />
</li>
<li><strong>Forward rollout</strong> on the nonlinear RK4 dynamics with
<span class="math display">\[
u_t^{\text{new}} = \bar u_t + \alpha\,k_t + K_t\big(x_t^{\text{new}}-\bar x_t\big),
\]</span>
using a backtracking line search over <span class="math inline">\(\alpha\in\{1,\frac{1}{2},\frac14,\dots\}\)</span> (note: <span class="math inline">\(\alpha\)</span> scales only the <strong>feedforward</strong> <span class="math inline">\(k_t\)</span>, not the feedback <span class="math inline">\(K_t\)</span>);<br />
</li>
<li>Update the nominal and repeat until cost reduction is small.</li>
</ol></li>
</ul>
<p>Fig. <a href="model-based-plan-optimize.html#fig:ilqr-rocket">4.5</a> plots the <strong>initial</strong>, <strong>intermediate</strong>, and <strong>final</strong> trajectories, and render the rocket as oriented rectangles (boxes) using <span class="math inline">\((p_x,p_y,\theta)\)</span> to visualize attitude along the descent. We can see iLQR successfully generated a soft landing trajectory.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/ilqr_rocket_landing.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ilqr-rocket"></span>
<img src="images/Model-based-optimization/ilqr_rocket_landing.png" alt="iLQR Trajectory Generation for Rocket Landing." width="100%" />
<p class="caption">
Figure 4.5: iLQR Trajectory Generation for Rocket Landing.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="ddp" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Differential Dynamic Programming<a href="model-based-plan-optimize.html#ddp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to iLQR, skipped for now.</p>
</div>
<div id="qp" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Quadratic Programming<a href="model-based-plan-optimize.html#qp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Trajectory optimization (TO) with nonlinear dynamics and objectives is well served by iLQR/DDP: at each outer iteration, they <em>linearize</em> the dynamics and <em>quadratize</em> the objective, then solve a <em>time-varying LQR</em> subproblem. This works remarkably well for <em>unconstrained</em> or softly constrained problems.</p>
<p>However, many TO tasks are <strong>constrained</strong>—e.g., obstacle avoidance, actuator limits, keep-out zones, terminal envelopes. Hard constraints are awkward for iLQR/DDP (they typically enter via penalties or saturations), and feasibility can be fragile. For such cases, it is often more natural to frame TO as a <strong>nonlinear program (NLP)</strong>—an optimization problem with general nonlinear objective and constraints. This brings the full machinery of modern numerical optimization (see, e.g., <span class="citation">(<a href="#ref-nocedal99book-numerical">Nocedal and Wright 1999</a>)</span>).</p>
<p>As a first step, we study the <strong>convex</strong> special case where the linearization already yields linear dynamics, affine constraints, and a quadratic objective (these are typically known as “constrained LQR” problems). This leads to <strong>Quadratic Programming (QP)</strong>, a cornerstone problem class with mature, efficient solvers. In the next section, we will lift these ideas to <strong>Sequential Quadratic Programming (SQP)</strong> to handle <em>general</em> constrained TO.</p>
<div id="to-qp" class="section level4 hasAnchor" number="4.3.3.1">
<h4><span class="header-section-number">4.3.3.1</span> From Trajectory Optimization to Quadratic Programming<a href="model-based-plan-optimize.html#to-qp" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Start from the constrained TO template in <a href="model-based-plan-optimize.html#eq:to-general">(4.29)</a>. Suppose:</p>
<ul>
<li><strong>Linear (time-varying) dynamics</strong> (from linearization or an intrinsically linear model):
<span class="math display">\[
x_{t+1} = A_t x_t + B_t u_t + a_t,\qquad t=0,\dots,N-1,
\]</span>
with given <span class="math inline">\(x_0=\hat x_0\)</span>.</li>
<li><strong>Quadratic objective</strong> (from quadratization or an Linear-Quadratic tracking design):
<span class="math display">\[
\Phi(x_N) + \sum_{t=0}^{N-1} \ell_t(x_t,u_t)
\;\equiv\;
\frac{1}{2}\,x_N^\top Q_N x_N + q_N^\top x_N
+ \sum_{t=0}^{N-1}\Big(\frac{1}{2}\,[x_t;u_t]^\top H_t [x_t;u_t] + h_t^\top [x_t;u_t]\Big),
\]</span>
with <span class="math inline">\(Q_N\succeq 0\)</span>, <span class="math inline">\(H_t=\begin{bmatrix}Q_t &amp; S_t\\ S_t^\top &amp; R_t\end{bmatrix} \succeq 0\)</span> and <span class="math inline">\(R_t\succ 0\)</span> for convexity.</li>
<li><strong>Affine path/terminal constraints</strong> (from linearized safety/goal constraints):
<span class="math display">\[
G_t^x x_t + G_t^u u_t \le g_t,\qquad
F_x x_N \leq f.
\]</span></li>
</ul>
<p>Define the stacked decision vector
<span class="math display">\[
z := \big[x_0^\top,\,u_0^\top,\,x_1^\top,\,u_1^\top,\,\dots,\,x_{N-1}^\top,\,u_{N-1}^\top,\,x_N^\top\big]^\top.
\]</span>
Then the horizon-wide problem is a <strong>QP</strong>:
<span class="math display" id="eq:qp-stacked">\[
\begin{aligned}
\min_{z}\quad &amp; \frac{1}{2}\, z^\top H\, z \;+\; h^\top z \\[2mm]
\text{s.t.}\quad
&amp; A_{\text{dyn}}\, z = b_{\text{dyn}} \quad \text{(stacked dynamics and } x_0=\hat x_0\text{)},\\
&amp; G\, z \le g \qquad\quad\ \ \text{(stacked affine path/terminal constraints)}\\
\end{aligned}
\tag{4.43}
\]</span>
Here <span class="math inline">\(H\succeq 0\)</span> is block-sparse (banded) due to the stagewise structure; the constraint matrices are also sparse/banded because each dynamic constraint couples only <span class="math inline">\((x_t,u_t,x_{t+1})\)</span>.</p>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-23" class="exercise"><strong>Exercise 4.1  </strong></span>Can you write down the blocks in <span class="math inline">\(H\)</span>, <span class="math inline">\(A\)</span>, and <span class="math inline">\(G\)</span>, as functions of <span class="math inline">\(H_t,A_t,B_t,G^x_t, G^u_t\)</span>? Then, observe the block-sparsity patterns.</p>
</div>
</div>
<p><strong>Convexity.</strong> If all stage Hessians <span class="math inline">\(H_t\succeq 0\)</span> and <span class="math inline">\(Q_N\succeq 0\)</span>, <a href="model-based-plan-optimize.html#eq:qp-stacked">(4.43)</a> is a <strong>convex QP</strong> with a <strong>unique</strong> minimizer when <span class="math inline">\(H\)</span> is positive definite on the feasible subspace (e.g., via <span class="math inline">\(R_t\succ 0\)</span>).</p>
</div>
<div id="solving-the-quadratic-program" class="section level4 hasAnchor" number="4.3.3.2">
<h4><span class="header-section-number">4.3.3.2</span> Solving the Quadratic Program<a href="model-based-plan-optimize.html#solving-the-quadratic-program" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We now discuss how to solve a general convex quadratic program (QP) containing both equality and inequality constraints:
<span class="math display" id="eq:qp-general">\[
\begin{aligned}
\min_{z \in \mathbb{R}^n}\quad &amp; \frac{1}{2}\, z^\top H\, z \;+\; h^\top z \\
\text{s.t.}\quad
&amp; A  z = b  \\
&amp; G z \le g
\end{aligned}
\tag{4.44}
\]</span>
where <span class="math inline">\(z \in \mathbb{R}^n\)</span> is the decision variable, <span class="math inline">\(H \in \mathbb{S}^{n}, h \in \mathbb{R}^n, A \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m, G \in \mathbb{R}^{p \times n}, g \in \mathbb{R}^p\)</span> are given problem data (e.g., generated as in Section <a href="model-based-plan-optimize.html#to-qp">4.3.3.1</a>). We assume <span class="math inline">\(H \succeq 0\)</span> is positive semidefinite.</p>
<p>There are multiple popular algorithms to solve <a href="model-based-plan-optimize.html#eq:qp-general">(4.44)</a>, e.g., the active set algorithm, the interior point algorithm, and the alternating direction method of multipliers. Here we only present the <em>primal–dual interior point method</em> (PD-IPM) due to its generality and robustness. Before presenting the PD-IPM algorithm, it is beneficial to review Newton’s method for solving a system of equations.</p>
<div class="highlightbox">
<p style="text-align: center;">
<b>Newton’s Method</b>
</p>
<p>Given a function <span class="math inline">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span> that is continuously differentiable, Newton’s method is designed to find a root of
<span class="math display">\[
f(x) = 0.
\]</span>
Given an initial iterate <span class="math inline">\(x^{(0)}\)</span>, Newton’s method works as follows
<span class="math display">\[
x^{(k+1)} = x^{(k)} - \frac{f(x^{(k)})}{f&#39;(x^{(k)})},
\]</span>
where <span class="math inline">\(f&#39;(x^{(k)})\)</span> denotes the derivative of <span class="math inline">\(f\)</span> at the current iterate <span class="math inline">\(x^{(k)}\)</span>. This simple algorithm is indeed the most important foundation of modern numerical optimization. Under mild conditions, Newton’s method has at least quadratic convergence rate, that is to say, if <span class="math inline">\(|x^{(k)} - x^\star| = \epsilon\)</span>, then <span class="math inline">\(|x^{(k+1)} - x^\star| = O(\epsilon^2)\)</span> (it should be noted that there exist pathological cases where even linear convergence is not guaranteed, e.g., when <span class="math inline">\(f&#39;(x^\star) = 0\)</span>).</p>
<p>Newton’s method can be generalized to find a point at which multiple functions vanish simultaneously. Given a function <span class="math inline">\(F: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}\)</span> that is continuously differentiable, and an initial iterate <span class="math inline">\(x^{(0)}\)</span>, Newton’s method reads
<span class="math display" id="eq:newton-method-vector">\[\begin{equation}
x^{(k+1)} = x^{(k)} - J_F(x^{(k)})^{-1} F(x^{(k)}),
\tag{4.45}
\end{equation}\]</span>
where <span class="math inline">\(J_F(\cdot)\)</span> denotes the Jacobian of <span class="math inline">\(F\)</span>. Iteration <a href="model-based-plan-optimize.html#eq:newton-method-vector">(4.45)</a> is equivalent to
<span class="math display" id="eq:newton-method-vector-1">\[\begin{equation}
\begin{split}
J_F(x^{(k)}) \Delta x^{(k)} &amp; = - F(x^{(k)}) \\
x^{(k+1)} &amp; = x^{(k)} + \Delta x^{(k)}
\end{split}
\tag{4.46}
\end{equation}\]</span>
i.e., one first solves a linear system of equations to find an update direction <span class="math inline">\(\Delta x^{(k)}\)</span>, and then take a step along the direction.</p>
<p>As we will see, PD-IPMs for solving convex QPs can be seen as applying Newton’s method to the perturbed KKT system of optimality conditions.</p>
</div>
<p><strong>Slacks, Lagrangian, and KKT Optimality.</strong> Introduce <span class="math inline">\(s\in\mathbb{R}^p\)</span> so that
<span class="math display" id="eq:ipm-slack">\[
Gz + s = g,\qquad s \ge 0.
\tag{4.47}
\]</span>
Let <span class="math inline">\(y\in\mathbb{R}^m\)</span> be the Lagrangian multipliers for <span class="math inline">\(Az=b\)</span>, <span class="math inline">\(\nu\in\mathbb{R}^p\)</span> for the equality <span class="math inline">\(Gz+s=g\)</span>, and <span class="math inline">\(w\in\mathbb{R}^p\)</span> (with <span class="math inline">\(w\ge 0\)</span>) for the inequality <span class="math inline">\(s\ge 0\)</span>. The Lagrangian for the QP <a href="model-based-plan-optimize.html#eq:qp-general">(4.44)</a> is
<span class="math display" id="eq:qp-Lagrangian">\[
\mathcal L(z,s,y,\nu,w)
= \frac{1}{2} z^\top H z + h^\top z
+ y^\top(Az-b) + \nu^\top(Gz+s-g) - w^\top s.
\tag{4.48}
\]</span>
From the Lagrangian, we can derive the KKT optimality conditions, i.e., under technical conditions (such as constraint qualification), a point <span class="math inline">\((z,s)\)</span> is a local minimizer of the QP if and only if there exists dual variables <span class="math inline">\((\nu,w)\)</span> satisfying:
<span class="math display" id="eq:ipm-kkt">\[
\begin{aligned}
\text{(stationarity)}&amp;:\quad
\nabla_z\mathcal L = H z + h + A^\top y + G^\top \nu = 0,\\
&amp;\quad \nabla_s\mathcal L = \nu - w = 0 \ \Longrightarrow\ \nu = w,\\[2pt]
\text{(primal feasibility)}&amp;:\quad
A z - b = 0,\qquad G z + s - g = 0, \qquad s \ge 0, \\
\text{(dual feasibility)}&amp;:\quad
w \ge 0,\\
\text{(complementarity)}&amp;:\quad
s_i w_i = 0,\quad i=1,\dots,p.
\end{aligned}
\tag{4.49}
\]</span>
Using <span class="math inline">\(\nu=w\)</span> we eliminate <span class="math inline">\(\nu\)</span> and keep variables <span class="math inline">\((z,s,y,w)\)</span>. Since the QP is convex, we know that any local minimizer is a global minimizer. Hence, if we can solve the KKT system <a href="model-based-plan-optimize.html#eq:ipm-kkt">(4.49)</a>, we can find an optimal solution of the QP.</p>
<blockquote>
<p>If you are not familiar with the Lagrangian and the KKT optimality conditions, make sure to review Appendix <a href="appconvex.html#lagrangian-dual">A.1.3</a> and <a href="appconvex.html#appconvex-theory-kkt">A.1.4</a>.</p>
</blockquote>
<p><strong>Central Path and Residuals.</strong> Replace complementarity by the <em>perturbed</em> condition
<span class="math display" id="eq:ipm-central">\[
S W \mathbf{1} = \sigma \mu\,\mathbf{1},
\qquad
\mu := \frac{1}{p}\, s^\top w, \qquad \sigma \in (0,1)
\tag{4.50}
\]</span>
with <span class="math inline">\(S=\mathrm{diag}(s)\)</span>, <span class="math inline">\(W=\mathrm{diag}(w)\)</span>.
At a current iterate <span class="math inline">\((z,s,y,w)\)</span> define residuals
<span class="math display" id="eq:ipm-residuals">\[
\begin{aligned}
r_{\mathrm{dual}} &amp;= H z + h + A^\top y + G^\top w,\\
r_{\mathrm{pe}}   &amp;= A z - b,\\
r_{\mathrm{pi}}   &amp;= G z + s - g,\\
r_{\mathrm{cent}} &amp;= S W \mathbf{1} - \sigma \mu\,\mathbf{1}.
\end{aligned}
\tag{4.51}
\]</span></p>
<p><strong>Newton System (primal–dual step).</strong> Now that we have arrived at the perturbed KKT system of equations in <a href="model-based-plan-optimize.html#eq:ipm-residuals">(4.51)</a> where we aim to drive all the residuals to zero. This is a system of <span class="math inline">\((n+m+p+p)\)</span> nonlinear equations in <span class="math inline">\((n + m + p + p)\)</span> variables. Therefore, we can apply Newton’s method to solve the system of equations.</p>
<blockquote>
<p>Note that we actually want to solve the original KKT system with <span class="math inline">\(\sigma \mu = 0\)</span>. However, this system is ill-conditioned and directly applying Newton’s method would lead to instability. Therefore, we solve the perturbed KKT system with <span class="math inline">\(\sigma \mu &gt; 0\)</span> and at each iteration we move closer to the original KKT system with <span class="math inline">\(\sigma \in (0,1)\)</span> so that in the limit we will converge (arbitrarily close) to a solution of the original KKT system.</p>
</blockquote>
<p>Solve for the Newton direction <span class="math inline">\((\Delta z,\Delta s,\Delta y,\Delta w)\)</span>:
<span class="math display" id="eq:ipm-newton">\[
\begin{aligned}
H\,\Delta z + A^\top \Delta y + G^\top \Delta w &amp;= -\,r_{\mathrm{dual}},\\
A\,\Delta z &amp;= -\,r_{\mathrm{pe}},\\
G\,\Delta z + \Delta s &amp;= -\,r_{\mathrm{pi}},\\
W\,\Delta s + S\,\Delta w &amp;= -\,r_{\mathrm{cent}}.
\end{aligned}
\tag{4.52}
\]</span>
Eliminate <span class="math inline">\(\Delta s = -r_{\mathrm{pi}} - G\Delta z\)</span> in the last equation to get
<span class="math display">\[
S\,\Delta w
= -\,r_{\mathrm{cent}} + W\,r_{\mathrm{pi}} + W\,G\,\Delta z
\quad\Longrightarrow\quad
\Delta w = S^{-1}\!\left(-r_{\mathrm{cent}} + W r_{\mathrm{pi}} + W G \Delta z\right).
\]</span>
Substitute into the first equation to obtain the <strong>reduced symmetric system</strong> in <span class="math inline">\((\Delta z,\Delta y)\)</span>:
<span class="math display" id="eq:ipm-reduced">\[
\begin{bmatrix}
H + G^\top D G &amp; A^\top\\
A &amp; \ \ 0
\end{bmatrix}
\begin{bmatrix}\Delta z\\ \Delta y\end{bmatrix}
=
-
\begin{bmatrix}
r_{\mathrm{dual}} - G^\top S^{-1} \big(r_{\mathrm{cent}} + W r_{\mathrm{pi}}\big)\\[2pt]
r_{\mathrm{pe}}
\end{bmatrix}
\tag{4.53}
\]</span>
with <span class="math inline">\(D := S^{-1} W\)</span>. Then recover
<span class="math display">\[
\Delta w = S^{-1}\!\left(-r_{\mathrm{cent}} + W r_{\mathrm{pi}} + W G \Delta z\right),\qquad
\Delta s = -r_{\mathrm{pi}} - G \Delta z.
\]</span></p>
<p><strong>Step Lengths.</strong> Choose step sizes to preserve positivity of <span class="math inline">\(s,w\)</span>:
<span class="math display" id="eq:ipm-steps">\[
\alpha_{\mathrm{pri}} = \min\!\Big(1,\ \eta \min_{\Delta s_i&lt;0}\frac{-s_i}{\Delta s_i}\Big),\qquad
\alpha_{\mathrm{du}}  = \min\!\Big(1,\ \eta \min_{\Delta w_i&lt;0}\frac{-w_i}{\Delta w_i}\Big),
\tag{4.54}
\]</span>
with <span class="math inline">\(\eta\in(0,1)\)</span> (e.g., <span class="math inline">\(0.99\)</span>). Update both primal and dual variables
<span class="math display">\[
\begin{aligned}
z &amp;\leftarrow z + \alpha_{\mathrm{pri}}\,\Delta z,\qquad
s \leftarrow s + \alpha_{\mathrm{pri}}\,\Delta s,\\
y &amp;\leftarrow y + \alpha_{\mathrm{du}}\,\Delta y,\qquad
w \leftarrow w + \alpha_{\mathrm{du}}\,\Delta w.
\end{aligned}
\]</span></p>
<!-- #### 5) Mehrotra predictor–corrector (recommended)

A robust adaptive centering strategy:

1. **Predictor (affine-scaling) step:** solve \@ref(eq:ipm-reduced) with \(\sigma=0\) to get \((\Delta z^{\mathrm{aff}},\Delta s^{\mathrm{aff}},\Delta y^{\mathrm{aff}},\Delta w^{\mathrm{aff}})\). Compute affine step lengths \(\alpha^{\mathrm{aff}}_{\mathrm{pri}},\alpha^{\mathrm{aff}}_{\mathrm{du}}\) and
   \[
   \mu_{\mathrm{aff}} = \frac{\big(s + \alpha^{\mathrm{aff}}_{\mathrm{pri}}\Delta s^{\mathrm{aff}}\big)^\top
   \big(w + \alpha^{\mathrm{aff}}_{\mathrm{du}}\Delta w^{\mathrm{aff}}\big)}{p}.
   \]
2. **Centering parameter:** set \(\sigma = (\mu_{\mathrm{aff}}/\mu)^3\) (power in \([2,3]\) is common).
3. **Corrector step:** form the **corrected** centering residual
   \[
   r_{\mathrm{cent}}^{\mathrm{corr}} = S W \mathbf{1} - \sigma \mu \mathbf{1}
   - \Delta S^{\mathrm{aff}}\Delta W^{\mathrm{aff}}\mathbf{1},
   \]
   and resolve \@ref(eq:ipm-reduced) using \(r_{\mathrm{cent}}^{\mathrm{corr}}\); then take fraction-to-boundary steps.

This typically improves robustness and practical convergence. -->
<p><strong>Initialization and Stopping.</strong></p>
<ul>
<li><strong>Initialization.</strong> Find any <span class="math inline">\(z\)</span> satisfying <span class="math inline">\(Az=b\)</span> (e.g., least-squares projection). Set
<span class="math display">\[
s := \max\{\mathbf{1},\, g - Gz\},\quad w := \mathbf{1},
\]</span>
to ensure strict positivity (<span class="math inline">\(s&gt;0,w&gt;0\)</span>); choose <span class="math inline">\(y:=0\)</span>.</li>
<li><strong>Stopping.</strong> Terminate when
<span class="math display">\[
\|r_{\mathrm{dual}}\|_\infty \le \varepsilon,\quad
\|r_{\mathrm{pe}}\|_\infty \le \varepsilon,\quad
\|r_{\mathrm{pi}}\|_\infty \le \varepsilon,\quad
\mu \le \varepsilon,
\]</span>
for a small tolerance <span class="math inline">\(\varepsilon\)</span> (e.g., <span class="math inline">\(10^{-6}\)</span>).</li>
</ul>
<p>The following pseudocode implements the PD-IPM algorithm for solving convex QP.</p>
<div class="highlightbox">
<p><strong>Algorithm: Primal–Dual Interior-Point for Convex QP</strong></p>
<p><strong>Input:</strong> <span class="math inline">\(H\succeq 0, h, A,b, G,g\)</span>; tolerance <span class="math inline">\(\varepsilon\)</span>; <span class="math inline">\(\eta=0.99\)</span>; <span class="math inline">\(\sigma \in (0,1)\)</span></p>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(z\)</span> with <span class="math inline">\(Az=b\)</span>; set <span class="math inline">\(s&gt;0, w&gt;0\)</span> (e.g., <span class="math inline">\(s=\max\{1,g-Gz\}\)</span>, <span class="math inline">\(w=\mathbf{1}\)</span>); set <span class="math inline">\(y=0\)</span>.</li>
<li>Repeat until convergence:
<ol style="list-style-type: decimal">
<li>Compute residuals <span class="math inline">\(r_{\mathrm{dual}}, r_{\mathrm{pe}}, r_{\mathrm{pi}}\)</span>, <span class="math inline">\(\mu=(s^\top w)/p\)</span>.</li>
<li>Solve the reduced system <a href="model-based-plan-optimize.html#eq:ipm-reduced">(4.53)</a> to obtain Newton direction.</li>
<li>Compute <span class="math inline">\(\alpha_{\mathrm{pri}},\alpha_{\mathrm{du}}\)</span> by <a href="model-based-plan-optimize.html#eq:ipm-steps">(4.54)</a>.</li>
<li>Update <span class="math inline">\((z,s,y,w)\)</span>.</li>
<li>Check stopping criteria; if satisfied, <strong>return</strong> <span class="math inline">\(z^\star\)</span>.</li>
</ol></li>
</ol>
</div>
<p><strong>Remarks.</strong></p>
<ul>
<li><p>For QPs obtained from trajectory optimization problems, the matrices are typically sparse (e.g., time-banded sparsity). This sparsity can be leveraged when forming and solving the Newton direction.</p></li>
<li><p>In practice, <a href="https://en.wikipedia.org/wiki/Mehrotra_predictor%E2%80%93corrector_method">Mehrotra’s predictor–corrector method</a> is used to improve the robustness and convergence of PD-IPM.</p></li>
</ul>
<p><strong>Software.</strong> It is important to understand the high-level algorithmic idea for solving a convex QP. However, in practice, there are many mature QP solvers and it takes just a few lines of code to call your favorite QP solver.</p>
<p>The following code snippet shows how to define a QP in <a href="https://www.cvxpy.org"><code>cvxpy</code></a> and then solve it using MOSEK (which implements PD-IPM).</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="model-based-plan-optimize.html#cb7-1" tabindex="-1"></a><span class="co"># Minimal dense QP with CVXPY</span></span>
<span id="cb7-2"><a href="model-based-plan-optimize.html#cb7-2" tabindex="-1"></a><span class="co">#   minimize    (1/2) x^T P x + q^T x</span></span>
<span id="cb7-3"><a href="model-based-plan-optimize.html#cb7-3" tabindex="-1"></a><span class="co">#   subject to  Ax &lt;= b, 1^T x = 1</span></span>
<span id="cb7-4"><a href="model-based-plan-optimize.html#cb7-4" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb7-5"><a href="model-based-plan-optimize.html#cb7-5" tabindex="-1"></a><span class="co"># pip install cvxpy</span></span>
<span id="cb7-6"><a href="model-based-plan-optimize.html#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="model-based-plan-optimize.html#cb7-7" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-8"><a href="model-based-plan-optimize.html#cb7-8" tabindex="-1"></a><span class="im">import</span> cvxpy <span class="im">as</span> cp</span>
<span id="cb7-9"><a href="model-based-plan-optimize.html#cb7-9" tabindex="-1"></a></span>
<span id="cb7-10"><a href="model-based-plan-optimize.html#cb7-10" tabindex="-1"></a><span class="co"># ----- QP data (dense) -----</span></span>
<span id="cb7-11"><a href="model-based-plan-optimize.html#cb7-11" tabindex="-1"></a>P <span class="op">=</span> np.array([</span>
<span id="cb7-12"><a href="model-based-plan-optimize.html#cb7-12" tabindex="-1"></a>    [<span class="fl">4.0</span>, <span class="fl">1.0</span>, <span class="fl">0.5</span>],</span>
<span id="cb7-13"><a href="model-based-plan-optimize.html#cb7-13" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">0.3</span>],</span>
<span id="cb7-14"><a href="model-based-plan-optimize.html#cb7-14" tabindex="-1"></a>    [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">1.5</span>]</span>
<span id="cb7-15"><a href="model-based-plan-optimize.html#cb7-15" tabindex="-1"></a>], dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb7-16"><a href="model-based-plan-optimize.html#cb7-16" tabindex="-1"></a><span class="co"># Make sure P is symmetric positive definite</span></span>
<span id="cb7-17"><a href="model-based-plan-optimize.html#cb7-17" tabindex="-1"></a>P <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (P <span class="op">+</span> P.T) <span class="op">+</span> <span class="fl">1e-9</span> <span class="op">*</span> np.eye(<span class="dv">3</span>)</span>
<span id="cb7-18"><a href="model-based-plan-optimize.html#cb7-18" tabindex="-1"></a></span>
<span id="cb7-19"><a href="model-based-plan-optimize.html#cb7-19" tabindex="-1"></a>q <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="op">-</span><span class="fl">3.0</span>])</span>
<span id="cb7-20"><a href="model-based-plan-optimize.html#cb7-20" tabindex="-1"></a></span>
<span id="cb7-21"><a href="model-based-plan-optimize.html#cb7-21" tabindex="-1"></a>A <span class="op">=</span> np.array([</span>
<span id="cb7-22"><a href="model-based-plan-optimize.html#cb7-22" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="fl">1.0</span>],   <span class="co"># linear inequality:  x1 - 2 x2 + x3 ≤ 2</span></span>
<span id="cb7-23"><a href="model-based-plan-optimize.html#cb7-23" tabindex="-1"></a>    [<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],   <span class="co"># x1 ≥ 0  -&gt;  -x1 ≤ 0</span></span>
<span id="cb7-24"><a href="model-based-plan-optimize.html#cb7-24" tabindex="-1"></a>    [<span class="fl">0.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.0</span>],   <span class="co"># x2 ≥ 0  -&gt;  -x2 ≤ 0</span></span>
<span id="cb7-25"><a href="model-based-plan-optimize.html#cb7-25" tabindex="-1"></a>    [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="op">-</span><span class="fl">1.0</span>],   <span class="co"># x3 ≥ 0  -&gt;  -x3 ≤ 0</span></span>
<span id="cb7-26"><a href="model-based-plan-optimize.html#cb7-26" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],    <span class="co"># x1 ≤ 1.5</span></span>
<span id="cb7-27"><a href="model-based-plan-optimize.html#cb7-27" tabindex="-1"></a>    [<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>],    <span class="co"># x2 ≤ 1.5</span></span>
<span id="cb7-28"><a href="model-based-plan-optimize.html#cb7-28" tabindex="-1"></a>    [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">1.0</span>],    <span class="co"># x3 ≤ 1.5</span></span>
<span id="cb7-29"><a href="model-based-plan-optimize.html#cb7-29" tabindex="-1"></a>])</span>
<span id="cb7-30"><a href="model-based-plan-optimize.html#cb7-30" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="fl">2.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">1.5</span>, <span class="fl">1.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb7-31"><a href="model-based-plan-optimize.html#cb7-31" tabindex="-1"></a></span>
<span id="cb7-32"><a href="model-based-plan-optimize.html#cb7-32" tabindex="-1"></a><span class="co"># Equality: sum(x) = 1</span></span>
<span id="cb7-33"><a href="model-based-plan-optimize.html#cb7-33" tabindex="-1"></a>e <span class="op">=</span> np.ones((<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb7-34"><a href="model-based-plan-optimize.html#cb7-34" tabindex="-1"></a>d <span class="op">=</span> np.array([<span class="fl">1.0</span>])</span>
<span id="cb7-35"><a href="model-based-plan-optimize.html#cb7-35" tabindex="-1"></a></span>
<span id="cb7-36"><a href="model-based-plan-optimize.html#cb7-36" tabindex="-1"></a><span class="co"># ----- CVXPY problem -----</span></span>
<span id="cb7-37"><a href="model-based-plan-optimize.html#cb7-37" tabindex="-1"></a>x <span class="op">=</span> cp.Variable(<span class="dv">3</span>)</span>
<span id="cb7-38"><a href="model-based-plan-optimize.html#cb7-38" tabindex="-1"></a></span>
<span id="cb7-39"><a href="model-based-plan-optimize.html#cb7-39" tabindex="-1"></a>objective <span class="op">=</span> cp.Minimize(<span class="fl">0.5</span> <span class="op">*</span> cp.quad_form(x, P) <span class="op">+</span> q <span class="op">@</span> x)</span>
<span id="cb7-40"><a href="model-based-plan-optimize.html#cb7-40" tabindex="-1"></a>constraints <span class="op">=</span> [</span>
<span id="cb7-41"><a href="model-based-plan-optimize.html#cb7-41" tabindex="-1"></a>    A <span class="op">@</span> x <span class="op">&lt;=</span> b,</span>
<span id="cb7-42"><a href="model-based-plan-optimize.html#cb7-42" tabindex="-1"></a>    e <span class="op">@</span> x <span class="op">==</span> d</span>
<span id="cb7-43"><a href="model-based-plan-optimize.html#cb7-43" tabindex="-1"></a>]</span>
<span id="cb7-44"><a href="model-based-plan-optimize.html#cb7-44" tabindex="-1"></a></span>
<span id="cb7-45"><a href="model-based-plan-optimize.html#cb7-45" tabindex="-1"></a>prob <span class="op">=</span> cp.Problem(objective, constraints)</span>
<span id="cb7-46"><a href="model-based-plan-optimize.html#cb7-46" tabindex="-1"></a><span class="co"># You can choose a solver; OSQP is common for QPs. ECOS/SCS also work.</span></span>
<span id="cb7-47"><a href="model-based-plan-optimize.html#cb7-47" tabindex="-1"></a>prob.solve(solver<span class="op">=</span>cp.MOSEK, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-48"><a href="model-based-plan-optimize.html#cb7-48" tabindex="-1"></a></span>
<span id="cb7-49"><a href="model-based-plan-optimize.html#cb7-49" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Status:&quot;</span>, prob.status)</span>
<span id="cb7-50"><a href="model-based-plan-optimize.html#cb7-50" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Optimal value:&quot;</span>, prob.value)</span>
<span id="cb7-51"><a href="model-based-plan-optimize.html#cb7-51" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;x* =&quot;</span>, x.value.<span class="bu">round</span>(<span class="dv">6</span>))</span>
<span id="cb7-52"><a href="model-based-plan-optimize.html#cb7-52" tabindex="-1"></a></span>
<span id="cb7-53"><a href="model-based-plan-optimize.html#cb7-53" tabindex="-1"></a><span class="co"># (Optional) check constraints</span></span>
<span id="cb7-54"><a href="model-based-plan-optimize.html#cb7-54" tabindex="-1"></a>ineq_res <span class="op">=</span> (A <span class="op">@</span> x.value <span class="op">-</span> b)</span>
<span id="cb7-55"><a href="model-based-plan-optimize.html#cb7-55" tabindex="-1"></a>eq_res <span class="op">=</span> (e <span class="op">@</span> x.value <span class="op">-</span> d)</span>
<span id="cb7-56"><a href="model-based-plan-optimize.html#cb7-56" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Max inequality residual (&lt;=0):&quot;</span>, np.<span class="bu">max</span>(ineq_res))</span>
<span id="cb7-57"><a href="model-based-plan-optimize.html#cb7-57" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Equality residual (≈0):&quot;</span>, eq_res.item())</span></code></pre></div>
<p>Running the code produces the following output. You should now be able to interpret the iterations of MOSEK.</p>
<pre><code>-------------------------------------------------------------------------------
                                Numerical solver                               
-------------------------------------------------------------------------------
(CVXPY) Nov 04 12:06:51 PM: Invoking solver MOSEK  to obtain a solution.


(CVXPY) Nov 04 12:06:52 PM: Problem
(CVXPY) Nov 04 12:06:52 PM:   Name                   :                 
(CVXPY) Nov 04 12:06:52 PM:   Objective sense        : maximize        
(CVXPY) Nov 04 12:06:52 PM:   Type                   : CONIC (conic optimization problem)
(CVXPY) Nov 04 12:06:52 PM:   Constraints            : 4               
(CVXPY) Nov 04 12:06:52 PM:   Affine conic cons.     : 0               
(CVXPY) Nov 04 12:06:52 PM:   Disjunctive cons.      : 0               
(CVXPY) Nov 04 12:06:52 PM:   Cones                  : 1               
(CVXPY) Nov 04 12:06:52 PM:   Scalar variables       : 13              
(CVXPY) Nov 04 12:06:52 PM:   Matrix variables       : 0               
(CVXPY) Nov 04 12:06:52 PM:   Integer variables      : 0               
(CVXPY) Nov 04 12:06:52 PM: 
(CVXPY) Nov 04 12:06:52 PM: Optimizer started.
(CVXPY) Nov 04 12:06:52 PM: Presolve started.
(CVXPY) Nov 04 12:06:52 PM: Linear dependency checker started.
(CVXPY) Nov 04 12:06:52 PM: Linear dependency checker terminated.
(CVXPY) Nov 04 12:06:52 PM: Eliminator started.
(CVXPY) Nov 04 12:06:52 PM: Freed constraints in eliminator : 1
(CVXPY) Nov 04 12:06:52 PM: Eliminator terminated.
(CVXPY) Nov 04 12:06:52 PM: Eliminator started.
(CVXPY) Nov 04 12:06:52 PM: Freed constraints in eliminator : 0
(CVXPY) Nov 04 12:06:52 PM: Eliminator terminated.
(CVXPY) Nov 04 12:06:52 PM: Eliminator - tries                  : 2                 time                   : 0.00            
(CVXPY) Nov 04 12:06:52 PM: Lin. dep.  - tries                  : 1                 time                   : 0.00            
(CVXPY) Nov 04 12:06:52 PM: Lin. dep.  - primal attempts        : 1                 successes              : 1               
(CVXPY) Nov 04 12:06:52 PM: Lin. dep.  - dual attempts          : 0                 successes              : 0               
(CVXPY) Nov 04 12:06:52 PM: Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               
(CVXPY) Nov 04 12:06:52 PM: Presolve terminated. Time: 0.00    
(CVXPY) Nov 04 12:06:52 PM: Optimizer  - threads                : 12              
(CVXPY) Nov 04 12:06:52 PM: Optimizer  - solved problem         : the primal      
(CVXPY) Nov 04 12:06:52 PM: Optimizer  - Constraints            : 3               
(CVXPY) Nov 04 12:06:52 PM: Optimizer  - Cones                  : 1               
(CVXPY) Nov 04 12:06:52 PM: Optimizer  - Scalar variables       : 10                conic                  : 5               
(CVXPY) Nov 04 12:06:52 PM: Optimizer  - Semi-definite variables: 0                 scalarized             : 0               
(CVXPY) Nov 04 12:06:52 PM: Factor     - setup time             : 0.00            
(CVXPY) Nov 04 12:06:52 PM: Factor     - dense det. time        : 0.00              GP order time          : 0.00            
(CVXPY) Nov 04 12:06:52 PM: Factor     - nonzeros before factor : 6                 after factor           : 6               
(CVXPY) Nov 04 12:06:52 PM: Factor     - dense dim.             : 0                 flops                  : 6.40e+01        
(CVXPY) Nov 04 12:06:52 PM: ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  
(CVXPY) Nov 04 12:06:52 PM: 0   1.3e+00  3.0e+00  2.0e+00  0.00e+00   -2.000000000e+00  -1.000000000e+00  1.0e+00  0.00  
(CVXPY) Nov 04 12:06:52 PM: 1   2.2e-01  5.3e-01  2.4e-01  1.05e-01   -1.810182709e+00  -1.723136430e+00  1.8e-01  0.00  
(CVXPY) Nov 04 12:06:52 PM: 2   3.7e-02  8.9e-02  1.6e-02  9.48e-01   -2.154687486e+00  -2.135415569e+00  3.0e-02  0.00  
(CVXPY) Nov 04 12:06:52 PM: 3   1.0e-02  2.5e-02  2.2e-03  1.01e+00   -2.236775806e+00  -2.230836084e+00  8.2e-03  0.00  
(CVXPY) Nov 04 12:06:52 PM: 4   2.8e-03  6.7e-03  3.2e-04  1.02e+00   -2.250313751e+00  -2.248643381e+00  2.2e-03  0.00  
(CVXPY) Nov 04 12:06:52 PM: 5   7.7e-04  1.8e-03  4.5e-05  1.00e+00   -2.256166317e+00  -2.255708191e+00  6.1e-04  0.00  
(CVXPY) Nov 04 12:06:52 PM: 6   2.1e-05  5.1e-05  2.1e-07  1.00e+00   -2.256863865e+00  -2.256850955e+00  1.7e-05  0.00  
(CVXPY) Nov 04 12:06:52 PM: 7   1.0e-07  2.4e-07  6.8e-11  1.00e+00   -2.256896295e+00  -2.256896233e+00  8.1e-08  0.00  
(CVXPY) Nov 04 12:06:52 PM: 8   8.5e-09  2.0e-08  1.6e-12  1.00e+00   -2.256896526e+00  -2.256896521e+00  6.7e-09  0.00  
(CVXPY) Nov 04 12:06:52 PM: Optimizer terminated. Time: 0.00    
(CVXPY) Nov 04 12:06:52 PM: 
(CVXPY) Nov 04 12:06:52 PM: 
(CVXPY) Nov 04 12:06:52 PM: Interior-point solution summary
(CVXPY) Nov 04 12:06:52 PM:   Problem status  : PRIMAL_AND_DUAL_FEASIBLE
(CVXPY) Nov 04 12:06:52 PM:   Solution status : OPTIMAL
(CVXPY) Nov 04 12:06:52 PM:   Primal.  obj: -2.2568965259e+00   nrm: 3e+00    Viol.  con: 2e-08    var: 1e-08    cones: 0e+00  
(CVXPY) Nov 04 12:06:52 PM:   Dual.    obj: -2.2568965208e+00   nrm: 1e+00    Viol.  con: 0e+00    var: 2e-08    cones: 0e+00  
-------------------------------------------------------------------------------
                                    Summary                                    
-------------------------------------------------------------------------------
(CVXPY) Nov 04 12:06:52 PM: Problem status: optimal
(CVXPY) Nov 04 12:06:52 PM: Optimal value: -2.257e+00
(CVXPY) Nov 04 12:06:52 PM: Compilation took 3.196e-03 seconds
(CVXPY) Nov 04 12:06:52 PM: Solver (including time spent in interface) took 1.100e+00 seconds
Status: optimal
Optimal value: -2.256896525744356
x* = [0.       0.068914 0.931086]
Max inequality residual (&lt;=0): -1.938693250380652e-08
Equality residual (≈0): 0.0</code></pre>
</div>
</div>
<div id="sqp" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Sequential Quadratic Programming<a href="model-based-plan-optimize.html#sqp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have seen that quadratic programs (QPs) gracefully handle <em>constrained LQR–style</em> trajectory optimization: convex quadratic costs, linear dynamics, and affine path/terminal constraints. With time-stacked sparsity, these problems are solved efficiently (e.g., via interior-point methods), making QP a strong tool for that regime.</p>
<p>The natural next step is the general trajectory optimization problem in <a href="model-based-plan-optimize.html#eq:to-general">(4.29)</a>, which features nonlinear dynamics, <em>nonconvex</em> objectives, and <em>nonlinear</em> constraints. To tackle this, we turn to <em>Sequential Quadratic Programming (SQP)</em>—a Newton-like framework that <em>iteratively</em> linearizes the dynamics/constraints and quadratizes the objective to form a sequence of QP subproblems. Each QP is solved to produce a step and updated multipliers; with globalization and appropriate Hessian modeling, the sequence converges to a <em>locally optimal</em> solution (a KKT point) of the original nonlinear TO problem. In short: QP handles the convex linearized case; SQP extends that logic to the full nonlinear setting by repeatedly building and solving the right QP at the current iterate. For an in-depth presentation of SQP for nonlinear programming, we refer to Chapter 18 of <span class="citation">(<a href="#ref-nocedal99book-numerical">Nocedal and Wright 1999</a>)</span>.</p>
<div id="problem-statement" class="section level4 hasAnchor" number="4.3.4.1">
<h4><span class="header-section-number">4.3.4.1</span> Problem Statement<a href="model-based-plan-optimize.html#problem-statement" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We will consider the following general nonlinear program (NLP)
<span class="math display" id="eq:sqp-nlp">\[
\begin{aligned}
\min_{x\in\mathbb{R}^n}\quad &amp; f(x) \\
\text{s.t.}\quad &amp; c(x)=0 \quad (c:\mathbb{R}^n\!\to\mathbb{R}^{m}),\\
&amp; d(x)\le 0 \quad (d:\mathbb{R}^n\!\to\mathbb{R}^{p}),
\end{aligned}
\tag{4.55}
\]</span>
with objective function <span class="math inline">\(f(x)\)</span>, equality constraints <span class="math inline">\(c(x)=0\)</span>, and inequality constraints <span class="math inline">\(d(x)\leq 0\)</span>. To obtain the NLP formulation <a href="model-based-plan-optimize.html#eq:sqp-nlp">(4.55)</a> from the TO template <a href="model-based-plan-optimize.html#eq:to-general">(4.29)</a>, one needs to define the set constraints <span class="math inline">\(x_t \in \mathcal{X}_t\)</span> and <span class="math inline">\(u_t \in \mathcal{U}_t\)</span> as general equality and inequality constraints. The decision variable <span class="math inline">\(x\)</span> contains the entire sequence of states and actions.</p>
<blockquote>
<p>While in many cases the functions <span class="math inline">\(f,c,d\)</span> are defined analytically, technically speaking, we only need zero-order and first-order oracles of these functions to implement numerical algorithms. That is, given a point <span class="math inline">\(x\)</span>, we need to evaluate <span class="math inline">\(f(x), c(x), d(x)\)</span> and their first-order derivatives.</p>
</blockquote>
<p>With dual variables <span class="math inline">\(\lambda \in \mathbb{R}^m\)</span> and <span class="math inline">\(\mu \in \mathbb{R}^p\)</span>, define the Lagrangian of <a href="model-based-plan-optimize.html#eq:sqp-nlp">(4.55)</a> as
<span class="math display">\[
\mathcal L(x,\lambda,\mu) \;=\; f(x) + \lambda^\top c(x) + \mu^\top d(x),\qquad \mu\ge 0,
\]</span>
and Jacobians <span class="math inline">\(J_c(x):=\nabla c(x)\in\mathbb{R}^{m\times n}\)</span>, <span class="math inline">\(J_d(x):=\nabla d(x)\in\mathbb{R}^{p\times n}\)</span>.</p>
</div>
<div id="high-level-intuition" class="section level4 hasAnchor" number="4.3.4.2">
<h4><span class="header-section-number">4.3.4.2</span> High-Level Intuition<a href="model-based-plan-optimize.html#high-level-intuition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>At a current iterate <span class="math inline">\(x_k\)</span>, we (1) Linearize the constraints and (2) quadratize the Lagrangian to build a local QP. Solving this QP yields a primal step <span class="math inline">\(p_k\)</span> and updated multipliers <span class="math inline">\((\lambda_{k+1},\mu_{k+1})\)</span> (from QP duals). A line-search (or trust-region) with a merit or filter globalization ensures convergence from remote starts.</p>
<p>Key ingredients:</p>
<ul>
<li><p>A Hessian (or quasi-Newton) approximation <span class="math inline">\(H_k \approx \nabla_{xx}^2\mathcal L(x_k,\lambda_k,\mu_k)\)</span>.</p></li>
<li><p>A QP subproblem capturing first-order feasibility and second-order optimality locally.</p></li>
<li><p>A globalization mechanism (<span class="math inline">\(\ell_1\)</span> merit or filter) + optional second-order correction (SOC) to mitigate linearization error in active inequalities.</p></li>
</ul>
</div>
<div id="the-sqp-qp-subproblem" class="section level4 hasAnchor" number="4.3.4.3">
<h4><span class="header-section-number">4.3.4.3</span> The SQP QP Subproblem<a href="model-based-plan-optimize.html#the-sqp-qp-subproblem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Given the current iterate <span class="math inline">\((x_k,\lambda_k,\mu_k)\)</span>, define
<span class="math display">\[
g_k := \nabla f(x_k),\qquad
c_k := c(x_k),\qquad
d_k := d(x_k),\qquad
A_k := J_c(x_k),\qquad
G_k := J_d(x_k).
\]</span>
Let <span class="math inline">\(H_k\)</span> be a symmetric approximation to <span class="math inline">\(\nabla_{xx}^2 \mathcal L(x_k,\lambda_k,\mu_k)\)</span> (see Section <a href="model-based-plan-optimize.html#sqp-Hessian-approximate">4.3.4.4</a>).</p>
<p>The QP subproblem in this step is
<span class="math display" id="eq:sqp-qp">\[
\begin{aligned}
\min_{p\in\mathbb{R}^n}\quad &amp;
\frac{1}{2} p^\top H_k\, p + g_k^\top p \\[1mm]
\text{s.t.}\quad &amp;
A_k\, p + c_k = 0, \\
&amp; G_k\, p + d_k \le 0.
\end{aligned}
\tag{4.56}
\]</span>
This defines a local quadratic model of <a href="model-based-plan-optimize.html#eq:sqp-nlp">(4.55)</a>: constraints are linearized; the objective is the second-order Taylor model of the Lagrangian (up to a constant).</p>
<p>Solving the QP subproblem <a href="model-based-plan-optimize.html#eq:sqp-qp">(4.56)</a> returns:</p>
<ul>
<li><strong>Primal step</strong> <span class="math inline">\(p_k\)</span>.</li>
<li><strong>Dual estimates</strong> <span class="math inline">\(\lambda_{k+1}^{\text{QP}}, \mu_{k+1}^{\text{QP}}\)</span> (the QP multipliers), which we use as new multipliers.</li>
</ul>
</div>
<div id="sqp-Hessian-approximate" class="section level4 hasAnchor" number="4.3.4.4">
<h4><span class="header-section-number">4.3.4.4</span> Hessian Approximation<a href="model-based-plan-optimize.html#sqp-Hessian-approximate" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The most natural choice for <span class="math inline">\(H_k\)</span> in the QP subproblem <a href="model-based-plan-optimize.html#eq:sqp-qp">(4.56)</a> is the exact Hessian:
<span class="math display">\[
H_k = \nabla_{xx}^2 \mathcal{L}(x_k, \lambda_k, \mu_k) = \nabla_{xx}^2 f(x_k) + \sum_{i} \lambda_{k,i} \nabla_{xx}^2 c_i(x_k) + \sum_{i} \mu_{k,i} \nabla_{xx}^2 d_i(x_k).
\]</span>
However, two potential issues with the exact Hessian are (i) it can be costly to build and store the analytic Hessians <span class="math inline">\(\nabla_{xx}^2 f, \nabla_{xx}^2 c_i, \nabla_{xx}^2 d_i\)</span>; (ii) the exact Hessian <span class="math inline">\(H_k\)</span> may not be positive semidefinite, which may lead to failure of convexity in the QP subproblem.</p>
<p>A cornerstone result in numerical optimization, due to <a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">Broyden–Fletcher–Goldfarb–Shanno</a> (BFGS), is to build an approximate Hessian from first-order derivatives. In particular, given two consecutive primal iterates <span class="math inline">\(x_{k+1}, x_k\)</span> (and their associated dual variables) and first-order gradients of the Lagrangian <span class="math inline">\(\nabla_x \mathcal{L}(x_{k+1}, \lambda_{k+1}, \mu_{k+1})\)</span>, <span class="math inline">\(\nabla_x \mathcal{L}(x_{k}, \lambda_{k}, \mu_{k})\)</span>, define
<span class="math display">\[
s_k := x_{k+1}-x_k,\qquad
   y_k := \nabla_x \mathcal L(x_{k+1},\lambda_{k+1},\mu_{k+1})
          - \nabla_x \mathcal L(x_k,\lambda_k,\mu_k).
\]</span>
The BFGS quasi-Newton method updates <span class="math inline">\(H_{k+1}\)</span> from <span class="math inline">\(H_k\)</span> as follows
<span class="math display" id="eq:BFGS">\[
H_{k+1} = H_k - \frac{H_k s_k s_k^\top H_k}{s_k^\top H_k s_k}
          + \frac{y_k y_k^\top}{s_k^\top y_k}.
\tag{4.57}
\]</span>
One can show that if <span class="math inline">\(y_k^\top s_k &gt; 0\)</span> holds, the BFGS Hessian approximation is always positive definite (provided <span class="math inline">\(H_0 \succ 0\)</span>). Therefore, the BFGS Hessian approximation ensures the QP subproblem is convex. If the curvature condition <span class="math inline">\(y_k^\top s_k &gt; 0\)</span> fails to hold, one can resort to damped BFGS, see <span class="citation">(<a href="#ref-nocedal99book-numerical">Nocedal and Wright 1999</a>)</span>.</p>
<blockquote>
<p>There is a broad family of quasi-Newton methods with BFGS being one of the most popular instances. For example, the <a href="https://en.wikipedia.org/wiki/Symmetric_rank-one">symmetric rank-one</a> (SR1) method is another popular quasi-Newton variant. In addition, the “limited memory” version of quasi-Newton methods (e.g., limited memory BFGS <span class="citation">(<a href="#ref-liu1989limited">Liu and Nocedal 1989</a>)</span>) can further reduce the price of Hessian approximation by only looking at the history of a small amount of gradients.</p>
</blockquote>
</div>
<div id="globalization-merit-or-filter" class="section level4 hasAnchor" number="4.3.4.5">
<h4><span class="header-section-number">4.3.4.5</span> Globalization: Merit or Filter<a href="model-based-plan-optimize.html#globalization-merit-or-filter" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To accept a step, we assess optimality improvement + feasibility improvement:</p>
<ul>
<li><p><strong><span class="math inline">\(\ell_1\)</span> merit</strong> (exact-penalty style): define the merit function
<span class="math display">\[
\phi_\rho(x) = f(x) + \rho \big(\|c(x)\|_1 + \|d^+(x)\|_1\big),\quad d^+ := \max(d,0),
\]</span>
with penalty <span class="math inline">\(\rho\)</span> large enough (<span class="math inline">\(\rho\)</span> can also be adaptive with respect to iterations). Use backtracking line search on <span class="math inline">\(\phi_\rho(x_k+\alpha p_k)\)</span> to ensure decrease of the merit function.</p></li>
<li><p><strong>Filter</strong> method: Maintain a set of pairs <span class="math inline">\((\text{feasibility},\text{objective})\)</span>. Accept steps that reduce <strong>either</strong> feasibility or objective sufficiently without worsening the other beyond the filter. Often paired with Second-order correction, see more details in <span class="citation">(<a href="#ref-nocedal99book-numerical">Nocedal and Wright 1999</a>)</span>.</p></li>
</ul>
<p>The following pseudocode implements a basic line-search SQP with quasi-Newton Hessian approximation.</p>
<div class="highlightbox">
<p><strong>Algorithm: Line-Search SQP</strong></p>
<p><strong>Inputs:</strong> <span class="math inline">\(x_0\)</span>, multipliers <span class="math inline">\((\lambda_0,\mu_0\ge 0)\)</span>, initial Hessian <span class="math inline">\(H_0\succ 0\)</span> (e.g., <span class="math inline">\(\gamma I\)</span>), globalization parameters.</p>
<p>For <span class="math inline">\(k=0,1,2,\dots\)</span></p>
<ol style="list-style-type: decimal">
<li><p><strong>Linearize &amp; build QP</strong>: form <span class="math inline">\(g_k, A_k, G_k, c_k, d_k\)</span> and <span class="math inline">\(H_k\)</span>, then solve the QP <a href="model-based-plan-optimize.html#eq:sqp-qp">(4.56)</a> to get <span class="math inline">\(p_k\)</span> and QP multipliers <span class="math inline">\((\hat\lambda_{k+1},\hat\mu_{k+1}\ge 0)\)</span>.</p></li>
<li><p><strong>Globalization</strong>: Choose step size <span class="math inline">\(\alpha_k\in(0,1]\)</span> by backtracking on the <span class="math inline">\(\ell_1\)</span>-merit.</p></li>
<li><p><strong>Update</strong>:
<span class="math display">\[
x_{k+1} = x_k + \alpha_k p_k,\qquad
\lambda_{k+1} = \hat\lambda_{k+1},\qquad
\mu_{k+1} = \Pi_{\ge 0}(\hat\mu_{k+1}).
\]</span></p></li>
<li><p><strong>Hessian (quasi-Newton) update</strong>: define
<span class="math display">\[
s_k := x_{k+1}-x_k,\qquad
y_k := \nabla_x \mathcal L(x_{k+1},\lambda_{k+1},\mu_{k+1})
       - \nabla_x \mathcal L(x_k,\lambda_k,\mu_k).
\]</span>
update
<span class="math display">\[
H_{k+1} = H_k - \frac{H_k s_k s_k^\top H_k}{s_k^\top H_k s_k}
          + \frac{y_k y_k^\top}{s_k^\top y_k}.
\]</span></p></li>
<li><p><strong>Stopping:</strong> if KKT residuals (stationarity, primal feasibility, complementarity) are below tolerance, <strong>terminate</strong>.</p></li>
</ol>
</div>
<p><strong>Notes.</strong></p>
<ul>
<li><p><strong>Trust-region SQP.</strong> An alternative globalization: add <span class="math inline">\(\|p\|\le \Delta\)</span> or a quadratic trust region, and update <span class="math inline">\(\Delta\)</span> by comparing predicted vs. actual reduction in a composite model.</p></li>
<li><p><strong>Software.</strong> The <code>scipy</code> package in Python implements SLSQP, which is basically the line-search SQP we presented above. A more advanced version of SQP is provided by the <a href="https://ccom.ucsd.edu/~optimizers/solvers/snopt/">SNOPT</a> commercial software. The <a href="https://github.com/ComputationalRobotics/CRISP">CRISP</a> software provides a C++ implementation of an SQP algorithm. Additionally, Matlab’s <a href="https://www.mathworks.com/help/optim/ug/fmincon.html"><code>fmincon</code></a> provides an implementation of SQP.</p></li>
</ul>
<div class="examplebox">
<div class="example">
<p><span id="exm:sqp-to-unicyle" class="example"><strong>Example 4.4  (Trajectory Optimization with SQP) </strong></span>We formulate a trajectory optimization (TO) problem for a unicycle robot that must travel from a start pose <span class="math inline">\(A\)</span> to a goal pose <span class="math inline">\(B\)</span> while avoiding circular (ball-shaped) obstacles.</p>
<p><strong>System Model.</strong> We use the standard unicycle (Dubins-like) kinematics in continuous time:
<span class="math display" id="eq:uni-cts">\[
\dot{x}(t)=
\begin{bmatrix}
\dot{p}_x\\[2pt]\dot{p}_y\\[2pt]\dot{\theta}
\end{bmatrix}
=
\begin{bmatrix}
v(t)\cos\theta(t)\\
v(t)\sin\theta(t)\\
\omega(t)
\end{bmatrix},
\qquad
x=[p_x,p_y,\theta]^\top,\ \ u=[v,\omega]^\top.
\tag{4.58}
\]</span></p>
<p>We discretize on a uniform grid <span class="math inline">\(t_k=k h,\ k=0,\dots,N\)</span> with step <span class="math inline">\(h&gt;0\)</span> by forward Euler:
<span class="math display" id="eq:uni-disc">\[
x_{k+1} \;=\; f_h(x_k,u_k)
\;:=\;
\begin{bmatrix}
p_{x,k} + h\, v_k \cos \theta_k\\
p_{y,k} + h\, v_k \sin \theta_k\\
\theta_k + h\, \omega_k
\end{bmatrix}.
\tag{4.59}
\]</span></p>
<p><strong>Decision Variables.</strong> We optimize over the state–control sequence
<span class="math display">\[
\{x_k\}_{k=0}^N,\quad \{u_k\}_{k=0}^{N-1},
\]</span>
and collect them into a single vector
<span class="math display" id="eq:uni-stacked-z">\[
z
=
\big[x_0^\top,\ u_0^\top,\ x_1^\top,\ u_1^\top,\ \dots,\ x_{N-1}^\top,\ u_{N-1}^\top,\ x_N^\top\big]^\top
\;\in\;\mathbb{R}^{(3+2)N+3}.
\tag{4.60}
\]</span></p>
<p><strong>Constraints.</strong> We impose the following constraints.</p>
<p><strong>(i) Initial condition.</strong>
<span class="math display" id="eq:uni-x0">\[
x_0 = A \in \mathbb{R}^3.
\tag{4.61}
\]</span></p>
<p><strong>(ii) System dynamics (equality constraints).</strong> For <span class="math inline">\(k=0,\dots,N-1\)</span>,
<span class="math display" id="eq:uni-dyn-eq">\[
x_{k+1} - f_h(x_k,u_k) = 0.
\tag{4.62}
\]</span></p>
<p><strong>(iii) Obstacle avoidance (inequalities).</strong> Let the set of circular obstacles be <span class="math inline">\(\mathcal{O}=\{(c_j,r_j)\}_{j=1}^{n_{\text{obs}}}\)</span> with centers <span class="math inline">\(c_j=[c_{x,j},c_{y,j}]^\top\)</span> and radii <span class="math inline">\(r_j&gt;0\)</span>. We require the robot’s position to stay outside each inflated disk of radius <span class="math inline">\(r_j+\delta\)</span> (safety margin <span class="math inline">\(\delta\ge 0\)</span>) at every knot:
<span class="math display">\[
\underbrace{(p_{x,k}-c_{x,j})^2 + (p_{y,k}-c_{y,j})^2}_{\text{dist}^2(x_k,\text{center}_j)}
\;\;\ge\;\; (r_j+\delta)^2,
\qquad
\forall k=0,\dots,N,\ \forall j.
\]</span>
In “<span class="math inline">\(c(x)\le 0\)</span>” form (e.g., for <code>fmincon</code>):
<span class="math display" id="eq:uni-obstacles">\[
c_{j,k}(x_k) \;:=\; (r_j+\delta)^2 - \big((p_{x,k}-c_{x,j})^2 + (p_{y,k}-c_{y,j})^2\big) \;\le\; 0.
\tag{4.63}
\]</span></p>
<p><strong>(iv) Simple bounds.</strong> Box limits on controls (and possibly states):
<span class="math display" id="eq:uni-bounds">\[
v_{\min} \le v_k \le v_{\max},\qquad
\omega_{\min} \le \omega_k \le \omega_{\max},\qquad k=0,\dots,N-1.
\tag{4.64}
\]</span></p>
<p><strong>Objective function.</strong> We use a smooth quadratic objective combining (a) terminal goal tracking, (b) control effort, and (c) control smoothness (temporal regularization):</p>
<ul>
<li><strong>Terminal goal tracking</strong> to a desired pose <span class="math inline">\(B=[p_x^\star,p_y^\star,\theta^\star]^\top\)</span>:
<span class="math display">\[
J_{\text{goal}}
\;=\;
w_{\text{pos}} \,\big\|x_N^{\text{pos}} - B^{\text{pos}}\big\|_2^2
\;+\;
w_{\theta}\,(\theta_N-\theta^\star)^2,
\quad
x_N^{\text{pos}}=[p_{x,N},p_{y,N}]^\top.
\]</span></li>
<li><strong>Control effort</strong>:
<span class="math display">\[
J_{u} \;=\; \sum_{k=0}^{N-1} w_u \,\|u_k\|_2^2
\;=\;
\sum_{k=0}^{N-1} w_u\,(v_k^2+\omega_k^2).
\]</span></li>
<li><strong>Control smoothness</strong> (discrete total-variation-like quadratic):
<span class="math display">\[
J_{\Delta u}
\;=\;
\sum_{k=0}^{N-2} w_{\Delta u}\,\|u_{k+1}-u_k\|_2^2.
\]</span></li>
</ul>
<p>The complete cost is
<span class="math display" id="eq:uni-obj">\[
J(x_{0:N},u_{0:N-1})
=
\frac{1}{2}\Big(
J_{\text{goal}} + J_u + J_{\Delta u}
\Big),
\tag{4.65}
\]</span>
where the outer factor <span class="math inline">\(\frac{1}{2}\)</span> is conventional in quadratic objectives.</p>
<p><strong>Complete optimization problem.</strong> Given <span class="math inline">\(A,B,\{(c_j,r_j)\},\delta,h,N\)</span>, choose <span class="math inline">\(\{x_k\}_{k=0}^N,\{u_k\}_{k=0}^{N-1}\)</span> to
<span class="math display">\[
\begin{aligned}
\min_{\{x_k,u_k\}} \quad
&amp; \frac{1}{2}\Big(
w_{\text{pos}} \|x_N^{\text{pos}}-B^{\text{pos}}\|_2^2
+ w_{\theta}(\theta_N-\theta^\star)^2
+ \sum_{k=0}^{N-1} w_u \|u_k\|_2^2
+ \sum_{k=0}^{N-2} w_{\Delta u} \|u_{k+1}-u_k\|_2^2
\Big) \\[2mm]
\text{s.t.}\quad
&amp; x_0 = A, \\[2pt]
&amp; x_{k+1} = f_h(x_k,u_k)\quad \text{for } k=0,\dots,N-1, \\[2pt]
&amp; (r_j+\delta)^2 - \big((p_{x,k}-c_{x,j})^2 + (p_{y,k}-c_{y,j})^2\big) \le 0,
\ \ \forall j,\ \forall k, \\[2pt]
&amp; v_{\min} \le v_k \le v_{\max},\quad
  \omega_{\min} \le \omega_k \le \omega_{\max},\quad k=0,\dots,N-1.
\end{aligned}
\]</span>
This is a smooth, sparse nonlinear program (NLP).</p>
<p><strong>Experimental setup.</strong></p>
<ul>
<li>Horizon <span class="math inline">\(N=60\)</span>, step <span class="math inline">\(h=0.1\)</span> s.<br />
</li>
<li>Start <span class="math inline">\(A=[0,0,0]^\top\)</span>, goal <span class="math inline">\(B=[6,5,0]^\top\)</span>.<br />
</li>
<li>Obstacles <span class="math inline">\(\{(c_j,r_j)\}\)</span> with a margin <span class="math inline">\(\delta=0.25\)</span> m.<br />
</li>
<li>Control bounds <span class="math inline">\(v\in[-1.5,1.5]\)</span> m/s, <span class="math inline">\(\omega\in[-2,2]\)</span> rad/s.<br />
</li>
<li>Weights <span class="math inline">\(w_{\text{pos}}=400,\ w_\theta=20,\ w_u=0.05,\ w_{\Delta u}=0.2\)</span>.<br />
</li>
<li><strong>Initialization:</strong> straight-line interpolation of positions from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span>, heading toward the line, constant <span class="math inline">\(v\)</span>, <span class="math inline">\(\omega=0\)</span>.</li>
</ul>
<p><strong>Two obstacles (success).</strong> Fig. <a href="model-based-plan-optimize.html#fig:unicycle-to-sqp-2obs">4.6</a> shows the results for trajectory optimization with two obstacles. As we can see, the SQP algorithm generates a smooth trajectory that avoids the two circular obstacles, despite the fact that the initial guess crosses both obstacles.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-to-sqp-2obs"></span>
<img src="images/Model-based-optimization/unicycle_to_sqp_2_obstacles.jpg" alt="Trajectory optimization for unicyle using SQP (two obstacles). Dotted line: initial guess; solid line: optimized trajectory." width="90%" />
<p class="caption">
Figure 4.6: Trajectory optimization for unicyle using SQP (two obstacles). Dotted line: initial guess; solid line: optimized trajectory.
</p>
</div>
<p><strong>Three obstacles (failure).</strong> However, when we add a third obstacle, Fig. <a href="model-based-plan-optimize.html#fig:unicycle-to-sqp-3obs">4.7</a> shows that the SQP algorithm converges to an infeasible solution that collides with the obstacles.</p>
<p>You can play with the Matlab code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/to_unicycle_sqp.m">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-to-sqp-3obs"></span>
<img src="images/Model-based-optimization/unicycle_to_sqp_3_obstacles.jpg" alt="Trajectory optimization for unicyle using SQP (three obstacles). Dotted line: initial guess; solid line: optimized trajectory." width="90%" />
<p class="caption">
Figure 4.7: Trajectory optimization for unicyle using SQP (three obstacles). Dotted line: initial guess; solid line: optimized trajectory.
</p>
</div>
</div>
</div>
<p>The example above highlights both the strengths and the limitations of solving TO with numerical NLP methods such as SQP. Because the TO problem is generally <em>nonconvex</em>, a <em>local</em> method’s outcome depends strongly on the <em>initialization</em>. With a <em>good</em> initial guess, local NLP solvers often converge quickly to a high-quality solution (e.g., Fig. <a href="model-based-plan-optimize.html#fig:unicycle-to-sqp-2obs">4.6</a>). With a <em>poor</em> initialization—especially when the landscape has many local minima—the solver may settle in a suboptimal basin or even fail to find a feasible trajectory (e.g., Fig. <a href="model-based-plan-optimize.html#fig:unicycle-to-sqp-3obs">4.7</a>).</p>
<p>Global optimization methods can avoid initialization sensitivity and provide <strong>global optimality guarantees</strong>. These techniques can be substantially more expensive and require additional structure or reformulation, but when applicable they yield powerful <em>initialization-free</em> solutions; see, e.g., <span class="citation">(<a href="#ref-kang2024fast">Kang et al. 2024</a>)</span> and references therein.</p>
</div>
</div>
<div id="ipm-nlp" class="section level3 hasAnchor" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> Interior-Point Methods<a href="model-based-plan-optimize.html#ipm-nlp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have seen that Primal–Dual Interior-Point Methods (PD-IPM) efficiently solve convex QPs (Section <a href="model-based-plan-optimize.html#qp">4.3.3</a>). The key idea was to write the KKT optimality conditions as a system of nonlinear equations and apply Newton’s method. We now extend this idea to the general NLP in <a href="model-based-plan-optimize.html#eq:sqp-nlp">(4.55)</a>:
<span class="math display">\[
\min_{x\in\mathbb{R}^n} f(x)\quad\text{s.t.}\quad c(x)=0,\;\; d(x)\le 0,
\]</span>
where <span class="math inline">\(f:\mathbb{R}^n \to \mathbb{R}, c:\mathbb{R}^n \to \mathbb{R}^m\)</span> and <span class="math inline">\(d:\mathbb{R}^n \to\mathbb{R}^p\)</span> are smooth.</p>
<div id="lagrangian-and-kkt" class="section level4 hasAnchor" number="4.3.5.1">
<h4><span class="header-section-number">4.3.5.1</span> Lagrangian and KKT<a href="model-based-plan-optimize.html#lagrangian-and-kkt" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Introduce slack variables <span class="math inline">\(s\in\mathbb{R}^p\)</span> to convert inequalities to equalities:
<span class="math display" id="eq:nlp-slack">\[
c(x)=0,\qquad d(x)+s=0,\qquad s\ge 0.
\tag{4.66}
\]</span></p>
<p>With equality multipliers <span class="math inline">\(y\in\mathbb{R}^m, \nu \in \mathbb{R}^p\)</span> and inequality multipliers <span class="math inline">\(\mu\in\mathbb{R}^p\)</span> (with <span class="math inline">\(\lambda \ge 0\)</span>), the <strong>Lagrangian</strong> of the slack-form problem is
<span class="math display" id="eq:nlp-L">\[
\mathcal L(x,s,y,\nu,\lambda) \;=\; f(x) + y^\top c(x) + \nu^\top \big(d(x)+s\big) - \lambda^\top s.
\tag{4.67}
\]</span></p>
<p>The KKT optimality conditions are (after eliminating <span class="math inline">\(\nu\)</span>)
<span class="math display" id="eq:nlp-kkt">\[
\begin{aligned}
\text{stationarity:}&amp;\quad
\nabla f(x) + J_c(x)^\top y + J_d(x)^\top \lambda = 0,\\
\text{primal feasibility:}&amp;\quad
c(x)=0,\;\; d(x)+s=0,\;\; s\ge 0,\\
\text{dual feasibility:}&amp;\quad
\lambda\ge 0,\\
\text{complementarity:}&amp;\quad
s_i\,\lambda_i = 0,\quad i=1,\dots,p,
\end{aligned}
\tag{4.68}
\]</span>
where <span class="math inline">\(J_c=\nabla c(x) \in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(J_d=\nabla d(x) \in \mathbb{R}^{p \times n}\)</span>.</p>
</div>
<div id="two-equivalent-views" class="section level4 hasAnchor" number="4.3.5.2">
<h4><span class="header-section-number">4.3.5.2</span> Two Equivalent Views<a href="model-based-plan-optimize.html#two-equivalent-views" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>There are two equivalent ways to derive primal–dual IPMs.</p>
<p><strong>Homotopy / Perturbed KKT (primal–dual view).</strong> Replace the complementarity slackness condition in the KKT system <a href="model-based-plan-optimize.html#eq:nlp-kkt">(4.68)</a> by a <em>perturbed</em> relation that defines the <em>central path</em>:
<span class="math display" id="eq:nlp-central-mu">\[
S\,\lambda \;=\; \mu \,\mathbf{1},\qquad S:=\operatorname{diag}(s),\;\; \mu &gt;0.
\tag{4.69}
\]</span>
Driving the parameter <span class="math inline">\(\mu \downarrow 0\)</span> yields iterates approaching a KKT point.</p>
<p><strong>Barrier (log-barrier view).</strong> Solve the barrier subproblem
<span class="math display" id="eq:nlp-barrier">\[
\min_{x,s&gt;0}\; f(x) - \mu \sum_{i=1}^p \log s_i
\quad\text{s.t.}\quad
c(x)=0,\;\; d(x)+s=0,
\tag{4.70}
\]</span>
then reduce <span class="math inline">\(\mu\)</span>. The KKT conditions of <a href="model-based-plan-optimize.html#eq:nlp-barrier">(4.70)</a> imply <span class="math inline">\(S\lambda = \mu \mathbf{1}\)</span>, hence the barrier and homotopy views are equivalent (different perspectives on the same central path).</p>
</div>
<div id="primaldual-residuals-and-newton-system" class="section level4 hasAnchor" number="4.3.5.3">
<h4><span class="header-section-number">4.3.5.3</span> Primal–Dual Residuals and Newton System<a href="model-based-plan-optimize.html#primaldual-residuals-and-newton-system" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Define residuals at <span class="math inline">\((x,s,y,\lambda)\)</span>:
<span class="math display" id="eq:nlp-res">\[
\begin{aligned}
r_{\mathrm{dual}} &amp;= \nabla f(x) + J_c^\top y + J_d^\top \lambda,\\
r_{\mathrm{pe}}   &amp;= c(x),\\
r_{\mathrm{pi}}   &amp;= d(x)+s,\\
r_{\mathrm{cent}} &amp;= S\lambda - \mu \,\mathbf{1}.
\end{aligned}
\tag{4.71}
\]</span>
Let
<span class="math display" id="eq:nlp-H">\[
H \;:=\; \nabla^2_{xx}\mathcal L(x,s,y,\lambda)
= \nabla^2 f(x) + \sum_{i=1}^m y_i \nabla^2 c_i(x) + \sum_{j=1}^p \lambda_j \nabla^2 d_j(x)
\tag{4.72}
\]</span>
be the exact Hessian of the Lagrangian with respect to <span class="math inline">\(x\)</span>.</p>
<p>A primal–dual Newton step <span class="math inline">\((\Delta x,\Delta s,\Delta y,\Delta \lambda)\)</span> solves
<span class="math display" id="eq:nlp-newton">\[
\begin{aligned}
H\,\Delta x + J_c^\top \Delta y + J_d^\top \Delta \lambda &amp;= -\,r_{\mathrm{dual}},\\
J_c\,\Delta x &amp;= -\,r_{\mathrm{pe}},\\
J_d\,\Delta x + \Delta s &amp;= -\,r_{\mathrm{pi}},\\
S\,\Delta \lambda + M\,\Delta s &amp;= -\,r_{\mathrm{cent}},\qquad M:=\operatorname{diag}(\lambda).
\end{aligned}
\tag{4.73}
\]</span></p>
<p>Eliminating <span class="math inline">\(\Delta s=-r_{\mathrm{pi}}-J_d\Delta x\)</span> gives
<span class="math display" id="eq:nlp-dmu">\[
\Delta \lambda = S^{-1}\!\left(-r_{\mathrm{cent}} + M\,r_{\mathrm{pi}} + M\,J_d\,\Delta x\right).
\tag{4.74}
\]</span>
Substitute into the first line to obtain the reduced symmetric system in <span class="math inline">\((\Delta x,\Delta y)\)</span>:
<span class="math display" id="eq:nlp-reduced">\[
\begin{bmatrix}
H + J_d^\top D\,J_d &amp; J_c^\top\\[2pt]
J_c &amp; 0
\end{bmatrix}
\begin{bmatrix}\Delta x\\ \Delta y\end{bmatrix}
= -
\begin{bmatrix}
r_{\mathrm{dual}} + J_d^\top S^{-1}\!\big(-r_{\mathrm{cent}} + M r_{\mathrm{pi}}\big)\\[2pt]
r_{\mathrm{pe}}
\end{bmatrix}
\tag{4.75}
\]</span>
with <span class="math inline">\(D:=S^{-1}M\)</span>. Then recover <span class="math inline">\(\Delta \lambda\)</span> via <a href="model-based-plan-optimize.html#eq:nlp-dmu">(4.74)</a> and <span class="math inline">\(\Delta s=-r_{\mathrm{pi}}-J_d\Delta x\)</span>.</p>
</div>
<div id="ipm-line-search" class="section level4 hasAnchor" number="4.3.5.4">
<h4><span class="header-section-number">4.3.5.4</span> Line-search IPM<a href="model-based-plan-optimize.html#ipm-line-search" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Step lengths (fraction-to-the-boundary).</strong> Choose positive step sizes that keep strict interiority:
<span class="math display" id="eq:nlp-steps">\[
\alpha_{\mathrm{pri}}=\min\!\Big(1,\ \eta \min_{\Delta s_i&lt;0}\frac{-s_i}{\Delta s_i}\Big),\quad
\alpha_{\mathrm{du}} =\min\!\Big(1,\ \eta \min_{\Delta \lambda_i&lt;0}\frac{-\lambda_i}{\Delta \lambda_i}\Big),\quad
\eta\in(0,1).
\tag{4.76}
\]</span></p>
<p><strong>Merit (or filter) globalization.</strong> Use a barrier merit for backtracking,
<span class="math display" id="eq:nlp-merit">\[
\Phi_{\mu}(x,s) \;=\; f(x) - \mu \sum_i \log s_i
+ \frac{\rho}{2}\,\|c(x)\|_2^2
+ \frac{\rho}{2}\,\|d(x)+s\|_2^2,
\tag{4.77}
\]</span>
or a filter that accepts steps reducing either infeasibility or the barrier objective.</p>
<p><strong>Mehrotra predictor–corrector (recommended).</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Predictor (affine) step:</strong> solve <a href="model-based-plan-optimize.html#eq:nlp-reduced">(4.75)</a> with <span class="math inline">\(\mu=0\)</span> to get <span class="math inline">\((\Delta x^{\mathrm{aff}},\Delta s^{\mathrm{aff}},\Delta y^{\mathrm{aff}},\Delta\lambda^{\mathrm{aff}})\)</span>, and affine step sizes <span class="math inline">\(\alpha_{\mathrm{pri}}^{\mathrm{aff}},\alpha_{\mathrm{du}}^{\mathrm{aff}}\)</span>.</p></li>
<li><p><strong>Centering:</strong> set
<span class="math display">\[
\tau_{\mathrm{aff}}=\frac{(s+\alpha_{\mathrm{pri}}^{\mathrm{aff}}\Delta s^{\mathrm{aff}})^\top
(\lambda+\alpha_{\mathrm{du}}^{\mathrm{aff}}\Delta\lambda^{\mathrm{aff}})}{p},\qquad
\sigma=\left(\frac{\tau_{\mathrm{aff}}}{\frac{s^\top\mu}{p}}\right)^{3},
\]</span>
and replace the complementarity RHS by <span class="math inline">\(\mu=\sigma\,\frac{s^\top\lambda}{p}\)</span>.</p></li>
<li><p><strong>Corrector:</strong> resolve <a href="model-based-plan-optimize.html#eq:nlp-reduced">(4.75)</a> with the corrected central residual
<span class="math display" id="eq:nlp-corr">\[
r_{\mathrm{cent}}^{\mathrm{corr}}
= S\lambda - \mu \,\mathbf{1} - \Delta S^{\mathrm{aff}}\Delta\lambda^{\mathrm{aff}}\mathbf{1}.
\tag{4.78}
\]</span></p></li>
<li><p><strong>Line search &amp; update:</strong> use <a href="model-based-plan-optimize.html#eq:nlp-steps">(4.76)</a> and backtrack on <span class="math inline">\(\Phi_{\mu}\)</span>.</p></li>
<li><p><strong>Stopping:</strong> terminate inner loop when <span class="math inline">\(\|r_{\mathrm{dual}}\|_\infty,\|r_{\mathrm{pe}}\|_\infty,\|r_{\mathrm{pi}}\|_\infty\)</span> and the average complementarity <span class="math inline">\(\frac{s^\top\mu}{p}\)</span> are below tolerance; then reduce <span class="math inline">\(\mu\)</span> and repeat.</p></li>
</ol>
<p><strong>Hessian Modeling.</strong> Use <span class="math inline">\(H=\nabla^2_{xx}\mathcal L\)</span>, or a damped (L-)BFGS / Gauss–Newton model as in Section <a href="model-based-plan-optimize.html#sqp-Hessian-approximate">4.3.4.4</a>.</p>
</div>
<div id="ipm-trust-region" class="section level4 hasAnchor" number="4.3.5.5">
<h4><span class="header-section-number">4.3.5.5</span> Trust-Region IPM<a href="model-based-plan-optimize.html#ipm-trust-region" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The trust-region (TR) IPM solves the barrier subproblem <a href="model-based-plan-optimize.html#eq:nlp-barrier">(4.70)</a> <em>inexactly</em> within a TR globalization that is scaled to the slacks to avoid steps that approach the boundary too aggressively.</p>
<p>The TR-IPM subproblem is the local quadratic model of the barrier problem together with
a metric that respects distance to the boundary.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Quadratic model of the barrier objective.</strong> Start from the barrier problem
<span class="math display">\[
\min_{x,s&gt;0}\ f(x)-\mu\sum_i\log s_i
\quad\text{s.t.}\quad c(x)=0,\ d(x)+s=0.
\]</span>
A second-order Taylor model at <span class="math inline">\((x,s)\)</span> gives (constants omitted)
<span class="math display">\[
m(p_x,p_s)\;\approx\; \nabla f^\top p_x+\frac{1}{2} p_x^\top H\,p_x
\;-\;\mu\,\mathbf 1^\top S^{-1}p_s
\;+\;\frac{1}{2}\,\mu\,\|S^{-1}p_s\|_2^2,
\]</span>
where <span class="math inline">\(H=\nabla^2_{xx}\mathcal L\)</span> and <span class="math inline">\(S=\operatorname{diag}(s)\)</span>. The linear term <span class="math inline">\(-\mu\,\mathbf1^\top S^{-1}p_s\)</span> is exactly the gradient of the log-barrier in the slack coordinates. The quadratic curvature in <span class="math inline">\(p_s\)</span> is <span class="math inline">\(\mu S^{-2}\)</span>.</p>
<p>Many implementations move (part of) this curvature into the TR metric, replacing the explicit <span class="math inline">\(+\frac{1}{2}\,\mu\|S^{-1}p_s\|^2\)</span> by the <em>scaled trust region</em> below. This avoids double-counting and makes the subproblem simpler while keeping the right geometry.</p></li>
<li><p><strong>Linearized constraints for a consistent local step.</strong><br />
<span class="math display">\[
J_c\,p_x + c(x)=0,\qquad J_d\,p_x + p_s + d(x)=0.
\]</span>
These are the first-order feasibility conditions of the barrier constraints.</p>
<!-- They enable the *normal/tangential split*: first reduce constraint violations (normal step), then reduce the model in the approximate nullspace (tangential step). --></li>
<li><p><strong>Scaled trust region <span class="math inline">\(\| (p_x,\;S^{-1}p_s)\|\le \Delta\)</span>.</strong> The scaling by <span class="math inline">\(S^{-1}\)</span> measures <span class="math inline">\(p_s\)</span> relative to the current distance to the boundary. If a slack <span class="math inline">\(s_i\)</span> is tiny, even a small absolute change <span class="math inline">\(p_{s,i}\)</span> is risky; the scaled norm automatically shrinks the allowable step in that direction. This yields:</p>
<ul>
<li><strong>Boundary awareness:</strong> steps cannot run into <span class="math inline">\(s_i\le0\)</span> unless the trust region is (incorrectly) large.<br />
</li>
<li><strong>Scale invariance:</strong> the step is insensitive to units or simple rescalings of the inequalities.<br />
</li>
<li><strong>Numerical stability:</strong> the local subproblem remains well conditioned near the boundary.</li>
</ul></li>
<li><p><strong>Fraction-to-the-boundary safeguard <span class="math inline">\(p_s\ge -\tau s\)</span>.</strong> This is a simple interiority constraint ensuring <span class="math inline">\(s+\alpha p_s&gt;0\)</span> for admissible step sizes.</p></li>
</ol>
<p>Putting these choices together yields the subproblem
<span class="math display" id="eq:tr-ipm-subproblem">\[
\min_{p_x,p_s}\ \nabla f^\top p_x+\frac{1}{2} p_x^\top H p_x - \mu\,\mathbf1^\top S^{-1}p_s
\quad\text{s.t.}\quad
\begin{cases}
J_c p_x + c(x)=0,\\
J_d p_x + p_s + d(x)=0,\\
\|(p_x,S^{-1}p_s)\|\le\Delta,\ p_s\ge-\tau s,
\end{cases}
\tag{4.79}
\]</span>
which is exactly a trust-region SQP step on the barrier problem in the barrier metric.</p>
<p>The outer loop adapts the trust-region radius <span class="math inline">\(\Delta\)</span> and the barrier parameter <span class="math inline">\(\mu\)</span> as follows.</p>
<ul>
<li><strong>Updating <span class="math inline">\(\Delta\)</span> (model–reality agreement).</strong> After computing a trial step <span class="math inline">\(p=(p_x,p_s)\)</span> from the scaled TR subproblem <a href="model-based-plan-optimize.html#eq:tr-ipm-subproblem">(4.79)</a>, compare the <em>predicted</em> reduction from the quadratic model to the <em>actual</em> reduction in the barrier merit:
<span class="math display">\[
\rho \;=\; \frac{\text{ared}}{\text{pred}}
\;=\;
\frac{\Phi_\mu(x,s)-\Phi_\mu(x+p_x,\,s+p_s)}
     {\text{model}(0)-\text{model}(p)}.
\]</span>
With thresholds <span class="math inline">\(0&lt;\eta_1&lt;\eta_2&lt;1\)</span> and factors <span class="math inline">\(\gamma_{\text{dec}}\in(0,1)\)</span>, <span class="math inline">\(\gamma_{\text{inc}}&gt;1\)</span>:
<ul>
<li>If <span class="math inline">\(\rho\ge \eta_2\)</span>: accept the step and enlarge the radius,
<span class="math inline">\(\Delta \leftarrow \min(\gamma_{\text{inc}}\Delta,\Delta_{\max})\)</span>.</li>
<li>If <span class="math inline">\(\eta_1 \le \rho &lt; \eta_2\)</span>: accept and keep <span class="math inline">\(\Delta\)</span>.</li>
<li>If <span class="math inline">\(\rho&lt;\eta_1\)</span>: reject the step and shrink the radius,
<span class="math inline">\(\Delta \leftarrow \gamma_{\text{dec}}\Delta\)</span>, then resolve the TR subproblem.</li>
</ul></li>
<li><strong>Updating <span class="math inline">\(\mu\)</span> (centrality progress).</strong> Decrease the barrier parameter <span class="math inline">\(\mu\)</span> only when the current barrier subproblem is solved to an accuracy commensurate with <span class="math inline">\(\mu\)</span>. Using KKT residuals and average complementarity
<span class="math display">\[
\tau \;:=\; \frac{s^\top \lambda}{p},
\]</span>
require (for some constants <span class="math inline">\(\kappa_{\rm dual},\kappa_{\rm pe},\kappa_{\rm pi},\kappa_{\rm cent}&gt;0\)</span>)
<span class="math display">\[
\|r_{\rm dual}\|_\infty \le \kappa_{\rm dual}\,\mu,\quad
\|r_{\rm pe}\|_\infty \le \kappa_{\rm pe}\,\mu,\quad
\|r_{\rm pi}\|_\infty \le \kappa_{\rm pi}\,\mu,\quad
|\tau-\mu| \le \kappa_{\rm cent}\,\mu.
\]</span>
When these hold, reduce <span class="math inline">\(\mu\)</span> (e.g., <span class="math inline">\(\mu\leftarrow\theta\,\mu\)</span> with <span class="math inline">\(\theta\in(0,1)\)</span>). If the tests are not met, hold <span class="math inline">\(\mu\)</span> fixed and continue improving the inner TR solve (possibly with an updated <span class="math inline">\(\Delta\)</span>).</li>
</ul>
<p><strong>When to favor TR-IPM.</strong> TR globalization is robust for nonconvex <span class="math inline">\(H\)</span>, allows inexact linear solves (e.g., Krylov), and integrates naturally with limited-memory updates and scaling.</p>
<p><strong>Software.</strong> Implementing a robust interior-point method that reliably solves <em>general</em> nonlinear programs is nontrivial. Fortunately, the open-source solver <a href="https://github.com/coin-or/Ipopt">IPOPT</a> <span class="citation">(<a href="#ref-wachter2006implementation">Wächter and Biegler 2006</a>)</span> is a mature, widely used option that exploits sparsity and supports exact or quasi-Newton Hessians. In what follows, we formulate a trajectory-optimization problem and solve it with IPOPT to illustrate end-to-end modeling and solver usage.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unicycle-ipopt" class="example"><strong>Example 4.5  (Trajectory Optimization with IPOPT) </strong></span>We solve a unicycle trajectory optimization (TO) problem (same as the one in Example <a href="model-based-plan-optimize.html#exm:sqp-to-unicyle">4.4</a>) from start pose <span class="math inline">\(A\)</span> to goal pose <span class="math inline">\(B\)</span> while avoiding circular obstacles, now using IPOPT via <a href="https://cyipopt.readthedocs.io/en/stable/"><code>cyipopt</code></a> in Python.</p>
<p><strong>Dynamics.</strong> With state <span class="math inline">\(x=[p_x,p_y,\theta]^\top\)</span> and control <span class="math inline">\(u=[v,\omega]^\top\)</span>,
<span class="math display" id="eq:ipopt-uni-cts">\[
\dot{x}(t)=
\begin{bmatrix}
v\cos\theta\\[2pt]
v\sin\theta\\
\omega
\end{bmatrix}.
\tag{4.80}
\]</span>
We discretize at <span class="math inline">\(t_k=kh\)</span> by forward Euler:
<span class="math display" id="eq:ipopt-uni-disc">\[
x_{k+1} = f_h(x_k,u_k) :=
\begin{bmatrix}
p_{x,k} + h\, v_k \cos \theta_k\\
p_{y,k} + h\, v_k \sin \theta_k\\
\theta_k + h\, \omega_k
\end{bmatrix},\quad k=0,\ldots,N-1.
\tag{4.81}
\]</span></p>
<p><strong>Decision vector.</strong> We stack the state and control trajectories into a single decision vector
<span class="math display" id="eq:ipopt-uni-z">\[
z=\big[x_0^\top,u_0^\top,x_1^\top,u_1^\top,\dots,x_{N-1}^\top,u_{N-1}^\top,x_N^\top\big]^\top
\in\mathbb{R}^{(3+2)N+3}.
\tag{4.82}
\]</span></p>
<p><strong>Constraints.</strong> The state and control trajectories need to satisfy the following constraints:</p>
<ul>
<li><p>Initial condition: <span class="math inline">\(x_0=A\)</span>.</p></li>
<li><p>Dynamics equalities: <span class="math inline">\(x_{k+1}-f_h(x_k,u_k)=0\)</span> for <span class="math inline">\(k=0,\dots,N-1\)</span>.</p></li>
<li><p>Obstacle inequalities: for each circular obstacle <span class="math inline">\((c_j=[c_{x,j},c_{y,j}]^\top,r_j)\)</span> and all <span class="math inline">\(k=0,\dots,N\)</span>,
<span class="math display" id="eq:ipopt-obs">\[
c_{j,k}(x_k):=(r_j+\delta)^2-\big((p_{x,k}-c_{x,j})^2+(p_{y,k}-c_{y,j})^2\big)\le 0,
\tag{4.83}
\]</span>
where <span class="math inline">\(\delta &gt; 0\)</span> is a safety margin.</p></li>
<li><p>Box bounds on controls:
<span class="math display" id="eq:ipopt-bounds">\[
v_{\min}\le v_k\le v_{\max},\qquad
\omega_{\min}\le \omega_k\le \omega_{\max}.
\tag{4.84}
\]</span></p></li>
</ul>
<p>In total, there are <span class="math inline">\(m = 3 + 3N\)</span> equality constraints and <span class="math inline">\(p = (N+1)O\)</span> inequality constraints, where <span class="math inline">\(O\)</span> is the number of obstacles.</p>
<p><strong>Objective.</strong> A smooth quadratic cost with terminal goal tracking, control effort, and control smoothness:
<span class="math display" id="eq:unicycle-objective">\[\begin{equation}
\hspace{-16mm} J = \frac12\Big(
w_{\text{pos}}\|x_N^{\text{pos}}-B^{\text{pos}}\|_2^2
+ w_\theta(\theta_N-\theta^\star)^2
+ \sum_{k=0}^{N-1} w_u \|u_k\|_2^2
+ \sum_{k=0}^{N-2} w_{\Delta u}\|u_{k+1}-u_k\|_2^2
\Big),
\tag{4.85}
\end{equation}\]</span>
with <span class="math inline">\(x_N^{\text{pos}}=[p_{x,N},p_{y,N}]^\top\)</span> the terminal position and <span class="math inline">\(B=[B_x,B_y,\theta^\star]^\top\)</span> the terminal pose. <span class="math inline">\(w_{\text{pos}}\)</span>, <span class="math inline">\(w_\theta\)</span>, <span class="math inline">\(w_u\)</span>, and <span class="math inline">\(w_{\Delta u}\)</span> are positive weights.</p>
<p><strong>Jacobians.</strong> We derive the gradient of the objective and the Jacobian of the constraints with respect to the stacked decision vector
<span class="math display">\[
z \;=\; \big[x_0^\top,\ u_0^\top,\ x_1^\top,\ u_1^\top,\ \dots,\ x_{N-1}^\top,\ u_{N-1}^\top,\ x_N^\top\big]^\top
\in \mathbb{R}^{(3+2)N+3},
\]</span>
where <span class="math inline">\(x_k=[p_{x,k},\,p_{y,k},\,\theta_k]^\top\)</span> and <span class="math inline">\(u_k=[v_k,\,\omega_k]^\top\)</span>.</p>
<p><strong>Objective gradient <span class="math inline">\(\nabla_z J\)</span>.</strong> Recall the objective function from <a href="model-based-plan-optimize.html#eq:unicycle-objective">(4.85)</a>. Let <span class="math inline">\(e_{\text{pos}} := x_N^{\text{pos}}-B^{\text{pos}}\)</span> and <span class="math inline">\(e_\theta := \theta_N-\theta^\star\)</span>, we have
<span class="math display">\[
\frac{\partial J}{\partial p_{x,N}} = w_{\text{pos}}\, e_{\text{pos},x},\qquad
\frac{\partial J}{\partial p_{y,N}} = w_{\text{pos}}\, e_{\text{pos},y},\qquad
\frac{\partial J}{\partial \theta_N} = w_\theta\, e_\theta.
\]</span></p>
<p>All other states <span class="math inline">\(x_k\)</span> for <span class="math inline">\(k&lt;N\)</span> do not appear in the objective, so
<span class="math display">\[
\frac{\partial J}{\partial x_k} = 0,\quad k=0,\dots,N-1.
\]</span></p>
<p>The control effort term in the objective has gradients w.r.t. controls:
<span class="math display">\[
J_u \;=\; \frac{1}{2}\sum_{k=0}^{N-1}  w_u \| u_k \|^2 \Rightarrow\;
\frac{\partial J_u}{\partial u_k} \;=\; w_u\,u_k
\quad\text{for }k=0,\dots,N-1.
\]</span></p>
<p>The control smoothness term in the objective reads:
<span class="math display">\[
J_{\Delta u} \;=\; \frac{1}{2}\sum_{k=0}^{N-2} w_{\Delta u}\,\|u_{k+1}-u_k\|_2^2.
\]</span>
By collecting contributions from the two adjacent differences that contain <span class="math inline">\(u_k\)</span>, we obtain the gradient
<span class="math display" id="eq:unicycle-objective-smoothness-gradient">\[
\frac{\partial J_{\Delta u}}{\partial u_k} =
\begin{cases}
w_{\Delta u}\,(u_0 - u_1), &amp; k=0,\\[4pt]
w_{\Delta u}\,(2u_k - u_{k-1} - u_{k+1}), &amp; k=1,\dots,N-2,\\[4pt]
w_{\Delta u}\,(u_{N-1} - u_{N-2}), &amp; k=N-1.
\end{cases}
\tag{4.86}
\]</span></p>
<p>Combining the above derivations, the only nonzero blocks of <span class="math inline">\(\nabla_z J\)</span> are:</p>
<ul>
<li><p>the <strong>terminal state block</strong> <span class="math inline">\(x_N\)</span>: entries shown above for <span class="math inline">\(p_{x,N},p_{y,N},\theta_N\)</span>;</p></li>
<li><p>the <strong>control blocks</strong> <span class="math inline">\(u_k\)</span>: <span class="math inline">\(w_u u_k\)</span> plus the smoothness terms <a href="model-based-plan-optimize.html#eq:unicycle-objective-smoothness-gradient">(4.86)</a>.</p></li>
</ul>
<p>All other entries are zero. Thus <span class="math inline">\(\nabla_z J\)</span> is extremely sparse.</p>
<p><strong>Constraint Jacobian <span class="math inline">\(\nabla_z g(z)\)</span>.</strong> We stack constraints as
<span class="math display">\[
g(z)=\begin{bmatrix}
g^{\text{init}}\\ g^{\text{dyn}}\\ g^{\text{obs}}
\end{bmatrix}
\in \mathbb{R}^{3+3N+(N+1)O}
\]</span>
in the order:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Initial condition</strong> <span class="math inline">\(g^{\text{init}} = x_0 - A = 0\)</span>.</p></li>
<li><p><strong>Dynamics</strong> for <span class="math inline">\(k=0,\dots,N-1\)</span> (forward Euler with step <span class="math inline">\(h\)</span>):
<span class="math display">\[
\begin{aligned}
g^{\text{dyn}}_{x,k}&amp;:= p_{x,k+1} - \big(p_{x,k} + h\,v_k\cos\theta_k\big) = 0,\\
g^{\text{dyn}}_{y,k}&amp;:= p_{y,k+1} - \big(p_{y,k} + h\,v_k\sin\theta_k\big) = 0,\\
g^{\text{dyn}}_{\theta,k}&amp;:= \theta_{k+1} - \big(\theta_k + h\,\omega_k\big) = 0.
\end{aligned}
\]</span></p></li>
<li><p><strong>Obstacle inequalities</strong> for each obstacle <span class="math inline">\(j=1,\dots,O\)</span> and each knot <span class="math inline">\(k=0,\dots,N\)</span>:
<span class="math display">\[
g^{\text{obs}}_{j,k} \;:=\; (r_j+\delta)^2 - \Big((p_{x,k}-c_{x,j})^2 + (p_{y,k}-c_{y,j})^2\Big) \;\le\; 0.
\]</span></p></li>
</ol>
<p>Below we list nonzero partial derivatives; all missing entries are <span class="math inline">\(0\)</span>.</p>
<p><strong>(i) Initial condition <span class="math inline">\(g^{\text{init}}=x_0-A\)</span></strong>
<span class="math display">\[
\frac{\partial g^{\text{init}}}{\partial x_0} = I_3.
\]</span></p>
<p><strong>(ii) Dynamics rows at time <span class="math inline">\(k\)</span></strong></p>
<ul>
<li><p><strong><span class="math inline">\(x\)</span>-row</strong> <span class="math inline">\(g^{\text{dyn}}_{x,k}\)</span>:
<span class="math display">\[
\frac{\partial g^{\text{dyn}}_{x,k}}{\partial p_{x,k+1}}=1,\quad
\frac{\partial g^{\text{dyn}}_{x,k}}{\partial p_{x,k}}=-1,\quad
\frac{\partial g^{\text{dyn}}_{x,k}}{\partial \theta_k}=h\,v_k\sin\theta_k,\quad
\frac{\partial g^{\text{dyn}}_{x,k}}{\partial v_k}=-h\cos\theta_k.
\]</span></p></li>
<li><p><strong><span class="math inline">\(y\)</span>-row</strong> <span class="math inline">\(g^{\text{dyn}}_{y,k}\)</span>:
<span class="math display">\[
\frac{\partial g^{\text{dyn}}_{y,k}}{\partial p_{y,k+1}}=1,\quad
\frac{\partial g^{\text{dyn}}_{y,k}}{\partial p_{y,k}}=-1,\quad
\frac{\partial g^{\text{dyn}}_{y,k}}{\partial \theta_k}=-h\,v_k\cos\theta_k,\quad
\frac{\partial g^{\text{dyn}}_{y,k}}{\partial v_k}=-h\sin\theta_k.
\]</span></p></li>
<li><p><strong>Heading row</strong> <span class="math inline">\(g^{\text{dyn}}_{\theta,k}\)</span>:
<span class="math display">\[
\frac{\partial g^{\text{dyn}}_{\theta,k}}{\partial \theta_{k+1}}=1,\quad
\frac{\partial g^{\text{dyn}}_{\theta,k}}{\partial \theta_k}=-1,\quad
\frac{\partial g^{\text{dyn}}_{\theta,k}}{\partial \omega_k}=-h.
\]</span></p></li>
</ul>
<p>All other partials in each row are zero. Each dynamics triple only touches the <strong>local block</strong> <span class="math inline">\((x_k,u_k,x_{k+1})\)</span>, yielding a <strong>banded, block-sparse</strong> Jacobian in time.</p>
<p><strong>(iii) Obstacle row for <span class="math inline">\((j,k)\)</span></strong>
<span class="math display">\[
g^{\text{obs}}_{j,k}=(r_j+\delta)^2 - \big((p_{x,k}-c_{x,j})^2+(p_{y,k}-c_{y,j})^2\big).
\]</span>
Nonzeros:
<span class="math display">\[
\frac{\partial g^{\text{obs}}_{j,k}}{\partial p_{x,k}}=-2\,(p_{x,k}-c_{x,j}),\qquad
\frac{\partial g^{\text{obs}}_{j,k}}{\partial p_{y,k}}=-2\,(p_{y,k}-c_{y,j}).
\]</span>
This row depends only on the position of <span class="math inline">\(x_k\)</span>, so each obstacle row touches exactly two columns <span class="math inline">\((p_{x,k},p_{y,k})\)</span>.</p>
<p>Combining the derivations above, we can see the Jacobian <span class="math inline">\(\nabla_z g\)</span> is block-banded in time.</p>
<ul>
<li><p>Initial block at <span class="math inline">\(x_0\)</span> is identity.</p></li>
<li><p>Each dynamics row touches <span class="math inline">\((x_k, u_k, x_{k+1})\)</span> with at most 4 nonzeros in the <span class="math inline">\(x\)</span>- and <span class="math inline">\(y\)</span>-rows and 3 nonzeros in the <span class="math inline">\(\theta\)</span>-row.</p></li>
<li><p>Each obstacle row touches only <span class="math inline">\((p_{x,k}, p_{y,k})\)</span>.</p></li>
</ul>
<p>Fig. <a href="model-based-plan-optimize.html#fig:unicycle-jacobian-sparsity">4.8</a> illustrates the sparsity structure of the constraint Jacobian.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-jacobian-sparsity"></span>
<img src="images/Model-based-optimization/jacobian-sparsity-unicycle.png" alt="Sparsity structure of constraint Jacobian." width="90%" />
<p class="caption">
Figure 4.8: Sparsity structure of constraint Jacobian.
</p>
</div>
<p>The following Python code snippet defines a problem class <code>UnicycleTO</code> with definitions of the objective, constraints, objective gradient, and constraints Jacobian. Note that the definition of the constraints Jacobian is highly involved because it defines the Jacobian as a sparse matrix with a predefined sparsity pattern. The solver can leverage this sparsity pattern to be highly efficient.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="model-based-plan-optimize.html#cb9-1" tabindex="-1"></a><span class="kw">class</span> UnicycleTO:</span>
<span id="cb9-2"><a href="model-based-plan-optimize.html#cb9-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, P: Params):</span>
<span id="cb9-3"><a href="model-based-plan-optimize.html#cb9-3" tabindex="-1"></a>        <span class="va">self</span>.P <span class="op">=</span> P</span>
<span id="cb9-4"><a href="model-based-plan-optimize.html#cb9-4" tabindex="-1"></a>        <span class="co"># --- Build bounds on variables ---</span></span>
<span id="cb9-5"><a href="model-based-plan-optimize.html#cb9-5" tabindex="-1"></a>        lb <span class="op">=</span> <span class="op">-</span>np.inf <span class="op">*</span> np.ones(P.Z_DIM)</span>
<span id="cb9-6"><a href="model-based-plan-optimize.html#cb9-6" tabindex="-1"></a>        ub <span class="op">=</span>  np.inf <span class="op">*</span> np.ones(P.Z_DIM)</span>
<span id="cb9-7"><a href="model-based-plan-optimize.html#cb9-7" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N):</span>
<span id="cb9-8"><a href="model-based-plan-optimize.html#cb9-8" tabindex="-1"></a>            iu <span class="op">=</span> idx_u(k, P)</span>
<span id="cb9-9"><a href="model-based-plan-optimize.html#cb9-9" tabindex="-1"></a>            lb[iu.start <span class="op">+</span> <span class="dv">0</span>] <span class="op">=</span> P.v_min</span>
<span id="cb9-10"><a href="model-based-plan-optimize.html#cb9-10" tabindex="-1"></a>            ub[iu.start <span class="op">+</span> <span class="dv">0</span>] <span class="op">=</span> P.v_max</span>
<span id="cb9-11"><a href="model-based-plan-optimize.html#cb9-11" tabindex="-1"></a>            lb[iu.start <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> P.w_min</span>
<span id="cb9-12"><a href="model-based-plan-optimize.html#cb9-12" tabindex="-1"></a>            ub[iu.start <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> P.w_max</span>
<span id="cb9-13"><a href="model-based-plan-optimize.html#cb9-13" tabindex="-1"></a>        <span class="va">self</span>.lb <span class="op">=</span> lb</span>
<span id="cb9-14"><a href="model-based-plan-optimize.html#cb9-14" tabindex="-1"></a>        <span class="va">self</span>.ub <span class="op">=</span> ub</span>
<span id="cb9-15"><a href="model-based-plan-optimize.html#cb9-15" tabindex="-1"></a></span>
<span id="cb9-16"><a href="model-based-plan-optimize.html#cb9-16" tabindex="-1"></a>        <span class="co"># --- Build bounds on constraints ---</span></span>
<span id="cb9-17"><a href="model-based-plan-optimize.html#cb9-17" tabindex="-1"></a>        cl <span class="op">=</span> np.zeros(P.M)</span>
<span id="cb9-18"><a href="model-based-plan-optimize.html#cb9-18" tabindex="-1"></a>        cu <span class="op">=</span> np.zeros(P.M)</span>
<span id="cb9-19"><a href="model-based-plan-optimize.html#cb9-19" tabindex="-1"></a>        <span class="co"># equalities: exactly 0</span></span>
<span id="cb9-20"><a href="model-based-plan-optimize.html#cb9-20" tabindex="-1"></a>        cl[:P.n_ceq] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb9-21"><a href="model-based-plan-optimize.html#cb9-21" tabindex="-1"></a>        cu[:P.n_ceq] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb9-22"><a href="model-based-plan-optimize.html#cb9-22" tabindex="-1"></a>        <span class="co"># inequalities: c &lt;= 0</span></span>
<span id="cb9-23"><a href="model-based-plan-optimize.html#cb9-23" tabindex="-1"></a>        cl[P.n_ceq:] <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb9-24"><a href="model-based-plan-optimize.html#cb9-24" tabindex="-1"></a>        cu[P.n_ceq:] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb9-25"><a href="model-based-plan-optimize.html#cb9-25" tabindex="-1"></a>        <span class="va">self</span>.cl <span class="op">=</span> cl</span>
<span id="cb9-26"><a href="model-based-plan-optimize.html#cb9-26" tabindex="-1"></a>        <span class="va">self</span>.cu <span class="op">=</span> cu</span>
<span id="cb9-27"><a href="model-based-plan-optimize.html#cb9-27" tabindex="-1"></a></span>
<span id="cb9-28"><a href="model-based-plan-optimize.html#cb9-28" tabindex="-1"></a>        <span class="co"># --- Precompute Jacobian sparsity (row, col) ---</span></span>
<span id="cb9-29"><a href="model-based-plan-optimize.html#cb9-29" tabindex="-1"></a>        <span class="va">self</span>.jac_rows, <span class="va">self</span>.jac_cols <span class="op">=</span> <span class="va">self</span>._build_jacobian_structure()</span>
<span id="cb9-30"><a href="model-based-plan-optimize.html#cb9-30" tabindex="-1"></a></span>
<span id="cb9-31"><a href="model-based-plan-optimize.html#cb9-31" tabindex="-1"></a>    <span class="co"># Objective</span></span>
<span id="cb9-32"><a href="model-based-plan-optimize.html#cb9-32" tabindex="-1"></a>    <span class="kw">def</span> objective(<span class="va">self</span>, z):</span>
<span id="cb9-33"><a href="model-based-plan-optimize.html#cb9-33" tabindex="-1"></a>        P <span class="op">=</span> <span class="va">self</span>.P</span>
<span id="cb9-34"><a href="model-based-plan-optimize.html#cb9-34" tabindex="-1"></a>        X, U <span class="op">=</span> unpack(z, P)</span>
<span id="cb9-35"><a href="model-based-plan-optimize.html#cb9-35" tabindex="-1"></a></span>
<span id="cb9-36"><a href="model-based-plan-optimize.html#cb9-36" tabindex="-1"></a>        <span class="co"># Terminal goal tracking</span></span>
<span id="cb9-37"><a href="model-based-plan-optimize.html#cb9-37" tabindex="-1"></a>        pos_err <span class="op">=</span> X[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>:<span class="dv">2</span>] <span class="op">-</span> P.B[<span class="dv">0</span>:<span class="dv">2</span>]</span>
<span id="cb9-38"><a href="model-based-plan-optimize.html#cb9-38" tabindex="-1"></a>        th_err  <span class="op">=</span> X[<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>]   <span class="op">-</span> P.B[<span class="dv">2</span>]</span>
<span id="cb9-39"><a href="model-based-plan-optimize.html#cb9-39" tabindex="-1"></a>        J_goal <span class="op">=</span> P.w_goal_pos <span class="op">*</span> np.dot(pos_err, pos_err) <span class="op">+</span> P.w_goal_theta <span class="op">*</span> (th_err<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb9-40"><a href="model-based-plan-optimize.html#cb9-40" tabindex="-1"></a></span>
<span id="cb9-41"><a href="model-based-plan-optimize.html#cb9-41" tabindex="-1"></a>        <span class="co"># Control effort</span></span>
<span id="cb9-42"><a href="model-based-plan-optimize.html#cb9-42" tabindex="-1"></a>        J_u <span class="op">=</span> P.w_u <span class="op">*</span> np.<span class="bu">sum</span>(U <span class="op">*</span> U)</span>
<span id="cb9-43"><a href="model-based-plan-optimize.html#cb9-43" tabindex="-1"></a></span>
<span id="cb9-44"><a href="model-based-plan-optimize.html#cb9-44" tabindex="-1"></a>        <span class="co"># Control smoothness</span></span>
<span id="cb9-45"><a href="model-based-plan-optimize.html#cb9-45" tabindex="-1"></a>        dU <span class="op">=</span> U[<span class="dv">1</span>:, :] <span class="op">-</span> U[:<span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb9-46"><a href="model-based-plan-optimize.html#cb9-46" tabindex="-1"></a>        J_du <span class="op">=</span> P.w_du <span class="op">*</span> np.<span class="bu">sum</span>(dU <span class="op">*</span> dU)</span>
<span id="cb9-47"><a href="model-based-plan-optimize.html#cb9-47" tabindex="-1"></a></span>
<span id="cb9-48"><a href="model-based-plan-optimize.html#cb9-48" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> (J_goal <span class="op">+</span> J_u <span class="op">+</span> J_du)</span>
<span id="cb9-49"><a href="model-based-plan-optimize.html#cb9-49" tabindex="-1"></a></span>
<span id="cb9-50"><a href="model-based-plan-optimize.html#cb9-50" tabindex="-1"></a>    <span class="co"># Gradient of objective</span></span>
<span id="cb9-51"><a href="model-based-plan-optimize.html#cb9-51" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, z):</span>
<span id="cb9-52"><a href="model-based-plan-optimize.html#cb9-52" tabindex="-1"></a>        P <span class="op">=</span> <span class="va">self</span>.P</span>
<span id="cb9-53"><a href="model-based-plan-optimize.html#cb9-53" tabindex="-1"></a>        X, U <span class="op">=</span> unpack(z, P)</span>
<span id="cb9-54"><a href="model-based-plan-optimize.html#cb9-54" tabindex="-1"></a>        grad <span class="op">=</span> np.zeros(P.Z_DIM)</span>
<span id="cb9-55"><a href="model-based-plan-optimize.html#cb9-55" tabindex="-1"></a></span>
<span id="cb9-56"><a href="model-based-plan-optimize.html#cb9-56" tabindex="-1"></a>        <span class="co"># Terminal contributions (no 0.5 after derivative: cancels 2)</span></span>
<span id="cb9-57"><a href="model-based-plan-optimize.html#cb9-57" tabindex="-1"></a>        pos_err <span class="op">=</span> X[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>:<span class="dv">2</span>] <span class="op">-</span> P.B[<span class="dv">0</span>:<span class="dv">2</span>]</span>
<span id="cb9-58"><a href="model-based-plan-optimize.html#cb9-58" tabindex="-1"></a>        th_err  <span class="op">=</span> X[<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>]   <span class="op">-</span> P.B[<span class="dv">2</span>]</span>
<span id="cb9-59"><a href="model-based-plan-optimize.html#cb9-59" tabindex="-1"></a>        gN <span class="op">=</span> np.array([P.w_goal_pos <span class="op">*</span> pos_err[<span class="dv">0</span>],</span>
<span id="cb9-60"><a href="model-based-plan-optimize.html#cb9-60" tabindex="-1"></a>                       P.w_goal_pos <span class="op">*</span> pos_err[<span class="dv">1</span>],</span>
<span id="cb9-61"><a href="model-based-plan-optimize.html#cb9-61" tabindex="-1"></a>                       P.w_goal_theta <span class="op">*</span> th_err])</span>
<span id="cb9-62"><a href="model-based-plan-optimize.html#cb9-62" tabindex="-1"></a>        grad[idx_x(P.N, P)] <span class="op">+=</span> gN</span>
<span id="cb9-63"><a href="model-based-plan-optimize.html#cb9-63" tabindex="-1"></a></span>
<span id="cb9-64"><a href="model-based-plan-optimize.html#cb9-64" tabindex="-1"></a>        <span class="co"># Control effort: 0.5 * 2 * w_u * u = w_u * u</span></span>
<span id="cb9-65"><a href="model-based-plan-optimize.html#cb9-65" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N):</span>
<span id="cb9-66"><a href="model-based-plan-optimize.html#cb9-66" tabindex="-1"></a>            iu <span class="op">=</span> idx_u(k, P)</span>
<span id="cb9-67"><a href="model-based-plan-optimize.html#cb9-67" tabindex="-1"></a>            grad[iu] <span class="op">+=</span> P.w_u <span class="op">*</span> U[k, :]</span>
<span id="cb9-68"><a href="model-based-plan-optimize.html#cb9-68" tabindex="-1"></a></span>
<span id="cb9-69"><a href="model-based-plan-optimize.html#cb9-69" tabindex="-1"></a>        <span class="co"># Control smoothness: 0.5 * w_du * sum ||u_{k+1}-u_k||^2</span></span>
<span id="cb9-70"><a href="model-based-plan-optimize.html#cb9-70" tabindex="-1"></a>        <span class="co"># d/d u_k:   -w_du*(u_{k+1}-u_k)  from (k,k+1)</span></span>
<span id="cb9-71"><a href="model-based-plan-optimize.html#cb9-71" tabindex="-1"></a>        <span class="co"># d/d u_{k+1}: +w_du*(u_{k+1}-u_k)</span></span>
<span id="cb9-72"><a href="model-based-plan-optimize.html#cb9-72" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb9-73"><a href="model-based-plan-optimize.html#cb9-73" tabindex="-1"></a>            du <span class="op">=</span> U[k <span class="op">+</span> <span class="dv">1</span>, :] <span class="op">-</span> U[k, :]</span>
<span id="cb9-74"><a href="model-based-plan-optimize.html#cb9-74" tabindex="-1"></a>            grad[idx_u(k, P)]     <span class="op">+=</span> <span class="op">-</span>P.w_du <span class="op">*</span> du</span>
<span id="cb9-75"><a href="model-based-plan-optimize.html#cb9-75" tabindex="-1"></a>            grad[idx_u(k <span class="op">+</span> <span class="dv">1</span>, P)] <span class="op">+=</span>  P.w_du <span class="op">*</span> du</span>
<span id="cb9-76"><a href="model-based-plan-optimize.html#cb9-76" tabindex="-1"></a></span>
<span id="cb9-77"><a href="model-based-plan-optimize.html#cb9-77" tabindex="-1"></a>        <span class="cf">return</span> grad</span>
<span id="cb9-78"><a href="model-based-plan-optimize.html#cb9-78" tabindex="-1"></a></span>
<span id="cb9-79"><a href="model-based-plan-optimize.html#cb9-79" tabindex="-1"></a>    <span class="co"># Constraints g(z)</span></span>
<span id="cb9-80"><a href="model-based-plan-optimize.html#cb9-80" tabindex="-1"></a>    <span class="kw">def</span> constraints(<span class="va">self</span>, z):</span>
<span id="cb9-81"><a href="model-based-plan-optimize.html#cb9-81" tabindex="-1"></a>        P <span class="op">=</span> <span class="va">self</span>.P</span>
<span id="cb9-82"><a href="model-based-plan-optimize.html#cb9-82" tabindex="-1"></a>        X, U <span class="op">=</span> unpack(z, P)</span>
<span id="cb9-83"><a href="model-based-plan-optimize.html#cb9-83" tabindex="-1"></a>        g <span class="op">=</span> np.zeros(P.M)</span>
<span id="cb9-84"><a href="model-based-plan-optimize.html#cb9-84" tabindex="-1"></a>        r <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-85"><a href="model-based-plan-optimize.html#cb9-85" tabindex="-1"></a></span>
<span id="cb9-86"><a href="model-based-plan-optimize.html#cb9-86" tabindex="-1"></a>        <span class="co"># Initial equality: X0 - A = 0</span></span>
<span id="cb9-87"><a href="model-based-plan-optimize.html#cb9-87" tabindex="-1"></a>        g[r:r<span class="op">+</span><span class="dv">3</span>] <span class="op">=</span> X[<span class="dv">0</span>, :] <span class="op">-</span> P.A</span>
<span id="cb9-88"><a href="model-based-plan-optimize.html#cb9-88" tabindex="-1"></a>        r <span class="op">+=</span> <span class="dv">3</span></span>
<span id="cb9-89"><a href="model-based-plan-optimize.html#cb9-89" tabindex="-1"></a></span>
<span id="cb9-90"><a href="model-based-plan-optimize.html#cb9-90" tabindex="-1"></a>        <span class="co"># Dynamics equalities</span></span>
<span id="cb9-91"><a href="model-based-plan-optimize.html#cb9-91" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N):</span>
<span id="cb9-92"><a href="model-based-plan-optimize.html#cb9-92" tabindex="-1"></a>            xk <span class="op">=</span> X[k, :]</span>
<span id="cb9-93"><a href="model-based-plan-optimize.html#cb9-93" tabindex="-1"></a>            uk <span class="op">=</span> U[k, :]</span>
<span id="cb9-94"><a href="model-based-plan-optimize.html#cb9-94" tabindex="-1"></a>            xnext <span class="op">=</span> f_disc(xk, uk, P.h)</span>
<span id="cb9-95"><a href="model-based-plan-optimize.html#cb9-95" tabindex="-1"></a>            g[r:r<span class="op">+</span><span class="dv">3</span>] <span class="op">=</span> X[k <span class="op">+</span> <span class="dv">1</span>, :] <span class="op">-</span> xnext</span>
<span id="cb9-96"><a href="model-based-plan-optimize.html#cb9-96" tabindex="-1"></a>            r <span class="op">+=</span> <span class="dv">3</span></span>
<span id="cb9-97"><a href="model-based-plan-optimize.html#cb9-97" tabindex="-1"></a></span>
<span id="cb9-98"><a href="model-based-plan-optimize.html#cb9-98" tabindex="-1"></a>        <span class="co"># Obstacle inequalities: (r+margin)^2 - ((px-cx)^2 + (py-cy)^2) &lt;= 0</span></span>
<span id="cb9-99"><a href="model-based-plan-optimize.html#cb9-99" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb9-100"><a href="model-based-plan-optimize.html#cb9-100" tabindex="-1"></a>            px, py <span class="op">=</span> X[k, <span class="dv">0</span>], X[k, <span class="dv">1</span>]</span>
<span id="cb9-101"><a href="model-based-plan-optimize.html#cb9-101" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(P.nObs):</span>
<span id="cb9-102"><a href="model-based-plan-optimize.html#cb9-102" tabindex="-1"></a>                cx, cy, r0 <span class="op">=</span> P.obstacles[j]</span>
<span id="cb9-103"><a href="model-based-plan-optimize.html#cb9-103" tabindex="-1"></a>                r_eff <span class="op">=</span> r0 <span class="op">+</span> P.safety_margin</span>
<span id="cb9-104"><a href="model-based-plan-optimize.html#cb9-104" tabindex="-1"></a>                g[r] <span class="op">=</span> (r_eff <span class="op">**</span> <span class="dv">2</span>) <span class="op">-</span> ((px <span class="op">-</span> cx) <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> (py <span class="op">-</span> cy) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb9-105"><a href="model-based-plan-optimize.html#cb9-105" tabindex="-1"></a>                r <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-106"><a href="model-based-plan-optimize.html#cb9-106" tabindex="-1"></a></span>
<span id="cb9-107"><a href="model-based-plan-optimize.html#cb9-107" tabindex="-1"></a>        <span class="cf">return</span> g</span>
<span id="cb9-108"><a href="model-based-plan-optimize.html#cb9-108" tabindex="-1"></a></span>
<span id="cb9-109"><a href="model-based-plan-optimize.html#cb9-109" tabindex="-1"></a>    <span class="co"># Jacobian sparsity</span></span>
<span id="cb9-110"><a href="model-based-plan-optimize.html#cb9-110" tabindex="-1"></a>    <span class="kw">def</span> jacobianstructure(<span class="va">self</span>):</span>
<span id="cb9-111"><a href="model-based-plan-optimize.html#cb9-111" tabindex="-1"></a>        <span class="cf">return</span> (np.array(<span class="va">self</span>.jac_rows, dtype<span class="op">=</span><span class="bu">int</span>),</span>
<span id="cb9-112"><a href="model-based-plan-optimize.html#cb9-112" tabindex="-1"></a>                np.array(<span class="va">self</span>.jac_cols, dtype<span class="op">=</span><span class="bu">int</span>))</span>
<span id="cb9-113"><a href="model-based-plan-optimize.html#cb9-113" tabindex="-1"></a></span>
<span id="cb9-114"><a href="model-based-plan-optimize.html#cb9-114" tabindex="-1"></a>    <span class="co"># Jacobian values (in the same order as jacobianstructure)</span></span>
<span id="cb9-115"><a href="model-based-plan-optimize.html#cb9-115" tabindex="-1"></a>    <span class="kw">def</span> jacobian(<span class="va">self</span>, z):</span>
<span id="cb9-116"><a href="model-based-plan-optimize.html#cb9-116" tabindex="-1"></a>        P <span class="op">=</span> <span class="va">self</span>.P</span>
<span id="cb9-117"><a href="model-based-plan-optimize.html#cb9-117" tabindex="-1"></a>        X, U <span class="op">=</span> unpack(z, P)</span>
<span id="cb9-118"><a href="model-based-plan-optimize.html#cb9-118" tabindex="-1"></a>        vals <span class="op">=</span> []</span>
<span id="cb9-119"><a href="model-based-plan-optimize.html#cb9-119" tabindex="-1"></a>        r <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-120"><a href="model-based-plan-optimize.html#cb9-120" tabindex="-1"></a></span>
<span id="cb9-121"><a href="model-based-plan-optimize.html#cb9-121" tabindex="-1"></a>        <span class="co"># Initial eq: d/dX0 is identity (one per row)</span></span>
<span id="cb9-122"><a href="model-based-plan-optimize.html#cb9-122" tabindex="-1"></a>        <span class="co"># rows r..r+2 with columns X0(px,py,th)</span></span>
<span id="cb9-123"><a href="model-based-plan-optimize.html#cb9-123" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb9-124"><a href="model-based-plan-optimize.html#cb9-124" tabindex="-1"></a>            vals.append(<span class="fl">1.0</span>)</span>
<span id="cb9-125"><a href="model-based-plan-optimize.html#cb9-125" tabindex="-1"></a>        r <span class="op">+=</span> <span class="dv">3</span></span>
<span id="cb9-126"><a href="model-based-plan-optimize.html#cb9-126" tabindex="-1"></a></span>
<span id="cb9-127"><a href="model-based-plan-optimize.html#cb9-127" tabindex="-1"></a>        <span class="co"># Dynamics eqs</span></span>
<span id="cb9-128"><a href="model-based-plan-optimize.html#cb9-128" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N):</span>
<span id="cb9-129"><a href="model-based-plan-optimize.html#cb9-129" tabindex="-1"></a>            xk <span class="op">=</span> X[k, :]</span>
<span id="cb9-130"><a href="model-based-plan-optimize.html#cb9-130" tabindex="-1"></a>            uk <span class="op">=</span> U[k, :]</span>
<span id="cb9-131"><a href="model-based-plan-optimize.html#cb9-131" tabindex="-1"></a>            th <span class="op">=</span> xk[<span class="dv">2</span>]</span>
<span id="cb9-132"><a href="model-based-plan-optimize.html#cb9-132" tabindex="-1"></a>            v  <span class="op">=</span> uk[<span class="dv">0</span>]</span>
<span id="cb9-133"><a href="model-based-plan-optimize.html#cb9-133" tabindex="-1"></a></span>
<span id="cb9-134"><a href="model-based-plan-optimize.html#cb9-134" tabindex="-1"></a>            <span class="co"># Row r: g1 = x_{k+1,px} - [px_k + h v cos(th_k)]</span></span>
<span id="cb9-135"><a href="model-based-plan-optimize.html#cb9-135" tabindex="-1"></a>            <span class="co"># d wrt X_{k+1,px}</span></span>
<span id="cb9-136"><a href="model-based-plan-optimize.html#cb9-136" tabindex="-1"></a>            vals.append(<span class="fl">1.0</span>)</span>
<span id="cb9-137"><a href="model-based-plan-optimize.html#cb9-137" tabindex="-1"></a>            <span class="co"># d wrt X_k,px</span></span>
<span id="cb9-138"><a href="model-based-plan-optimize.html#cb9-138" tabindex="-1"></a>            vals.append(<span class="op">-</span><span class="fl">1.0</span>)</span>
<span id="cb9-139"><a href="model-based-plan-optimize.html#cb9-139" tabindex="-1"></a>            <span class="co"># d wrt X_k,theta ( + h v sin(th) )</span></span>
<span id="cb9-140"><a href="model-based-plan-optimize.html#cb9-140" tabindex="-1"></a>            vals.append(P.h <span class="op">*</span> v <span class="op">*</span> np.sin(th))</span>
<span id="cb9-141"><a href="model-based-plan-optimize.html#cb9-141" tabindex="-1"></a>            <span class="co"># d wrt U_k,v ( - h cos(th) )</span></span>
<span id="cb9-142"><a href="model-based-plan-optimize.html#cb9-142" tabindex="-1"></a>            vals.append(<span class="op">-</span>P.h <span class="op">*</span> np.cos(th))</span>
<span id="cb9-143"><a href="model-based-plan-optimize.html#cb9-143" tabindex="-1"></a></span>
<span id="cb9-144"><a href="model-based-plan-optimize.html#cb9-144" tabindex="-1"></a>            <span class="co"># Row r+1: g2 = x_{k+1,py} - [py_k + h v sin(th_k)]</span></span>
<span id="cb9-145"><a href="model-based-plan-optimize.html#cb9-145" tabindex="-1"></a>            vals.append(<span class="fl">1.0</span>)                  <span class="co"># d wrt X_{k+1,py}</span></span>
<span id="cb9-146"><a href="model-based-plan-optimize.html#cb9-146" tabindex="-1"></a>            vals.append(<span class="op">-</span><span class="fl">1.0</span>)                 <span class="co"># d wrt X_k,py</span></span>
<span id="cb9-147"><a href="model-based-plan-optimize.html#cb9-147" tabindex="-1"></a>            vals.append(<span class="op">-</span>P.h <span class="op">*</span> v <span class="op">*</span> np.cos(th))<span class="co"># d wrt X_k,theta</span></span>
<span id="cb9-148"><a href="model-based-plan-optimize.html#cb9-148" tabindex="-1"></a>            vals.append(<span class="op">-</span>P.h <span class="op">*</span> np.sin(th))    <span class="co"># d wrt U_k,v</span></span>
<span id="cb9-149"><a href="model-based-plan-optimize.html#cb9-149" tabindex="-1"></a></span>
<span id="cb9-150"><a href="model-based-plan-optimize.html#cb9-150" tabindex="-1"></a>            <span class="co"># Row r+2: g3 = x_{k+1,th} - [th_k + h w_k]</span></span>
<span id="cb9-151"><a href="model-based-plan-optimize.html#cb9-151" tabindex="-1"></a>            vals.append(<span class="fl">1.0</span>)   <span class="co"># d wrt X_{k+1,theta}</span></span>
<span id="cb9-152"><a href="model-based-plan-optimize.html#cb9-152" tabindex="-1"></a>            vals.append(<span class="op">-</span><span class="fl">1.0</span>)  <span class="co"># d wrt X_k,theta</span></span>
<span id="cb9-153"><a href="model-based-plan-optimize.html#cb9-153" tabindex="-1"></a>            vals.append(<span class="op">-</span>P.h)  <span class="co"># d wrt U_k,omega</span></span>
<span id="cb9-154"><a href="model-based-plan-optimize.html#cb9-154" tabindex="-1"></a></span>
<span id="cb9-155"><a href="model-based-plan-optimize.html#cb9-155" tabindex="-1"></a>            r <span class="op">+=</span> <span class="dv">3</span></span>
<span id="cb9-156"><a href="model-based-plan-optimize.html#cb9-156" tabindex="-1"></a></span>
<span id="cb9-157"><a href="model-based-plan-optimize.html#cb9-157" tabindex="-1"></a>        <span class="co"># Obstacle inequalities</span></span>
<span id="cb9-158"><a href="model-based-plan-optimize.html#cb9-158" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb9-159"><a href="model-based-plan-optimize.html#cb9-159" tabindex="-1"></a>            px, py <span class="op">=</span> X[k, <span class="dv">0</span>], X[k, <span class="dv">1</span>]</span>
<span id="cb9-160"><a href="model-based-plan-optimize.html#cb9-160" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(P.nObs):</span>
<span id="cb9-161"><a href="model-based-plan-optimize.html#cb9-161" tabindex="-1"></a>                cx, cy, _ <span class="op">=</span> P.obstacles[j]</span>
<span id="cb9-162"><a href="model-based-plan-optimize.html#cb9-162" tabindex="-1"></a>                <span class="co"># d/d px_k: -2(px - cx)</span></span>
<span id="cb9-163"><a href="model-based-plan-optimize.html#cb9-163" tabindex="-1"></a>                vals.append(<span class="op">-</span><span class="fl">2.0</span> <span class="op">*</span> (px <span class="op">-</span> cx))</span>
<span id="cb9-164"><a href="model-based-plan-optimize.html#cb9-164" tabindex="-1"></a>                <span class="co"># d/d py_k: -2(py - cy)</span></span>
<span id="cb9-165"><a href="model-based-plan-optimize.html#cb9-165" tabindex="-1"></a>                vals.append(<span class="op">-</span><span class="fl">2.0</span> <span class="op">*</span> (py <span class="op">-</span> cy))</span>
<span id="cb9-166"><a href="model-based-plan-optimize.html#cb9-166" tabindex="-1"></a>                r <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-167"><a href="model-based-plan-optimize.html#cb9-167" tabindex="-1"></a></span>
<span id="cb9-168"><a href="model-based-plan-optimize.html#cb9-168" tabindex="-1"></a>        <span class="cf">return</span> np.array(vals, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb9-169"><a href="model-based-plan-optimize.html#cb9-169" tabindex="-1"></a></span>
<span id="cb9-170"><a href="model-based-plan-optimize.html#cb9-170" tabindex="-1"></a>    <span class="co"># --- Internal: build Jacobian sparsity pattern once ---</span></span>
<span id="cb9-171"><a href="model-based-plan-optimize.html#cb9-171" tabindex="-1"></a>    <span class="kw">def</span> _build_jacobian_structure(<span class="va">self</span>):</span>
<span id="cb9-172"><a href="model-based-plan-optimize.html#cb9-172" tabindex="-1"></a>        P <span class="op">=</span> <span class="va">self</span>.P</span>
<span id="cb9-173"><a href="model-based-plan-optimize.html#cb9-173" tabindex="-1"></a>        rows <span class="op">=</span> []</span>
<span id="cb9-174"><a href="model-based-plan-optimize.html#cb9-174" tabindex="-1"></a>        cols <span class="op">=</span> []</span>
<span id="cb9-175"><a href="model-based-plan-optimize.html#cb9-175" tabindex="-1"></a>        r <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-176"><a href="model-based-plan-optimize.html#cb9-176" tabindex="-1"></a></span>
<span id="cb9-177"><a href="model-based-plan-optimize.html#cb9-177" tabindex="-1"></a>        <span class="co"># Initial equality: g0..g2 depend on X0(px,py,th) diagonally</span></span>
<span id="cb9-178"><a href="model-based-plan-optimize.html#cb9-178" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb9-179"><a href="model-based-plan-optimize.html#cb9-179" tabindex="-1"></a>            rows.append(r <span class="op">+</span> i)</span>
<span id="cb9-180"><a href="model-based-plan-optimize.html#cb9-180" tabindex="-1"></a>            cols.append(idx_x(<span class="dv">0</span>, P).start <span class="op">+</span> i)</span>
<span id="cb9-181"><a href="model-based-plan-optimize.html#cb9-181" tabindex="-1"></a>        r <span class="op">+=</span> <span class="dv">3</span></span>
<span id="cb9-182"><a href="model-based-plan-optimize.html#cb9-182" tabindex="-1"></a></span>
<span id="cb9-183"><a href="model-based-plan-optimize.html#cb9-183" tabindex="-1"></a>        <span class="co"># Dynamics equalities</span></span>
<span id="cb9-184"><a href="model-based-plan-optimize.html#cb9-184" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N):</span>
<span id="cb9-185"><a href="model-based-plan-optimize.html#cb9-185" tabindex="-1"></a>            ixk   <span class="op">=</span> idx_x(k, P)</span>
<span id="cb9-186"><a href="model-based-plan-optimize.html#cb9-186" tabindex="-1"></a>            ixkp1 <span class="op">=</span> idx_x(k <span class="op">+</span> <span class="dv">1</span>, P)</span>
<span id="cb9-187"><a href="model-based-plan-optimize.html#cb9-187" tabindex="-1"></a>            iuk   <span class="op">=</span> idx_u(k, P)</span>
<span id="cb9-188"><a href="model-based-plan-optimize.html#cb9-188" tabindex="-1"></a></span>
<span id="cb9-189"><a href="model-based-plan-optimize.html#cb9-189" tabindex="-1"></a>            <span class="co"># Row r (px)</span></span>
<span id="cb9-190"><a href="model-based-plan-optimize.html#cb9-190" tabindex="-1"></a>            rows.extend([r, r, r, r])</span>
<span id="cb9-191"><a href="model-based-plan-optimize.html#cb9-191" tabindex="-1"></a>            cols.extend([</span>
<span id="cb9-192"><a href="model-based-plan-optimize.html#cb9-192" tabindex="-1"></a>                ixkp1.start <span class="op">+</span> <span class="dv">0</span>,   <span class="co"># X_{k+1,px}</span></span>
<span id="cb9-193"><a href="model-based-plan-optimize.html#cb9-193" tabindex="-1"></a>                ixk.start   <span class="op">+</span> <span class="dv">0</span>,   <span class="co"># X_k,px</span></span>
<span id="cb9-194"><a href="model-based-plan-optimize.html#cb9-194" tabindex="-1"></a>                ixk.start   <span class="op">+</span> <span class="dv">2</span>,   <span class="co"># X_k,theta</span></span>
<span id="cb9-195"><a href="model-based-plan-optimize.html#cb9-195" tabindex="-1"></a>                iuk.start   <span class="op">+</span> <span class="dv">0</span>    <span class="co"># U_k,v</span></span>
<span id="cb9-196"><a href="model-based-plan-optimize.html#cb9-196" tabindex="-1"></a>            ])</span>
<span id="cb9-197"><a href="model-based-plan-optimize.html#cb9-197" tabindex="-1"></a>            <span class="co"># Row r+1 (py)</span></span>
<span id="cb9-198"><a href="model-based-plan-optimize.html#cb9-198" tabindex="-1"></a>            rows.extend([r <span class="op">+</span> <span class="dv">1</span>, r <span class="op">+</span> <span class="dv">1</span>, r <span class="op">+</span> <span class="dv">1</span>, r <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb9-199"><a href="model-based-plan-optimize.html#cb9-199" tabindex="-1"></a>            cols.extend([</span>
<span id="cb9-200"><a href="model-based-plan-optimize.html#cb9-200" tabindex="-1"></a>                ixkp1.start <span class="op">+</span> <span class="dv">1</span>,   <span class="co"># X_{k+1,py}</span></span>
<span id="cb9-201"><a href="model-based-plan-optimize.html#cb9-201" tabindex="-1"></a>                ixk.start   <span class="op">+</span> <span class="dv">1</span>,   <span class="co"># X_k,py</span></span>
<span id="cb9-202"><a href="model-based-plan-optimize.html#cb9-202" tabindex="-1"></a>                ixk.start   <span class="op">+</span> <span class="dv">2</span>,   <span class="co"># X_k,theta</span></span>
<span id="cb9-203"><a href="model-based-plan-optimize.html#cb9-203" tabindex="-1"></a>                iuk.start   <span class="op">+</span> <span class="dv">0</span>    <span class="co"># U_k,v</span></span>
<span id="cb9-204"><a href="model-based-plan-optimize.html#cb9-204" tabindex="-1"></a>            ])</span>
<span id="cb9-205"><a href="model-based-plan-optimize.html#cb9-205" tabindex="-1"></a>            <span class="co"># Row r+2 (theta)</span></span>
<span id="cb9-206"><a href="model-based-plan-optimize.html#cb9-206" tabindex="-1"></a>            rows.extend([r <span class="op">+</span> <span class="dv">2</span>, r <span class="op">+</span> <span class="dv">2</span>, r <span class="op">+</span> <span class="dv">2</span>])</span>
<span id="cb9-207"><a href="model-based-plan-optimize.html#cb9-207" tabindex="-1"></a>            cols.extend([</span>
<span id="cb9-208"><a href="model-based-plan-optimize.html#cb9-208" tabindex="-1"></a>                ixkp1.start <span class="op">+</span> <span class="dv">2</span>,   <span class="co"># X_{k+1,theta}</span></span>
<span id="cb9-209"><a href="model-based-plan-optimize.html#cb9-209" tabindex="-1"></a>                ixk.start   <span class="op">+</span> <span class="dv">2</span>,   <span class="co"># X_k,theta</span></span>
<span id="cb9-210"><a href="model-based-plan-optimize.html#cb9-210" tabindex="-1"></a>                iuk.start   <span class="op">+</span> <span class="dv">1</span>    <span class="co"># U_k,omega</span></span>
<span id="cb9-211"><a href="model-based-plan-optimize.html#cb9-211" tabindex="-1"></a>            ])</span>
<span id="cb9-212"><a href="model-based-plan-optimize.html#cb9-212" tabindex="-1"></a>            r <span class="op">+=</span> <span class="dv">3</span></span>
<span id="cb9-213"><a href="model-based-plan-optimize.html#cb9-213" tabindex="-1"></a></span>
<span id="cb9-214"><a href="model-based-plan-optimize.html#cb9-214" tabindex="-1"></a>        <span class="co"># Obstacle inequalities: each depends only on px_k, py_k</span></span>
<span id="cb9-215"><a href="model-based-plan-optimize.html#cb9-215" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(P.N <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb9-216"><a href="model-based-plan-optimize.html#cb9-216" tabindex="-1"></a>            ixk <span class="op">=</span> idx_x(k, P)</span>
<span id="cb9-217"><a href="model-based-plan-optimize.html#cb9-217" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(P.nObs):</span>
<span id="cb9-218"><a href="model-based-plan-optimize.html#cb9-218" tabindex="-1"></a>                rows.extend([r, r])</span>
<span id="cb9-219"><a href="model-based-plan-optimize.html#cb9-219" tabindex="-1"></a>                cols.extend([ixk.start <span class="op">+</span> <span class="dv">0</span>, ixk.start <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb9-220"><a href="model-based-plan-optimize.html#cb9-220" tabindex="-1"></a>                r <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb9-221"><a href="model-based-plan-optimize.html#cb9-221" tabindex="-1"></a></span>
<span id="cb9-222"><a href="model-based-plan-optimize.html#cb9-222" tabindex="-1"></a>        <span class="cf">assert</span> r <span class="op">==</span> P.M, <span class="st">&quot;Jacobian structure row count mismatch&quot;</span></span>
<span id="cb9-223"><a href="model-based-plan-optimize.html#cb9-223" tabindex="-1"></a>        <span class="cf">return</span> rows, cols</span></code></pre></div>
<p>After defining the problem class, we pass it to the interface of IPOPT using the folowing snippet.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="model-based-plan-optimize.html#cb10-1" tabindex="-1"></a><span class="co"># Initial guess</span></span>
<span id="cb10-2"><a href="model-based-plan-optimize.html#cb10-2" tabindex="-1"></a>z0 <span class="op">=</span> initial_guess(P)</span>
<span id="cb10-3"><a href="model-based-plan-optimize.html#cb10-3" tabindex="-1"></a></span>
<span id="cb10-4"><a href="model-based-plan-optimize.html#cb10-4" tabindex="-1"></a><span class="co"># Problem + IPOPT</span></span>
<span id="cb10-5"><a href="model-based-plan-optimize.html#cb10-5" tabindex="-1"></a>problem <span class="op">=</span> UnicycleTO(P)</span>
<span id="cb10-6"><a href="model-based-plan-optimize.html#cb10-6" tabindex="-1"></a>nlp <span class="op">=</span> cyipopt.Problem(</span>
<span id="cb10-7"><a href="model-based-plan-optimize.html#cb10-7" tabindex="-1"></a>    n<span class="op">=</span>P.Z_DIM, m<span class="op">=</span>P.M,</span>
<span id="cb10-8"><a href="model-based-plan-optimize.html#cb10-8" tabindex="-1"></a>    problem_obj<span class="op">=</span>problem,</span>
<span id="cb10-9"><a href="model-based-plan-optimize.html#cb10-9" tabindex="-1"></a>    lb<span class="op">=</span>problem.lb, ub<span class="op">=</span>problem.ub,</span>
<span id="cb10-10"><a href="model-based-plan-optimize.html#cb10-10" tabindex="-1"></a>    cl<span class="op">=</span>problem.cl, cu<span class="op">=</span>problem.cu</span>
<span id="cb10-11"><a href="model-based-plan-optimize.html#cb10-11" tabindex="-1"></a>)</span>
<span id="cb10-12"><a href="model-based-plan-optimize.html#cb10-12" tabindex="-1"></a></span>
<span id="cb10-13"><a href="model-based-plan-optimize.html#cb10-13" tabindex="-1"></a><span class="co"># Options (tweak as desired)</span></span>
<span id="cb10-14"><a href="model-based-plan-optimize.html#cb10-14" tabindex="-1"></a>nlp.add_option(<span class="st">&quot;tol&quot;</span>, <span class="fl">1e-6</span>)</span>
<span id="cb10-15"><a href="model-based-plan-optimize.html#cb10-15" tabindex="-1"></a>nlp.add_option(<span class="st">&quot;dual_inf_tol&quot;</span>, <span class="fl">1e-6</span>)</span>
<span id="cb10-16"><a href="model-based-plan-optimize.html#cb10-16" tabindex="-1"></a>nlp.add_option(<span class="st">&quot;constr_viol_tol&quot;</span>, <span class="fl">1e-6</span>)</span>
<span id="cb10-17"><a href="model-based-plan-optimize.html#cb10-17" tabindex="-1"></a>nlp.add_option(<span class="st">&quot;compl_inf_tol&quot;</span>, <span class="fl">1e-6</span>)</span>
<span id="cb10-18"><a href="model-based-plan-optimize.html#cb10-18" tabindex="-1"></a>nlp.add_option(<span class="st">&quot;max_iter&quot;</span>, <span class="dv">2000</span>)</span>
<span id="cb10-19"><a href="model-based-plan-optimize.html#cb10-19" tabindex="-1"></a>nlp.add_option(<span class="st">&quot;hessian_approximation&quot;</span>, <span class="st">&quot;limited-memory&quot;</span>)</span>
<span id="cb10-20"><a href="model-based-plan-optimize.html#cb10-20" tabindex="-1"></a>nlp.add_option(<span class="st">&quot;print_level&quot;</span>, <span class="dv">5</span>)</span>
<span id="cb10-21"><a href="model-based-plan-optimize.html#cb10-21" tabindex="-1"></a></span>
<span id="cb10-22"><a href="model-based-plan-optimize.html#cb10-22" tabindex="-1"></a>z_star, info <span class="op">=</span> nlp.solve(z0)</span></code></pre></div>
<p>In the case of three obstacles (<span class="math inline">\(\delta=0\)</span>), we obtain the trajectory shown in Fig. <a href="model-based-plan-optimize.html#fig:unicycle-to-ipopt-all-zero">4.9</a> with an all-zero initialization; the trajectory shown in Fig. <a href="model-based-plan-optimize.html#fig:unicycle-to-ipopt-straight">4.10</a> with a straight-line initialization; and the trajectory shown in Fig. @ref(fig: unicycle-to-ipopt-random) with a random initialization.</p>
<p>You can play with the full code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/to_unicycle_ipopt.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-to-ipopt-all-zero"></span>
<img src="images/Model-based-optimization/to_unicycle_ipopt_all-zero.png" alt="Trajectory optimization for unicyle using IPOPT (all-zero initialization)." width="90%" />
<p class="caption">
Figure 4.9: Trajectory optimization for unicyle using IPOPT (all-zero initialization).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-to-ipopt-straight"></span>
<img src="images/Model-based-optimization/to_unicycle_ipopt_straight.png" alt="Trajectory optimization for unicyle using IPOPT (straight-line initialization)." width="90%" />
<p class="caption">
Figure 4.10: Trajectory optimization for unicyle using IPOPT (straight-line initialization).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unicycle-to-ipopt-random"></span>
<img src="images/Model-based-optimization/to_unicycle_ipopt_random-init.png" alt="Trajectory optimization for unicyle using IPOPT (random initialization)." width="90%" />
<p class="caption">
Figure 4.11: Trajectory optimization for unicyle using IPOPT (random initialization).
</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="mpc" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Model Predictive Control<a href="model-based-plan-optimize.html#mpc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Trajectory optimization (TO) computes (locally or globally) optimal trajectories that are dynamically feasible with respect to a chosen discrete-time transcription and satisfy system constraints at the discretization grid. As a consequence, the resulting plan is open loop: it neither accounts for disturbances nor provides feedback. A common remedy is LQR trajectory tracking (Section <a href="model-based-plan-optimize.html#lqr-tracking">4.2</a>), which linearizes the dynamics along the nominal trajectory to synthesize a local feedback controller. While effective near the nominal path, this approach can struggle in the presence of obstacles or whenever the reference trajectory itself must change over time.</p>
<p>Model predictive control (MPC) addresses these limitations by turning TO into a receding-horizon feedback policy. At each control step, MPC resolves a finite-horizon optimal control problem with the current measured state as the initial condition, applies only the first control action, and then repeats the procedure at the next step. This closes the loop, providing robustness to model mismatch and disturbances while naturally adapting the trajectory as the environment or task evolves. In this section we introduce the core ideas behind MPC; for a comprehensive treatment, see <span class="citation">(<a href="#ref-rawlings2020model">Rawlings, Mayne, and Diehl 2020</a>)</span>.</p>
<p><strong>Dynamics.</strong> Let us consider a discrete-time dynamical system
<span class="math display" id="eq:mpc-dynamics">\[\begin{equation}
s_{t+1} = f(s_t, a_t, w_t),
\tag{4.87}
\end{equation}\]</span>
where <span class="math inline">\(s_t \in \mathbb{R}^n\)</span> represents the state, <span class="math inline">\(a_t \in \mathbb{R}^m\)</span> represents the control/action, and <span class="math inline">\(w_t \in \mathbb{R}^d\)</span> represents a stochastic disturbance. With slight abuse of notation, let us use
<span class="math display">\[
s_{t+1} = f (s_t, a_t) \equiv f (s_t, a_t, 0)
\]</span>
to denote the system dynamics without the disturbance.</p>
<blockquote>
<p>Here we have used <span class="math inline">\((s_t, a_t)\)</span> to represent the state and control. This departs from the notation <a href="model-based-plan-optimize.html#eq:planning-discrete-dynamics">(4.1)</a> used in the beginning. The reason for doing so will become clear soon.</p>
</blockquote>
<p><strong>Trajectory Optimization.</strong> At time step <span class="math inline">\(t\)</span>, let the system’s state be <span class="math inline">\(s_t\)</span> as given (e.g., measured from sensor data). MPC solves the following open-loop TO problem, adapted from <a href="model-based-plan-optimize.html#eq:to-general">(4.29)</a>:
<span class="math display" id="eq:to-general-mpc">\[\begin{equation}
\begin{split}
\min_{\{x_k,u_k\}} \quad &amp;
\Phi(x_N) + \sum_{k=0}^{N-1} \ell_k(x_k,u_k) \\[2mm]
\text{s.t.}\quad &amp;
x_{k+1} = f (x_k,u_k), \qquad t=0,\dots,N-1,\\
&amp; \boxed{x_0 = s_t} \ \ \text{(given)},\\
&amp; x_k \in \mathcal X_k,\quad u_k \in \mathcal U_k \quad \text{(bounds)},\\
&amp; g_k(x_k,u_k) \le 0,\quad h_k(x_k,u_k)=0 \quad \text{(path/terminal constraints).}
\end{split}
\tag{4.88}
\end{equation}\]</span>
Notice that in <a href="model-based-plan-optimize.html#eq:to-general-mpc">(4.88)</a>:</p>
<ul>
<li><p>We used <span class="math inline">\(k\)</span> to denote the time step in the TO problem, in contrast to <span class="math inline">\(t\)</span> in the system dynamics.</p></li>
<li><p>We used <span class="math inline">\((x_k,u_k)\)</span> to denote the state and control in the TO problem, as opposed to <span class="math inline">\((s_t, a_t)\)</span> in the system dynamics.</p></li>
<li><p>We enforce the TO problem starts at <span class="math inline">\(x_0 = s_t\)</span>, i.e., the initial state in trajectory planning aligns with the current system state at time <span class="math inline">\(t\)</span>.</p></li>
</ul>
<p><strong>Receding Horizon Control.</strong> Let
<span class="math display" id="eq:mpc-solution-t">\[\begin{equation}
(x_0^\star, u_0^\star, x_1^\star, u_1^\star, \dots, x_{N-1}^\star, u_{N-1}^\star, x_N^\star)
\tag{4.89}
\end{equation}\]</span>
be an optimal solution of the TO problem <a href="model-based-plan-optimize.html#eq:to-general-mpc">(4.88)</a> (e.g., obtained from IPOPT). Instead of executing the entire sequence of optimal controls <span class="math inline">\((u_0^\star, u_1^\star, \dots,  u_{N-1}^\star)\)</span>, MPC will only execute the first element. Formally, the actual control action applied to the system, denoted as <span class="math inline">\(a_t\)</span>, is
<span class="math display" id="eq:mpc-receding">\[\begin{equation}
a_t = u_0^\star.
\tag{4.90}
\end{equation}\]</span>
Applying <span class="math inline">\(a_t = u_0^\star\)</span> to the real dynamics <a href="model-based-plan-optimize.html#eq:mpc-dynamics">(4.87)</a>, the system will step into a new state <span class="math inline">\(s_{t+1}\)</span> at time <span class="math inline">\(t+1\)</span>:
<span class="math display">\[
s_{t+1} = f(s_t, u_0^\star, w_t).
\]</span>
Then, MPC solves a new TO problem that is exactly the same as the problem <a href="model-based-plan-optimize.html#eq:to-general-mpc">(4.88)</a> at time <span class="math inline">\(t\)</span>, except that the initial state is changed to <span class="math inline">\(s_{t+1}\)</span>:
<span class="math display" id="eq:to-general-mpc-t-1">\[\begin{equation}
\begin{split}
\min_{\{x_k,u_k\}} \quad &amp;
\Phi(x_N) + \sum_{k=0}^{N-1} \ell_k(x_k,u_k) \\[2mm]
\text{s.t.}\quad &amp;
x_{k+1} = f (x_k,u_k), \qquad t=0,\dots,N-1,\\
&amp; \boxed{x_0 = s_{t+1}} \ \ \text{(given)},\\
&amp; x_k \in \mathcal X_k,\quad u_k \in \mathcal U_k \quad \text{(bounds)},\\
&amp; g_k(x_k,u_k) \le 0,\quad h_k(x_k,u_k)=0 \quad \text{(path/terminal constraints).}
\end{split}
\tag{4.91}
\end{equation}\]</span>
Problems <a href="model-based-plan-optimize.html#eq:to-general-mpc">(4.88)</a> and <a href="model-based-plan-optimize.html#eq:to-general-mpc-t-1">(4.91)</a> are called <em>parametric optimization problems</em>, in the sense that the form of the optimization problem remains the same, but the value of the parameter <span class="math inline">\(s_t\)</span> that defines the equality constraint has changed.</p>
<p>For this reason, we should restate the solution in <a href="model-based-plan-optimize.html#eq:mpc-solution-t">(4.89)</a> as
<span class="math display" id="eq:mpc-solution-t-implicit">\[\begin{equation}
(x_0^\star(s_t), u_0^\star(s_t), x_1^\star(s_t), u_1^\star(s_t), \dots, x_{N-1}^\star(s_t), u_{N-1}^\star(s_t), x_N^\star(s_t))
\tag{4.92}
\end{equation}\]</span>
because they are all implicit functions of the parameter <span class="math inline">\(s_t\)</span>.</p>
<p>Similarly, the solution to the TO problem <a href="model-based-plan-optimize.html#eq:to-general-mpc-t-1">(4.91)</a> at time <span class="math inline">\(t+1\)</span> should be denoted as
<span class="math display" id="eq:mpc-solution-t-1-implicit">\[\begin{equation}
(x_0^\star(s_{t+1}), u_0^\star(s_{t+1}), x_1^\star(s_{t+1}), u_1^\star(s_{t+1}), \dots, x_{N-1}^\star(s_{t+1}), u_{N-1}^\star(s_{t+1}), x_N^\star(s_{t+1}))
\tag{4.93}
\end{equation}\]</span>
to indicate that they are functions of <span class="math inline">\(s_{t+1}\)</span>.</p>
<p>After solving the TO problem at time <span class="math inline">\(t+1\)</span>, MPC executes the first element:
<span class="math display">\[
a_{t+1} = u_0^\star (s_{t+1}),
\]</span>
and the system steps into a new state <span class="math inline">\(s_{t+2}\)</span>, from which MPC solves a new TO problem with parameter <span class="math inline">\(s_{t+2}\)</span>, and the process continues.</p>
<p><strong>Implicit Feedback.</strong> Through receding horizon control, MPC creates an implicit feedback control policy. Let the system’s state at time <span class="math inline">\(t\)</span> be <span class="math inline">\(s_t\)</span>, the feedback policy is:
<span class="math display" id="eq:mpc-feedback-policy">\[\begin{equation}
a_t = u^\star_0 (s_t) := \mu (s_t) = \arg\min \text{ (the TO problem with parameter } s_t).
\tag{4.94}
\end{equation}\]</span>
Unlike the policy in the case of RL, which is an explicit neural network, the policy of MPC is implicit and comes from the optimal solution of the parametric TO problem.</p>
<p><strong>Closed-Loop System.</strong> The closed-loop system under the MPC policy is therefore
<span class="math display" id="eq:mpc-closed-loop">\[\begin{equation}
s_{t+1} = f(s_t, \mu(s_t), w_t) := f_{\text{CL}}(s_t, w_t),
\tag{4.95}
\end{equation}\]</span>
which becomes an uncontrolled system with disturbance <span class="math inline">\(w_t\)</span>. The diagram of the closed-loop system is shown in Fig. <a href="model-based-plan-optimize.html#fig:mpc-diagram">4.12</a>. Lots of research have studied properties of the closed-loop system, such as stability and robustness to disturbance. One of the most important results states that, as long as the terminal loss function <span class="math inline">\(\Phi(\cdot)\)</span> satisfies a technical condition (being a control Lyapunov function), then the closed-loop system is stable. We do not go deep into these results for MPC analysis and refer the reader to <span class="citation">(<a href="#ref-rawlings2020model">Rawlings, Mayne, and Diehl 2020</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mpc-diagram"></span>
<img src="images/Model-based-optimization/mpc-diagram.png" alt="Model Predictive Control." width="90%" />
<p class="caption">
Figure 4.12: Model Predictive Control.
</p>
</div>
<p><strong>Warmstart for Parametric Optimization.</strong> The requirement that an MPC feedback policy solve an optimization problem online makes it computationally expensive. In some domains, e.g., chemical process control, control updates are infrequent (for example, every 10 minutes) and MPC is practical. In other domains such as robotics, control rates are much higher (e.g., 10,Hz), so reducing MPC’s computational cost is critical. A common strategy is <em>warm-starting</em>: when solving the optimization for the new parameter <span class="math inline">\(s_{t+1}\)</span> at time <span class="math inline">\(t+1\)</span>, initialize the numerical solver with the previous solution <a href="model-based-plan-optimize.html#eq:mpc-solution-t-implicit">(4.92)</a> computed for <span class="math inline">\(s_t\)</span> at time <span class="math inline">\(t\)</span>. If the change from <span class="math inline">\(s_t\)</span> to <span class="math inline">\(s_{t+1}\)</span> is small, the optimal solution at <span class="math inline">\(t+1\)</span> is likely close to that at <span class="math inline">\(t\)</span>, and warm-starting typically reduces the solver’s required iterations.</p>
<p>We now apply MPC to the stabilization of a double integrator with control constraints.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:mpc-double-integrator" class="example"><strong>Example 4.6  (MPC for Double Integrator) </strong></span>This example implements Model Predictive Control (MPC) for a 1D double-integrator (position–velocity) system using a quadratic program (QP).</p>
<p><strong>Dynamics of the System.</strong> We model the kinematics of a point mass with (discrete-time) constant-acceleration physics. Let the state be
<span class="math display">\[
x_k = \begin{bmatrix} p_k \\ v_k \end{bmatrix} \in \mathbb{R}^2,
\quad
u_k \in \mathbb{R}
\]</span>
where <span class="math inline">\(p_k\)</span> is position, <span class="math inline">\(v_k\)</span> is velocity, and <span class="math inline">\(u_k\)</span> is the commanded acceleration. With sampling time <span class="math inline">\(\Delta t\)</span>, the discrete-time, <em>nominal</em> dynamics are
<span class="math display">\[
x_{k+1} = A x_k + B u_k,
\quad
A = \begin{bmatrix} 1 &amp; \Delta t \\ 0 &amp; 1 \end{bmatrix},
\quad
B = \begin{bmatrix} \tfrac{1}{2}\Delta t^2 \\ \Delta t \end{bmatrix}.
\]</span></p>
<p><strong>Control Goal.</strong> We consider <em>regulation to a fixed target</em> <span class="math inline">\(x_{\text{goal}}\in\mathbb{R}^2\)</span>, typically the origin:
<span class="math display">\[
x_{\text{goal}}=\begin{bmatrix}0\\0\end{bmatrix}.
\]</span>
(Tracking a moving reference is immediate by updating <span class="math inline">\(x_{\text{goal}}\)</span> online.)</p>
<p><strong>Finite-Horizon Trajectory Optimization (QP).</strong> At each control time <span class="math inline">\(t\)</span>, with measured state <span class="math inline">\(s_t\)</span>, MPC solves a length-<span class="math inline">\(N\)</span> open-loop trajectory optimization (TO) problem with decision variables <span class="math inline">\(\{x_k,u_k\}_{k=0}^{N}\)</span> (with <span class="math inline">\(u_N\)</span> unused):
<span class="math display" id="eq:di-mpc-to">\[
\begin{aligned}
\hspace{-16mm} \min_{\{x_k,u_k\}} \quad &amp;
\underbrace{(x_N - x_{\text{goal}})^\top Q_f (x_N - x_{\text{goal}})}_{\text{terminal cost}}
\;+\;
\sum_{k=0}^{N-1}\Big[
\underbrace{(x_k - x_{\text{goal}})^\top Q (x_k - x_{\text{goal}})}_{\text{state cost}}
+
\underbrace{u_k^\top R u_k}_{\text{effort}}
\Big] \\[1.0ex]
\hspace{-16mm}  \text{s.t.}\quad &amp;
x_{k+1} = A x_k + B u_k,\quad k=0,\dots,N-1, \\[0.3ex]
&amp; \boxed{x_0 = s_t}\quad\text{(enforces the current initial condition)}, \\[0.3ex]
&amp; x_{\min} \le x_k \le x_{\max},\quad k=0,\dots,N, \\[0.3ex]
&amp; u_{\min} \le u_k \le u_{\max},\quad k=0,\dots,N-1.
\end{aligned}
\tag{4.96}
\]</span></p>
<p>The problem in <a href="model-based-plan-optimize.html#eq:di-mpc-to">(4.96)</a> is a convex QP (quadratic cost; linear dynamics and box constraints). We will write a Python code that builds this QP once with CVXPY and then only updates the Parameter for the measured state <span class="math inline">\(s_t\)</span> and (optionally) <span class="math inline">\(x_{\text{goal}}\)</span> at each MPC step.</p>
<p><strong>Receding Horizon (Implicit Feedback).</strong> Let <span class="math inline">\(\{x_k^\star,u_k^\star\}\)</span> be the optimizer of <a href="model-based-plan-optimize.html#eq:di-mpc-to">(4.96)</a>. MPC applies only the first input
<span class="math display">\[
a_t = u_0^\star,
\]</span>
then measures the new state, shifts/warm-starts the QP, and resolves. This closes the loop and yields an implicit feedback policy <span class="math inline">\(a_t=\mu(s_t)=u_0^\star(s_t)\)</span>.</p>
<p><strong>True Dynamics with Disturbance.</strong> To test robustness, the closed-loop plant advances with additive disturbance:
<span class="math display">\[
s_{t+1} = A\,s_t + B\,a_t + w_t,
\]</span>
where <span class="math inline">\(w_t\)</span> is zero-mean noise. In the code, <span class="math inline">\(w_t\)</span> is a small Gaussian perturbation added to both position and velocity updates. This models sensor/actuator errors or unmodeled effects and illustrates how MPC corrects by re-solving at every step.</p>
<p><strong>Experiment Setup.</strong></p>
<ul>
<li><p><strong>Initialization.</strong> Start from <span class="math inline">\(x_0=[4,0]^\top\)</span> (far from the goal). Fix <span class="math inline">\(x_{\text{goal}}=[0,0]^\top\)</span>.</p></li>
<li><p><strong>Horizon and sampling.</strong> <span class="math inline">\(N=20\)</span>, <span class="math inline">\(\Delta t=0.1\)</span> s; run for <span class="math inline">\(T=120\)</span> MPC steps (12 s).</p></li>
<li><p><strong>QP solver and structure.</strong> We solve with OSQP via CVXPY. The problem is constructed once; at each step we update the parameter <span class="math inline">\(x_0=s_t\)</span>. This preserves the QP structure (matrices <span class="math inline">\(P\)</span> and <span class="math inline">\(A\)</span>), enabling factorization reuse and faster solves.</p></li>
<li><p><strong>Warm-start.</strong> Shift the previous optimal sequence <span class="math inline">\(u^\star\)</span> left by one step and hold the last value, then roll out the dynamics from the new <span class="math inline">\(s_t\)</span> to seed a feasible <span class="math inline">\(x\)</span>-trajectory guess. These are passed to the solver via <code>warm_start=True</code>.</p></li>
</ul>
<p>Fig. <a href="model-based-plan-optimize.html#fig:mpc-double-integrator-small-noise">4.13</a> shows the closed-loop position and velocity under the MPC controller in the case of small disturbancen (the standard deviations of the two elements of <span class="math inline">\(w_t\)</span> are <span class="math inline">\(0.002\)</span> and <span class="math inline">\(0.005\)</span>).</p>
<p>Fig. <a href="model-based-plan-optimize.html#fig:mpc-double-integrator-large-noise">4.14</a> shows the closed-loop position and velocity under the MPC controller in the case of large disturbancen (the standard deviations of the two elements of <span class="math inline">\(w_t\)</span> are <span class="math inline">\(0.02\)</span> and <span class="math inline">\(0.05\)</span>).</p>
<p>In both cases, the MPC policy successfully regulates the system around the origin, illustrating robustness of the MPC policy to disturbances.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/mpc_double_integrator.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mpc-double-integrator-small-noise"></span>
<img src="images/Model-based-optimization/mpc_double_integrator-1.png" alt="MPC for double integrator (small disturbance)." width="80%" />
<p class="caption">
Figure 4.13: MPC for double integrator (small disturbance).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mpc-double-integrator-large-noise"></span>
<img src="images/Model-based-optimization/mpc_double_integrator-2.png" alt="MPC for double integrator (large disturbance)." width="80%" />
<p class="caption">
Figure 4.14: MPC for double integrator (large disturbance).
</p>
</div>
</div>
</div>
<!-- This immediately creates closed-loop feedback. To see this, note that $u_0^\star$ is an optimal solution of \@ref(eq:to-general-mpc), and $x_t$ appears in the problem  -->

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-arnold84ieee-generalized" class="csl-entry">
Arnold, William F, and Alan J Laub. 1984. <span>“Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.”</span> <em>Proceedings of the IEEE</em> 72 (12): 1746–54.
</div>
<div id="ref-kang2024fast" class="csl-entry">
Kang, Shucheng, Xiaoyang Xu, Jay Sarva, Ling Liang, and Heng Yang. 2024. <span>“Fast and Certifiable Trajectory Optimization.”</span> In <em>International Workshop on the Algorithmic Foundations of Robotics</em>.
</div>
<div id="ref-liu1989limited" class="csl-entry">
Liu, Dong C, and Jorge Nocedal. 1989. <span>“On the Limited Memory BFGS Method for Large Scale Optimization.”</span> <em>Mathematical Programming</em> 45 (1): 503–28.
</div>
<div id="ref-nocedal99book-numerical" class="csl-entry">
Nocedal, Jorge, and Stephen J Wright. 1999. <em>Numerical Optimization</em>. Springer.
</div>
<div id="ref-rawlings2020model" class="csl-entry">
Rawlings, James Blake, David Q Mayne, and Moritz Diehl. 2020. <em>Model Predictive Control: Theory, Computation, and Design</em>. Vol. 2. Nob Hill Publishing Madison, WI.
</div>
<div id="ref-wachter2006implementation" class="csl-entry">
Wächter, Andreas, and Lorenz T Biegler. 2006. <span>“On the Implementation of an Interior-Point Filter Line-Search Algorithm for Large-Scale Nonlinear Programming.”</span> <em>Mathematical Programming</em> 106 (1): 25–57.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="policy-gradient.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advanced-materials.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/04-model-based-control.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
