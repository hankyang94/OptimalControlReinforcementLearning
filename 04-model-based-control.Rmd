# Model-based Planning and Optimization {#model-based-plan-optimize}

In Chapter \@ref(mdp), we studied tabular MDPs with known dynamics where both the state and action spaces are finite, and we saw how dynamic-programming methods—Policy Iteration (PI) and Value Iteration (VI)—recover optimal policies.

Chapters \@ref(value-rl) and \@ref(policy-gradient) generalized these ideas to unknown dynamics and continuous spaces by introducing function approximation for value functions and policies. Those methods are *model-free*: they assume no access to the transition model and rely solely on data collected from interaction.

This chapter turns to the complementary regime: **known dynamics** with **continuous state and action spaces**. Our goal is to develop model-based planning and optimization methods that exploit the known dynamics to compute high-quality decisions efficiently.

We proceed in three steps:

1. **Linear-quadratic systems.** For linear dynamics and quadratic stage/terminal rewards (or costs), dynamic programming yields a linear optimal policy that can be computed efficiently via Riccati recursions. This setting serves as a tractable, illuminating baseline and a recurring building block in more general algorithms.

2. **Trajectory optimization (TO) for nonlinear systems.** When dynamics are nonlinear, we focus on planning an optimal state–action trajectory from a given initial condition, maximizing the cumulative reward (or minimizing cumulative cost) subject to dynamics and constraints. Unlike RL, which seeks an optimal *feedback policy* valid for all states, TO computes an *open-loop plan* (often with a time-varying local feedback around a nominal trajectory). Although less ambitious, TO naturally accommodates state/control constraints—common in motion planning under safety, actuation, and environmental limits—and is widely used in robotics and control.

3. **Model predictive control (MPC).** MPC converts open-loop plans into a feedback controller by repeatedly solving a short-horizon TO problem at each time step, applying only the first action, and receding the horizon. This receding-horizon strategy brings robustness to disturbances and model mismatch while retaining the constraint-handling benefits of TO.

We adopt the standard discrete-time dynamical system notation 
\[
x_{t+1} = f_t(x_t, u_t, w_t),
\]
where \(x_t \in \mathbb{R}^n\) is the state, \(u_t \in \mathbb{R}^m\) is the control/action, \(w_t \in \mathbb{R}^d\) is a (possibly stochastic) disturbance, and \(f_t\) is a known transition function, potentially nonlinear or nonsmooth. The goal is to find a policy that maximizes a sum of stage rewards \(r(x_t,u_t)\) and optional terminal reward \(r_T(x_T)\). We will often use the cost-minimization form \(c = -r\).
State and action constraints are written as
\[
x_t \in \mathcal{X}, \qquad u_t \in \mathcal{U}.
\]

## Linear Quadratic Regulator {#lqr}

In this section, we focus on the case when $f_t$ is a linear function, and the rewards/costs are quadratic in $x$ and $u$. This family of problems is known as linear quadratic regulator (LQR).

### Finite-Horizon LQR

Consider a linear discrete-time dynamical system 
\begin{equation}
x_{k+1} = A_k x_k + B_k u_k + w_k, \quad k=0,1,\dots,N-1,
(\#eq:lqr-linear-system)
\end{equation}
where $x_k \in \mathbb{R}^n$ the state, $u_k \in \mathbb{R}^m$ the control, $w_k \in \mathbb{R}^n$ the independent, zero-mean disturbance with given probability distribution that does not depend on $x_k,u_k$, and $A_k \in \mathbb{R}^{n \times n}, B_k \in \mathbb{R}^{n \times m}$ are known matrices determining the transition dynamics.

We want to solve the following optimal control problem 
\begin{equation}
\min_{\mu_0,\dots,\mu_{N-1}} \mathbb{E} \left\{ x_N^\top Q_N x_N + \sum_{k=0}^{N-1} \left( x_k^\top Q_k x_k + u_k^\top R_k u_k \right) \right\},
(\#eq:lqr-formulation)
\end{equation}
where $\mu_0,\dots,\mu_{N-1}$ are feedback policies/controllers that map states to actions and the expectation is taken over the randomness in $w_0,\dots,w_{N-1}$. In \@ref(eq:lqr-formulation), $\{Q_k \}_{k=0}^N$ are positive semidefinite matrices, and $\{ R_k \}_{k=0}^{N-1}$ are positive definite matrices. The formulation \@ref(eq:lqr-formulation) is known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to "regulate" the system around the origin $x=0$. 

The Bellman Optimality condition introduced in Theorem \@ref(thm:FiniteHorizonMDPBellmanOptimality) still holds for continuous state and action spaces. Therefore, we will try to follow the dynamic programming (DP) algorithm in Section \@ref(dp) to solve for the optimal policy.

The DP algorithm computes the optimal cost-to-go backwards in time. 
The terminal cost is 
$$
J_N(x_N) = x_N^\top Q_N x_N
$$
by definition.

The optimal cost-to-go at time $N-1$ is equal to
\begin{equation}
\begin{split}
J_{N-1}(x_{N-1}) = \min_{u_{N-1}} \mathbb{E}_{w_{N-1}} \{ x_{N-1}^\top Q_{N-1} x_{N-1} + u_{N-1}^\top R_{N-1} u_{N-1} + \\ \Vert \underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \Vert^2_{Q_N} \}
\end{split}
(\#eq:lqr-cost-N-1)
\end{equation}
where $\Vert v \Vert_Q^2 = v^\top Q v$ for $Q \succeq 0$. Now observe that the objective in \@ref(eq:lqr-cost-N-1) is 
\begin{equation}
\begin{split}
x_{N-1}^\top Q_{N-1} x_{N-1} + u_{N-1}^\top R_{N-1} u_{N-1} + \Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \Vert_{Q_N}^2 + \\
\mathbb{E}_{w_{N-1}} \left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^\top Q_{N-1} w_{N-1} \right] + \\
\mathbb{E}_{w_{N-1}} \left[ w_{N-1}^\top Q_N w_{N-1} \right]
\end{split}
\end{equation}
where the second line is zero due to $\mathbb{E}[w_{N-1}] = 0$ and the third line is a constant with respect to $u_{N-1}$. Consequently, the optimal control $u_{N-1}^\star$ can be computed by setting the derivative of the objective with respect to $u_{N-1}$ equal to zero 
\begin{equation}
u_{N-1}^\star = - \left[ \left( R_{N-1} + B_{N-1}^\top Q_N B_{N-1} \right)^{-1} B_{N-1}^\top Q_N A_{N-1} \right] x_{N-1}.
(\#eq:optimal-u-N-1)
\end{equation}
Plugging the optimal controller $u^\star_{N-1}$ back to the objective of \@ref(eq:lqr-cost-N-1) leads to
\begin{equation}
J_{N-1}(x_{N-1}) = x_{N-1}^\top S_{N-1} x_{N-1} + \mathbb{E} \left[ w_{N-1}^\top Q_N w_{N-1} \right],
(\#eq:optimal-cost-N-1)
\end{equation}
with 
$$
S_{N-1} = Q_{N-1} + A_{N-1}^\top \left[ Q_N - Q_N B_{N-1} \left( R_{N-1} + B_{N-1}^\top Q_N B_{N-1} \right)^{-1} B_{N-1}^\top Q_N \right] A_{N-1}.
$$
We note that $S_{N-1}$ is positive semidefinite (this is an exercise for you to convince yourself). 

Now we realize that something surprising and nice has happened.

1. The optimal controller $u^{\star}_{N-1}$ in \@ref(eq:optimal-u-N-1) is a linear feedback policy of the state $x_{N-1}$, and 

2. The optimal cost-to-go $J_{N-1}(x_{N-1})$ in \@ref(eq:optimal-cost-N-1) is quadratic in $x_{N-1}$, just the same as $J_{N}(x_N)$.

This implies that, if we continue to compute the optimal cost-to-go at time $N-2$, we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is, 

> The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time. 

We summarize the solution for the LQR problem \@ref(eq:lqr-formulation) as follows.

::: {.theorembox}
::: {.proposition #discretetimefinitehorizonlqrsolution name="Solution of Discrete-Time Finite-Horizon LQR"}
The optimal controller for the LQR problem \@ref(eq:lqr-formulation) is a linear state-feedback policy
\begin{equation}
\mu_k^\star(x_k) = - K_k x_k, \quad k=0,\dots,N-1.
(\#eq:lqr-solution-control)
\end{equation}
The gain matrix $K_k$ can be computed as
$$
K_k = \left( R_k + B_k^\top S_{k+1} B_k  \right)^{-1} B_k^\top S_{k+1} A_k,
$$
where the matrix $S_k$ satisfies the following backwards recursion
\begin{equation}
\hspace{-6mm}
\begin{split}
S_N &= Q_N \\
S_k &= Q_k + A_k^\top \left[ S_{k+1} - S_{k+1}B_k \left( R_k + B_k^\top S_{k+1} B_k  \right)^{-1}  B_k^\top S_{k+1}  \right] A_k, k=N-1,\dots,0.
\end{split}
(\#eq:finite-discrete-lqr-riccati)
\end{equation}
The optimal cost-to-go is given by 
$$
J_0(x_0) = x_0^\top S_0 x_0 + \sum_{k=0}^{N-1} \mathbb{E} \left[ w_k^\top S_{k+1} w_k\right].
$$
The recursion \@ref(eq:finite-discrete-lqr-riccati) is called the _discrete-time Riccati equation_.
:::
:::

Proposition \@ref(prp:discretetimefinitehorizonlqrsolution) states that, to evaluate the optimal policy \@ref(eq:lqr-solution-control), one can first run the backwards Riccati equation \@ref(eq:finite-discrete-lqr-riccati) to compute all the positive definite matrices $S_k$, and then compute the gain matrices $K_k$. For systems of reasonable dimensions, evalutating the matrix inversion in \@ref(eq:finite-discrete-lqr-riccati) should be fairly efficient.




### Infinite-Horizon LQR {#infinite-horizon-lqr}

We now switch to the infinite-horizon LQR problem
\begin{align}
\min_{\mu} & \quad  \sum_{k=0}^{\infty} \left( x_k^\top Q x_k + u_k^\top R u_k \right) (\#eq:infinite-horizon-lqr-cost) \\
\text{subject to} & \quad x_{k+1} = A x_k + B u_k, \quad k=0,\dots,\infty, (\#eq:infinite-horizon-lqr-system)
\end{align}
where $Q \succeq 0$, $R \succ 0$, $A,B$ are constant matrices, and we seek a stationary policy $\mu$ that maps states to actions. Note that here we remove the disturbance $w_k$ because in general adding $w_k$ will make the objective function unbounded. To handle $w_k$, we will have to either add a discount factor $\gamma$, or switch to an average cost objective function. 

For infinite-horizon problems, the Bellman Optimality condition changes from a recursion to an equation. Specifically, according to Theorem \@ref(thm:BellmanOptimalityInfiniteHorizon) and equation \@ref(eq:BellmanOptimalityInfiniteHorizonStateValue), the optimal value function should satisfy the following Bellman optimality equation, restated for the case of cost minimization instead of reward maximization:
\begin{equation}
J^\star (x) = \min_{u} \left[ c(x,u) + \sum_{x'} P(x' \mid x, u) J^\star (x') \right], \quad \forall x,
(\#eq:BellmanOptimalityInfiniteHorizonRestateMin)
\end{equation}
where $c(x,u)$ is the cost function.

**Guess A Solution.** Based on our derivation in the finite-horizon case, we might as well guess that the optimal value function is a quadratic function:
$$
J(x) = x^\top S x, \quad \forall x,
$$
for some positive definite matrix $S$. Then, our guessed solution must satisfy the Bellman optimality stated in \@ref(eq:BellmanOptimalityInfiniteHorizonRestateMin):
\begin{equation}
x^\top S x = J(x) = \min_{u} \left\{ x^\top Q x + u^\top R u + \Vert \underbrace{A x + B u}_{x'} \Vert_S^2  \right\}.
(\#eq:infinite-horizon-lqr-invoke-dp)
\end{equation}
The minimization over $u$ in \@ref(eq:infinite-horizon-lqr-invoke-dp) can again be solved in closed-form by setting the gradient of the objective with respect to $u$ to be zero
\begin{equation}
u^\star = - \underbrace{\left[ \left( R + B^\top S B \right)^{-1} B^\top S A \right]}_{K} x.
(\#eq:infinite-horizon-lqr-control)
\end{equation}
Plugging the optimal $u^\star$ back into \@ref(eq:infinite-horizon-lqr-invoke-dp), we see that the matrix $S$ has to satisfy the following equation
\begin{equation}
S = Q + A^\top \left[  S - SB \left( R + B^\top S B  \right)^{-1} B^\top S \right] A.
(\#eq:algebraic-riccati)
\end{equation}
Equation \@ref(eq:algebraic-riccati) is known as the _discrete algebraic Riccati equation_ (DARE). 

So the question boils down to if the DARE has a solution $S$ that is positive definite?

::: {.theorembox}
::: {.proposition #infinitehorizonlqrsolution name="Solution of Discrete-Time Infinite-Horizon LQR"} 
Consider a linear system 
$$
x_{k+1} = A x_k + B u_k,
$$
with $(A,B)$ controllable (see Section \@ref(linear-system-basics)). Let $Q \succeq 0$ in \@ref(eq:infinite-horizon-lqr-cost) be such that $Q$ can be written as $Q = C^\top C$ with $(A,C)$ observable. 

Then the optimal controller for the infinite-horizon LQR problem \@ref(eq:infinite-horizon-lqr-cost) is a stationary linear policy
$$
\mu^\star (x) = - K x,
$$
with 
$$
K = \left( R + B^\top S B \right)^{-1} B^\top S A.
$$
The matrix $S$ is the unique positive definite matrix that satisfies the discrete algebraic Riccati equation 
\begin{equation}
S = Q + A^\top \left[  S - SB \left( R + B^\top S B  \right)^{-1} B^\top S \right] A.
(\#eq:discrete-algebraic-riccati-equation)
\end{equation}

Moreover, the closed-loop system 
$$
x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k
$$
is stable, i.e., the eigenvalues of the matrix $A - BK$ are strictly within the unit circle (see Appendix \@ref(app-lti-stability-dt)).
:::
:::

*Remark.* The assumptions of $(A,B)$ being controllable and $(A,C)$ being observable can be relaxted to $(A,B)$ being stabilizable and $(A,C)$ being detectable (for definitions of stabilizability and detectability, see Appendix \@ref(app-lti-system-theory)).

We have not discussed how to solve the algebraic Riccati equation \@ref(eq:discrete-algebraic-riccati-equation). It is clear that \@ref(eq:discrete-algebraic-riccati-equation) is not a linear system of equations in $S$. In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see [@arnold84ieee-generalized]. Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab [`dlqr`](https://www.mathworks.com/help/control/ref/dlqr.html) and the Python [`scipy.linalg.solve_discrete_are`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_discrete_are.html) function computes the $K$ and $S$ matrices from $A,B,Q,R$. 

Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum.


::: {.examplebox}
::: {.example #lqr-pendulum-stabilization name="Pendulum Stabilization by LQR"}
Consider the simple pendulum in Fig. \@ref(fig:pendulum-drawing) with dynamics
\begin{equation}
x = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}, \quad 
\dot{x} = f(x,u) = \begin{bmatrix}
\dot{\theta} \\
-\frac{1}{ml^2}(b \dot{\theta} + mgl \sin \theta) + \frac{1}{ml^2} u
\end{bmatrix}
(\#eq:lqr-pendulum-dynamics)
\end{equation}
where $m$ is the mass of the pendulum, $l$ is the length of the pole, $g$ is the gravitational constant, $b$ is the damping ratio, and $u$ is the torque applied to the pendulum. 

We are interested in applying the LQR controller to balance the pendulum in the upright position $x_d = [\pi,0]^\top$ with a zero velocity.

```{r pendulum-drawing, out.width='40%', fig.show='hold', fig.cap='A Simple Pendulum.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/pendulum-drawing.png')
```

Let us first shift the dynamics so that "$0$" is the upright position. This can be done by defining a new variable $z = x - x_d = [\theta - \pi, \dot{\theta}]^\top$, which leads to 
\begin{equation}
\dot{z} = \dot{x} = f(x,u) = f(z + x_d,u) = \begin{bmatrix}
z_2 \\
\frac{1}{ml^2} \left( u - b z_2 + mgl \sin z_1  \right)
\end{bmatrix} = f'(z,u).
(\#eq:pendulum-dynamics-z-coordinate)
\end{equation}
We then linearize the nonlinear dynamics $\dot{z} = f'(z,u)$ at the point $z^\star = 0, u^\star = 0$:
\begin{align}
\dot{z} & \approx f'(z^\star,u^\star) + \left( \frac{\partial f'}{\partial z} \right)_{z^\star,u^\star} (z - z^\star) + \left( \frac{\partial f'}{\partial u} \right)_{z^\star,u^\star} (u - u^\star) \\
& = \begin{bmatrix}
0 & 1 \\
\frac{g}{l} \cos z_1 & - \frac{b}{ml^2}
\end{bmatrix}_{z^\star, u^\star} z + 
\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix} u \\
& = \underbrace{\begin{bmatrix}
0 & 1 \\
\frac{g}{l} & - \frac{b}{ml^2}
\end{bmatrix}}_{A_c} z  + 
\underbrace{\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix}}_{B_c} u.
\end{align} 
Finally, we convert the continuous-time dynamics to discrete time with a fixed discretization $h$
$$
z_{k+1} = \dot{z}_k \cdot h + z_k = \underbrace{(h \cdot A_c + I )}_{A} z_k + \underbrace{(h \cdot B_c)}_{B} u_k.
$$

We are now ready to implement the LQR controller. In the formulation \@ref(eq:infinite-horizon-lqr-cost), we choose $Q = I$, $R = I$, and compute the gain matrix $K$ by solving the DARE.

Fig. \@ref(fig:pendulum-stabilization-sim) shows the simulation result for $m=1,l=1,b=0.1$, $g = 9.8$, and $h = 0.01$, with an initial condition $z^0 = [0.1,0.1]^\top$. We can see that the LQR controller successfully stabilizes the pendulum at $z^\star$, the upright position.

You can play with the Python code [here](https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_lqr_stabilization.py). 

Alternatively, the Matlab code can be found [here](https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/pendulum_stabilization_lqr.m).

```{r pendulum-stabilization-sim, out.width='60%', fig.show='hold', fig.cap='LQR stabilization of a simple pendulum.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/Model-based-optimization/pendulum_lqr_stabilization.png')
```
:::
:::

### Linear System Basics {#linear-system-basics}

Consider the discrete-time linear time-invariant (LTI) system
\[
x_{k+1}=Ax_k+Bu_k,\qquad y_k=Cx_k+Du_k,
\]
with \(x_k\in\mathbb{R}^n,\;u_k\in\mathbb{R}^m,\;y_k\in\mathbb{R}^p\).

We provide a very brief review of linear system theory to understand Proposition \@ref(prp:infinitehorizonlqrsolution). More details can be found in Appendix \@ref(app-lti-system-theory).


**Stability.** The autonomous system \(x_{k+1}=Ax_k\) is (asymptotically) stable if for every \(x_0\) we have \(x_k\to 0\) as \(k\to\infty\).

**Equivalent characterizations.**

- \(A\) is **Schur**: all eigenvalues satisfy \(|\lambda_i(A)|<1\). 

- Lyapunov: \(\exists P\succ 0\) s.t. \(A^\top P A - P \prec 0\).  


**Controllability (reachability).** The pair \((A,B)\) is controllable if for any \(x_0,x_f\) there exists a *finite* input sequence \(\{u_0,\dots,u_{N-1}\}\) that drives the state from \(x_0\) to \(x_N=x_f\).

**Kalman controllability matrix.**
  \[
  \mathcal C \;=\; [\,B\; AB\; A^2B\;\cdots\; A^{n-1}B\,],\quad
  \text{\((A,B)\) controllable} \iff \operatorname{rank}(\mathcal C)=n.
  \]

**Popov-Belevitch-Hautus (PBH) test.**
  \[
  \text{\((A,B)\) controllable} \iff 
  \operatorname{rank}\!\begin{bmatrix}\lambda I - A & B\end{bmatrix} = n
  \ \text{for all}\ \lambda\in\mathbb{C}.
  \]
It suffices to check \(\lambda\) equal to the eigenvalues of \(A\).

**Observability.** The pair \((A,C)\) is observable if the initial state \(x_0\) can be uniquely determined from a finite sequence of outputs (and known inputs), e.g., from \(\{y_0,\dots,y_{n-1}\}\).

**Observability matrix.**
  \[
  \mathcal O \;=\; \begin{bmatrix} C \\ CA \\ \vdots \\ CA^{n-1}\end{bmatrix},\quad
  \text{\((A,C)\) observable} \iff \operatorname{rank}(\mathcal O)=n.
  \]

**PBH test.**
  \[
  \text{\((A,C)\) observable} \iff 
  \operatorname{rank}\!\begin{bmatrix}\lambda I - A^\top & C^\top\end{bmatrix} = n
  \ \text{for all}\ \lambda\in\mathbb{C}.
  \]
Dual to controllability: \((A,C)\) observable \(\Leftrightarrow\) \((A^\top,C^\top)\) controllable.


## LQR Trajectory Tracking {#lqr-tracking}

Classical LQR delivers an optimal linear state-feedback when dynamics are linear and the objective is quadratic. In many planning problems, however, we do not seek a single stationary feedback for *all* states but rather a *local stabilizer around a given (possibly time-varying) trajectory*—for instance, a motion plan from a trajectory optimizer or MPC's rolling nominal (see Section \@ref(traj-opt)). LQR Tracking (also called time-varying LQR, TVLQR) provides exactly this: a *time-varying* linear feedback that stabilizes the system near a nominal state–control sequence and rejects small disturbances.

**Problem Setup.** Let the nominal (i.e., ignoring the disturbance) discrete-time system be
\[
x_{t+1} \;=\; f_t(x_t,u_t), \qquad t=0,\dots,N-1,
\]
and suppose we have a *nominal trajectory* \(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\) satisfying
\[
\bar x_{t+1} \;=\; f_t(\bar x_t,\bar u_t).
\]

Our goal is to design a controller that can stabilize the system with disturbance, i.e., $x_{t+1} = f_t(x_t, u_t, w_t)$, around the nominal trajectory.

Towards this, we define *deviations* from the nominal trajectory as
\[
\delta x_t := x_t-\bar x_t, \qquad \delta u_t := u_t-\bar u_t.
\]

If the true system is linear time-varying (or we linearize a nonlinear system along the nominal), we obtain the *deviation dynamics*
\[
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,
\quad
A_t := \left.\frac{\partial f_t}{\partial x}\right|_{(\bar x_t,\bar u_t)},\quad
B_t := \left.\frac{\partial f_t}{\partial u}\right|_{(\bar x_t,\bar u_t)}.
(\#eq:tvlqr-deviation-dynamics)
\]

We penalize deviations with a quadratic cost
\[
J \;=\; \delta x_N^\top Q_N \delta x_N
\;+\;\sum_{t=0}^{N-1} \Big(\delta x_t^\top Q_t \delta x_t \;+\; \delta u_t^\top R_t \delta u_t\Big),
\quad Q_t\succeq 0,\; R_t\succ 0.
(\#eq:tvlqr-deviation-cost)
\]

**LQR Tracking Algorithm.** The tracking controller takes the *affine* form
\[
u_t \;=\; \bar u_t \;-\; K_t\,(x_t-\bar x_t),
\]
where \(\{K_t\}_{t=0}^{N-1}\) are time-varying gains computed by a backward Riccati recursion on the deviation system \@ref(eq:tvlqr-deviation-dynamics) with cost \@ref(eq:tvlqr-deviation-cost). 

From Proposition \@ref(prp:discretetimefinitehorizonlqrsolution), we know the gains can be computed as follows. 

Initialize at terminal time:
\[
S_N \;=\; Q_N.
\]
For \(t = N-1,\,N-2,\,\dots,\,0\):
\begin{equation}
\begin{split}
K_t &= \Big(R_t + B_t^\top S_{t+1} B_t\Big)^{-1} B_t^\top S_{t+1} A_t, \\[2mm]
S_t &= Q_t \;+\; A_t^\top \!\Big(S_{t+1} - S_{t+1} B_t \big(R_t + B_t^\top S_{t+1} B_t\big)^{-1} B_t^\top S_{t+1}\Big) A_t.
\end{split}
(\#eq:tvlqr-riccati)
\end{equation}

Given the gains \(\{K_t\}\), apply at runtime:
\begin{equation}
u_t \;=\; \bar u_t - K_t\,(x_t-\bar x_t), \qquad t=0,\dots,N-1.
(\#eq:tvlqr-control-law)
\end{equation}

The following pseudocode summarizes the algorithm.

::: {.highlightbox}
**Algorithm: LQR Trajectory Tracking (TVLQR)**

**Inputs:** nominal \(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\), weights \(\{Q_t,R_t\}\), terminal \(Q_N\).

1. **Linearize** along the nominal to get \(A_t,B_t\) via \@ref(eq:tvlqr-deviation-dynamics).  
2. **Backward pass:** compute \(K_t\) and \(S_t\) via \@ref(eq:tvlqr-riccati).  
3. **Apply feedback:** \(u_t=\bar u_t - K_t(x_t-\bar x_t)\) as in \@ref(eq:tvlqr-control-law).  

**Output:** time-varying gains \(\{K_t\}\) giving a local stabilizer around the trajectory.
:::

We now apply TVLQR to a vehicle trajectory tracking problem.


::: {.examplebox}
::: {.example #tvlqr-unicyle name="LQR Trajectory Tracking for Unicyle"}

We (i) define the dynamics in continuous and discrete time, (ii) specify a circular *nominal trajectory*, (iii) linearize the nonlinear dynamics *along the nominal*, (iv) state the deviation-cost weights \(Q,R\) (and terminal \(Q_N\)), and (v) list the *experiment setup* (discretization and horizon length).


**Unicycle Dynamics (Continuous and Discrete).**

**State and input.**  
\[
x=\begin{bmatrix}p_x\\ p_y\\ \theta\end{bmatrix}\in\mathbb{R}^3,
\qquad
u=\begin{bmatrix}v\\ \omega\end{bmatrix}\in\mathbb{R}^2,
\]
where \((p_x,p_y)\) is planar position, \(\theta\) is heading, \(v\) is forward speed, and \(\omega\) is yaw rate.

**Continuous time:**
\begin{equation}
\dot p_x = v\cos\theta,\qquad
\dot p_y = v\sin\theta,\qquad
\dot\theta = \omega.
(\#eq:unicycle-ct)
\end{equation}

**Discrete time (forward Euler with step \(h>0\)):**
\begin{equation}
x_{t+1} \;=\; f(x_t,u_t)
:= \begin{bmatrix}
p_{x,t} + h\, v_t\cos\theta_t\\[2pt]
p_{y,t} + h\, v_t\sin\theta_t\\[2pt]
\theta_t + h\,\omega_t
\end{bmatrix}.
(\#eq:unicycle-dt)
\end{equation}

**Nominal Trajectory (Circular Motion).** We track a circle of radius \(R=\dfrac{\bar v}{\bar\omega}\) using *constant nominal inputs*
\begin{equation}
\bar u_t \equiv \begin{bmatrix}\bar v\\ \bar\omega\end{bmatrix},\qquad t=0,\dots,N-1,
(\#eq:unicycle-nominal-control)
\end{equation}
and generate the *nominal state* recursively under the discrete dynamics \@ref(eq:unicycle-dt):
\begin{equation}
\bar x_{t+1} \;=\; f(\bar x_t,\bar u_t),\qquad \bar x_0 = \begin{bmatrix}R\\ 0\\ \tfrac{\pi}{2}\end{bmatrix}.
(\#eq:unicycle-discrete-nominal)
\end{equation}

We define *deviations* from the nominal:
\[
\delta x_t := x_t-\bar x_t,\qquad \delta u_t := u_t-\bar u_t.
\]

**Linearization Along the Nominal.** Linearize \@ref(eq:unicycle-dt) at \((\bar x_t,\bar u_t)\) to obtain the deviation dynamics
\begin{equation}
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,
(\#eq:unicycle-dev-dyn)
\end{equation}
with Jacobians (using \(c_t:=\cos\bar\theta_t,\ s_t:=\sin\bar\theta_t\))
\begin{equation}
A_t \;=\; \frac{\partial f}{\partial x}\Big|_{(\bar x_t,\bar u_t)}
= \begin{bmatrix}
1 & 0 & -h\,\bar v\,s_t\\
0 & 1 & \ \ h\,\bar v\,c_t\\
0 & 0 & 1
\end{bmatrix},
\qquad
B_t \;=\; \frac{\partial f}{\partial u}\Big|_{(\bar x_t,\bar u_t)}
= \begin{bmatrix}
h\,c_t & 0\\
h\,s_t & 0\\
0 & h
\end{bmatrix}.
(\#eq:unicycle-linearization)
\end{equation}

**Deviation Cost (Weights \(Q,R,Q_N\)).** We penalize deviations with a quadratic stage/terminal cost
\[
J \;=\; \delta x_N^\top Q_N\,\delta x_N
\;+\;\sum_{t=0}^{N-1}\Big(\delta x_t^\top Q\,\delta x_t+\delta u_t^\top R\,\delta u_t\Big),
\]
using the weights:
\begin{equation}
Q=\mathrm{diag}(30,\;30,\;5),\qquad
Q_N=\mathrm{diag}(60,\;60,\;8),\qquad
R=\mathrm{diag}(0.2,\;0.2).
(\#eq:unicycle-weights)
\end{equation}
These reflect a stronger emphasis on position tracking, a moderate penalty on heading error, and a mild penalty on control *deviations* from the nominal.

**Experiment Setup.**

- **Discretization step:** \(h = 0.02\ \mathrm{s}\).  
- **Horizon length:** \(T_{\mathrm{final}} = 12\ \mathrm{s}\).  
- **Number of steps:** \(N = T_{\mathrm{final}}/h = \mathbf{600}\).  
- **Nominal inputs:** \(\bar v = 1.2\ \mathrm{m/s},\ \bar\omega = 0.4\ \mathrm{rad/s}\) (radius \(R=\bar v/\bar\omega\)).  
- **Initialization:** the nominal starts at \(\bar x_0 = [\,R,\,0,\,\pi/2\,]^\top\); the actual system may start with a small offset (see code).

With \((A_t,B_t)\) from \@ref(eq:unicycle-linearization) and weights \@ref(eq:unicycle-weights), the TVLQR backward Riccati recursion (Section \@ref(lqr-tracking)) yields gains \(K_t\). We then apply the **local feedback**
\[
u_t \;=\; \bar u_t - K_t\,(x_t-\bar x_t),
\]
to robustly track the circular nominal under small disturbances.

**Disturbances.** To test robustness, we inject additive process disturbances into the discrete dynamics:
\[
x_{t+1} \;=\; f(x_t,u_t)\;+\; w_t,\qquad t=0,\dots,N-1,
\]
where
\[
w_t \;=\; \underbrace{\eta_t}_{\text{i.i.d. Gaussian noise}} \;+\; \underbrace{g_t}_{\text{deterministic gust}}.
\]

1) Small i.i.d. Gaussian process noise. We draw \(\eta_t \sim \mathcal N(0,W)\) independently at each step with
\[
W \;=\; \mathrm{diag}\!\big(\sigma_x^2,\ \sigma_y^2,\ \sigma_\theta^2\big),
\qquad
\sigma_x = \sigma_y = 0.01\ \text{m},\quad
\sigma_\theta = \mathrm{deg2rad}(0.2).
\]
This noise perturbs the post-update state components \((p_x,p_y,\theta)\).

2) Finite-duration "gust" impulse.
In addition to \(\eta_t\), we apply a brief deterministic bias over a window
\[
t \in [t_g,\ t_g+\Delta] \;=\; [\,4.0\,\mathrm{s},\ 4.8\,\mathrm{s}\,),
\]
implemented at the discrete indices \(\{t_g,\dots,t_g+\Delta\}\). During this window we set
\[
g_t \;=\; \begin{bmatrix}
\delta p_x \\[1mm] 0 \\[1mm] \delta \theta
\end{bmatrix},
\qquad
\delta p_x = 0.01\ \text{m per step},\quad
\delta \theta = \mathrm{deg2rad}(1.8)\ \text{per step},
\]
and \(g_t=\mathbf{0}\) otherwise. This models a short-lived lateral drift and a heading kick.

**Results.** Fig. \@ref(fig:unicycle-lqr-tracking-trajectory) visualizes the nominal trajectory (the dotted circle) and the TVLQR-stabilized trajectory in blue. To clearly see the impact of closed-loop LQR tracking, we also plotted the open-loop trajectory, i.e., the system's trajectory if no feedback is applied. We can observe that the TVLQR controller effectively rejects the disturbances and stabilizes the closed-loop trajectory along the nominal path.

Fig. \@ref(fig:unicycle-lqr-tracking-error) visualizes the state tracking error (position and heading error) as well as compares the closed-loop control with open-loop control. 

You can play with the code [here](https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/lqr_tracking.py).


```{r unicycle-lqr-tracking-trajectory, out.width='60%', fig.show='hold', fig.cap='LQR Trajectory Tracking for Unicyle: comparison between nominal trajectory, open-loop trajectory, and closed-loop trajectory with feedback.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/Model-based-optimization/tvlqr_trajectory.png')
```

```{r unicycle-lqr-tracking-error, out.width='90%', fig.show='hold', fig.cap='LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom).', fig.align='center', echo=FALSE}
knitr::include_graphics('images/Model-based-optimization/tvlqr_state_error.png')
knitr::include_graphics('images/Model-based-optimization/tvlqr_control.png')
```

:::
:::

## Trajectory Optimization {#traj-opt}





