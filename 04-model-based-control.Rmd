# Model-based Planning and Optimization {#model-based-plan-optimize}

In Chapter \@ref(mdp), we studied tabular MDPs with known dynamics where both the state and action spaces are finite, and we saw how dynamic-programming methods—Policy Iteration (PI) and Value Iteration (VI)—recover optimal policies.

Chapters \@ref(value-rl) and \@ref(policy-gradient) generalized these ideas to unknown dynamics and continuous spaces by introducing function approximation for value functions and policies. Those methods are *model-free*: they assume no access to the transition model and rely solely on data collected from interaction.

This chapter turns to the complementary regime: **known dynamics** with **continuous state and action spaces**. Our goal is to develop model-based planning and optimization methods that exploit the known dynamics to compute high-quality decisions efficiently.

We proceed in three steps:

1. **Linear-quadratic systems.** For linear dynamics and quadratic stage/terminal rewards (or costs), dynamic programming yields a linear optimal policy that can be computed efficiently via Riccati recursions. This setting serves as a tractable, illuminating baseline and a recurring building block in more general algorithms.

2. **Trajectory optimization (TO) for nonlinear systems.** When dynamics are nonlinear, we focus on planning an optimal state–action trajectory from a given initial condition, maximizing the cumulative reward (or minimizing cumulative cost) subject to dynamics and constraints. Unlike RL, which seeks an optimal *feedback policy* valid for all states, TO computes an *open-loop plan* (often with a time-varying local feedback around a nominal trajectory). Although less ambitious, TO naturally accommodates state/control constraints—common in motion planning under safety, actuation, and environmental limits—and is widely used in robotics and control.

3. **Model predictive control (MPC).** MPC converts open-loop plans into a feedback controller by repeatedly solving a short-horizon TO problem at each time step, applying only the first action, and receding the horizon. This receding-horizon strategy brings robustness to disturbances and model mismatch while retaining the constraint-handling benefits of TO.

We adopt the standard discrete-time dynamical system notation 
\[
x_{t+1} = f_t(x_t, u_t, w_t),
\]
where \(x_t \in \mathbb{R}^n\) is the state, \(u_t \in \mathbb{R}^m\) is the control/action, \(w_t \in \mathbb{R}^d\) is a (possibly stochastic) disturbance, and \(f_t\) is a known transition function, potentially nonlinear or nonsmooth. The goal is to find a policy that maximizes a sum of stage rewards \(r(x_t,u_t)\) and optional terminal reward \(r_T(x_T)\). We will often use the cost-minimization form \(c = -r\).
State and action constraints are written as
\[
x_t \in \mathcal{X}, \qquad u_t \in \mathcal{U}.
\]

## Linear Quadratic Regulator {#lqr}

In this section, we focus on the case when $f_t$ is a linear function, and the rewards/costs are quadratic in $x$ and $u$. This family of problems is known as linear quadratic regulator (LQR).

### Finite-Horizon LQR

Consider a linear discrete-time dynamical system 
\begin{equation}
x_{k+1} = A_k x_k + B_k u_k + w_k, \quad k=0,1,\dots,N-1,
(\#eq:lqr-linear-system)
\end{equation}
where $x_k \in \mathbb{R}^n$ the state, $u_k \in \mathbb{R}^m$ the control, $w_k \in \mathbb{R}^n$ the independent, zero-mean disturbance with given probability distribution that does not depend on $x_k,u_k$, and $A_k \in \mathbb{R}^{n \times n}, B_k \in \mathbb{R}^{n \times m}$ are known matrices determining the transition dynamics.

We want to solve the following optimal control problem 
\begin{equation}
\min_{\mu_0,\dots,\mu_{N-1}} \mathbb{E} \left\{ x_N^\top Q_N x_N + \sum_{k=0}^{N-1} \left( x_k^\top Q_k x_k + u_k^\top R_k u_k \right) \right\},
(\#eq:lqr-formulation)
\end{equation}
where $\mu_0,\dots,\mu_{N-1}$ are feedback policies/controllers that map states to actions and the expectation is taken over the randomness in $w_0,\dots,w_{N-1}$. In \@ref(eq:lqr-formulation), $\{Q_k \}_{k=0}^N$ are positive semidefinite matrices, and $\{ R_k \}_{k=0}^{N-1}$ are positive definite matrices. The formulation \@ref(eq:lqr-formulation) is known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to "regulate" the system around the origin $x=0$. 

The Bellman Optimality condition introduced in Theorem \@ref(thm:FiniteHorizonMDPBellmanOptimality) still holds for continuous state and action spaces. Therefore, we will try to follow the dynamic programming (DP) algorithm in Section \@ref(dp) to solve for the optimal policy.

The DP algorithm computes the optimal cost-to-go backwards in time. 
The terminal cost is 
$$
J_N(x_N) = x_N^\top Q_N x_N
$$
by definition.

The optimal cost-to-go at time $N-1$ is equal to
\begin{equation}
\begin{split}
J_{N-1}(x_{N-1}) = \min_{u_{N-1}} \mathbb{E}_{w_{N-1}} \{ x_{N-1}^\top Q_{N-1} x_{N-1} + u_{N-1}^\top R_{N-1} u_{N-1} + \\ \Vert \underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \Vert^2_{Q_N} \}
\end{split}
(\#eq:lqr-cost-N-1)
\end{equation}
where $\Vert v \Vert_Q^2 = v^\top Q v$ for $Q \succeq 0$. Now observe that the objective in \@ref(eq:lqr-cost-N-1) is 
\begin{equation}
\begin{split}
x_{N-1}^\top Q_{N-1} x_{N-1} + u_{N-1}^\top R_{N-1} u_{N-1} + \Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \Vert_{Q_N}^2 + \\
\mathbb{E}_{w_{N-1}} \left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^\top Q_{N-1} w_{N-1} \right] + \\
\mathbb{E}_{w_{N-1}} \left[ w_{N-1}^\top Q_N w_{N-1} \right]
\end{split}
\end{equation}
where the second line is zero due to $\mathbb{E}[w_{N-1}] = 0$ and the third line is a constant with respect to $u_{N-1}$. Consequently, the optimal control $u_{N-1}^\star$ can be computed by setting the derivative of the objective with respect to $u_{N-1}$ equal to zero 
\begin{equation}
u_{N-1}^\star = - \left[ \left( R_{N-1} + B_{N-1}^\top Q_N B_{N-1} \right)^{-1} B_{N-1}^\top Q_N A_{N-1} \right] x_{N-1}.
(\#eq:optimal-u-N-1)
\end{equation}
Plugging the optimal controller $u^\star_{N-1}$ back to the objective of \@ref(eq:lqr-cost-N-1) leads to
\begin{equation}
J_{N-1}(x_{N-1}) = x_{N-1}^\top S_{N-1} x_{N-1} + \mathbb{E} \left[ w_{N-1}^\top Q_N w_{N-1} \right],
(\#eq:optimal-cost-N-1)
\end{equation}
with 
$$
S_{N-1} = Q_{N-1} + A_{N-1}^\top \left[ Q_N - Q_N B_{N-1} \left( R_{N-1} + B_{N-1}^\top Q_N B_{N-1} \right)^{-1} B_{N-1}^\top Q_N \right] A_{N-1}.
$$
We note that $S_{N-1}$ is positive semidefinite (this is an exercise for you to convince yourself). 

Now we realize that something surprising and nice has happened.

1. The optimal controller $u^{\star}_{N-1}$ in \@ref(eq:optimal-u-N-1) is a linear feedback policy of the state $x_{N-1}$, and 

2. The optimal cost-to-go $J_{N-1}(x_{N-1})$ in \@ref(eq:optimal-cost-N-1) is quadratic in $x_{N-1}$, just the same as $J_{N}(x_N)$.

This implies that, if we continue to compute the optimal cost-to-go at time $N-2$, we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is, 

> The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time. 

We summarize the solution for the LQR problem \@ref(eq:lqr-formulation) as follows.

::: {.theorembox}
::: {.proposition #discretetimefinitehorizonlqrsolution name="Solution of Discrete-Time Finite-Horizon LQR"}
The optimal controller for the LQR problem \@ref(eq:lqr-formulation) is a linear state-feedback policy
\begin{equation}
\mu_k^\star(x_k) = - K_k x_k, \quad k=0,\dots,N-1.
(\#eq:lqr-solution-control)
\end{equation}
The gain matrix $K_k$ can be computed as
$$
K_k = \left( R_k + B_k^\top S_{k+1} B_k  \right)^{-1} B_k^\top S_{k+1} A_k,
$$
where the matrix $S_k$ satisfies the following backwards recursion
\begin{equation}
\hspace{-6mm}
\begin{split}
S_N &= Q_N \\
S_k &= Q_k + A_k^\top \left[ S_{k+1} - S_{k+1}B_k \left( R_k + B_k^\top S_{k+1} B_k  \right)^{-1}  B_k^\top S_{k+1}  \right] A_k, k=N-1,\dots,0.
\end{split}
(\#eq:finite-discrete-lqr-riccati)
\end{equation}
The optimal cost-to-go is given by 
$$
J_0(x_0) = x_0^\top S_0 x_0 + \sum_{k=0}^{N-1} \mathbb{E} \left[ w_k^\top S_{k+1} w_k\right].
$$
The recursion \@ref(eq:finite-discrete-lqr-riccati) is called the _discrete-time Riccati equation_.
:::
:::

Proposition \@ref(prp:discretetimefinitehorizonlqrsolution) states that, to evaluate the optimal policy \@ref(eq:lqr-solution-control), one can first run the backwards Riccati equation \@ref(eq:finite-discrete-lqr-riccati) to compute all the positive definite matrices $S_k$, and then compute the gain matrices $K_k$. For systems of reasonable dimensions, evalutating the matrix inversion in \@ref(eq:finite-discrete-lqr-riccati) should be fairly efficient.




### Infinite-Horizon LQR {#infinite-horizon-lqr}

We now switch to the infinite-horizon LQR problem
\begin{align}
\min_{\mu} & \quad  \sum_{k=0}^{\infty} \left( x_k^\top Q x_k + u_k^\top R u_k \right) (\#eq:infinite-horizon-lqr-cost) \\
\text{subject to} & \quad x_{k+1} = A x_k + B u_k, \quad k=0,\dots,\infty, (\#eq:infinite-horizon-lqr-system)
\end{align}
where $Q \succeq 0$, $R \succ 0$, $A,B$ are constant matrices, and we seek a stationary policy $\mu$ that maps states to actions. Note that here we remove the disturbance $w_k$ because in general adding $w_k$ will make the objective function unbounded. To handle $w_k$, we will have to either add a discount factor $\gamma$, or switch to an average cost objective function. 

For infinite-horizon problems, the Bellman Optimality condition changes from a recursion to an equation. Specifically, according to Theorem \@ref(thm:BellmanOptimalityInfiniteHorizon) and equation \@ref(eq:BellmanOptimalityInfiniteHorizonStateValue), the optimal value function should satisfy the following Bellman optimality equation, restated for the case of cost minimization instead of reward maximization:
\begin{equation}
J^\star (x) = \min_{u} \left[ c(x,u) + \sum_{x'} P(x' \mid x, u) J^\star (x') \right], \quad \forall x,
(\#eq:BellmanOptimalityInfiniteHorizonRestateMin)
\end{equation}
where $c(x,u)$ is the cost function.

**Guess A Solution.** Based on our derivation in the finite-horizon case, we might as well guess that the optimal value function is a quadratic function:
$$
J(x) = x^\top S x, \quad \forall x,
$$
for some positive definite matrix $S$. Then, our guessed solution must satisfy the Bellman optimality stated in \@ref(eq:BellmanOptimalityInfiniteHorizonRestateMin):
\begin{equation}
x^\top S x = J(x) = \min_{u} \left\{ x^\top Q x + u^\top R u + \Vert \underbrace{A x + B u}_{x'} \Vert_S^2  \right\}.
(\#eq:infinite-horizon-lqr-invoke-dp)
\end{equation}
The minimization over $u$ in \@ref(eq:infinite-horizon-lqr-invoke-dp) can again be solved in closed-form by setting the gradient of the objective with respect to $u$ to be zero
\begin{equation}
u^\star = - \underbrace{\left[ \left( R + B^\top S B \right)^{-1} B^\top S A \right]}_{K} x.
(\#eq:infinite-horizon-lqr-control)
\end{equation}
Plugging the optimal $u^\star$ back into \@ref(eq:infinite-horizon-lqr-invoke-dp), we see that the matrix $S$ has to satisfy the following equation
\begin{equation}
S = Q + A^\top \left[  S - SB \left( R + B^\top S B  \right)^{-1} B^\top S \right] A.
(\#eq:algebraic-riccati)
\end{equation}
Equation \@ref(eq:algebraic-riccati) is known as the _discrete algebraic Riccati equation_ (DARE). 

So the question boils down to if the DARE has a solution $S$ that is positive definite?

::: {.theorembox}
::: {.proposition #infinitehorizonlqrsolution name="Solution of Discrete-Time Infinite-Horizon LQR"} 
Consider a linear system 
$$
x_{k+1} = A x_k + B u_k,
$$
with $(A,B)$ controllable (see Section \@ref(linear-system-basics)). Let $Q \succeq 0$ in \@ref(eq:infinite-horizon-lqr-cost) be such that $Q$ can be written as $Q = C^\top C$ with $(A,C)$ observable. 

Then the optimal controller for the infinite-horizon LQR problem \@ref(eq:infinite-horizon-lqr-cost) is a stationary linear policy
$$
\mu^\star (x) = - K x,
$$
with 
$$
K = \left( R + B^\top S B \right)^{-1} B^\top S A.
$$
The matrix $S$ is the unique positive definite matrix that satisfies the discrete algebraic Riccati equation 
\begin{equation}
S = Q + A^\top \left[  S - SB \left( R + B^\top S B  \right)^{-1} B^\top S \right] A.
(\#eq:discrete-algebraic-riccati-equation)
\end{equation}

Moreover, the closed-loop system 
$$
x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k
$$
is stable, i.e., the eigenvalues of the matrix $A - BK$ are strictly within the unit circle (see Appendix \@ref(app-lti-stability-dt)).
:::
:::

*Remark.* The assumptions of $(A,B)$ being controllable and $(A,C)$ being observable can be relaxted to $(A,B)$ being stabilizable and $(A,C)$ being detectable (for definitions of stabilizability and detectability, see Appendix \@ref(app-lti-system-theory)).

We have not discussed how to solve the algebraic Riccati equation \@ref(eq:discrete-algebraic-riccati-equation). It is clear that \@ref(eq:discrete-algebraic-riccati-equation) is not a linear system of equations in $S$. In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see [@arnold84ieee-generalized]. Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab [`dlqr`](https://www.mathworks.com/help/control/ref/dlqr.html) and the Python [`scipy.linalg.solve_discrete_are`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_discrete_are.html) function computes the $K$ and $S$ matrices from $A,B,Q,R$. 

Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum.


::: {.examplebox}
::: {.example #lqr-pendulum-stabilization name="Pendulum Stabilization by LQR"}
Consider the simple pendulum in Fig. \@ref(fig:pendulum-drawing) with dynamics
\begin{equation}
x = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}, \quad 
\dot{x} = f(x,u) = \begin{bmatrix}
\dot{\theta} \\
-\frac{1}{ml^2}(b \dot{\theta} + mgl \sin \theta) + \frac{1}{ml^2} u
\end{bmatrix}
(\#eq:lqr-pendulum-dynamics)
\end{equation}
where $m$ is the mass of the pendulum, $l$ is the length of the pole, $g$ is the gravitational constant, $b$ is the damping ratio, and $u$ is the torque applied to the pendulum. 

We are interested in applying the LQR controller to balance the pendulum in the upright position $x_d = [\pi,0]^\top$ with a zero velocity.

```{r pendulum-drawing, out.width='40%', fig.show='hold', fig.cap='A Simple Pendulum.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/pendulum-drawing.png')
```

Let us first shift the dynamics so that "$0$" is the upright position. This can be done by defining a new variable $z = x - x_d = [\theta - \pi, \dot{\theta}]^\top$, which leads to 
\begin{equation}
\dot{z} = \dot{x} = f(x,u) = f(z + x_d,u) = \begin{bmatrix}
z_2 \\
\frac{1}{ml^2} \left( u - b z_2 + mgl \sin z_1  \right)
\end{bmatrix} = f'(z,u).
(\#eq:pendulum-dynamics-z-coordinate)
\end{equation}
We then linearize the nonlinear dynamics $\dot{z} = f'(z,u)$ at the point $z^\star = 0, u^\star = 0$:
\begin{align}
\dot{z} & \approx f'(z^\star,u^\star) + \left( \frac{\partial f'}{\partial z} \right)_{z^\star,u^\star} (z - z^\star) + \left( \frac{\partial f'}{\partial u} \right)_{z^\star,u^\star} (u - u^\star) \\
& = \begin{bmatrix}
0 & 1 \\
\frac{g}{l} \cos z_1 & - \frac{b}{ml^2}
\end{bmatrix}_{z^\star, u^\star} z + 
\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix} u \\
& = \underbrace{\begin{bmatrix}
0 & 1 \\
\frac{g}{l} & - \frac{b}{ml^2}
\end{bmatrix}}_{A_c} z  + 
\underbrace{\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix}}_{B_c} u.
\end{align} 
Finally, we convert the continuous-time dynamics to discrete time with a fixed discretization $h$
$$
z_{k+1} = \dot{z}_k \cdot h + z_k = \underbrace{(h \cdot A_c + I )}_{A} z_k + \underbrace{(h \cdot B_c)}_{B} u_k.
$$

We are now ready to implement the LQR controller. In the formulation \@ref(eq:infinite-horizon-lqr-cost), we choose $Q = I$, $R = I$, and compute the gain matrix $K$ by solving the DARE.

Fig. \@ref(fig:pendulum-stabilization-sim) shows the simulation result for $m=1,l=1,b=0.1$, $g = 9.8$, and $h = 0.01$, with an initial condition $z^0 = [0.1,0.1]^\top$. We can see that the LQR controller successfully stabilizes the pendulum at $z^\star$, the upright position.

You can play with the Python code [here](https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_lqr_stabilization.py). 

Alternatively, the Matlab code can be found [here](https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/pendulum_stabilization_lqr.m).

```{r pendulum-stabilization-sim, out.width='60%', fig.show='hold', fig.cap='LQR stabilization of a simple pendulum.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/Model-based-optimization/pendulum_lqr_stabilization.png')
```
:::
:::

### Linear System Basics {#linear-system-basics}

Consider the discrete-time linear time-invariant (LTI) system
\[
x_{k+1}=Ax_k+Bu_k,\qquad y_k=Cx_k+Du_k,
\]
with \(x_k\in\mathbb{R}^n,\;u_k\in\mathbb{R}^m,\;y_k\in\mathbb{R}^p\).

We provide a very brief review of linear system theory to understand Proposition \@ref(prp:infinitehorizonlqrsolution). More details can be found in Appendix \@ref(app-lti-system-theory).


**Stability.** The autonomous system \(x_{k+1}=Ax_k\) is (asymptotically) stable if for every \(x_0\) we have \(x_k\to 0\) as \(k\to\infty\).

**Equivalent characterizations.**

- \(A\) is **Schur**: all eigenvalues satisfy \(|\lambda_i(A)|<1\). 

- Lyapunov: \(\exists P\succ 0\) s.t. \(A^\top P A - P \prec 0\).  


**Controllability (reachability).** The pair \((A,B)\) is controllable if for any \(x_0,x_f\) there exists a *finite* input sequence \(\{u_0,\dots,u_{N-1}\}\) that drives the state from \(x_0\) to \(x_N=x_f\).

**Kalman controllability matrix.**
  \[
  \mathcal C \;=\; [\,B\; AB\; A^2B\;\cdots\; A^{n-1}B\,],\quad
  \text{\((A,B)\) controllable} \iff \operatorname{rank}(\mathcal C)=n.
  \]

**Popov-Belevitch-Hautus (PBH) test.**
  \[
  \text{\((A,B)\) controllable} \iff 
  \operatorname{rank}\!\begin{bmatrix}\lambda I - A & B\end{bmatrix} = n
  \ \text{for all}\ \lambda\in\mathbb{C}.
  \]
It suffices to check \(\lambda\) equal to the eigenvalues of \(A\).

**Observability.** The pair \((A,C)\) is observable if the initial state \(x_0\) can be uniquely determined from a finite sequence of outputs (and known inputs), e.g., from \(\{y_0,\dots,y_{n-1}\}\).

**Observability matrix.**
  \[
  \mathcal O \;=\; \begin{bmatrix} C \\ CA \\ \vdots \\ CA^{n-1}\end{bmatrix},\quad
  \text{\((A,C)\) observable} \iff \operatorname{rank}(\mathcal O)=n.
  \]

**PBH test.**
  \[
  \text{\((A,C)\) observable} \iff 
  \operatorname{rank}\!\begin{bmatrix}\lambda I - A^\top & C^\top\end{bmatrix} = n
  \ \text{for all}\ \lambda\in\mathbb{C}.
  \]
Dual to controllability: \((A,C)\) observable \(\Leftrightarrow\) \((A^\top,C^\top)\) controllable.


## LQR Trajectory Tracking {#lqr-tracking}

Classical LQR delivers an optimal linear state-feedback when dynamics are linear and the objective is quadratic. In many planning problems, however, we do not seek a single stationary feedback for *all* states but rather a *local stabilizer around a given (possibly time-varying) trajectory*—for instance, a motion plan from a trajectory optimizer or MPC's rolling nominal (see Section \@ref(traj-opt)). LQR Tracking (also called time-varying LQR, TVLQR) provides exactly this: a *time-varying* linear feedback that stabilizes the system near a nominal state–control sequence and rejects small disturbances.

**Problem Setup.** Let the nominal (i.e., ignoring the disturbance) discrete-time system be
\[
x_{t+1} \;=\; f_t(x_t,u_t), \qquad t=0,\dots,N-1,
\]
and suppose we have a *nominal trajectory* \(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\) satisfying
\[
\bar x_{t+1} \;=\; f_t(\bar x_t,\bar u_t).
\]

Our goal is to design a controller that can stabilize the system with disturbance, i.e., $x_{t+1} = f_t(x_t, u_t, w_t)$, around the nominal trajectory.

Towards this, we define *deviations* from the nominal trajectory as
\[
\delta x_t := x_t-\bar x_t, \qquad \delta u_t := u_t-\bar u_t.
\]

If the true system is linear time-varying (or we linearize a nonlinear system along the nominal), we obtain the *deviation dynamics*
\[
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,
\quad
A_t := \left.\frac{\partial f_t}{\partial x}\right|_{(\bar x_t,\bar u_t)},\quad
B_t := \left.\frac{\partial f_t}{\partial u}\right|_{(\bar x_t,\bar u_t)}.
(\#eq:tvlqr-deviation-dynamics)
\]

We penalize deviations with a quadratic cost
\[
J \;=\; \delta x_N^\top Q_N \delta x_N
\;+\;\sum_{t=0}^{N-1} \Big(\delta x_t^\top Q_t \delta x_t \;+\; \delta u_t^\top R_t \delta u_t\Big),
\quad Q_t\succeq 0,\; R_t\succ 0.
(\#eq:tvlqr-deviation-cost)
\]

**LQR Tracking Algorithm.** The tracking controller takes the *affine* form
\[
u_t \;=\; \bar u_t \;-\; K_t\,(x_t-\bar x_t),
\]
where \(\{K_t\}_{t=0}^{N-1}\) are time-varying gains computed by a backward Riccati recursion on the deviation system \@ref(eq:tvlqr-deviation-dynamics) with cost \@ref(eq:tvlqr-deviation-cost). 

From Proposition \@ref(prp:discretetimefinitehorizonlqrsolution), we know the gains can be computed as follows. 

Initialize at terminal time:
\[
S_N \;=\; Q_N.
\]
For \(t = N-1,\,N-2,\,\dots,\,0\):
\begin{equation}
\begin{split}
K_t &= \Big(R_t + B_t^\top S_{t+1} B_t\Big)^{-1} B_t^\top S_{t+1} A_t, \\[2mm]
S_t &= Q_t \;+\; A_t^\top \!\Big(S_{t+1} - S_{t+1} B_t \big(R_t + B_t^\top S_{t+1} B_t\big)^{-1} B_t^\top S_{t+1}\Big) A_t.
\end{split}
(\#eq:tvlqr-riccati)
\end{equation}

Given the gains \(\{K_t\}\), apply at runtime:
\begin{equation}
u_t \;=\; \bar u_t - K_t\,(x_t-\bar x_t), \qquad t=0,\dots,N-1.
(\#eq:tvlqr-control-law)
\end{equation}

The following pseudocode summarizes the algorithm.

::: {.highlightbox}
**Algorithm: LQR Trajectory Tracking (TVLQR)**

**Inputs:** nominal \(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\), weights \(\{Q_t,R_t\}\), terminal \(Q_N\).

1. **Linearize** along the nominal to get \(A_t,B_t\) via \@ref(eq:tvlqr-deviation-dynamics).  
2. **Backward pass:** compute \(K_t\) and \(S_t\) via \@ref(eq:tvlqr-riccati).  
3. **Apply feedback:** \(u_t=\bar u_t - K_t(x_t-\bar x_t)\) as in \@ref(eq:tvlqr-control-law).  

**Output:** time-varying gains \(\{K_t\}\) giving a local stabilizer around the trajectory.
:::

We now apply TVLQR to a vehicle trajectory tracking problem.


::: {.examplebox}
::: {.example #tvlqr-unicyle name="LQR Trajectory Tracking for Unicyle"}

We (i) define the dynamics in continuous and discrete time, (ii) specify a circular *nominal trajectory*, (iii) linearize the nonlinear dynamics *along the nominal*, (iv) state the deviation-cost weights \(Q,R\) (and terminal \(Q_N\)), and (v) list the *experiment setup* (discretization and horizon length).


**Unicycle Dynamics (Continuous and Discrete).**

**State and input.**  
\[
x=\begin{bmatrix}p_x\\ p_y\\ \theta\end{bmatrix}\in\mathbb{R}^3,
\qquad
u=\begin{bmatrix}v\\ \omega\end{bmatrix}\in\mathbb{R}^2,
\]
where \((p_x,p_y)\) is planar position, \(\theta\) is heading, \(v\) is forward speed, and \(\omega\) is yaw rate.

**Continuous time:**
\begin{equation}
\dot p_x = v\cos\theta,\qquad
\dot p_y = v\sin\theta,\qquad
\dot\theta = \omega.
(\#eq:unicycle-ct)
\end{equation}

**Discrete time (forward Euler with step \(h>0\)):**
\begin{equation}
x_{t+1} \;=\; f(x_t,u_t)
:= \begin{bmatrix}
p_{x,t} + h\, v_t\cos\theta_t\\[2pt]
p_{y,t} + h\, v_t\sin\theta_t\\[2pt]
\theta_t + h\,\omega_t
\end{bmatrix}.
(\#eq:unicycle-dt)
\end{equation}

**Nominal Trajectory (Circular Motion).** We track a circle of radius \(R=\dfrac{\bar v}{\bar\omega}\) using *constant nominal inputs*
\begin{equation}
\bar u_t \equiv \begin{bmatrix}\bar v\\ \bar\omega\end{bmatrix},\qquad t=0,\dots,N-1,
(\#eq:unicycle-nominal-control)
\end{equation}
and generate the *nominal state* recursively under the discrete dynamics \@ref(eq:unicycle-dt):
\begin{equation}
\bar x_{t+1} \;=\; f(\bar x_t,\bar u_t),\qquad \bar x_0 = \begin{bmatrix}R\\ 0\\ \tfrac{\pi}{2}\end{bmatrix}.
(\#eq:unicycle-discrete-nominal)
\end{equation}

We define *deviations* from the nominal:
\[
\delta x_t := x_t-\bar x_t,\qquad \delta u_t := u_t-\bar u_t.
\]

**Linearization Along the Nominal.** Linearize \@ref(eq:unicycle-dt) at \((\bar x_t,\bar u_t)\) to obtain the deviation dynamics
\begin{equation}
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,
(\#eq:unicycle-dev-dyn)
\end{equation}
with Jacobians (using \(c_t:=\cos\bar\theta_t,\ s_t:=\sin\bar\theta_t\))
\begin{equation}
A_t \;=\; \frac{\partial f}{\partial x}\Big|_{(\bar x_t,\bar u_t)}
= \begin{bmatrix}
1 & 0 & -h\,\bar v\,s_t\\
0 & 1 & \ \ h\,\bar v\,c_t\\
0 & 0 & 1
\end{bmatrix},
\qquad
B_t \;=\; \frac{\partial f}{\partial u}\Big|_{(\bar x_t,\bar u_t)}
= \begin{bmatrix}
h\,c_t & 0\\
h\,s_t & 0\\
0 & h
\end{bmatrix}.
(\#eq:unicycle-linearization)
\end{equation}

**Deviation Cost (Weights \(Q,R,Q_N\)).** We penalize deviations with a quadratic stage/terminal cost
\[
J \;=\; \delta x_N^\top Q_N\,\delta x_N
\;+\;\sum_{t=0}^{N-1}\Big(\delta x_t^\top Q\,\delta x_t+\delta u_t^\top R\,\delta u_t\Big),
\]
using the weights:
\begin{equation}
Q=\mathrm{diag}(30,\;30,\;5),\qquad
Q_N=\mathrm{diag}(60,\;60,\;8),\qquad
R=\mathrm{diag}(0.2,\;0.2).
(\#eq:unicycle-weights)
\end{equation}
These reflect a stronger emphasis on position tracking, a moderate penalty on heading error, and a mild penalty on control *deviations* from the nominal.

**Experiment Setup.**

- **Discretization step:** \(h = 0.02\ \mathrm{s}\).  
- **Horizon length:** \(T_{\mathrm{final}} = 12\ \mathrm{s}\).  
- **Number of steps:** \(N = T_{\mathrm{final}}/h = \mathbf{600}\).  
- **Nominal inputs:** \(\bar v = 1.2\ \mathrm{m/s},\ \bar\omega = 0.4\ \mathrm{rad/s}\) (radius \(R=\bar v/\bar\omega\)).  
- **Initialization:** the nominal starts at \(\bar x_0 = [\,R,\,0,\,\pi/2\,]^\top\); the actual system may start with a small offset (see code).

With \((A_t,B_t)\) from \@ref(eq:unicycle-linearization) and weights \@ref(eq:unicycle-weights), the TVLQR backward Riccati recursion (Section \@ref(lqr-tracking)) yields gains \(K_t\). We then apply the **local feedback**
\[
u_t \;=\; \bar u_t - K_t\,(x_t-\bar x_t),
\]
to robustly track the circular nominal under small disturbances.

**Disturbances.** To test robustness, we inject additive process disturbances into the discrete dynamics:
\[
x_{t+1} \;=\; f(x_t,u_t)\;+\; w_t,\qquad t=0,\dots,N-1,
\]
where
\[
w_t \;=\; \underbrace{\eta_t}_{\text{i.i.d. Gaussian noise}} \;+\; \underbrace{g_t}_{\text{deterministic gust}}.
\]

1) Small i.i.d. Gaussian process noise. We draw \(\eta_t \sim \mathcal N(0,W)\) independently at each step with
\[
W \;=\; \mathrm{diag}\!\big(\sigma_x^2,\ \sigma_y^2,\ \sigma_\theta^2\big),
\qquad
\sigma_x = \sigma_y = 0.01\ \text{m},\quad
\sigma_\theta = \mathrm{deg2rad}(0.2).
\]
This noise perturbs the post-update state components \((p_x,p_y,\theta)\).

2) Finite-duration "gust" impulse.
In addition to \(\eta_t\), we apply a brief deterministic bias over a window
\[
t \in [t_g,\ t_g+\Delta] \;=\; [\,4.0\,\mathrm{s},\ 4.8\,\mathrm{s}\,),
\]
implemented at the discrete indices \(\{t_g,\dots,t_g+\Delta\}\). During this window we set
\[
g_t \;=\; \begin{bmatrix}
\delta p_x \\[1mm] 0 \\[1mm] \delta \theta
\end{bmatrix},
\qquad
\delta p_x = 0.01\ \text{m per step},\quad
\delta \theta = \mathrm{deg2rad}(1.8)\ \text{per step},
\]
and \(g_t=\mathbf{0}\) otherwise. This models a short-lived lateral drift and a heading kick.

**Results.** Fig. \@ref(fig:unicycle-lqr-tracking-trajectory) visualizes the nominal trajectory (the dotted circle) and the TVLQR-stabilized trajectory in blue. To clearly see the impact of closed-loop LQR tracking, we also plotted the open-loop trajectory, i.e., the system's trajectory if no feedback is applied. We can observe that the TVLQR controller effectively rejects the disturbances and stabilizes the closed-loop trajectory along the nominal path.

Fig. \@ref(fig:unicycle-lqr-tracking-error) visualizes the state tracking error (position and heading error) as well as compares the closed-loop control with open-loop control. 

You can play with the code [here](https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/lqr_tracking.py).


```{r unicycle-lqr-tracking-trajectory, out.width='60%', fig.show='hold', fig.cap='LQR Trajectory Tracking for Unicyle: comparison between nominal trajectory, open-loop trajectory, and closed-loop trajectory with feedback.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/Model-based-optimization/tvlqr_trajectory.png')
```

```{r unicycle-lqr-tracking-error, out.width='90%', fig.show='hold', fig.cap='LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom).', fig.align='center', echo=FALSE}
knitr::include_graphics('images/Model-based-optimization/tvlqr_state_error.png')
knitr::include_graphics('images/Model-based-optimization/tvlqr_control.png')
```
:::
:::

## Trajectory Optimization {#traj-opt}

In Section \@ref(lqr-tracking) we saw that TVLQR gives a powerful *local stabilizer* around a nominal state–control sequence \((\bar x_t,\bar u_t)\). This raises a natural question:

> Where do nominal trajectories come from?

In many robotics tasks (maneuvering a car, landing a rocket, walking with a robot), we must compute a *feasible, high-quality open-loop plan* that respects the dynamics and constraints. **Trajectory Optimization (TO)** does exactly this: it searches over sequences \(\{x_t,u_t\}\) to minimize a cumulative cost while satisfying the system dynamics and constraints.

Moreover, if we can solve TO **quickly** (or approximately, but reliably), then by re-solving over a short horizon at each time step and applying only the first control, we obtain **Model Predictive Control (MPC)**—a feedback controller that blends optimization with robustness (see Section \@ref(mpc) later). Thus, TO is both a **planner** and the engine behind **feedback via MPC**.

**General Nonlinear Trajectory Optimization Problem.** We adopt the standard discrete-time nonlinear system
\[
x_{t+1} = f_t(x_t,u_t),\qquad t=0,\dots,N-1,
\]
with state \(x_t\in\mathbb{R}^n\) and control \(u_t\in\mathbb{R}^m\). A generic finite-horizon TO problem is
\begin{equation}
\begin{split}
\min_{\{x_t,u_t\}} \quad &
\Phi(x_N) + \sum_{t=0}^{N-1} \ell_t(x_t,u_t) \\[2mm]
\text{s.t.}\quad &
x_{t+1} = f_t(x_t,u_t), \qquad t=0,\dots,N-1,\\
& x_0 = \hat x_0 \ \ \text{(given)},\\
& x_t \in \mathcal X_t,\quad u_t \in \mathcal U_t \quad \text{(bounds)},\\
& g_t(x_t,u_t) \le 0,\quad h_t(x_t,u_t)=0 \quad \text{(path/terminal constraints).}
\end{split}
(\#eq:to-general)
\end{equation}
Here \(\ell_t\) and \(\Phi\) encode performance (e.g., energy, time, tracking error), \(\mathcal X_t,\mathcal U_t\) capture box limits and safety sets, and \(g_t,h_t\) represent additional nonlinear constraints (obstacles, terminal goals, etc.). 

Solving \@ref(eq:to-general) directly is difficult in general. A widely used strategy is to iteratively approximate it by *quadratic* subproblems that can be solved efficiently. This leads to **iLQR** and its second-order cousin **DDP** (see Section \@ref(ddp)). 

### Iterative LQR {#traj-opt-ilqr}

**High-level intuition.** iLQR (iterative LQR) alternates between:

1. **Local modeling:** around a *current* nominal trajectory \(\{(\bar x_t,\bar u_t)\}\),  
   - **linearize** the dynamics,  
   - **quadratically approximate** the cost.

2. **LQR step:** solve the resulting *time-varying LQR* subproblem to obtain a *time-varying affine policy*
   \[
   \delta u_t = k_t + K_t\,\delta x_t,\quad \delta x_t:=x_t-\bar x_t,\ \delta u_t:=u_t-\bar u_t,
   \]
   which gives both a feedforward step \(k_t\) (to change the nominal control) and a feedback gain \(K_t\) (to stabilize the rollout).

3. **Forward rollout + line search:** apply \(u_t^{\text{new}}=\bar u_t+\alpha k_t + K_t(x_t^{\text{new}}-\bar x_t)\) to the true nonlinear dynamics, producing a new nominal trajectory \(\{(\bar x_t,\bar u_t)\}\). Here we choose \(\alpha\in(0,1]\) to reduce the cost and respect constraints.

4. **Repeat** until convergence (cost decrease and dynamics residuals are small).

iLQR can be viewed as a [Gauss–Newton method](https://en.wikipedia.org/wiki/Gauss–Newton_algorithm) on trajectories: it uses first-order dynamics and second-order cost, capturing the dominant curvature while remaining numerically robust and fast.

#### LQR Subproblem (one iLQR outer iteration)

Given a nominal trajectory \(\{(\bar x_t,\bar u_t)\}_{t=0}^{N-1}\) with \(\bar x_{t+1}=f_t(\bar x_t,\bar u_t)\), define deviations
\[
\delta x_t := x_t-\bar x_t,\qquad \delta u_t := u_t-\bar u_t,\qquad \delta x_0\ \text{given.}
\]

**Linearized Dynamics.** We linearize the dynamics along the nominal trajectory
\[
\delta x_{t+1} \;\approx\; A_t\,\delta x_t + B_t\,\delta u_t,\quad
A_t:=\left.\frac{\partial f_t}{\partial x}\right|_{(\bar x_t,\bar u_t)},\ \ 
B_t:=\left.\frac{\partial f_t}{\partial u}\right|_{(\bar x_t,\bar u_t)}.
(\#eq:ilqr-linearize)
\]

**Quadratic Cost Approximation.** We perform a quadratic approximation of the objective function about \((\bar x_t,\bar u_t)\)
\[
\begin{aligned}
\ell_t(x_t,u_t) &\approx \ell_t 
+ \ell_{x,t}^\top \delta x_t + \ell_{u,t}^\top \delta u_t
+ \frac{1}{2}
\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}^{\!\top}
\!\begin{bmatrix}\ell_{xx,t} & \ell_{xu,t}\\ \ell_{ux,t} & \ell_{uu,t}\end{bmatrix}
\!\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix},\\
\Phi(x_N) &\approx \Phi + \Phi_x^\top \delta x_N + \frac{1}{2}\,\delta x_N^\top \Phi_{xx}\,\delta x_N.
\end{aligned}
(\#eq:ilqr-quadratic-cost)
\]


**The LQR Subproblem.** With \@ref(eq:ilqr-linearize)–\@ref(eq:ilqr-quadratic-cost), the iLQR subproblem at this outer iteration is the **finite-horizon linear–quadratic program in deviations**:
\[
\begin{aligned}
\min_{\{\delta x_t,\delta u_t\}} \quad
& \underbrace{\frac{1}{2}\,\delta x_N^\top \Phi_{xx}\,\delta x_N + \Phi_x^\top \delta x_N}_{\text{terminal}}
\;+\; \\
& \sum_{t=0}^{N-1}
\underbrace{\Big(
\frac{1}{2}
\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}^{\!\top}
\!\begin{bmatrix}\ell_{xx,t} & \ell_{xu,t}\\ \ell_{ux,t} & \ell_{uu,t}\end{bmatrix}
\!\begin{bmatrix}\delta x_t\\ \delta u_t\end{bmatrix}
+ \ell_{x,t}^\top\delta x_t + \ell_{u,t}^\top\delta u_t
\Big)}_{\text{stage}} \\[1mm]
\text{s.t.}\quad &
\delta x_{t+1} = A_t\,\delta x_t + B_t\,\delta u_t,\qquad t=0,\dots,N-1,\\
& \delta x_0\ \text{given.}
\end{aligned}
(\#eq:ilqr-lqr-subproblem)
\]

> **Notes.** The iLQR subproblem \@ref(eq:ilqr-lqr-subproblem) is slightly different from the previous finite-horizon LQR formulation \@ref(eq:lqr-formulation) in the sense that the objective function of \@ref(eq:ilqr-lqr-subproblem) also contains **linear terms** in \(\delta x_t,\delta u_t\), and those linear terms come from the Taylor expansion of the original nonlinear objective fuctions. In this case, we will see in the following that the optimal policy is **affine** (feedforward \(k_t\) + feedback \(K_t\)).

#### Solving the Subproblem by Dynamic Programming

We posit a **quadratic value approximator** at each time:
\[
V_{t}(\delta x_{t})
\;\approx\;
V_{t}
+ V_{x,t}^\top \delta x_t
+ \frac{1}{2}\,\delta x_t^\top V_{xx,t}\,\delta x_t,
\qquad
V_{x,N}=\Phi_x,\; V_{xx,N}=\Phi_{xx}.
(\#eq:ilqr-value)
\]
Note that this quadratic value approximator also contains linear and constant terms because the objective function contains linear terms.

Define the local Q-function at stage \(t\) by substituting the linear dynamics into the next-step value (this is our familiar Q-value in RL):
\[
Q_t(\delta x_t,\delta u_t)
\;=\;
\ell_t(x_t,u_t) + V_{t+1} \big(A_t\delta x_t + B_t\delta u_t\big),
\]
which, after collecting terms, yields the iLQR blocks
\[
\begin{aligned}
Q_{x,t}&=\ell_{x,t}+A_t^\top V_{x,t+1},\qquad
Q_{u,t}=\ell_{u,t}+B_t^\top V_{x,t+1},\\
Q_{xx,t}&=\ell_{xx,t}+A_t^\top V_{xx,t+1}A_t,\quad
Q_{ux,t}=\ell_{ux,t}+B_t^\top V_{xx,t+1}A_t,\\
Q_{uu,t}&=\ell_{uu,t}+B_t^\top V_{xx,t+1}B_t.
\end{aligned}
(\#eq:ilqr-Q)
\]
The iLQR blocks assemble into a big matrix such that
\[
Q_t(\delta x_t,\delta u_t)
\;=\;
\frac{1}{2}\, \begin{bmatrix} 1 \\ \delta x_t \\ \delta u_t \end{bmatrix}^\top
\begin{bmatrix}
2c_t & Q_{x,t}^\top & Q_{u,t}^\top\\[2pt]
Q_{x,t} & Q_{xx,t} & Q_{xu,t}\\[2pt]
Q_{u,t} & Q_{ux,t} & Q_{uu,t}
\end{bmatrix}
\begin{bmatrix} 1 \\ \delta x_t \\ \delta u_t \end{bmatrix}.
(\#eq:ilqr-Q-aug)
\]
where \(c_t\) collects all stage/terminal constants e.g., \(\ell_t+\!V_{t+1}\).

**Solving the local Q (backward pass).** Set the first-order condition w.r.t. \(\delta u\):
\[
0 \;=\; \partial_{\delta u} Q_t \;=\; Q_{u,t} + Q_{ux,t}\delta x + Q_{uu,t}\delta u.
\]
Solve for the affine control law
\[
\delta u_t^\star \;=\; k_t + K_t\,\delta x,\qquad
k_t = -Q_{uu,t}^{-1} Q_{u,t},\quad
K_t = -Q_{uu,t}^{-1} Q_{ux,t}\!,
(\#eq:ilqr-kK)
\]
which is exactly the LQR solution for the quadratic \(Q_t\).

Substitute \(\delta u_t^\star\) back into \@ref(eq:ilqr-Q-aug). The minimized Q becomes a quadratic in \(\delta x\) with coefficients given by 
\[
\begin{aligned}
V_{x,t}  &= Q_{x,t} + Q_{xu,t}k_t + K_t^\top Q_{uu,t}k_t + K_t^\top Q_{u,t},\\
V_{xx,t} &= Q_{xx,t} + Q_{xu,t}K_t + K_t^\top Q_{ux,t} + K_t^\top Q_{uu,t}K_t,
\end{aligned}
(\#eq:ilqr-V)
\]
with terminal
\[
V_{x,N} = \Phi_x, V_{xx,N} = \Phi_{xx}.
\]

**Forward Pass (apply the computed policy).** Given \(\{k_t,K_t\}\), produce a candidate trajectory on the *true* nonlinear dynamics using a line search \(\alpha\in(0,1]\):
\[
\begin{aligned}
u_t^{\text{cand}} &= \bar u_t + \alpha k_t + K_t\big(x_t^{\text{cand}}-\bar x_t\big) ,\\
x_{t+1}^{\text{cand}} &= f_t\big(x_t^{\text{cand}},u_t^{\text{cand}}\big),\qquad x_0^{\text{cand}}=\hat x_0.
\end{aligned}
(\#eq:ilqr-forward)
\]
Choose \(\alpha\) (e.g., \(\{1,\frac{1}{2},\tfrac14,\dots\}\)) to reduce the **true** cost and respect constraints, then update the nominal:
\[
(\bar x_t,\bar u_t)\ \leftarrow\ (x_t^{\text{cand}},u_t^{\text{cand}}).
\]

The following pseudocode summarizes iLQR.

::: {.highlightbox}

**Algorithm: iLQR (Trajectory Generation)**

**Inputs:** dynamics \(f_t\), initial state \(\hat x_0\), horizon \(N\), stage/terminal costs \(\ell_t,\Phi\), initial guess \(\{\bar u_t\}\).

1. **Initialize** nominal rollout \(\{\bar x_t,\bar u_t\}\) from \(\hat x_0\).
2. **Linearize & quadratize** at \(\{(\bar x_t,\bar u_t)\}\): build \(A_t,B_t\) and cost derivatives.
3. **Backward pass (TVLQR):** compute \(\{k_t,K_t\}\) using \@ref(eq:ilqr-kK) and update \(V_{x,t},V_{xx,t}\) via \@ref(eq:ilqr-V).
4. **Forward rollout:** apply \(u_t^{\text{new}}=\bar u_t+\alpha k_t+K_t(x_t^{\text{new}}-\bar x_t)\) on the **true** dynamics, pick \(\alpha\) by line search.
5. **Convergence check:** stop if the cost decrease and dynamics residuals fall below thresholds; otherwise, set the new nominal and **repeat** from Step 2.
:::

The next example applies iLQR to trajectory generation for rocket landing.

::: {.examplebox}
::: {.example #iLQR-rocket-landing name="iLQR for Rocket Landing"}
We model a planar (2D) rocket with state and control
\[
x=\begin{bmatrix}p_x & p_y & v_x & v_y & \theta & \omega\end{bmatrix}^\top,\qquad
u=\begin{bmatrix}T & \tau\end{bmatrix}^\top,
\]
where \((p_x,p_y)\) is position, \((v_x,v_y)\) is velocity, \(\theta\) is attitude (pitch) and \(\omega\) its angular rate. The thrust \(T\ge 0\) acts **along the body axis** (pointing out of the engine), and \(\tau\) is a planar torque about the center of mass. Continuous-time dynamics are
\[
\begin{aligned}
\dot p_x &= v_x, &
\dot p_y &= v_y, \\
\dot v_x &= \frac{T}{m}\sin\theta, &
\dot v_y &= \frac{T}{m}\cos\theta - g, \\
\dot\theta &= \omega, &
\dot\omega &= \frac{\tau}{I_{zz}}.
\end{aligned}
(\#eq:rocket-ct)
\]
In simulation we use RK4 with stepsize \(h\) to propagate the true dynamics \@ref(eq:rocket-ct). For iLQR's local subproblems we form the continuous Jacobians \((A_c,B_c)=\big(\tfrac{\partial f}{\partial x},\tfrac{\partial f}{\partial u}\big)\) at the current nominal and use the standard first-order discrete map
\[
A_t \;\approx\; I + h\,A_c(\bar x_t,\bar u_t),\qquad
B_t \;\approx\; h\,B_c(\bar x_t,\bar u_t).
(\#eq:rocket-disc-lin)
\]

**Soft-Landing Objective.** The goal is a **soft, upright landing** at the origin:
\[
x_{\mathrm{goal}} = \mathbf{0}
\quad\Longleftrightarrow\quad
p_x=p_y=0,\; v_x=v_y=0,\; \theta=0,\; \omega=0.
\]
We penalize deviations from this goal along the entire trajectory and especially at the terminal state to encourage low touchdown velocities and an upright attitude.

**Cost Function.** With horizon \(N\) and step \(h\), the discrete objective is
\[
J \;=\; \tfrac12\,(x_N-x_g)^\top Q_f (x_N-x_g)
\;+\;\sum_{t=0}^{N-1}\Big[
\tfrac12\,(x_t-x_g)^\top Q (x_t-x_g) \;+\; \tfrac12\,u_t^\top R u_t
\Big],
(\#eq:rocket-cost)
\]
where \(x_g=\mathbf{0}\). In the example:
\[
\begin{aligned}
Q&=\mathrm{diag}(1,\ 2,\ 0.5,\ 0.5,\ 2,\ 0.5),\\
Q_f&=\mathrm{diag}(200,\ 300,\ 50,\ 50,\ 300,\ 50),\\
R&=\mathrm{diag}(10^{-3},\ 10^{-3}).
\end{aligned}
(\#eq:rocket-weights)
\]
These weights place strong emphasis on terminal altitude and attitude (\(p_y,\theta\)), moderate emphasis on velocities and lateral position, and a light regularization on the controls.

**Experiment Setup.**

- **Physical parameters.** Gravity \(g=9.81\,\mathrm{m/s^2}\), mass \(m=1.0\,\mathrm{kg}\), planar inertia \(I_{zz}=0.2\,\mathrm{kg\,m^2}\).

- **Discretization.** Stepsize \(h=0.05\,\mathrm{s}\); horizon \(T=6.0\,\mathrm{s}\); number of steps \(N=T/h=120\).

- **Initial state.** 
  \[
  x_0=\big[\,5.0,\ 10.0,\ -0.5,\ -1.0,\ \mathrm{deg2rad}(10),\ 0\,\big]^\top,
  \]
  i.e., 10 m altitude, lateral offset, small descent and slight pitch.
- **Initial nominal controls.** Constant hover thrust and zero torque:
  \[
  \bar u_t = [\,m g,\ 0\,]^\top,\qquad t=0,\dots,N-1.
  \]

- **iLQR procedure.** Each outer iteration:
  1) Linearize dynamics and quadratize the cost along the current nominal (\@ref(eq:rocket-disc-lin), \@ref(eq:rocket-cost));  
  2) Solve the **time-varying LQR** subproblem to get affine updates \(\delta u_t = k_t + K_t\,\delta x_t\);  
  3) **Forward rollout** on the nonlinear RK4 dynamics with
  \[
  u_t^{\text{new}} = \bar u_t + \alpha\,k_t + K_t\big(x_t^{\text{new}}-\bar x_t\big),
  \]
  using a backtracking line search over \(\alpha\in\{1,\tfrac12,\tfrac14,\dots\}\) (note: \(\alpha\) scales only the **feedforward** \(k_t\), not the feedback \(K_t\));  
  4) Update the nominal and repeat until cost reduction is small.

Fig. \@ref(fig:ilqr-rocket) plots the **initial**, **intermediate**, and **final** trajectories, and render the rocket as oriented rectangles (boxes) using \((p_x,p_y,\theta)\) to visualize attitude along the descent. We can see iLQR successfully generated a soft landing trajectory.

You can play with the code [here](https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/ilqr_rocket_landing.py).

```{r ilqr-rocket, out.width='100%', fig.show='hold', fig.cap='iLQR Trajectory Generation for Rocket Landing.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/Model-based-optimization/ilqr_rocket_landing.png')
```
:::
:::

### Differential Dynamic Programming {#ddp}

## Model Predictive Control {#mpc}





