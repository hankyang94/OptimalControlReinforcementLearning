\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{apalike}
\HyPL@Entry{0<</S/D>>}
\newlabel{preface}{{}{5}{Preface}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Preface}{5}{chapter*.2}\protected@file@percent }
\newlabel{feedback}{{}{5}{Feedback}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{Feedback}{5}{section*.3}\protected@file@percent }
\newlabel{offerings}{{}{5}{Offerings}{section*.4}{}}
\@writefile{toc}{\contentsline {section}{Offerings}{5}{section*.4}\protected@file@percent }
\newlabel{fall}{{}{5}{2025 Fall}{section*.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{2025 Fall}{5}{section*.5}\protected@file@percent }
\newlabel{fall-1}{{}{5}{2023 Fall}{section*.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{2023 Fall}{5}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Markov Decision Process}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{mdp}{{1}{7}{Markov Decision Process}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Finite-Horizon MDP}{7}{section.1.1}\protected@file@percent }
\newlabel{FiniteHorizonMDP}{{1.1}{7}{Finite-Horizon MDP}{section.1.1}{}}
\newlabel{eq:policy-tuple}{{1.1}{8}{Finite-Horizon MDP}{equation.1.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Value Functions}{9}{subsection.1.1.1}\protected@file@percent }
\newlabel{FiniteHorizonMDP-Value}{{1.1.1}{9}{Value Functions}{subsection.1.1.1}{}}
\newlabel{eq:FiniteHorizonMDP-state-value}{{1.2}{9}{Value Functions}{equation.1.1.2}{}}
\newlabel{eq:FiniteHorizonMDP-action-value}{{1.3}{9}{Value Functions}{equation.1.1.3}{}}
\newlabel{eq:FiniteHorizonMDP-state-value-from-action-value}{{1.4}{9}{Value Functions}{equation.1.1.4}{}}
\newlabel{eq:FiniteHorizonMDP-action-value-from-state-value}{{1.5}{9}{Value Functions}{equation.1.1.5}{}}
\newlabel{prp:BellmanConsistency}{{1.1}{9}{Bellman Consistency (Finite Horizon)}{proposition.1.1}{}}
\newlabel{eq:BellmanConsistency-State-Value}{{1.6}{10}{Bellman Consistency (Finite Horizon)}{equation.1.1.6}{}}
\newlabel{eq:BellmanConsistency-Action-Value}{{1.7}{10}{Bellman Consistency (Finite Horizon)}{equation.1.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Policy Evaluation}{10}{subsection.1.1.2}\protected@file@percent }
\newlabel{policy-evaluation}{{1.1.2}{10}{Policy Evaluation}{subsection.1.1.2}{}}
\newlabel{exm:MDPExampleGraph}{{1.1}{10}{MDP, Transition Graph, and Policy Evaluation}{example.1.1}{}}
\gdef \LT@i {\LT@entry 
    {1}{67.79906pt}\LT@entry 
    {1}{82.63962pt}\LT@entry 
    {1}{117.95471pt}\LT@entry 
    {1}{76.63962pt}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A Simple Transition Graph.}}{11}{figure.1.1}\protected@file@percent }
\newlabel{fig:mdp-robot-transition-graph}{{1.1}{11}{A Simple Transition Graph}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Hangover Transition Graph.}}{13}{figure.1.2}\protected@file@percent }
\newlabel{fig:mdp-hangover-transition-graph}{{1.2}{13}{Hangover Transition Graph}{figure.1.2}{}}
\newlabel{eq:HangoverRandomValueFunction}{{1.13}{16}{MDP, Transition Graph, and Policy Evaluation}{equation.1.1.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Principle of Optimality}{16}{subsection.1.1.3}\protected@file@percent }
\newlabel{optimality}{{1.1.3}{16}{Principle of Optimality}{subsection.1.1.3}{}}
\newlabel{eq:FiniteHorizonMDPRLProblem}{{1.14}{16}{Principle of Optimality}{equation.1.1.14}{}}
\newlabel{thm:FiniteHorizonMDPBellmanOptimality}{{1.1}{16}{Bellman Optimality (Finite Horizon, State-Value)}{theorem.1.1}{}}
\newlabel{eq:FiniteHorizonMDPBellmanOptimality}{{1.15}{16}{Bellman Optimality (Finite Horizon, State-Value)}{equation.1.1.15}{}}
\newlabel{eq:FiniteHorizonMDPStatewiseDominance}{{1.16}{16}{Bellman Optimality (Finite Horizon, State-Value)}{equation.1.1.16}{}}
\newlabel{eq:FiniteHorizonMDPOptimalPolicy}{{1.17}{17}{Bellman Optimality (Finite Horizon, State-Value)}{equation.1.1.17}{}}
\newlabel{cor:FiniteHorizonMDPBellmanOptimalityActionValue}{{1.1}{17}{Bellman Optimality (Finite Horizon, Action-Value)}{corollary.1.1}{}}
\newlabel{eq:FiniteHorizonMDPOptimalActionValue}{{1.18}{18}{Bellman Optimality (Finite Horizon, Action-Value)}{equation.1.1.18}{}}
\newlabel{eq:FiniteHorizonMDPStateValueActionValue}{{1.19}{18}{Bellman Optimality (Finite Horizon, Action-Value)}{equation.1.1.19}{}}
\newlabel{eq:FiniteHorizonMDPBellmanRecursionActionValue}{{1.20}{18}{Bellman Optimality (Finite Horizon, Action-Value)}{equation.1.1.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming}{18}{subsection.1.1.4}\protected@file@percent }
\newlabel{dp}{{1.1.4}{18}{Dynamic Programming}{subsection.1.1.4}{}}
\newlabel{exm:HangoverDynamicProgramming}{{1.2}{18}{Dynamic Programming for Hangover MDP}{example.1.2}{}}
\newlabel{eq:HangoverOptimalValueFunction}{{1.21}{21}{Dynamic Programming for Hangover MDP}{equation.1.1.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Infinite-Horizon MDP}{22}{section.1.2}\protected@file@percent }
\newlabel{InfiniteHorizonMDP}{{1.2}{22}{Infinite-Horizon MDP}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Value Functions}{23}{subsection.1.2.1}\protected@file@percent }
\newlabel{value-functions}{{1.2.1}{23}{Value Functions}{subsection.1.2.1}{}}
\newlabel{eq:InfiniteHorizonMDPStateValue}{{1.23}{23}{Value Functions}{equation.1.2.23}{}}
\newlabel{eq:InfiniteHorizonMDPActionValue}{{1.24}{23}{Value Functions}{equation.1.2.24}{}}
\newlabel{eq:InfiniteHorizonMDPStateValueActionValueRelation-1}{{1.25}{23}{Value Functions}{equation.1.2.25}{}}
\newlabel{eq:InfiniteHorizonMDPStateValueActionValueRelation-2}{{1.26}{23}{Value Functions}{equation.1.2.26}{}}
\newlabel{prp:BellmanConsistencyInfiniteHorizon}{{1.2}{23}{Bellman Consistency (Infinite Horizon)}{proposition.1.2}{}}
\newlabel{eq:BellmanConsistency-InfiniteHorizon-State-Value}{{1.27}{23}{Bellman Consistency (Infinite Horizon)}{equation.1.2.27}{}}
\newlabel{eq:BellmanConsistency-InfiniteHorizon-Action-Value}{{1.28}{24}{Bellman Consistency (Infinite Horizon)}{equation.1.2.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Policy Evaluation}{24}{subsection.1.2.2}\protected@file@percent }
\newlabel{policy-evaluation-1}{{1.2.2}{24}{Policy Evaluation}{subsection.1.2.2}{}}
\newlabel{prp:PolicyEvaluationInfiniteHorizonStateValue}{{1.3}{24}{Policy Evaluation (Infinite Horizon, State-Value)}{proposition.1.3}{}}
\newlabel{eq:PolicyEvaluationInfiniteHorizonStateValue}{{1.29}{24}{Policy Evaluation (Infinite Horizon, State-Value)}{equation.1.2.29}{}}
\newlabel{eq:BellmanOperatorPolicy}{{1.30}{25}{Policy Evaluation}{equation.1.2.30}{}}
\newlabel{eq:gamma-contraction-Bellman-operator}{{1.31}{25}{Policy Evaluation}{equation.1.2.31}{}}
\newlabel{prp:PolicyEvaluationInfiniteHorizonActionValue}{{1.4}{26}{Policy Evaluation (Infinite Horizon, Action-Value)}{proposition.1.4}{}}
\newlabel{eq:PolicyEvaluationInfiniteHorizonActionValue}{{1.32}{26}{Policy Evaluation (Infinite Horizon, Action-Value)}{equation.1.2.32}{}}
\newlabel{exm:InfiniteHorizonMDPPolicyEvaluation}{{1.3}{26}{Policy Evaluation for Inverted Pendulum}{example.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Inverted Pendulum.}}{27}{figure.1.3}\protected@file@percent }
\newlabel{fig:mdp-pendulum-illustration}{{1.3}{27}{Inverted Pendulum}{figure.1.3}{}}
\newlabel{eq:PendulumDynamicsDiscrete}{{1.33}{27}{Policy Evaluation for Inverted Pendulum}{equation.1.2.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Value Function from Policy Evaluation.}}{31}{figure.1.4}\protected@file@percent }
\newlabel{fig:mdp-pendulum-value-function-policy-evaluation}{{1.4}{31}{Value Function from Policy Evaluation}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Principle of Optimality}{31}{subsection.1.2.3}\protected@file@percent }
\newlabel{principle-of-optimality}{{1.2.3}{31}{Principle of Optimality}{subsection.1.2.3}{}}
\newlabel{thm:BellmanOptimalityInfiniteHorizon}{{1.2}{32}{Bellman Optimality (Infinite Horizon)}{theorem.1.2}{}}
\newlabel{eq:BellmanOptimalityInfiniteHorizonStateValue}{{1.34}{32}{Bellman Optimality (Infinite Horizon)}{equation.1.2.34}{}}
\newlabel{eq:InfiniteHorizonOptimalActionValue}{{1.35}{32}{Bellman Optimality (Infinite Horizon)}{equation.1.2.35}{}}
\newlabel{eq:BellmanOptimalityInfiniteHorizonActionValue}{{1.36}{32}{Bellman Optimality (Infinite Horizon)}{equation.1.2.36}{}}
\newlabel{eq:InfiniteHorizonOptimalPolicy}{{1.37}{32}{Bellman Optimality (Infinite Horizon)}{equation.1.2.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Policy Improvement}{33}{subsection.1.2.4}\protected@file@percent }
\newlabel{policy-improvement}{{1.2.4}{33}{Policy Improvement}{subsection.1.2.4}{}}
\newlabel{lem:InfiniteHorizonPolicyImprovement}{{1.1}{33}{Policy Improvement}{lemma.1.1}{}}
\newlabel{eq:ProofPolicyImprovementStepOne}{{1.38}{34}{Policy Improvement}{equation.1.2.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Policy Iteration}{35}{subsection.1.2.5}\protected@file@percent }
\newlabel{policy-iteration}{{1.2.5}{35}{Policy Iteration}{subsection.1.2.5}{}}
\newlabel{thm:PolicyIterationConvergence}{{1.3}{35}{Convergence of Policy Iteration}{theorem.1.3}{}}
\newlabel{exm:InvertedPendulumPolicyIteration}{{1.4}{36}{Policy Iteration for Inverted Pendulum}{example.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Optimal Value Function after Policy Iteration}}{40}{figure.1.5}\protected@file@percent }
\newlabel{fig:mdp-pendulum-PI-value}{{1.5}{40}{Optimal Value Function after Policy Iteration}{figure.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Optimal Policy after Policy Iteration}}{40}{figure.1.6}\protected@file@percent }
\newlabel{fig:mdp-pendulum-PI-policy}{{1.6}{40}{Optimal Policy after Policy Iteration}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Optimal Trajectory of Pendulum Swing-Up}}{41}{figure.1.7}\protected@file@percent }
\newlabel{fig:mdp-pendulum-PI-rollout-trajectory}{{1.7}{41}{Optimal Trajectory of Pendulum Swing-Up}{figure.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Value Iteration}{41}{subsection.1.2.6}\protected@file@percent }
\newlabel{value-iteration}{{1.2.6}{41}{Value Iteration}{subsection.1.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function}}{42}{figure.1.8}\protected@file@percent }
\newlabel{fig:mdp-pendulum-PI-rollout-trajectory-value}{{1.8}{42}{Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function}{figure.1.8}{}}
\newlabel{thm:ValueIterationConvergence}{{1.4}{42}{Convergence of Value Iteration}{theorem.1.4}{}}
\newlabel{exm:InvertedPendulumValueIteration}{{1.5}{43}{Value Iteration for Inverted Pendulum}{example.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Value-based Reinforcement Learning}{47}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{value-rl}{{2}{47}{Value-based Reinforcement Learning}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Tabular Methods}{47}{section.2.1}\protected@file@percent }
\newlabel{tabular-methods}{{2.1}{47}{Tabular Methods}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Policy Evaluation}{48}{subsection.2.1.1}\protected@file@percent }
\newlabel{policy-evaluation-2}{{2.1.1}{48}{Policy Evaluation}{subsection.2.1.1}{}}
\newlabel{eq:InfiniteHorizonStateValueRestate}{{2.1}{48}{Policy Evaluation}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.1}Monte Carlo Estimation}{48}{subsubsection.2.1.1.1}\protected@file@percent }
\newlabel{monte-carlo-estimation}{{2.1.1.1}{48}{Monte Carlo Estimation}{subsubsection.2.1.1.1}{}}
\newlabel{eq:return-MC}{{2.2}{48}{Monte Carlo Estimation}{equation.2.1.2}{}}
\newlabel{eq:StateValueMCEstimate}{{2.3}{49}{Monte Carlo Estimation}{equation.2.1.3}{}}
\newlabel{eq:mc-incremental}{{2.4}{49}{Monte Carlo Estimation}{equation.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.2}Temporal-Difference Learning}{50}{subsubsection.2.1.1.2}\protected@file@percent }
\newlabel{temporal-difference-learning}{{2.1.1.2}{50}{Temporal-Difference Learning}{subsubsection.2.1.1.2}{}}
\newlabel{eq:InfiniteHorizonBellmanConsistencyRestate}{{2.8}{50}{Temporal-Difference Learning}{equation.2.1.8}{}}
\newlabel{eq:TDZeroUpdate}{{2.9}{50}{Temporal-Difference Learning}{equation.2.1.9}{}}
\newlabel{eq:TDError}{{2.10}{50}{Temporal-Difference Learning}{equation.2.1.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.3}Multi-Step TD Learning}{51}{subsubsection.2.1.1.3}\protected@file@percent }
\newlabel{multi-step-td-learning}{{2.1.1.3}{51}{Multi-Step TD Learning}{subsubsection.2.1.1.3}{}}
\newlabel{eq:nStepReturn}{{2.11}{52}{Multi-Step TD Learning}{equation.2.1.11}{}}
\newlabel{eq:nStepTDUpdate}{{2.12}{52}{Multi-Step TD Learning}{equation.2.1.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.4}Eligibility Traces and TD(\(\mitlambda \))}{52}{subsubsection.2.1.1.4}\protected@file@percent }
\newlabel{eligibility-traces-and-tdlambda}{{2.1.1.4}{52}{\texorpdfstring {Eligibility Traces and TD(\(\lambda \))}{Eligibility Traces and TD(\textbackslash lambda)}}{subsubsection.2.1.1.4}{}}
\newlabel{eq:LambdaReturn}{{2.13}{53}{\texorpdfstring {Eligibility Traces and TD(\(\lambda \))}{Eligibility Traces and TD(\textbackslash lambda)}}{equation.2.1.13}{}}
\newlabel{eq:lambda-return-episodic}{{2.14}{53}{}{equation.2.1.14}{}}
\newlabel{eq:EligibilityTrace}{{2.15}{54}{\texorpdfstring {Eligibility Traces and TD(\(\lambda \))}{Eligibility Traces and TD(\textbackslash lambda)}}{equation.2.1.15}{}}
\newlabel{eq:TDLambdaUpdate}{{2.16}{54}{\texorpdfstring {Eligibility Traces and TD(\(\lambda \))}{Eligibility Traces and TD(\textbackslash lambda)}}{equation.2.1.16}{}}
\newlabel{prp:ForwardBackwardEquivalence}{{2.1}{54}{Forwardâ€“Backward Equivalence}{proposition.2.1}{}}
\newlabel{exm:PolicyEvaluationRandomWalk}{{2.1}{56}{Policy Evaluation (MC and TD Family)}{example.2.1}{}}
\newlabel{eq:trueV-rw}{{2.17}{56}{Policy Evaluation (MC and TD Family)}{equation.2.1.17}{}}
\newlabel{eq:mse-metric}{{2.18}{56}{Policy Evaluation (MC and TD Family)}{equation.2.1.18}{}}
\citation{kearns2000bias}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Policy Evaluation, MC versus TD Family, Fixed Step Size}}{57}{figure.2.1}\protected@file@percent }
\newlabel{fig:policy-evaluation-random-walk-fixed-step-size}{{2.1}{57}{Policy Evaluation, MC versus TD Family, Fixed Step Size}{figure.2.1}{}}
\newlabel{eq:per-state-decay}{{2.19}{57}{Policy Evaluation (MC and TD Family)}{equation.2.1.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Convergence Proof of TD Learning}{57}{subsection.2.1.2}\protected@file@percent }
\newlabel{convergence-proof-of-td-learning}{{2.1.2}{57}{Convergence Proof of TD Learning}{subsection.2.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Policy Evaluation, MC versus TD Family, Diminishing Step Size}}{58}{figure.2.2}\protected@file@percent }
\newlabel{fig:policy-evaluation-random-walk-diminishing-step-size}{{2.2}{58}{Policy Evaluation, MC versus TD Family, Diminishing Step Size}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}On-Policy Control}{58}{subsection.2.1.3}\protected@file@percent }
\newlabel{on-policy-control}{{2.1.3}{58}{On-Policy Control}{subsection.2.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.1}Monte Carlo Control}{58}{subsubsection.2.1.3.1}\protected@file@percent }
\newlabel{monte-carlo-control}{{2.1.3.1}{58}{Monte Carlo Control}{subsubsection.2.1.3.1}{}}
\newlabel{eq:epsilon-soft-policy}{{2.20}{58}{Monte Carlo Control}{equation.2.1.20}{}}
\newlabel{eq:MCControl-QUpdate}{{2.21}{59}{Monte Carlo Control}{equation.2.1.21}{}}
\newlabel{eq:MCControl-PI}{{2.22}{59}{Monte Carlo Control}{equation.2.1.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.2}SARSA (On-Policy TD Control)}{60}{subsubsection.2.1.3.2}\protected@file@percent }
\newlabel{sarsa-on-policy-td-control}{{2.1.3.2}{60}{SARSA (On-Policy TD Control)}{subsubsection.2.1.3.2}{}}
\newlabel{eq:SARSA-TDTarget}{{2.23}{60}{SARSA (On-Policy TD Control)}{equation.2.1.23}{}}
\newlabel{eq:SARSA-QUpdate}{{2.24}{60}{SARSA (On-Policy TD Control)}{equation.2.1.24}{}}
\newlabel{eq:SARSA-PolicyUpdate}{{2.25}{60}{SARSA (On-Policy TD Control)}{equation.2.1.25}{}}
\newlabel{eq:ExpectedSARSA-Target}{{2.26}{60}{SARSA (On-Policy TD Control)}{equation.2.1.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Off-Policy Control}{61}{subsection.2.1.4}\protected@file@percent }
\newlabel{off-policy-control}{{2.1.4}{61}{Off-Policy Control}{subsection.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.1}Importance Sampling for Policy Evaluation}{61}{subsubsection.2.1.4.1}\protected@file@percent }
\newlabel{importance-sampling-for-policy-evaluation}{{2.1.4.1}{61}{Importance Sampling for Policy Evaluation}{subsubsection.2.1.4.1}{}}
\newlabel{eq:IS-SupportCondition}{{2.27}{61}{Importance Sampling for Policy Evaluation}{equation.2.1.27}{}}
\newlabel{eq:IS-Trajectory-LikelihoodRatio}{{2.28}{62}{Importance Sampling for Policy Evaluation}{equation.2.1.28}{}}
\newlabel{eq:IS-LikelihoodRatio}{{2.29}{62}{Importance Sampling for Policy Evaluation}{equation.2.1.29}{}}
\newlabel{eq:IS-Value}{{2.30}{62}{Importance Sampling for Policy Evaluation}{equation.2.1.30}{}}
\newlabel{eq:IS-ActionValue}{{2.31}{62}{Importance Sampling for Policy Evaluation}{equation.2.1.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.2}Off-Policy Monte Carlo Control}{63}{subsubsection.2.1.4.2}\protected@file@percent }
\newlabel{off-policy-monte-carlo-control}{{2.1.4.2}{63}{Off-Policy Monte Carlo Control}{subsubsection.2.1.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.3}Q-Learning}{64}{subsubsection.2.1.4.3}\protected@file@percent }
\newlabel{q-learning}{{2.1.4.3}{64}{Q-Learning}{subsubsection.2.1.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.4}Double Q-Learning}{64}{subsubsection.2.1.4.4}\protected@file@percent }
\newlabel{double-q-learning}{{2.1.4.4}{64}{Double Q-Learning}{subsubsection.2.1.4.4}{}}
\newlabel{exm:GridWorldMCControl}{{2.2}{66}{Value-based RL for Grid World}{example.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Grid World}}{66}{figure.2.3}\protected@file@percent }
\newlabel{fig:grid-world}{{2.3}{66}{Grid World}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Convergence of Estimated Q Values.}}{67}{figure.2.4}\protected@file@percent }
\newlabel{fig:grid-world-Q-convergence}{{2.4}{67}{Convergence of Estimated Q Values}{figure.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Function Approximation}{67}{section.2.2}\protected@file@percent }
\newlabel{function-approximation}{{2.2}{67}{Function Approximation}{section.2.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Policy Gradients}{69}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{policy-gradient}{{3}{69}{Policy Gradients}{chapter.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Convex Analysis and Optimization}{71}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appconvex}{{A}{71}{Convex Analysis and Optimization}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Theory}{71}{section.A.1}\protected@file@percent }
\newlabel{appconvex-theory}{{A.1}{71}{Theory}{section.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Sets}{71}{subsection.A.1.1}\protected@file@percent }
\newlabel{sets}{{A.1.1}{71}{Sets}{subsection.A.1.1}{}}
\newlabel{def:affineset}{{A.1}{71}{Affine set}{definition.A.1}{}}
\newlabel{def:convexset}{{A.2}{71}{Convex set}{definition.A.2}{}}
\newlabel{def:cone}{{A.3}{71}{Cone}{definition.A.3}{}}
\newlabel{def:convexcone}{{A.4}{71}{Convex Cone}{definition.A.4}{}}
\newlabel{def:hyperplane}{{A.5}{71}{Hyperplane}{definition.A.5}{}}
\newlabel{def:halfspaces}{{A.6}{72}{Halfspaces}{definition.A.6}{}}
\newlabel{def:balls}{{A.7}{72}{Balls}{definition.A.7}{}}
\newlabel{def:ellipsoids}{{A.8}{72}{Ellipsoids}{definition.A.8}{}}
\newlabel{def:polyhedra}{{A.9}{72}{Polyhedra}{definition.A.9}{}}
\newlabel{def:normball}{{A.10}{72}{Norm ball}{definition.A.10}{}}
\newlabel{def:normcone}{{A.11}{72}{Norm cone}{definition.A.11}{}}
\newlabel{def:simplex}{{A.12}{72}{Simplex}{definition.A.12}{}}
\newlabel{def:symmetricmatrices}{{A.13}{72}{Symmetric,positive semidefinite,positive definite matrices}{definition.A.13}{}}
\newlabel{prp:operationpreserveconvexity}{{A.1}{73}{}{proposition.A.1}{}}
\newlabel{def:affinehull}{{A.14}{73}{Affine hull}{definition.A.14}{}}
\newlabel{def:relint}{{A.15}{73}{Relative Interior}{definition.A.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}Convex function}{73}{subsection.A.1.2}\protected@file@percent }
\newlabel{appconvex-theory-convexfunction}{{A.1.2}{73}{Convex function}{subsection.A.1.2}{}}
\newlabel{def:defcvxfunc}{{A.16}{73}{Convex function}{definition.A.16}{}}
\newlabel{prp:decidecvx}{{A.2}{74}{Conditions for Convex function}{proposition.A.2}{}}
\newlabel{prp:preservecvx}{{A.3}{74}{Operations that preserve convexity}{proposition.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.3}Lagrange dual}{74}{subsection.A.1.3}\protected@file@percent }
\newlabel{lagrange-dual}{{A.1.3}{74}{Lagrange dual}{subsection.A.1.3}{}}
\newlabel{def:defdualfunc}{{A.17}{75}{Lagrange dual function}{definition.A.17}{}}
\newlabel{def:defdualprob}{{A.18}{75}{Lagrange dual problem}{definition.A.18}{}}
\newlabel{thm:slater}{{A.1}{75}{Slater's constraint qualification}{theorem.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.4}KKT condition}{76}{subsection.A.1.4}\protected@file@percent }
\newlabel{appconvex-theory-kkt}{{A.1.4}{76}{KKT condition}{subsection.A.1.4}{}}
\newlabel{thm:KKT}{{A.2}{76}{Karush-Kuhn-Tucker(KKT) Conditions}{theorem.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Practice}{77}{section.A.2}\protected@file@percent }
\newlabel{appconvex-practice}{{A.2}{77}{Practice}{section.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}CVX Introduction}{77}{subsection.A.2.1}\protected@file@percent }
\newlabel{cvx-introduction}{{A.2.1}{77}{CVX Introduction}{subsection.A.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.2}Linear Programming (LP)}{77}{subsection.A.2.2}\protected@file@percent }
\newlabel{linear-programming-lp}{{A.2.2}{77}{Linear Programming (LP)}{subsection.A.2.2}{}}
\newlabel{eq:app-lpdef}{{A.5}{77}{Linear Programming (LP)}{equation.A.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.3}Quadratic Programming (QP)}{78}{subsection.A.2.3}\protected@file@percent }
\newlabel{quadratic-programming-qp}{{A.2.3}{78}{Quadratic Programming (QP)}{subsection.A.2.3}{}}
\newlabel{eq:app-qpdef}{{A.6}{78}{Quadratic Programming (QP)}{equation.A.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.4}Quadratically Constrained Quadratic Programming (QCQP)}{80}{subsection.A.2.4}\protected@file@percent }
\newlabel{quadratically-constrained-quadratic-programming-qcqp}{{A.2.4}{80}{Quadratically Constrained Quadratic Programming (QCQP)}{subsection.A.2.4}{}}
\newlabel{eq:app-qcqpdef}{{A.9}{80}{Quadratically Constrained Quadratic Programming (QCQP)}{equation.A.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.5}Second-Order Cone Programming (SOCP)}{82}{subsection.A.2.5}\protected@file@percent }
\newlabel{second-order-cone-programming-socp}{{A.2.5}{82}{Second-Order Cone Programming (SOCP)}{subsection.A.2.5}{}}
\newlabel{eq:app-socpdef}{{A.12}{82}{Second-Order Cone Programming (SOCP)}{equation.A.2.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.6}Semidefinite Programming (SDP)}{84}{subsection.A.2.6}\protected@file@percent }
\newlabel{semidefinite-programming-sdp}{{A.2.6}{84}{Semidefinite Programming (SDP)}{subsection.A.2.6}{}}
\newlabel{eq:app-sdpdef}{{A.25}{84}{Semidefinite Programming (SDP)}{equation.A.2.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.7}CVXPY Introduction and Examples}{86}{subsection.A.2.7}\protected@file@percent }
\newlabel{cvxpy-introduction-and-examples}{{A.2.7}{86}{CVXPY Introduction and Examples}{subsection.A.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.1}LP}{86}{subsubsection.A.2.7.1}\protected@file@percent }
\newlabel{lp}{{A.2.7.1}{86}{LP}{subsubsection.A.2.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.2}QP}{87}{subsubsection.A.2.7.2}\protected@file@percent }
\newlabel{qp}{{A.2.7.2}{87}{QP}{subsubsection.A.2.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.3}QCQP}{88}{subsubsection.A.2.7.3}\protected@file@percent }
\newlabel{qcqp}{{A.2.7.3}{88}{QCQP}{subsubsection.A.2.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.4}SOCP}{89}{subsubsection.A.2.7.4}\protected@file@percent }
\newlabel{socp}{{A.2.7.4}{89}{SOCP}{subsubsection.A.2.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.5}SDP}{90}{subsubsection.A.2.7.5}\protected@file@percent }
\newlabel{sdp}{{A.2.7.5}{90}{SDP}{subsubsection.A.2.7.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Linear System Theory}{93}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app-lti-system-theory}{{B}{93}{Linear System Theory}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Stability}{93}{section.B.1}\protected@file@percent }
\newlabel{app-lti-stability}{{B.1}{93}{Stability}{section.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}Continuous-Time Stability}{93}{subsection.B.1.1}\protected@file@percent }
\newlabel{app-lti-stability-ct}{{B.1.1}{93}{Continuous-Time Stability}{subsection.B.1.1}{}}
\newlabel{eq:app-stability-ct-linear-system}{{B.1}{93}{Continuous-Time Stability}{equation.B.1.1}{}}
\newlabel{def:ltistable}{{B.1}{93}{Asymptotic and Marginal Stability}{definition.B.1}{}}
\newlabel{thm:ltistable}{{B.1}{93}{Stability of Continuous-Time LTI System}{theorem.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.2}Discrete-Time Stability}{94}{subsection.B.1.2}\protected@file@percent }
\newlabel{app-lti-stability-dt}{{B.1.2}{94}{Discrete-Time Stability}{subsection.B.1.2}{}}
\newlabel{eq:app-stability-dt-linear-system}{{B.2}{94}{Discrete-Time Stability}{equation.B.1.2}{}}
\newlabel{thm:dtltisystemstability}{{B.2}{94}{Stability of Discrete-Time LTI System}{theorem.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.3}Lyapunov Analysis}{95}{subsection.B.1.3}\protected@file@percent }
\newlabel{lyapunov-analysis}{{B.1.3}{95}{Lyapunov Analysis}{subsection.B.1.3}{}}
\newlabel{thm:lyapunovequation}{{B.3}{95}{Lyapunov Equation}{theorem.B.3}{}}
\newlabel{eq:lyapunov-equation}{{B.3}{95}{Lyapunov Equation}{equation.B.1.3}{}}
\newlabel{cor:bestconvergencerate}{{B.1}{96}{Maximum Convergence Rate Estimation}{corollary.B.1}{}}
\citation{chen1984book-linear}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Controllability and Observability}{97}{section.B.2}\protected@file@percent }
\newlabel{app-lti-controllable-observable}{{B.2}{97}{Controllability and Observability}{section.B.2}{}}
\newlabel{eq:app-linear-system}{{B.14}{97}{Controllability and Observability}{equation.B.2.14}{}}
\newlabel{eq:app-lti-xy}{{B.15}{97}{Controllability and Observability}{equation.B.2.15}{}}
\newlabel{def:lticontrollable}{{B.2}{97}{Controllability}{definition.B.2}{}}
\newlabel{def:ltiobservable}{{B.3}{98}{Observability}{definition.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}Cayley-Hamilton Theorem}{98}{subsection.B.2.1}\protected@file@percent }
\newlabel{cayley-hamilton-theorem}{{B.2.1}{98}{Cayley-Hamilton Theorem}{subsection.B.2.1}{}}
\newlabel{thm:cayham}{{B.4}{98}{Cayley-Hamilton}{theorem.B.4}{}}
\newlabel{eq:adjugate-1}{{B.16}{98}{Cayley-Hamilton Theorem}{equation.B.2.16}{}}
\newlabel{eq:adjugate-2}{{B.17}{98}{Cayley-Hamilton Theorem}{equation.B.2.17}{}}
\newlabel{cor:cayham-1}{{B.2}{99}{}{corollary.B.2}{}}
\newlabel{cor:cayham-2}{{B.3}{99}{}{corollary.B.3}{}}
\newlabel{cor:cayham-3}{{B.4}{100}{}{corollary.B.4}{}}
\newlabel{cor:cayham-4}{{B.5}{101}{}{corollary.B.5}{}}
\citation{chen1984book-linear}
\citation{zhou1996book-robust}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.2}Equivalent Statements for Controllability}{102}{subsection.B.2.2}\protected@file@percent }
\newlabel{equivalent-statements-for-controllability}{{B.2.2}{102}{Equivalent Statements for Controllability}{subsection.B.2.2}{}}
\newlabel{thm:lticontrollable}{{B.5}{102}{Equivalent Statements for Controllability}{theorem.B.5}{}}
\citation{davison1968tac-poleassign}
\citation{chen1984book-linear}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.3}Duality}{106}{subsection.B.2.3}\protected@file@percent }
\newlabel{duality}{{B.2.3}{106}{Duality}{subsection.B.2.3}{}}
\newlabel{thm:lti-dual-conobs}{{B.6}{107}{Theorem of Duality}{theorem.B.6}{}}
\citation{chen1984book-linear}
\citation{zhou1996book-robust}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.4}Equivalent Statements for Observability}{108}{subsection.B.2.4}\protected@file@percent }
\newlabel{equivalent-statements-for-observability}{{B.2.4}{108}{Equivalent Statements for Observability}{subsection.B.2.4}{}}
\newlabel{thm:ltiobservable}{{B.7}{108}{Equivalent Statements for Observability}{theorem.B.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Stabilizability And Detectability}{108}{section.B.3}\protected@file@percent }
\newlabel{stabilizability-and-detectability}{{B.3}{108}{Stabilizability And Detectability}{section.B.3}{}}
\citation{zhou1996book-robust}
\newlabel{def:ltisystemmode}{{B.4}{109}{System Mode}{definition.B.4}{}}
\newlabel{def:ltistabilizable}{{B.5}{109}{Stabilizability}{definition.B.5}{}}
\newlabel{def:ltidetectable}{{B.6}{109}{Detectability}{definition.B.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Equivalent Statements for Stabilizability}{109}{subsection.B.3.1}\protected@file@percent }
\newlabel{equivalent-statements-for-stabilizability}{{B.3.1}{109}{Equivalent Statements for Stabilizability}{subsection.B.3.1}{}}
\newlabel{thm:ltistabilizable}{{B.8}{109}{Equivalent Statements for Stabilizability}{theorem.B.8}{}}
\citation{zhou1996book-robust}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.2}Equivalent Statements for Detectability}{111}{subsection.B.3.2}\protected@file@percent }
\newlabel{equivalent-statements-for-detectability}{{B.3.2}{111}{Equivalent Statements for Detectability}{subsection.B.3.2}{}}
\newlabel{thm:ltidetectable}{{B.9}{111}{Equivalent Statements for Detectability}{theorem.B.9}{}}
\bibdata{book.bib,packages.bib}
\bibcite{chen1984book-linear}{{1}{1984}{{Chen}}{{}}}
\bibcite{davison1968tac-poleassign}{{2}{1968}{{Davison and Wonham}}{{}}}
\bibcite{kearns2000bias}{{3}{2000}{{Kearns and Singh}}{{}}}
\bibcite{zhou1996book-robust}{{4}{1996}{{Zhou et~al.}}{{}}}
\gdef \@abspage@last{113}
