\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{apalike}
\HyPL@Entry{0<</S/D>>}
\newlabel{preface}{{}{5}{Preface}{chapter*.2}{}}
\@writefile{toc}{\contentsline {chapter}{Preface}{5}{chapter*.2}\protected@file@percent }
\newlabel{feedback}{{}{5}{Feedback}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{Feedback}{5}{section*.3}\protected@file@percent }
\newlabel{offerings}{{}{5}{Offerings}{section*.4}{}}
\@writefile{toc}{\contentsline {section}{Offerings}{5}{section*.4}\protected@file@percent }
\newlabel{fall}{{}{5}{2025 Fall}{section*.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{2025 Fall}{5}{section*.5}\protected@file@percent }
\newlabel{fall-1}{{}{5}{2023 Fall}{section*.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{2023 Fall}{5}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Markov Decision Process}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{mdp}{{1}{7}{Markov Decision Process}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Finite-Horizon MDP}{7}{section.1.1}\protected@file@percent }
\newlabel{FiniteHorizonMDP}{{1.1}{7}{Finite-Horizon MDP}{section.1.1}{}}
\newlabel{eq:policy-tuple}{{1.1}{8}{Finite-Horizon MDP}{equation.1.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Value Functions}{9}{subsection.1.1.1}\protected@file@percent }
\newlabel{FiniteHorizonMDP-Value}{{1.1.1}{9}{Value Functions}{subsection.1.1.1}{}}
\newlabel{eq:FiniteHorizonMDP-state-value}{{1.2}{9}{Value Functions}{equation.1.1.2}{}}
\newlabel{eq:FiniteHorizonMDP-action-value}{{1.3}{9}{Value Functions}{equation.1.1.3}{}}
\newlabel{eq:FiniteHorizonMDP-state-value-from-action-value}{{1.4}{9}{Value Functions}{equation.1.1.4}{}}
\newlabel{eq:FiniteHorizonMDP-action-value-from-state-value}{{1.5}{9}{Value Functions}{equation.1.1.5}{}}
\newlabel{prp:BellmanConsistency}{{1.1}{9}{Bellman Consistency (Finite Horizon)}{proposition.1.1}{}}
\newlabel{eq:BellmanConsistency-State-Value}{{1.6}{10}{Bellman Consistency (Finite Horizon)}{equation.1.1.6}{}}
\newlabel{eq:BellmanConsistency-Action-Value}{{1.7}{10}{Bellman Consistency (Finite Horizon)}{equation.1.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Policy Evaluation}{10}{subsection.1.1.2}\protected@file@percent }
\newlabel{policy-evaluation}{{1.1.2}{10}{Policy Evaluation}{subsection.1.1.2}{}}
\newlabel{exm:MDPExampleGraph}{{1.1}{10}{MDP, Transition Graph, and Policy Evaluation}{example.1.1}{}}
\gdef \LT@i {\LT@entry 
    {1}{67.79906pt}\LT@entry 
    {1}{82.63962pt}\LT@entry 
    {1}{117.95471pt}\LT@entry 
    {1}{76.63962pt}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A Simple Transition Graph.}}{11}{figure.1.1}\protected@file@percent }
\newlabel{fig:mdp-robot-transition-graph}{{1.1}{11}{A Simple Transition Graph}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Hangover Transition Graph.}}{13}{figure.1.2}\protected@file@percent }
\newlabel{fig:mdp-hangover-transition-graph}{{1.2}{13}{Hangover Transition Graph}{figure.1.2}{}}
\newlabel{eq:HangoverRandomValueFunction}{{1.13}{16}{MDP, Transition Graph, and Policy Evaluation}{equation.1.1.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Principle of Optimality}{16}{subsection.1.1.3}\protected@file@percent }
\newlabel{optimality}{{1.1.3}{16}{Principle of Optimality}{subsection.1.1.3}{}}
\newlabel{eq:FiniteHorizonMDPRLProblem}{{1.14}{16}{Principle of Optimality}{equation.1.1.14}{}}
\newlabel{thm:FiniteHorizonMDPBellmanOptimality}{{1.1}{16}{Bellman Optimality (Finite Horizon, State-Value)}{theorem.1.1}{}}
\newlabel{eq:FiniteHorizonMDPBellmanOptimality}{{1.15}{16}{Bellman Optimality (Finite Horizon, State-Value)}{equation.1.1.15}{}}
\newlabel{eq:FiniteHorizonMDPStatewiseDominance}{{1.16}{16}{Bellman Optimality (Finite Horizon, State-Value)}{equation.1.1.16}{}}
\newlabel{eq:FiniteHorizonMDPOptimalPolicy}{{1.17}{17}{Bellman Optimality (Finite Horizon, State-Value)}{equation.1.1.17}{}}
\newlabel{cor:FiniteHorizonMDPBellmanOptimalityActionValue}{{1.1}{17}{Bellman Optimality (Finite Horizon, Action-Value)}{corollary.1.1}{}}
\newlabel{eq:FiniteHorizonMDPOptimalActionValue}{{1.18}{18}{Bellman Optimality (Finite Horizon, Action-Value)}{equation.1.1.18}{}}
\newlabel{eq:FiniteHorizonMDPStateValueActionValue}{{1.19}{18}{Bellman Optimality (Finite Horizon, Action-Value)}{equation.1.1.19}{}}
\newlabel{eq:FiniteHorizonMDPBellmanRecursionActionValue}{{1.20}{18}{Bellman Optimality (Finite Horizon, Action-Value)}{equation.1.1.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming}{18}{subsection.1.1.4}\protected@file@percent }
\newlabel{dp}{{1.1.4}{18}{Dynamic Programming}{subsection.1.1.4}{}}
\newlabel{exm:HangoverDynamicProgramming}{{1.2}{18}{Dynamic Programming for Hangover MDP}{example.1.2}{}}
\newlabel{eq:HangoverOptimalValueFunction}{{1.21}{21}{Dynamic Programming for Hangover MDP}{equation.1.1.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Infinite-Horizon MDP}{22}{section.1.2}\protected@file@percent }
\newlabel{InfiniteHorizonMDP}{{1.2}{22}{Infinite-Horizon MDP}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Value Functions}{23}{subsection.1.2.1}\protected@file@percent }
\newlabel{value-functions}{{1.2.1}{23}{Value Functions}{subsection.1.2.1}{}}
\newlabel{eq:InfiniteHorizonMDPStateValue}{{1.23}{23}{Value Functions}{equation.1.2.23}{}}
\newlabel{eq:InfiniteHorizonMDPActionValue}{{1.24}{23}{Value Functions}{equation.1.2.24}{}}
\newlabel{eq:InfiniteHorizonMDPStateValueActionValueRelation-1}{{1.25}{23}{Value Functions}{equation.1.2.25}{}}
\newlabel{eq:InfiniteHorizonMDPStateValueActionValueRelation-2}{{1.26}{23}{Value Functions}{equation.1.2.26}{}}
\newlabel{prp:BellmanConsistencyInfiniteHorizon}{{1.2}{23}{Bellman Consistency (Infinite Horizon)}{proposition.1.2}{}}
\newlabel{eq:BellmanConsistency-InfiniteHorizon-State-Value}{{1.27}{23}{Bellman Consistency (Infinite Horizon)}{equation.1.2.27}{}}
\newlabel{eq:BellmanConsistency-InfiniteHorizon-Action-Value}{{1.28}{24}{Bellman Consistency (Infinite Horizon)}{equation.1.2.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Policy Evaluation}{24}{subsection.1.2.2}\protected@file@percent }
\newlabel{policy-evaluation-1}{{1.2.2}{24}{Policy Evaluation}{subsection.1.2.2}{}}
\newlabel{prp:PolicyEvaluationInfiniteHorizonStateValue}{{1.3}{24}{Policy Evaluation (Infinite Horizon, State-Value)}{proposition.1.3}{}}
\newlabel{eq:PolicyEvaluationInfiniteHorizonStateValue}{{1.29}{24}{Policy Evaluation (Infinite Horizon, State-Value)}{equation.1.2.29}{}}
\newlabel{eq:BellmanOperatorPolicy}{{1.30}{25}{Policy Evaluation}{equation.1.2.30}{}}
\newlabel{eq:gamma-contraction-Bellman-operator}{{1.31}{25}{Policy Evaluation}{equation.1.2.31}{}}
\newlabel{prp:PolicyEvaluationInfiniteHorizonActionValue}{{1.4}{26}{Policy Evaluation (Infinite Horizon, Action-Value)}{proposition.1.4}{}}
\newlabel{eq:PolicyEvaluationInfiniteHorizonActionValue}{{1.32}{26}{Policy Evaluation (Infinite Horizon, Action-Value)}{equation.1.2.32}{}}
\newlabel{exm:InfiniteHorizonMDPPolicyEvaluation}{{1.3}{26}{Policy Evaluation for Inverted Pendulum}{example.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Inverted Pendulum.}}{27}{figure.1.3}\protected@file@percent }
\newlabel{fig:mdp-pendulum-illustration}{{1.3}{27}{Inverted Pendulum}{figure.1.3}{}}
\newlabel{eq:PendulumDynamicsDiscrete}{{1.33}{27}{Policy Evaluation for Inverted Pendulum}{equation.1.2.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Value Function from Policy Evaluation.}}{31}{figure.1.4}\protected@file@percent }
\newlabel{fig:mdp-pendulum-value-function-policy-evaluation}{{1.4}{31}{Value Function from Policy Evaluation}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Principle of Optimality}{31}{subsection.1.2.3}\protected@file@percent }
\newlabel{principle-of-optimality}{{1.2.3}{31}{Principle of Optimality}{subsection.1.2.3}{}}
\newlabel{thm:BellmanOptimalityInfiniteHorizon}{{1.2}{32}{Bellman Optimality (Infinite Horizon)}{theorem.1.2}{}}
\newlabel{eq:BellmanOptimalityInfiniteHorizonStateValue}{{1.34}{32}{Bellman Optimality (Infinite Horizon)}{equation.1.2.34}{}}
\newlabel{eq:InfiniteHorizonOptimalActionValue}{{1.35}{32}{Bellman Optimality (Infinite Horizon)}{equation.1.2.35}{}}
\newlabel{eq:BellmanOptimalityInfiniteHorizonActionValue}{{1.36}{32}{Bellman Optimality (Infinite Horizon)}{equation.1.2.36}{}}
\newlabel{eq:InfiniteHorizonOptimalPolicy}{{1.37}{32}{Bellman Optimality (Infinite Horizon)}{equation.1.2.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Policy Improvement}{33}{subsection.1.2.4}\protected@file@percent }
\newlabel{policy-improvement}{{1.2.4}{33}{Policy Improvement}{subsection.1.2.4}{}}
\newlabel{lem:InfiniteHorizonPolicyImprovement}{{1.1}{33}{Policy Improvement}{lemma.1.1}{}}
\newlabel{eq:ProofPolicyImprovementStepOne}{{1.38}{34}{Policy Improvement}{equation.1.2.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Policy Iteration}{35}{subsection.1.2.5}\protected@file@percent }
\newlabel{policy-iteration}{{1.2.5}{35}{Policy Iteration}{subsection.1.2.5}{}}
\newlabel{thm:PolicyIterationConvergence}{{1.3}{35}{Convergence of Policy Iteration}{theorem.1.3}{}}
\newlabel{exm:InvertedPendulumPolicyIteration}{{1.4}{36}{Policy Iteration for Inverted Pendulum}{example.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Optimal Value Function after Policy Iteration}}{40}{figure.1.5}\protected@file@percent }
\newlabel{fig:mdp-pendulum-PI-value}{{1.5}{40}{Optimal Value Function after Policy Iteration}{figure.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Optimal Policy after Policy Iteration}}{40}{figure.1.6}\protected@file@percent }
\newlabel{fig:mdp-pendulum-PI-policy}{{1.6}{40}{Optimal Policy after Policy Iteration}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Optimal Trajectory of Pendulum Swing-Up}}{41}{figure.1.7}\protected@file@percent }
\newlabel{fig:mdp-pendulum-PI-rollout-trajectory}{{1.7}{41}{Optimal Trajectory of Pendulum Swing-Up}{figure.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Value Iteration}{41}{subsection.1.2.6}\protected@file@percent }
\newlabel{value-iteration}{{1.2.6}{41}{Value Iteration}{subsection.1.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function}}{42}{figure.1.8}\protected@file@percent }
\newlabel{fig:mdp-pendulum-PI-rollout-trajectory-value}{{1.8}{42}{Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function}{figure.1.8}{}}
\newlabel{thm:ValueIterationConvergence}{{1.4}{42}{Convergence of Value Iteration}{theorem.1.4}{}}
\newlabel{exm:InvertedPendulumValueIteration}{{1.5}{43}{Value Iteration for Inverted Pendulum}{example.1.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Value-based Reinforcement Learning}{47}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{value-rl}{{2}{47}{Value-based Reinforcement Learning}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Tabular Methods}{47}{section.2.1}\protected@file@percent }
\newlabel{tabular-methods}{{2.1}{47}{Tabular Methods}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Policy Evaluation}{48}{subsection.2.1.1}\protected@file@percent }
\newlabel{tabular-PE}{{2.1.1}{48}{Policy Evaluation}{subsection.2.1.1}{}}
\newlabel{eq:InfiniteHorizonStateValueRestate}{{2.1}{48}{Policy Evaluation}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.1}Monte Carlo Estimation}{48}{subsubsection.2.1.1.1}\protected@file@percent }
\newlabel{monte-carlo-estimation}{{2.1.1.1}{48}{Monte Carlo Estimation}{subsubsection.2.1.1.1}{}}
\newlabel{eq:return-MC}{{2.2}{48}{Monte Carlo Estimation}{equation.2.1.2}{}}
\newlabel{eq:StateValueMCEstimate}{{2.3}{49}{Monte Carlo Estimation}{equation.2.1.3}{}}
\newlabel{eq:mc-incremental}{{2.4}{49}{Monte Carlo Estimation}{equation.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.2}Temporal-Difference Learning}{50}{subsubsection.2.1.1.2}\protected@file@percent }
\newlabel{temporal-difference-learning}{{2.1.1.2}{50}{Temporal-Difference Learning}{subsubsection.2.1.1.2}{}}
\newlabel{eq:InfiniteHorizonBellmanConsistencyRestate}{{2.8}{50}{Temporal-Difference Learning}{equation.2.1.8}{}}
\newlabel{eq:TDZeroUpdate}{{2.9}{50}{Temporal-Difference Learning}{equation.2.1.9}{}}
\newlabel{eq:TDError}{{2.10}{50}{Temporal-Difference Learning}{equation.2.1.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.3}Multi-Step TD Learning}{51}{subsubsection.2.1.1.3}\protected@file@percent }
\newlabel{PE-MultiStepTD}{{2.1.1.3}{51}{Multi-Step TD Learning}{subsubsection.2.1.1.3}{}}
\newlabel{eq:nStepReturn}{{2.11}{52}{Multi-Step TD Learning}{equation.2.1.11}{}}
\newlabel{eq:nStepTDUpdate}{{2.12}{52}{Multi-Step TD Learning}{equation.2.1.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.4}Eligibility Traces and TD(\(\mitlambda \))}{52}{subsubsection.2.1.1.4}\protected@file@percent }
\newlabel{TDlambda}{{2.1.1.4}{52}{\texorpdfstring {Eligibility Traces and TD(\(\lambda \))}{Eligibility Traces and TD(\textbackslash lambda)}}{subsubsection.2.1.1.4}{}}
\newlabel{eq:LambdaReturn}{{2.13}{53}{\texorpdfstring {Eligibility Traces and TD(\(\lambda \))}{Eligibility Traces and TD(\textbackslash lambda)}}{equation.2.1.13}{}}
\newlabel{eq:lambda-return-episodic}{{2.14}{53}{}{equation.2.1.14}{}}
\newlabel{eq:EligibilityTrace}{{2.15}{54}{\texorpdfstring {Eligibility Traces and TD(\(\lambda \))}{Eligibility Traces and TD(\textbackslash lambda)}}{equation.2.1.15}{}}
\newlabel{eq:TDLambdaUpdate}{{2.16}{54}{\texorpdfstring {Eligibility Traces and TD(\(\lambda \))}{Eligibility Traces and TD(\textbackslash lambda)}}{equation.2.1.16}{}}
\newlabel{prp:ForwardBackwardEquivalence}{{2.1}{54}{Forward–Backward Equivalence}{proposition.2.1}{}}
\newlabel{exm:PolicyEvaluationRandomWalk}{{2.1}{56}{Policy Evaluation (MC and TD Family)}{example.2.1}{}}
\newlabel{eq:trueV-rw}{{2.17}{56}{Policy Evaluation (MC and TD Family)}{equation.2.1.17}{}}
\newlabel{eq:mse-metric}{{2.18}{56}{Policy Evaluation (MC and TD Family)}{equation.2.1.18}{}}
\citation{kearns2000bias}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Policy Evaluation, MC versus TD Family, Fixed Step Size}}{57}{figure.2.1}\protected@file@percent }
\newlabel{fig:policy-evaluation-random-walk-fixed-step-size}{{2.1}{57}{Policy Evaluation, MC versus TD Family, Fixed Step Size}{figure.2.1}{}}
\newlabel{eq:per-state-decay}{{2.19}{57}{Policy Evaluation (MC and TD Family)}{equation.2.1.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Convergence Proof of TD Learning}{57}{subsection.2.1.2}\protected@file@percent }
\newlabel{value-rl-convergence-td}{{2.1.2}{57}{Convergence Proof of TD Learning}{subsection.2.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Policy Evaluation, MC versus TD Family, Diminishing Step Size}}{58}{figure.2.2}\protected@file@percent }
\newlabel{fig:policy-evaluation-random-walk-diminishing-step-size}{{2.2}{58}{Policy Evaluation, MC versus TD Family, Diminishing Step Size}{figure.2.2}{}}
\newlabel{eq:TD0update-proof}{{2.20}{58}{Convergence Proof of TD Learning}{equation.2.1.20}{}}
\newlabel{eq:StationaryDistribution-Tabular}{{2.21}{58}{Convergence Proof of TD Learning}{equation.2.1.21}{}}
\citation{robbins1971convergence}
\newlabel{eq:TabularStateOnlyTransition}{{2.22}{59}{Convergence Proof of TD Learning}{equation.2.1.22}{}}
\newlabel{eq:StationaryDistribution-Tabular-State-Only}{{2.23}{59}{Convergence Proof of TD Learning}{equation.2.1.23}{}}
\newlabel{eq:BellmanOperatorRestate-TDConvergenceProof}{{2.24}{59}{Convergence Proof of TD Learning}{equation.2.1.24}{}}
\newlabel{thm:TD0ConvergenceTabular}{{2.1}{59}{TD(0) Convergence (Tabular)}{theorem.2.1}{}}
\newlabel{lem:RobbinsSiegmund}{{2.1}{59}{Robbins-Siegmund Lemma}{lemma.2.1}{}}
\newlabel{lem:TDConvergenceLemma}{{2.2}{59}{}{lemma.2.2}{}}
\newlabel{eq:def-tilde-P}{{2.25}{60}{Convergence Proof of TD Learning}{equation.2.1.25}{}}
\newlabel{eq:InnerProductInequality}{{2.27}{60}{Convergence Proof of TD Learning}{equation.2.1.27}{}}
\newlabel{eq:TDStochasticApproximation}{{2.29}{61}{Convergence Proof of TD Learning}{equation.2.1.29}{}}
\newlabel{eq:DifferenceBetweenLyapunov}{{2.30}{61}{Convergence Proof of TD Learning}{equation.2.1.30}{}}
\newlabel{eq:DifferenceBetweenLyapunov-ConditionalExpectation}{{2.31}{62}{Convergence Proof of TD Learning}{equation.2.1.31}{}}
\newlabel{eq:TD0ConvergenceProof-RS-Form}{{2.32}{62}{Convergence Proof of TD Learning}{equation.2.1.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}On-Policy Control}{62}{subsection.2.1.3}\protected@file@percent }
\newlabel{on-policy-control}{{2.1.3}{62}{On-Policy Control}{subsection.2.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.1}Monte Carlo Control}{62}{subsubsection.2.1.3.1}\protected@file@percent }
\newlabel{monte-carlo-control}{{2.1.3.1}{62}{Monte Carlo Control}{subsubsection.2.1.3.1}{}}
\newlabel{eq:epsilon-soft-policy}{{2.33}{63}{Monte Carlo Control}{equation.2.1.33}{}}
\newlabel{eq:MCControl-QUpdate}{{2.34}{63}{Monte Carlo Control}{equation.2.1.34}{}}
\newlabel{eq:MCControl-PI}{{2.35}{63}{Monte Carlo Control}{equation.2.1.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.2}SARSA (On-Policy TD Control)}{64}{subsubsection.2.1.3.2}\protected@file@percent }
\newlabel{sarsa-on-policy-td-control}{{2.1.3.2}{64}{SARSA (On-Policy TD Control)}{subsubsection.2.1.3.2}{}}
\newlabel{eq:SARSA-TDTarget}{{2.36}{64}{SARSA (On-Policy TD Control)}{equation.2.1.36}{}}
\newlabel{eq:SARSA-QUpdate}{{2.37}{64}{SARSA (On-Policy TD Control)}{equation.2.1.37}{}}
\newlabel{eq:SARSA-PolicyUpdate}{{2.38}{65}{SARSA (On-Policy TD Control)}{equation.2.1.38}{}}
\newlabel{eq:ExpectedSARSA-Target}{{2.39}{65}{SARSA (On-Policy TD Control)}{equation.2.1.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Off-Policy Control}{65}{subsection.2.1.4}\protected@file@percent }
\newlabel{off-policy-control}{{2.1.4}{65}{Off-Policy Control}{subsection.2.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.1}Importance Sampling for Policy Evaluation}{65}{subsubsection.2.1.4.1}\protected@file@percent }
\newlabel{importance-sampling-for-policy-evaluation}{{2.1.4.1}{65}{Importance Sampling for Policy Evaluation}{subsubsection.2.1.4.1}{}}
\newlabel{eq:IS-SupportCondition}{{2.40}{66}{Importance Sampling for Policy Evaluation}{equation.2.1.40}{}}
\newlabel{eq:IS-Trajectory-LikelihoodRatio}{{2.41}{66}{Importance Sampling for Policy Evaluation}{equation.2.1.41}{}}
\newlabel{eq:IS-LikelihoodRatio}{{2.42}{66}{Importance Sampling for Policy Evaluation}{equation.2.1.42}{}}
\newlabel{eq:IS-Value}{{2.43}{66}{Importance Sampling for Policy Evaluation}{equation.2.1.43}{}}
\newlabel{eq:IS-ActionValue}{{2.44}{67}{Importance Sampling for Policy Evaluation}{equation.2.1.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.2}Off-Policy Monte Carlo Control}{67}{subsubsection.2.1.4.2}\protected@file@percent }
\newlabel{off-policy-monte-carlo-control}{{2.1.4.2}{67}{Off-Policy Monte Carlo Control}{subsubsection.2.1.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.3}Q-Learning}{68}{subsubsection.2.1.4.3}\protected@file@percent }
\newlabel{q-learning}{{2.1.4.3}{68}{Q-Learning}{subsubsection.2.1.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.4}Double Q-Learning}{69}{subsubsection.2.1.4.4}\protected@file@percent }
\newlabel{double-q-learning}{{2.1.4.4}{69}{Double Q-Learning}{subsubsection.2.1.4.4}{}}
\newlabel{exm:GridWorldMCControl}{{2.2}{70}{Value-based RL for Grid World}{example.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Grid World}}{70}{figure.2.3}\protected@file@percent }
\newlabel{fig:grid-world}{{2.3}{70}{Grid World}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Convergence of Estimated Q Values.}}{71}{figure.2.4}\protected@file@percent }
\newlabel{fig:grid-world-Q-convergence}{{2.4}{71}{Convergence of Estimated Q Values}{figure.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Function Approximation}{72}{section.2.2}\protected@file@percent }
\newlabel{function-approximation}{{2.2}{72}{Function Approximation}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Basics of Continuous MDP}{72}{subsection.2.2.1}\protected@file@percent }
\newlabel{basics-of-continuous-mdp}{{2.2.1}{72}{Basics of Continuous MDP}{subsection.2.2.1}{}}
\newlabel{eq:StateOnlyMarkovKernel}{{2.45}{72}{Basics of Continuous MDP}{equation.2.2.45}{}}
\newlabel{eq:StateOnlyMarkovKernel-density}{{2.46}{72}{Basics of Continuous MDP}{equation.2.2.46}{}}
\newlabel{eq:StationaryDistribution-definition}{{2.47}{72}{Basics of Continuous MDP}{equation.2.2.47}{}}
\newlabel{eq:StationaryDistribution-definition-density}{{2.48}{73}{Basics of Continuous MDP}{equation.2.2.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Policy Evaluation}{73}{subsection.2.2.2}\protected@file@percent }
\newlabel{policy-evaluation-2}{{2.2.2}{73}{Policy Evaluation}{subsection.2.2.2}{}}
\newlabel{eq:BellmanConsistencyContinuousStateFiniteAction}{{2.49}{73}{Policy Evaluation}{equation.2.2.49}{}}
\newlabel{eq:BellmanConsistencyContinuousStateFiniteAction-1}{{2.50}{73}{Policy Evaluation}{equation.2.2.50}{}}
\newlabel{eq:BellmanOperator}{{2.51}{73}{Policy Evaluation}{equation.2.2.51}{}}
\newlabel{eq:LinearV}{{2.52}{73}{Policy Evaluation}{equation.2.2.52}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.1}Monte Carlo Estimation}{74}{subsubsection.2.2.2.1}\protected@file@percent }
\newlabel{monte-carlo-estimation-1}{{2.2.2.1}{74}{Monte Carlo Estimation}{subsubsection.2.2.2.1}{}}
\newlabel{eq:MCEstimationGoal}{{2.53}{74}{Monte Carlo Estimation}{equation.2.2.53}{}}
\newlabel{eq:MCOneStepGD}{{2.54}{74}{Monte Carlo Estimation}{equation.2.2.54}{}}
\newlabel{eq:convergence-of-MC}{{2.55}{75}{Monte Carlo Estimation}{equation.2.2.55}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2.2}Semi-Gradient TD(0)}{75}{subsubsection.2.2.2.2}\protected@file@percent }
\newlabel{semi-gradient-td0}{{2.2.2.2}{75}{Semi-Gradient TD(0)}{subsubsection.2.2.2.2}{}}
\newlabel{eq:SemiGradientTD0}{{2.56}{75}{Semi-Gradient TD(0)}{equation.2.2.56}{}}
\newlabel{eq:ResidualGradientTD0}{{2.57}{76}{Semi-Gradient TD(0)}{equation.2.2.57}{}}
\newlabel{eq:ProjectedBellmanOperator}{{2.58}{76}{Semi-Gradient TD(0)}{equation.2.2.58}{}}
\newlabel{eq:PBEFixedPoint}{{2.59}{77}{Semi-Gradient TD(0)}{equation.2.2.59}{}}
\newlabel{eq:TDLinearSystem-again}{{2.60}{77}{Semi-Gradient TD(0)}{equation.2.2.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}On-Policy Control}{78}{subsection.2.2.3}\protected@file@percent }
\newlabel{on-policy-control-1}{{2.2.3}{78}{On-Policy Control}{subsection.2.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3.1}Semi-Gradient SARSA(0)}{78}{subsubsection.2.2.3.1}\protected@file@percent }
\newlabel{semi-gradient-sarsa0}{{2.2.3.1}{78}{Semi-Gradient SARSA(0)}{subsubsection.2.2.3.1}{}}
\newlabel{eq:SARSA-TDerror}{{2.61}{78}{Semi-Gradient SARSA(0)}{equation.2.2.61}{}}
\newlabel{eq:SARSA-Update}{{2.62}{78}{Semi-Gradient SARSA(0)}{equation.2.2.62}{}}
\newlabel{eq:ExpectedSARSA}{{2.63}{79}{Semi-Gradient SARSA(0)}{equation.2.2.63}{}}
\newlabel{exm:MountainCarSemiGradientSARSA}{{2.3}{79}{Semi-Gradient SARSA for Mountain Car}{example.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Mountain Car from Gym}}{79}{figure.2.5}\protected@file@percent }
\newlabel{fig:Mountain-car-plot}{{2.5}{79}{Mountain Car from Gym}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Average return w.r.t. episode (Semi-Gradient SARSA)}}{80}{figure.2.6}\protected@file@percent }
\newlabel{fig:Mountain-car-sarsa-episode-return}{{2.6}{80}{Average return w.r.t. episode (Semi-Gradient SARSA)}{figure.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Example rollout (Semi-Gradient SARSA)}}{80}{figure.2.7}\protected@file@percent }
\newlabel{fig:Mountain-car-sarsa-rollout}{{2.7}{80}{Example rollout (Semi-Gradient SARSA)}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Average return w.r.t. episode (Semi-Gradient SARSA + Experience Replay)}}{81}{figure.2.8}\protected@file@percent }
\newlabel{fig:Mountain-car-sarsa-er-episode-return}{{2.8}{81}{Average return w.r.t. episode (Semi-Gradient SARSA + Experience Replay)}{figure.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Example rollout (Semi-Gradient SARSA + Experience Replay)}}{81}{figure.2.9}\protected@file@percent }
\newlabel{fig:Mountain-car-sarsa-er-rollout}{{2.9}{81}{Example rollout (Semi-Gradient SARSA + Experience Replay)}{figure.2.9}{}}
\citation{baird1995residual}
\citation{mnih2015human}
\citation{riedmiller2005neural}
\citation{fan2020theoretical}
\citation{sutton1998reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Off-Policy Control}{82}{subsection.2.2.4}\protected@file@percent }
\newlabel{off-policy-control-1}{{2.2.4}{82}{Off-Policy Control}{subsection.2.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4.1}Off-Policy Semi-Gradient TD(0)}{82}{subsubsection.2.2.4.1}\protected@file@percent }
\newlabel{off-policy-semi-gradient-td0}{{2.2.4.1}{82}{Off-Policy Semi-Gradient TD(0)}{subsubsection.2.2.4.1}{}}
\newlabel{eq:FAOffPolicySemiGradientTD0}{{2.65}{82}{Off-Policy Semi-Gradient TD(0)}{equation.2.2.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Baird Counterexample}}{83}{figure.2.10}\protected@file@percent }
\newlabel{fig:Baird-example}{{2.10}{83}{Baird Counterexample}{figure.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Baird Counterexample: divergence}}{83}{figure.2.11}\protected@file@percent }
\newlabel{fig:Baird-example-value}{{2.11}{83}{Baird Counterexample: divergence}{figure.2.11}{}}
\citation{sutton2008convergent}
\citation{sutton2009fast}
\citation{mahmood2015emphatic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4.2}Deep Q Network}{85}{subsubsection.2.2.4.2}\protected@file@percent }
\newlabel{DQN}{{2.2.4.2}{85}{Deep Q Network}{subsubsection.2.2.4.2}{}}
\newlabel{eq:naiveQLearningFA}{{2.66}{85}{Deep Q Network}{equation.2.2.66}{}}
\citation{mnih2015human}
\citation{antos2007fitted}
\citation{munos2008finite}
\newlabel{exm:MountainCarQLearningFA}{{2.4}{86}{DQN for Mountain Car}{example.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Average return w.r.t. episode (Naive Q Learning)}}{87}{figure.2.12}\protected@file@percent }
\newlabel{fig:Mountain-car-q-episode-return}{{2.12}{87}{Average return w.r.t. episode (Naive Q Learning)}{figure.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Example rollout (Naive Q Learning)}}{87}{figure.2.13}\protected@file@percent }
\newlabel{fig:Mountain-car-q-rollout}{{2.13}{87}{Example rollout (Naive Q Learning)}{figure.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Average return w.r.t. episode (DQN)}}{88}{figure.2.14}\protected@file@percent }
\newlabel{fig:Mountain-car-dqn-episode-return}{{2.14}{88}{Average return w.r.t. episode (DQN)}{figure.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Example rollout (DQN)}}{88}{figure.2.15}\protected@file@percent }
\newlabel{fig:Mountain-car-dqn-rollout}{{2.15}{88}{Example rollout (DQN)}{figure.2.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Policy Gradient Methods}{89}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{policy-gradient}{{3}{89}{Policy Gradient Methods}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Gradient-based Optimization}{90}{section.3.1}\protected@file@percent }
\newlabel{gradient-optimization}{{3.1}{90}{Gradient-based Optimization}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Basic Setup}{90}{subsection.3.1.1}\protected@file@percent }
\newlabel{basic-setup}{{3.1.1}{90}{Basic Setup}{subsection.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Gradient Ascent and Descent}{90}{subsection.3.1.2}\protected@file@percent }
\newlabel{gradient-ascent-and-descent}{{3.1.2}{90}{Gradient Ascent and Descent}{subsection.3.1.2}{}}
\citation{nesterov2018lectures}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2.1}Convergence Guarantees}{91}{subsubsection.3.1.2.1}\protected@file@percent }
\newlabel{convergence-guarantees}{{3.1.2.1}{91}{Convergence Guarantees}{subsubsection.3.1.2.1}{}}
\newlabel{eq:PG-GO-convexity}{{3.1}{91}{Convergence Guarantees}{equation.3.1.1}{}}
\newlabel{eq:PG-GO-Lsmooth}{{3.2}{91}{Convergence Guarantees}{equation.3.1.2}{}}
\newlabel{eq:PG-GO-descent-lemma}{{3.3}{91}{Convergence Guarantees}{equation.3.1.3}{}}
\newlabel{thm:gd-convex-smooth}{{3.1}{91}{GD on smooth convex function}{theorem.3.1}{}}
\newlabel{eq:GD-SmoothConvex-Value}{{3.4}{91}{GD on smooth convex function}{equation.3.1.4}{}}
\newlabel{eq:GD-SmoothConvex-optimal-rate}{{3.5}{91}{GD on smooth convex function}{equation.3.1.5}{}}
\citation{nesterov2018lectures}
\newlabel{eq:PG-GO-strongly-convex}{{3.6}{92}{Convergence Guarantees}{equation.3.1.6}{}}
\newlabel{thm:gd-strongly-convex}{{3.2}{92}{GD on smooth strongly convex function}{theorem.3.2}{}}
\newlabel{eq:GD-Strongly-Convex-1}{{3.7}{92}{GD on smooth strongly convex function}{equation.3.1.7}{}}
\newlabel{eq:GD-Strongly-Convex-2}{{3.8}{92}{GD on smooth strongly convex function}{equation.3.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Stochastic Gradients}{92}{subsection.3.1.3}\protected@file@percent }
\newlabel{stochastic-gradients}{{3.1.3}{92}{Stochastic Gradients}{subsection.3.1.3}{}}
\citation{garrigos2023handbook}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3.1}Convergence Guarantees}{93}{subsubsection.3.1.3.1}\protected@file@percent }
\newlabel{convergence-guarantees-1}{{3.1.3.1}{93}{Convergence Guarantees}{subsubsection.3.1.3.1}{}}
\newlabel{thm:sgd-convex-rate}{{3.3}{93}{SGD on smooth convex function}{theorem.3.3}{}}
\newlabel{eq:SGD-convex-fixed-step-size}{{3.9}{93}{SGD on smooth convex function}{equation.3.1.9}{}}
\newlabel{eq:SGD-convex-diminishing-step-size}{{3.10}{93}{SGD on smooth convex function}{equation.3.1.10}{}}
\citation{garrigos2023handbook}
\newlabel{thm:sgd-strong-rate}{{3.4}{94}{SGD on smooth strongly convex function}{theorem.3.4}{}}
\newlabel{eq:SGD-Strongly-Convex}{{3.11}{94}{SGD on smooth strongly convex function}{equation.3.1.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Beyond Vanilla Gradient Methods}{94}{subsection.3.1.4}\protected@file@percent }
\newlabel{beyond-vanilla-gradient-methods}{{3.1.4}{94}{Beyond Vanilla Gradient Methods}{subsection.3.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Policy Gradients}{95}{section.3.2}\protected@file@percent }
\newlabel{policy-gradients}{{3.2}{95}{Policy Gradients}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Setup}{95}{subsection.3.2.1}\protected@file@percent }
\newlabel{setup}{{3.2.1}{95}{Setup}{subsection.3.2.1}{}}
\newlabel{eq:trajectory-density}{{3.12}{95}{Setup}{equation.3.2.12}{}}
\newlabel{eq:PG-Trajectory-Return}{{3.13}{95}{Setup}{equation.3.2.13}{}}
\newlabel{eq:PG-return-to-go}{{3.14}{95}{Setup}{equation.3.2.14}{}}
\newlabel{eq:PG-objective}{{3.15}{95}{Setup}{equation.3.2.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.1}Policy models}{95}{subsubsection.3.2.1.1}\protected@file@percent }
\newlabel{policy-models}{{3.2.1.1}{95}{Policy models}{subsubsection.3.2.1.1}{}}
\newlabel{eq:finite-action-policy}{{3.16}{95}{Policy models}{equation.3.2.16}{}}
\newlabel{eq:continuous-action-policy}{{3.17}{96}{Policy models}{equation.3.2.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}The Policy Gradient Lemma}{96}{subsection.3.2.2}\protected@file@percent }
\newlabel{the-policy-gradient-lemma}{{3.2.2}{96}{The Policy Gradient Lemma}{subsection.3.2.2}{}}
\newlabel{thm:policy-gradient-lemma}{{3.5}{96}{Policy Gradient Lemma}{theorem.3.5}{}}
\newlabel{eq:PG-PGLemma-1}{{3.18}{96}{Policy Gradient Lemma}{equation.3.2.18}{}}
\newlabel{eq:PG-PGLemma-2}{{3.19}{96}{Policy Gradient Lemma}{equation.3.2.19}{}}
\newlabel{eq:PG-PGLemma-3}{{3.20}{96}{Policy Gradient Lemma}{equation.3.2.20}{}}
\newlabel{eq:state-visitation-distribution}{{3.21}{96}{Policy Gradient Lemma}{equation.3.2.21}{}}
\newlabel{eq:log-derivative-trick}{{3.22}{97}{The Policy Gradient Lemma}{equation.3.2.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}REINFORCE}{98}{subsection.3.2.3}\protected@file@percent }
\newlabel{reinforce}{{3.2.3}{98}{REINFORCE}{subsection.3.2.3}{}}
\newlabel{eq:PG-Estimator-1}{{3.23}{98}{REINFORCE}{equation.3.2.23}{}}
\citation{barto2012neuronlike}
\newlabel{eq:PG-Estimator-2}{{3.24}{99}{REINFORCE}{equation.3.2.24}{}}
\newlabel{exm:cartpole-reinforce}{{3.1}{99}{REINFORCE for Cart-Pole Balancing}{example.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Cart Pole balance.}}{100}{figure.3.1}\protected@file@percent }
\newlabel{fig:cart-pole-illustration}{{3.1}{100}{Cart Pole balance}{figure.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Learning curve (Naive REINFORCE).}}{100}{figure.3.2}\protected@file@percent }
\newlabel{fig:cart-pole-learning-curve-reinforce}{{3.2}{100}{Learning curve (Naive REINFORCE)}{figure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Learning curve (Minibatch REINFORCE).}}{101}{figure.3.3}\protected@file@percent }
\newlabel{fig:cart-pole-learning-curve-minibatch-reinforce}{{3.3}{101}{Learning curve (Minibatch REINFORCE)}{figure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Policy rollout (Minibatch REINFORCE).}}{101}{figure.3.4}\protected@file@percent }
\newlabel{fig:cart-pole-policy-rollout-minibatch-reinforce}{{3.4}{101}{Policy rollout (Minibatch REINFORCE)}{figure.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Baselines and Variance Reduction}{101}{subsection.3.2.4}\protected@file@percent }
\newlabel{baselines-and-variance-reduction}{{3.2.4}{101}{Baselines and Variance Reduction}{subsection.3.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4.1}Baseline}{101}{subsubsection.3.2.4.1}\protected@file@percent }
\newlabel{baseline}{{3.2.4.1}{101}{Baseline}{subsubsection.3.2.4.1}{}}
\newlabel{eq:baseline-estimator}{{3.25}{101}{Baseline}{equation.3.2.25}{}}
\newlabel{thm:baseline-invariance}{{3.6}{102}{Baseline Invariance}{theorem.3.6}{}}
\newlabel{eq:PG-baseline-form}{{3.26}{102}{Baseline Invariance}{equation.3.2.26}{}}
\newlabel{eq:baseline-estimator-Qvalue}{{3.27}{102}{Baseline Invariance}{equation.3.2.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4.2}Optimal Baseline and Advantage}{103}{subsubsection.3.2.4.2}\protected@file@percent }
\newlabel{optimal-baseline-and-advantage}{{3.2.4.2}{103}{Optimal Baseline and Advantage}{subsubsection.3.2.4.2}{}}
\newlabel{thm:variance-minimizing-baseline}{{3.7}{104}{Variance-Minimizing Baseline (per-state)}{theorem.3.7}{}}
\newlabel{eq:best-baseline}{{3.7}{104}{Variance-Minimizing Baseline (per-state)}{theorem.3.7}{}}
\newlabel{eq:advantage-def}{{3.29}{105}{Optimal Baseline and Advantage}{equation.3.2.29}{}}
\newlabel{eq:reinforce-adv-estimator}{{3.30}{105}{Optimal Baseline and Advantage}{equation.3.2.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4.3}Intuition for the Advantage}{105}{subsubsection.3.2.4.3}\protected@file@percent }
\newlabel{intuition-for-the-advantage}{{3.2.4.3}{105}{Intuition for the Advantage}{subsubsection.3.2.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4.4}REINFORCE with a Learned Value Baseline}{106}{subsubsection.3.2.4.4}\protected@file@percent }
\newlabel{REINFORCE-LearnedValue}{{3.2.4.4}{106}{REINFORCE with a Learned Value Baseline}{subsubsection.3.2.4.4}{}}
\newlabel{exm:cartpole-reinforce-learned-value}{{3.2}{107}{REINFORCE with a Learned Value Baseline for Cart-Pole}{example.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Learning curve (Minibatch REINFORCE with a Learned Value Baseline).}}{108}{figure.3.5}\protected@file@percent }
\newlabel{fig:cart-pole-learning-curve-minibatch-reinforce-learned-value}{{3.5}{108}{Learning curve (Minibatch REINFORCE with a Learned Value Baseline)}{figure.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Policy rollout (Minibatch REINFORCE with a Learned Value Baseline).}}{108}{figure.3.6}\protected@file@percent }
\newlabel{fig:cart-pole-policy-rollout-minibatch-reinforce-learned-value}{{3.6}{108}{Policy rollout (Minibatch REINFORCE with a Learned Value Baseline)}{figure.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Actor--Critic Methods}{109}{section.3.3}\protected@file@percent }
\newlabel{actorcritic-methods}{{3.3}{109}{Actor--Critic Methods}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Anatomy of an Actor--Critic}{109}{subsection.3.3.1}\protected@file@percent }
\newlabel{anatomy-of-an-actorcritic}{{3.3.1}{109}{Anatomy of an Actor--Critic}{subsection.3.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}On-Policy Actor--Critic with TD(0)}{109}{subsection.3.3.2}\protected@file@percent }
\newlabel{ActorCriticTD}{{3.3.2}{109}{On-Policy Actor--Critic with TD(0)}{subsection.3.3.2}{}}
\newlabel{eq:td0}{{3.3.2}{109}{On-Policy Actor--Critic with TD(0)}{subsection.3.3.2}{}}
\newlabel{eq:ac-actor-delta}{{3.3.2}{109}{On-Policy Actor--Critic with TD(0)}{subsection.3.3.2}{}}
\newlabel{exm:cartpole-actor-critic-TD}{{3.3}{110}{Actor–Critic with One-Step Bootstrap for Cart-Pole}{example.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Learning curve (Actor--Critic with One-Step Bootstrap).}}{110}{figure.3.7}\protected@file@percent }
\newlabel{fig:cart-pole-learning-curve-actor-critic-TD}{{3.7}{110}{Learning curve (Actor--Critic with One-Step Bootstrap)}{figure.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Policy rollout (Actor--Critic with One-Step Bootstrap).}}{110}{figure.3.8}\protected@file@percent }
\newlabel{fig:cart-pole-policy-rollout-actor-critic-TD}{{3.8}{110}{Policy rollout (Actor--Critic with One-Step Bootstrap)}{figure.3.8}{}}
\citation{schulman2015high}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Generalized Advantage Estimation (GAE)}{111}{subsection.3.3.3}\protected@file@percent }
\newlabel{PG-GAE}{{3.3.3}{111}{Generalized Advantage Estimation (GAE)}{subsection.3.3.3}{}}
\newlabel{eq:nstep-target}{{3.31}{111}{Generalized Advantage Estimation (GAE)}{equation.3.3.31}{}}
\newlabel{eq:lambda-return}{{3.32}{111}{Generalized Advantage Estimation (GAE)}{equation.3.3.32}{}}
\newlabel{eq:td-residual}{{3.3.3}{111}{Generalized Advantage Estimation (GAE)}{equation.3.3.32}{}}
\newlabel{eq:gae-def}{{3.3.3}{111}{Generalized Advantage Estimation (GAE)}{equation.3.3.32}{}}
\newlabel{eq:gae-recursion}{{3.3.3}{111}{Generalized Advantage Estimation (GAE)}{equation.3.3.32}{}}
\newlabel{eq:gae-equals-mix}{{3.3.3}{111}{Generalized Advantage Estimation (GAE)}{equation.3.3.32}{}}
\newlabel{eq:gae-vtarget}{{3.3.3}{112}{Generalized Advantage Estimation (GAE)}{equation.3.3.32}{}}
\newlabel{eq:true-pg}{{3.3.3}{112}{Generalized Advantage Estimation (GAE)}{equation.3.3.32}{}}
\newlabel{eq:cond-unbiased}{{3.3.3}{112}{Generalized Advantage Estimation (GAE)}{equation.3.3.32}{}}
\newlabel{exm:cartpole-gae}{{3.4}{113}{GAE for Cart-Pole Balancing}{example.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Learning curve (Actor--Critic with GAE).}}{113}{figure.3.9}\protected@file@percent }
\newlabel{fig:cart-pole-learning-curve-gae}{{3.9}{113}{Learning curve (Actor--Critic with GAE)}{figure.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Off-Policy Actor--Critic}{113}{subsection.3.3.4}\protected@file@percent }
\newlabel{off-policy-actorcritic}{{3.3.4}{113}{Off-Policy Actor--Critic}{subsection.3.3.4}{}}
\newlabel{eq:is-ratio}{{3.3.4}{113}{Off-Policy Actor--Critic}{subsection.3.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Policy rollout (Actor--Critic with GAE).}}{114}{figure.3.10}\protected@file@percent }
\newlabel{fig:cart-pole-policy-rollout-gae}{{3.10}{114}{Policy rollout (Actor--Critic with GAE)}{figure.3.10}{}}
\newlabel{eq:offpolicy-actor}{{3.33}{114}{Off-Policy Actor--Critic}{equation.3.3.33}{}}
\newlabel{eq:offpolicy-critic}{{3.3.4}{114}{Off-Policy Actor--Critic}{equation.3.3.33}{}}
\newlabel{eq:offpolicy-adv}{{3.3.4}{114}{Off-Policy Actor--Critic}{equation.3.3.33}{}}
\newlabel{exm:cart-pole-off-policy-actor-critic}{{3.5}{116}{Off-Policy Actor-Critic for Cart-Pole Balancing}{example.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Learning curve (Off-Policy Actor--Critic).}}{116}{figure.3.11}\protected@file@percent }
\newlabel{fig:cart-pole-learning-curve-off-policy-ac}{{3.11}{116}{Learning curve (Off-Policy Actor--Critic)}{figure.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Policy rollout (Off-Policy Actor--Critic).}}{116}{figure.3.12}\protected@file@percent }
\newlabel{fig:cart-pole-policy-rollout-off-policy-ac}{{3.12}{116}{Policy rollout (Off-Policy Actor--Critic)}{figure.3.12}{}}
\newlabel{exm:pendulum-off-policy-ac}{{3.6}{117}{Off-Policy Actor-Critic for Inverted Pendulum}{example.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Illustration of Inverted Pendulum in Gym.}}{117}{figure.3.13}\protected@file@percent }
\newlabel{fig:pendulum-diagram}{{3.13}{117}{Illustration of Inverted Pendulum in Gym}{figure.3.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Learning curve (Off-Policy Actor-Critic).}}{117}{figure.3.14}\protected@file@percent }
\newlabel{fig:pendulum-learning-curve-off-policy-ac}{{3.14}{117}{Learning curve (Off-Policy Actor-Critic)}{figure.3.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Advanced Policy Gradients}{117}{section.3.4}\protected@file@percent }
\newlabel{advanced-policy-gradients}{{3.4}{117}{Advanced Policy Gradients}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Revisiting Generalized Policy Iteration}{117}{subsection.3.4.1}\protected@file@percent }
\newlabel{revisiting-generalized-policy-iteration}{{3.4.1}{117}{Revisiting Generalized Policy Iteration}{subsection.3.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Policy rollout (Off-Policy Actor-Critic).}}{118}{figure.3.15}\protected@file@percent }
\newlabel{fig:pendulum-policy-rollout-off-policy-ac}{{3.15}{118}{Policy rollout (Off-Policy Actor-Critic)}{figure.3.15}{}}
\newlabel{eq:policy-gradient-advantage-gpi}{{3.34}{119}{Revisiting Generalized Policy Iteration}{equation.3.4.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Performance Difference Lemma}{119}{subsection.3.4.2}\protected@file@percent }
\newlabel{performance-difference-lemma}{{3.4.2}{119}{Performance Difference Lemma}{subsection.3.4.2}{}}
\newlabel{thm:pdl}{{3.8}{119}{Performance Difference Lemma}{theorem.3.8}{}}
\newlabel{eq:PDL}{{3.35}{119}{Performance Difference Lemma}{equation.3.4.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Trust Region Constraint}{120}{subsection.3.4.3}\protected@file@percent }
\newlabel{trust-region-constraint}{{3.4.3}{120}{Trust Region Constraint}{subsection.3.4.3}{}}
\newlabel{eq:KL-divergence-def}{{3.36}{120}{Trust Region Constraint}{equation.3.4.36}{}}
\newlabel{eq:trpo-surrogate}{{3.37}{120}{Trust Region Constraint}{equation.3.4.37}{}}
\newlabel{eq:trpo-kl}{{3.38}{120}{Trust Region Constraint}{equation.3.4.38}{}}
\citation{kakade2001natural}
\newlabel{eq:constrained-policy-optimization}{{3.39}{121}{Trust Region Constraint}{equation.3.4.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Natural Policy Gradient}{121}{subsection.3.4.4}\protected@file@percent }
\newlabel{natural-policy-gradient}{{3.4.4}{121}{Natural Policy Gradient}{subsection.3.4.4}{}}
\newlabel{eq:npg-qp}{{3.40}{121}{Natural Policy Gradient}{equation.3.4.40}{}}
\newlabel{eq:npg-policy-gradient}{{3.41}{121}{Natural Policy Gradient}{equation.3.4.41}{}}
\newlabel{eq:npg-fisher}{{3.42}{121}{Natural Policy Gradient}{equation.3.4.42}{}}
\newlabel{eq:npg-step}{{3.43}{121}{Natural Policy Gradient}{equation.3.4.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Proof of Fisher Information}{122}{subsection.3.4.5}\protected@file@percent }
\newlabel{proof-fisher}{{3.4.5}{122}{Proof of Fisher Information}{subsection.3.4.5}{}}
\citation{schulman2015trust}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}Trust Region Policy Optimization}{124}{subsection.3.4.6}\protected@file@percent }
\newlabel{trust-region-policy-optimization}{{3.4.6}{124}{Trust Region Policy Optimization}{subsection.3.4.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.6.1}Backtracking Line Search}{125}{subsubsection.3.4.6.1}\protected@file@percent }
\newlabel{backtracking-line-search}{{3.4.6.1}{125}{Backtracking Line Search}{subsubsection.3.4.6.1}{}}
\citation{schulman2017proximal}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Proximal Policy Optimization}{126}{subsection.3.4.7}\protected@file@percent }
\newlabel{proximal-policy-optimization}{{3.4.7}{126}{Proximal Policy Optimization}{subsection.3.4.7}{}}
\newlabel{eq:PPO-regularized}{{3.44}{126}{Proximal Policy Optimization}{equation.3.4.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.7.1}Gradient of the KL--Regularized Surrogate}{126}{subsubsection.3.4.7.1}\protected@file@percent }
\newlabel{gradient-of-the-klregularized-surrogate}{{3.4.7.1}{126}{Gradient of the KL--Regularized Surrogate}{subsubsection.3.4.7.1}{}}
\newlabel{eq:PPO-penalty-gradient}{{3.4.7.1}{126}{Gradient of the KL--Regularized Surrogate}{subsubsection.3.4.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.7.2}From the Lagrangian Relaxation to PPO Updates}{126}{subsubsection.3.4.7.2}\protected@file@percent }
\newlabel{from-the-lagrangian-relaxation-to-ppo-updates}{{3.4.7.2}{126}{From the Lagrangian Relaxation to PPO Updates}{subsubsection.3.4.7.2}{}}
\newlabel{eq:PPO-clip-obj}{{3.45}{127}{From the Lagrangian Relaxation to PPO Updates}{equation.3.4.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces The clipped objective function in PPO (from the original PPO paper).}}{128}{figure.3.16}\protected@file@percent }
\newlabel{fig:ppo-clip-obj}{{3.16}{128}{The clipped objective function in PPO (from the original PPO paper)}{figure.3.16}{}}
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.8}Soft Actor--Critic}{129}{subsection.3.4.8}\protected@file@percent }
\newlabel{soft-actorcritic}{{3.4.8}{129}{Soft Actor--Critic}{subsection.3.4.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.8.1}SAC for Discrete Actions}{129}{subsubsection.3.4.8.1}\protected@file@percent }
\newlabel{sac-for-discrete-actions}{{3.4.8.1}{129}{SAC for Discrete Actions}{subsubsection.3.4.8.1}{}}
\newlabel{eq:entropy}{{3.46}{129}{SAC for Discrete Actions}{equation.3.4.46}{}}
\newlabel{eq:soft-objective}{{3.48}{130}{SAC for Discrete Actions}{equation.3.4.48}{}}
\newlabel{eq:soft-value-functions}{{3.49}{130}{SAC for Discrete Actions}{equation.3.4.49}{}}
\newlabel{eq:Bellman-Consistency-Soft-Q}{{3.50}{130}{SAC for Discrete Actions}{equation.3.4.50}{}}
\newlabel{eq:sac-target}{{3.51}{130}{SAC for Discrete Actions}{equation.3.4.51}{}}
\newlabel{eq:sac-critic-loss}{{3.52}{130}{SAC for Discrete Actions}{equation.3.4.52}{}}
\newlabel{eq:sac-actor-discrete}{{3.53}{130}{SAC for Discrete Actions}{equation.3.4.53}{}}
\newlabel{eq:sac-actor-grad-discrete}{{3.54}{131}{SAC for Discrete Actions}{equation.3.4.54}{}}
\newlabel{exm:cartpole-sac}{{3.7}{132}{SAC for Cart-pole Balancing}{example.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Learning curve (Soft Actor--Critic).}}{132}{figure.3.17}\protected@file@percent }
\newlabel{fig:cart-pole-learning-curve-sac}{{3.17}{132}{Learning curve (Soft Actor--Critic)}{figure.3.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Policy rollout (Soft Actor--Critic).}}{132}{figure.3.18}\protected@file@percent }
\newlabel{fig:cart-pole-policy-rollout-sac}{{3.18}{132}{Policy rollout (Soft Actor--Critic)}{figure.3.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.8.2}SAC for Continuous Actions}{132}{subsubsection.3.4.8.2}\protected@file@percent }
\newlabel{sac-for-continuous-actions}{{3.4.8.2}{132}{SAC for Continuous Actions}{subsubsection.3.4.8.2}{}}
\newlabel{eq:cont-target}{{3.55}{133}{SAC for Continuous Actions}{equation.3.4.55}{}}
\newlabel{eq:cont-actor}{{3.56}{133}{SAC for Continuous Actions}{equation.3.4.56}{}}
\citation{silver2014deterministic}
\newlabel{exm:pendulum-sac}{{3.8}{135}{SAC for Inverted Pendulum}{example.3.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Learning curve (Soft Actor--Critic).}}{135}{figure.3.19}\protected@file@percent }
\newlabel{fig:pendulum-learning-curve-sac}{{3.19}{135}{Learning curve (Soft Actor--Critic)}{figure.3.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Policy rollout (Soft Actor--Critic).}}{135}{figure.3.20}\protected@file@percent }
\newlabel{fig:pendulum-policy-rollout-sac}{{3.20}{135}{Policy rollout (Soft Actor--Critic)}{figure.3.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.9}Deterministic Policy Gradient}{136}{subsection.3.4.9}\protected@file@percent }
\newlabel{deterministic-policy-gradient}{{3.4.9}{136}{Deterministic Policy Gradient}{subsection.3.4.9}{}}
\newlabel{eq:DPG-obj-on}{{3.57}{136}{Deterministic Policy Gradient}{equation.3.4.57}{}}
\newlabel{eq:DPG-obj-off}{{3.58}{136}{Deterministic Policy Gradient}{equation.3.4.58}{}}
\newlabel{eq:DPG-grad-on}{{3.59}{136}{Deterministic Policy Gradient}{equation.3.4.59}{}}
\newlabel{eq:DPG-grad-off}{{3.60}{136}{Deterministic Policy Gradient}{equation.3.4.60}{}}
\citation{lillicrap2015continuous}
\newlabel{thm:dpg-common-foc}{{3.9}{137}{Common First-Order Optima}{theorem.3.9}{}}
\newlabel{eq:per-state-gradient}{{3.61}{137}{Common First-Order Optima}{equation.3.4.61}{}}
\newlabel{exm:pendulum-ddpg}{{3.9}{139}{DDPG for Inverted Pendulum}{example.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Learning curve (DDPG).}}{139}{figure.3.21}\protected@file@percent }
\newlabel{fig:pendulum-learning-curve-ddpg}{{3.21}{139}{Learning curve (DDPG)}{figure.3.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces Policy rollout (DDPG).}}{139}{figure.3.22}\protected@file@percent }
\newlabel{fig:pendulum-policy-rollout-ddpg}{{3.22}{139}{Policy rollout (DDPG)}{figure.3.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Model-based Policy Optimization}{139}{section.3.5}\protected@file@percent }
\newlabel{model-based-policy-optimization}{{3.5}{139}{Model-based Policy Optimization}{section.3.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Model-based Planning and Optimization}{141}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{model-based-plan-optimize}{{4}{141}{Model-based Planning and Optimization}{chapter.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Convex Analysis and Optimization}{143}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appconvex}{{A}{143}{Convex Analysis and Optimization}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Theory}{143}{section.A.1}\protected@file@percent }
\newlabel{appconvex-theory}{{A.1}{143}{Theory}{section.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Sets}{143}{subsection.A.1.1}\protected@file@percent }
\newlabel{sets}{{A.1.1}{143}{Sets}{subsection.A.1.1}{}}
\newlabel{def:affineset}{{A.1}{143}{Affine set}{definition.A.1}{}}
\newlabel{def:convexset}{{A.2}{143}{Convex set}{definition.A.2}{}}
\newlabel{def:cone}{{A.3}{143}{Cone}{definition.A.3}{}}
\newlabel{def:convexcone}{{A.4}{143}{Convex Cone}{definition.A.4}{}}
\newlabel{def:hyperplane}{{A.5}{143}{Hyperplane}{definition.A.5}{}}
\newlabel{def:halfspaces}{{A.6}{144}{Halfspaces}{definition.A.6}{}}
\newlabel{def:balls}{{A.7}{144}{Balls}{definition.A.7}{}}
\newlabel{def:ellipsoids}{{A.8}{144}{Ellipsoids}{definition.A.8}{}}
\newlabel{def:polyhedra}{{A.9}{144}{Polyhedra}{definition.A.9}{}}
\newlabel{def:normball}{{A.10}{144}{Norm ball}{definition.A.10}{}}
\newlabel{def:normcone}{{A.11}{144}{Norm cone}{definition.A.11}{}}
\newlabel{def:simplex}{{A.12}{144}{Simplex}{definition.A.12}{}}
\newlabel{def:symmetricmatrices}{{A.13}{144}{Symmetric,positive semidefinite,positive definite matrices}{definition.A.13}{}}
\newlabel{prp:operationpreserveconvexity}{{A.1}{145}{}{proposition.A.1}{}}
\newlabel{def:affinehull}{{A.14}{145}{Affine hull}{definition.A.14}{}}
\newlabel{def:relint}{{A.15}{145}{Relative Interior}{definition.A.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}Convex function}{145}{subsection.A.1.2}\protected@file@percent }
\newlabel{appconvex-theory-convexfunction}{{A.1.2}{145}{Convex function}{subsection.A.1.2}{}}
\newlabel{def:defcvxfunc}{{A.16}{145}{Convex function}{definition.A.16}{}}
\newlabel{prp:decidecvx}{{A.2}{146}{Conditions for Convex function}{proposition.A.2}{}}
\newlabel{prp:preservecvx}{{A.3}{146}{Operations that preserve convexity}{proposition.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.3}Lagrange dual}{146}{subsection.A.1.3}\protected@file@percent }
\newlabel{lagrange-dual}{{A.1.3}{146}{Lagrange dual}{subsection.A.1.3}{}}
\newlabel{def:defdualfunc}{{A.17}{147}{Lagrange dual function}{definition.A.17}{}}
\newlabel{def:defdualprob}{{A.18}{147}{Lagrange dual problem}{definition.A.18}{}}
\newlabel{thm:slater}{{A.1}{147}{Slater's constraint qualification}{theorem.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.4}KKT condition}{148}{subsection.A.1.4}\protected@file@percent }
\newlabel{appconvex-theory-kkt}{{A.1.4}{148}{KKT condition}{subsection.A.1.4}{}}
\newlabel{thm:KKT}{{A.2}{148}{Karush-Kuhn-Tucker(KKT) Conditions}{theorem.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Practice}{149}{section.A.2}\protected@file@percent }
\newlabel{appconvex-practice}{{A.2}{149}{Practice}{section.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}CVX Introduction}{149}{subsection.A.2.1}\protected@file@percent }
\newlabel{cvx-introduction}{{A.2.1}{149}{CVX Introduction}{subsection.A.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.2}Linear Programming (LP)}{149}{subsection.A.2.2}\protected@file@percent }
\newlabel{linear-programming-lp}{{A.2.2}{149}{Linear Programming (LP)}{subsection.A.2.2}{}}
\newlabel{eq:app-lpdef}{{A.5}{149}{Linear Programming (LP)}{equation.A.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.3}Quadratic Programming (QP)}{150}{subsection.A.2.3}\protected@file@percent }
\newlabel{quadratic-programming-qp}{{A.2.3}{150}{Quadratic Programming (QP)}{subsection.A.2.3}{}}
\newlabel{eq:app-qpdef}{{A.6}{150}{Quadratic Programming (QP)}{equation.A.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.4}Quadratically Constrained Quadratic Programming (QCQP)}{152}{subsection.A.2.4}\protected@file@percent }
\newlabel{quadratically-constrained-quadratic-programming-qcqp}{{A.2.4}{152}{Quadratically Constrained Quadratic Programming (QCQP)}{subsection.A.2.4}{}}
\newlabel{eq:app-qcqpdef}{{A.9}{152}{Quadratically Constrained Quadratic Programming (QCQP)}{equation.A.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.5}Second-Order Cone Programming (SOCP)}{154}{subsection.A.2.5}\protected@file@percent }
\newlabel{second-order-cone-programming-socp}{{A.2.5}{154}{Second-Order Cone Programming (SOCP)}{subsection.A.2.5}{}}
\newlabel{eq:app-socpdef}{{A.12}{154}{Second-Order Cone Programming (SOCP)}{equation.A.2.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.6}Semidefinite Programming (SDP)}{156}{subsection.A.2.6}\protected@file@percent }
\newlabel{semidefinite-programming-sdp}{{A.2.6}{156}{Semidefinite Programming (SDP)}{subsection.A.2.6}{}}
\newlabel{eq:app-sdpdef}{{A.25}{156}{Semidefinite Programming (SDP)}{equation.A.2.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.7}CVXPY Introduction and Examples}{158}{subsection.A.2.7}\protected@file@percent }
\newlabel{cvxpy-introduction-and-examples}{{A.2.7}{158}{CVXPY Introduction and Examples}{subsection.A.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.1}LP}{158}{subsubsection.A.2.7.1}\protected@file@percent }
\newlabel{lp}{{A.2.7.1}{158}{LP}{subsubsection.A.2.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.2}QP}{159}{subsubsection.A.2.7.2}\protected@file@percent }
\newlabel{qp}{{A.2.7.2}{159}{QP}{subsubsection.A.2.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.3}QCQP}{160}{subsubsection.A.2.7.3}\protected@file@percent }
\newlabel{qcqp}{{A.2.7.3}{160}{QCQP}{subsubsection.A.2.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.4}SOCP}{161}{subsubsection.A.2.7.4}\protected@file@percent }
\newlabel{socp}{{A.2.7.4}{161}{SOCP}{subsubsection.A.2.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.7.5}SDP}{162}{subsubsection.A.2.7.5}\protected@file@percent }
\newlabel{sdp}{{A.2.7.5}{162}{SDP}{subsubsection.A.2.7.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Linear System Theory}{165}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app-lti-system-theory}{{B}{165}{Linear System Theory}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Stability}{165}{section.B.1}\protected@file@percent }
\newlabel{app-lti-stability}{{B.1}{165}{Stability}{section.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}Continuous-Time Stability}{165}{subsection.B.1.1}\protected@file@percent }
\newlabel{app-lti-stability-ct}{{B.1.1}{165}{Continuous-Time Stability}{subsection.B.1.1}{}}
\newlabel{eq:app-stability-ct-linear-system}{{B.1}{165}{Continuous-Time Stability}{equation.B.1.1}{}}
\newlabel{def:ltistable}{{B.1}{165}{Asymptotic and Marginal Stability}{definition.B.1}{}}
\newlabel{thm:ltistable}{{B.1}{165}{Stability of Continuous-Time LTI System}{theorem.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.2}Discrete-Time Stability}{166}{subsection.B.1.2}\protected@file@percent }
\newlabel{app-lti-stability-dt}{{B.1.2}{166}{Discrete-Time Stability}{subsection.B.1.2}{}}
\newlabel{eq:app-stability-dt-linear-system}{{B.2}{166}{Discrete-Time Stability}{equation.B.1.2}{}}
\newlabel{thm:dtltisystemstability}{{B.2}{166}{Stability of Discrete-Time LTI System}{theorem.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.3}Lyapunov Analysis}{167}{subsection.B.1.3}\protected@file@percent }
\newlabel{lyapunov-analysis}{{B.1.3}{167}{Lyapunov Analysis}{subsection.B.1.3}{}}
\newlabel{thm:lyapunovequation}{{B.3}{167}{Lyapunov Equation}{theorem.B.3}{}}
\newlabel{eq:lyapunov-equation}{{B.3}{167}{Lyapunov Equation}{equation.B.1.3}{}}
\newlabel{cor:bestconvergencerate}{{B.1}{168}{Maximum Convergence Rate Estimation}{corollary.B.1}{}}
\citation{chen1984book-linear}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Controllability and Observability}{169}{section.B.2}\protected@file@percent }
\newlabel{app-lti-controllable-observable}{{B.2}{169}{Controllability and Observability}{section.B.2}{}}
\newlabel{eq:app-linear-system}{{B.14}{169}{Controllability and Observability}{equation.B.2.14}{}}
\newlabel{eq:app-lti-xy}{{B.15}{169}{Controllability and Observability}{equation.B.2.15}{}}
\newlabel{def:lticontrollable}{{B.2}{169}{Controllability}{definition.B.2}{}}
\newlabel{def:ltiobservable}{{B.3}{170}{Observability}{definition.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}Cayley-Hamilton Theorem}{170}{subsection.B.2.1}\protected@file@percent }
\newlabel{cayley-hamilton-theorem}{{B.2.1}{170}{Cayley-Hamilton Theorem}{subsection.B.2.1}{}}
\newlabel{thm:cayham}{{B.4}{170}{Cayley-Hamilton}{theorem.B.4}{}}
\newlabel{eq:adjugate-1}{{B.16}{170}{Cayley-Hamilton Theorem}{equation.B.2.16}{}}
\newlabel{eq:adjugate-2}{{B.17}{170}{Cayley-Hamilton Theorem}{equation.B.2.17}{}}
\newlabel{cor:cayham-1}{{B.2}{171}{}{corollary.B.2}{}}
\newlabel{cor:cayham-2}{{B.3}{171}{}{corollary.B.3}{}}
\newlabel{cor:cayham-3}{{B.4}{172}{}{corollary.B.4}{}}
\newlabel{cor:cayham-4}{{B.5}{173}{}{corollary.B.5}{}}
\citation{chen1984book-linear}
\citation{zhou1996book-robust}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.2}Equivalent Statements for Controllability}{174}{subsection.B.2.2}\protected@file@percent }
\newlabel{equivalent-statements-for-controllability}{{B.2.2}{174}{Equivalent Statements for Controllability}{subsection.B.2.2}{}}
\newlabel{thm:lticontrollable}{{B.5}{174}{Equivalent Statements for Controllability}{theorem.B.5}{}}
\citation{davison1968tac-poleassign}
\citation{chen1984book-linear}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.3}Duality}{178}{subsection.B.2.3}\protected@file@percent }
\newlabel{duality}{{B.2.3}{178}{Duality}{subsection.B.2.3}{}}
\newlabel{thm:lti-dual-conobs}{{B.6}{179}{Theorem of Duality}{theorem.B.6}{}}
\citation{chen1984book-linear}
\citation{zhou1996book-robust}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.4}Equivalent Statements for Observability}{180}{subsection.B.2.4}\protected@file@percent }
\newlabel{equivalent-statements-for-observability}{{B.2.4}{180}{Equivalent Statements for Observability}{subsection.B.2.4}{}}
\newlabel{thm:ltiobservable}{{B.7}{180}{Equivalent Statements for Observability}{theorem.B.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Stabilizability And Detectability}{180}{section.B.3}\protected@file@percent }
\newlabel{stabilizability-and-detectability}{{B.3}{180}{Stabilizability And Detectability}{section.B.3}{}}
\citation{zhou1996book-robust}
\newlabel{def:ltisystemmode}{{B.4}{181}{System Mode}{definition.B.4}{}}
\newlabel{def:ltistabilizable}{{B.5}{181}{Stabilizability}{definition.B.5}{}}
\newlabel{def:ltidetectable}{{B.6}{181}{Detectability}{definition.B.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Equivalent Statements for Stabilizability}{181}{subsection.B.3.1}\protected@file@percent }
\newlabel{equivalent-statements-for-stabilizability}{{B.3.1}{181}{Equivalent Statements for Stabilizability}{subsection.B.3.1}{}}
\newlabel{thm:ltistabilizable}{{B.8}{181}{Equivalent Statements for Stabilizability}{theorem.B.8}{}}
\citation{zhou1996book-robust}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.2}Equivalent Statements for Detectability}{183}{subsection.B.3.2}\protected@file@percent }
\newlabel{equivalent-statements-for-detectability}{{B.3.2}{183}{Equivalent Statements for Detectability}{subsection.B.3.2}{}}
\newlabel{thm:ltidetectable}{{B.9}{183}{Equivalent Statements for Detectability}{theorem.B.9}{}}
\bibdata{book.bib,packages.bib}
\bibcite{antos2007fitted}{{1}{2007}{{Antos et~al.}}{{}}}
\bibcite{baird1995residual}{{2}{1995}{{Baird et~al.}}{{}}}
\bibcite{barto2012neuronlike}{{3}{2012}{{Barto et~al.}}{{}}}
\bibcite{chen1984book-linear}{{4}{1984}{{Chen}}{{}}}
\bibcite{davison1968tac-poleassign}{{5}{1968}{{Davison and Wonham}}{{}}}
\bibcite{fan2020theoretical}{{6}{2020}{{Fan et~al.}}{{}}}
\bibcite{garrigos2023handbook}{{7}{2023}{{Garrigos and Gower}}{{}}}
\bibcite{haarnoja2018soft}{{8}{2018}{{Haarnoja et~al.}}{{}}}
\bibcite{kakade2001natural}{{9}{2001}{{Kakade}}{{}}}
\bibcite{kearns2000bias}{{10}{2000}{{Kearns and Singh}}{{}}}
\bibcite{lillicrap2015continuous}{{11}{2015}{{Lillicrap et~al.}}{{}}}
\bibcite{mahmood2015emphatic}{{12}{2015}{{Mahmood et~al.}}{{}}}
\bibcite{mnih2015human}{{13}{2015}{{Mnih et~al.}}{{}}}
\bibcite{munos2008finite}{{14}{2008}{{Munos and Szepesv{\'a}ri}}{{}}}
\bibcite{nesterov2018lectures}{{15}{2018}{{Nesterov}}{{}}}
\bibcite{riedmiller2005neural}{{16}{2005}{{Riedmiller}}{{}}}
\bibcite{robbins1971convergence}{{17}{1971}{{Robbins and Siegmund}}{{}}}
\bibcite{schulman2015trust}{{18}{2015a}{{Schulman et~al.}}{{}}}
\bibcite{schulman2015high}{{19}{2015b}{{Schulman et~al.}}{{}}}
\bibcite{schulman2017proximal}{{20}{2017}{{Schulman et~al.}}{{}}}
\bibcite{silver2014deterministic}{{21}{2014}{{Silver et~al.}}{{}}}
\bibcite{sutton1998reinforcement}{{22}{1998}{{Sutton and Barto}}{{}}}
\bibcite{sutton2009fast}{{23}{2009}{{Sutton et~al.}}{{}}}
\bibcite{sutton2008convergent}{{24}{2008}{{Sutton et~al.}}{{}}}
\bibcite{zhou1996book-robust}{{25}{1996}{{Zhou et~al.}}{{}}}
\gdef \@abspage@last{186}
