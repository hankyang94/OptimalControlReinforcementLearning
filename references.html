<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>References | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="References | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="References | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-11-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="app-lti-system-theory.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.1.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.1.4" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.1.4</b> Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.2</b> Infinite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mdp.html"><a href="mdp.html#value-functions"><i class="fa fa-check"></i><b>1.2.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation-1"><i class="fa fa-check"></i><b>1.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.2.3" data-path="mdp.html"><a href="mdp.html#principle-of-optimality"><i class="fa fa-check"></i><b>1.2.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.2.4" data-path="mdp.html"><a href="mdp.html#policy-improvement"><i class="fa fa-check"></i><b>1.2.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="1.2.5" data-path="mdp.html"><a href="mdp.html#policy-iteration"><i class="fa fa-check"></i><b>1.2.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="1.2.6" data-path="mdp.html"><a href="mdp.html#value-iteration"><i class="fa fa-check"></i><b>1.2.6</b> Value Iteration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="value-rl.html"><a href="value-rl.html"><i class="fa fa-check"></i><b>2</b> Value-based Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="value-rl.html"><a href="value-rl.html#tabular-methods"><i class="fa fa-check"></i><b>2.1</b> Tabular Methods</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="value-rl.html"><a href="value-rl.html#tabular-PE"><i class="fa fa-check"></i><b>2.1.1</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.1.2" data-path="value-rl.html"><a href="value-rl.html#value-rl-convergence-td"><i class="fa fa-check"></i><b>2.1.2</b> Convergence Proof of TD Learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control"><i class="fa fa-check"></i><b>2.1.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.1.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control"><i class="fa fa-check"></i><b>2.1.4</b> Off-Policy Control</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="value-rl.html"><a href="value-rl.html#function-approximation"><i class="fa fa-check"></i><b>2.2</b> Function Approximation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="value-rl.html"><a href="value-rl.html#basics-of-continuous-mdp"><i class="fa fa-check"></i><b>2.2.1</b> Basics of Continuous MDP</a></li>
<li class="chapter" data-level="2.2.2" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-2"><i class="fa fa-check"></i><b>2.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.2.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control-1"><i class="fa fa-check"></i><b>2.2.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.2.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control-1"><i class="fa fa-check"></i><b>2.2.4</b> Off-Policy Control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="policy-gradient.html"><a href="policy-gradient.html"><i class="fa fa-check"></i><b>3</b> Policy Gradient Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-optimization"><i class="fa fa-check"></i><b>3.1</b> Gradient-based Optimization</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="policy-gradient.html"><a href="policy-gradient.html#basic-setup"><i class="fa fa-check"></i><b>3.1.1</b> Basic Setup</a></li>
<li class="chapter" data-level="3.1.2" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-ascent-and-descent"><i class="fa fa-check"></i><b>3.1.2</b> Gradient Ascent and Descent</a></li>
<li class="chapter" data-level="3.1.3" data-path="policy-gradient.html"><a href="policy-gradient.html#stochastic-gradients"><i class="fa fa-check"></i><b>3.1.3</b> Stochastic Gradients</a></li>
<li class="chapter" data-level="3.1.4" data-path="policy-gradient.html"><a href="policy-gradient.html#beyond-vanilla-gradient-methods"><i class="fa fa-check"></i><b>3.1.4</b> Beyond Vanilla Gradient Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#policy-gradients"><i class="fa fa-check"></i><b>3.2</b> Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="policy-gradient.html"><a href="policy-gradient.html#setup"><i class="fa fa-check"></i><b>3.2.1</b> Setup</a></li>
<li class="chapter" data-level="3.2.2" data-path="policy-gradient.html"><a href="policy-gradient.html#the-policy-gradient-lemma"><i class="fa fa-check"></i><b>3.2.2</b> The Policy Gradient Lemma</a></li>
<li class="chapter" data-level="3.2.3" data-path="policy-gradient.html"><a href="policy-gradient.html#reinforce"><i class="fa fa-check"></i><b>3.2.3</b> REINFORCE</a></li>
<li class="chapter" data-level="3.2.4" data-path="policy-gradient.html"><a href="policy-gradient.html#baselines-and-variance-reduction"><i class="fa fa-check"></i><b>3.2.4</b> Baselines and Variance Reduction</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#actorcritic-methods"><i class="fa fa-check"></i><b>3.3</b> Actor–Critic Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#anatomy-of-an-actorcritic"><i class="fa fa-check"></i><b>3.3.1</b> Anatomy of an Actor–Critic</a></li>
<li class="chapter" data-level="3.3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#ActorCriticTD"><i class="fa fa-check"></i><b>3.3.2</b> On-Policy Actor–Critic with TD(0)</a></li>
<li class="chapter" data-level="3.3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#PG-GAE"><i class="fa fa-check"></i><b>3.3.3</b> Generalized Advantage Estimation (GAE)</a></li>
<li class="chapter" data-level="3.3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#off-policy-actorcritic"><i class="fa fa-check"></i><b>3.3.4</b> Off-Policy Actor–Critic</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#advanced-policy-gradients"><i class="fa fa-check"></i><b>3.4</b> Advanced Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="policy-gradient.html"><a href="policy-gradient.html#revisiting-generalized-policy-iteration"><i class="fa fa-check"></i><b>3.4.1</b> Revisiting Generalized Policy Iteration</a></li>
<li class="chapter" data-level="3.4.2" data-path="policy-gradient.html"><a href="policy-gradient.html#performance-difference-lemma"><i class="fa fa-check"></i><b>3.4.2</b> Performance Difference Lemma</a></li>
<li class="chapter" data-level="3.4.3" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-constraint"><i class="fa fa-check"></i><b>3.4.3</b> Trust Region Constraint</a></li>
<li class="chapter" data-level="3.4.4" data-path="policy-gradient.html"><a href="policy-gradient.html#natural-policy-gradient"><i class="fa fa-check"></i><b>3.4.4</b> Natural Policy Gradient</a></li>
<li class="chapter" data-level="3.4.5" data-path="policy-gradient.html"><a href="policy-gradient.html#proof-fisher"><i class="fa fa-check"></i><b>3.4.5</b> Proof of Fisher Information</a></li>
<li class="chapter" data-level="3.4.6" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-policy-optimization"><i class="fa fa-check"></i><b>3.4.6</b> Trust Region Policy Optimization</a></li>
<li class="chapter" data-level="3.4.7" data-path="policy-gradient.html"><a href="policy-gradient.html#proximal-policy-optimization"><i class="fa fa-check"></i><b>3.4.7</b> Proximal Policy Optimization</a></li>
<li class="chapter" data-level="3.4.8" data-path="policy-gradient.html"><a href="policy-gradient.html#soft-actorcritic"><i class="fa fa-check"></i><b>3.4.8</b> Soft Actor–Critic</a></li>
<li class="chapter" data-level="3.4.9" data-path="policy-gradient.html"><a href="policy-gradient.html#deterministic-policy-gradient"><i class="fa fa-check"></i><b>3.4.9</b> Deterministic Policy Gradient</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="policy-gradient.html"><a href="policy-gradient.html#model-based-policy-optimization"><i class="fa fa-check"></i><b>3.5</b> Model-based Policy Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html"><i class="fa fa-check"></i><b>4</b> Model-based Planning and Optimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr"><i class="fa fa-check"></i><b>4.1</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#finite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.1</b> Finite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#infinite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.2</b> Infinite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#linear-system-basics"><i class="fa fa-check"></i><b>4.1.3</b> Linear System Basics</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr-tracking"><i class="fa fa-check"></i><b>4.2</b> LQR Trajectory Tracking</a></li>
<li class="chapter" data-level="4.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#traj-opt"><i class="fa fa-check"></i><b>4.3</b> Trajectory Optimization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#traj-opt-ilqr"><i class="fa fa-check"></i><b>4.3.1</b> Iterative LQR</a></li>
<li class="chapter" data-level="4.3.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#ddp"><i class="fa fa-check"></i><b>4.3.2</b> Differential Dynamic Programming</a></li>
<li class="chapter" data-level="4.3.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#qp"><i class="fa fa-check"></i><b>4.3.3</b> Quadratic Programming</a></li>
<li class="chapter" data-level="4.3.4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#sqp"><i class="fa fa-check"></i><b>4.3.4</b> Sequential Quadratic Programming</a></li>
<li class="chapter" data-level="4.3.5" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#ipm-nlp"><i class="fa fa-check"></i><b>4.3.5</b> Interior-Point Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#mpc"><i class="fa fa-check"></i><b>4.4</b> Model Predictive Control</a></li>
<li class="chapter" data-level="4.5" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#mppi"><i class="fa fa-check"></i><b>4.5</b> Model Predictive Path Integral Control</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#sec:mppi-it"><i class="fa fa-check"></i><b>4.5.1</b> Information-Theoretic Derivation of MPPI</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-materials.html"><a href="advanced-materials.html"><i class="fa fa-check"></i><b>5</b> Advanced Materials</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrangian-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1 unnumbered hasAnchor">
<h1>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h1>

<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div class="csl-entry">
Antos, András, Csaba Szepesvári, and Rémi Munos. 2007. <span>“Fitted q-Iteration in Continuous Action-Space MDPs.”</span> <em>Advances in Neural Information Processing Systems</em> 20.
</div>
<div class="csl-entry">
Arnold, William F, and Alan J Laub. 1984. <span>“Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.”</span> <em>Proceedings of the IEEE</em> 72 (12): 1746–54.
</div>
<div class="csl-entry">
Baird, Leemon et al. 1995. <span>“Residual Algorithms: Reinforcement Learning with Function Approximation.”</span> In <em>Proceedings of the Twelfth International Conference on Machine Learning</em>, 30–37.
</div>
<div class="csl-entry">
Barto, Andrew G, Richard S Sutton, and Charles W Anderson. 2012. <span>“Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, no. 5: 834–46.
</div>
<div class="csl-entry">
Chen, Chi-Tsong. 1984. <em>Linear System Theory and Design</em>. Saunders college publishing.
</div>
<div class="csl-entry">
Davison, E., and W. Wonham. 1968. <span>“On Pole Assignment in Multivariable Linear Systems.”</span> <em>IEEE Transactions on Automatic Control</em> 13 (6): 747–48. <a href="https://doi.org/10.1109/TAC.1968.1099056">https://doi.org/10.1109/TAC.1968.1099056</a>.
</div>
<div class="csl-entry">
Fan, Jianqing, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. 2020. <span>“A Theoretical Analysis of Deep q-Learning.”</span> In <em>Learning for Dynamics and Control</em>, 486–89. PMLR.
</div>
<div class="csl-entry">
Garrigos, Guillaume, and Robert M Gower. 2023. <span>“Handbook of Convergence Theorems for (Stochastic) Gradient Methods.”</span> <em>arXiv Preprint arXiv:2301.11235</em>.
</div>
<div class="csl-entry">
Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. <span>“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</span> In <em>International Conference on Machine Learning</em>, 1861–70. Pmlr.
</div>
<div class="csl-entry">
Janner, Michael, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. <span>“When to Trust Your Model: Model-Based Policy Optimization.”</span> <em>Advances in Neural Information Processing Systems</em> 32.
</div>
<div class="csl-entry">
Kakade, Sham M. 2001. <span>“A Natural Policy Gradient.”</span> <em>Advances in Neural Information Processing Systems</em> 14.
</div>
<div class="csl-entry">
Kang, Shucheng, Xiaoyang Xu, Jay Sarva, Ling Liang, and Heng Yang. 2024. <span>“Fast and Certifiable Trajectory Optimization.”</span> In <em>International Workshop on the Algorithmic Foundations of Robotics</em>.
</div>
<div class="csl-entry">
Kearns, Michael J, and Satinder Singh. 2000. <span>“Bias-Variance Error Bounds for Temporal Difference Updates.”</span> In <em>COLT</em>, 142–47.
</div>
<div class="csl-entry">
Lillicrap, Timothy P, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. <span>“Continuous Control with Deep Reinforcement Learning.”</span> <em>arXiv Preprint arXiv:1509.02971</em>.
</div>
<div class="csl-entry">
Liu, Dong C, and Jorge Nocedal. 1989. <span>“On the Limited Memory BFGS Method for Large Scale Optimization.”</span> <em>Mathematical Programming</em> 45 (1): 503–28.
</div>
<div class="csl-entry">
Mahmood, A Rupam, Huizhen Yu, Martha White, and Richard S Sutton. 2015. <span>“Emphatic Temporal-Difference Learning.”</span> <em>arXiv Preprint arXiv:1507.01569</em>.
</div>
<div class="csl-entry">
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. <span>“Human-Level Control Through Deep Reinforcement Learning.”</span> <em>Nature</em> 518 (7540): 529–33.
</div>
<div class="csl-entry">
Munos, Rémi, and Csaba Szepesvári. 2008. <span>“Finite-Time Bounds for Fitted Value Iteration.”</span> <em>Journal of Machine Learning Research</em> 9 (5).
</div>
<div class="csl-entry">
Nesterov, Yurii. 2018. <em>Lectures on Convex Optimization</em>. Vol. 137. Springer.
</div>
<div class="csl-entry">
Nocedal, Jorge, and Stephen J Wright. 1999. <em>Numerical Optimization</em>. Springer.
</div>
<div class="csl-entry">
Rawlings, James Blake, David Q Mayne, and Moritz Diehl. 2020. <em>Model Predictive Control: Theory, Computation, and Design</em>. Vol. 2. Nob Hill Publishing Madison, WI.
</div>
<div class="csl-entry">
Riedmiller, Martin. 2005. <span>“Neural Fitted q Iteration–First Experiences with a Data Efficient Neural Reinforcement Learning Method.”</span> In <em>European Conference on Machine Learning</em>, 317–28. Springer.
</div>
<div class="csl-entry">
Robbins, Herbert, and David Siegmund. 1971. <span>“A Convergence Theorem for Non Negative Almost Supermartingales and Some Applications.”</span> In <em>Optimizing Methods in Statistics</em>, 233–57. Elsevier.
</div>
<div class="csl-entry">
Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. <span>“Trust Region Policy Optimization.”</span> In <em>International Conference on Machine Learning</em>, 1889–97. PMLR.
</div>
<div class="csl-entry">
Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. <span>“High-Dimensional Continuous Control Using Generalized Advantage Estimation.”</span> <em>arXiv Preprint arXiv:1506.02438</em>.
</div>
<div class="csl-entry">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal Policy Optimization Algorithms.”</span> <em>arXiv Preprint arXiv:1707.06347</em>.
</div>
<div class="csl-entry">
Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. <span>“Deterministic Policy Gradient Algorithms.”</span> In <em>International Conference on Machine Learning</em>, 387–95. Pmlr.
</div>
<div class="csl-entry">
Sutton, Richard S, and Andrew G Barto. 1998. <em>Reinforcement Learning: An Introduction</em>. Vol. 1. 1. MIT press Cambridge.
</div>
<div class="csl-entry">
Sutton, Richard S, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári, and Eric Wiewiora. 2009. <span>“Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation.”</span> In <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>, 993–1000.
</div>
<div class="csl-entry">
Sutton, Richard S, Csaba Szepesvári, and Hamid Reza Maei. 2008. <span>“A Convergent o(n) Algorithm for Off-Policy Temporal-Difference Learning with Linear Function Approximation.”</span> <em>Advances in Neural Information Processing Systems</em> 21 (21): 1609–16.
</div>
<div class="csl-entry">
Wächter, Andreas, and Lorenz T Biegler. 2006. <span>“On the Implementation of an Interior-Point Filter Line-Search Algorithm for Large-Scale Nonlinear Programming.”</span> <em>Mathematical Programming</em> 106 (1): 25–57.
</div>
<div class="csl-entry">
Williams, Grady, Paul Drews, Brian Goldfain, James M Rehg, and Evangelos A Theodorou. 2016. <span>“Aggressive Driving with Model Predictive Path Integral Control.”</span> In <em>2016 IEEE International Conference on Robotics and Automation (ICRA)</em>, 1433–40. IEEE.
</div>
<div class="csl-entry">
Zhou, Kemin, JC Doyle, and Keither Glover. 1996. <span>“Robust and Optimal Control.”</span> <em>Control Engineering Practice</em> 4 (8): 1189–90.
</div>
</div>
</div>






            </section>

          </div>
        </div>
      </div>
<a href="app-lti-system-theory.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/12-references.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
