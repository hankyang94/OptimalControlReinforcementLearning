<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 The Optimal Control Formulation | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 The Optimal Control Formulation | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 The Optimal Control Formulation | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-03-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="exactdp.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="formulation.html"><a href="formulation.html"><i class="fa fa-check"></i><b>1</b> The Optimal Control Formulation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="formulation.html"><a href="formulation.html#the-basic-problem"><i class="fa fa-check"></i><b>1.1</b> The Basic Problem</a></li>
<li class="chapter" data-level="1.2" data-path="formulation.html"><a href="formulation.html#dynamic-programming-and-principle-of-optimality"><i class="fa fa-check"></i><b>1.2</b> Dynamic Programming and Principle of Optimality</a></li>
<li class="chapter" data-level="1.3" data-path="formulation.html"><a href="formulation.html#infinite-horizon"><i class="fa fa-check"></i><b>1.3</b> Infinite-horizon Formulation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exactdp.html"><a href="exactdp.html"><i class="fa fa-check"></i><b>2</b> Exact Dynamic Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path="exactdp.html"><a href="exactdp.html#lqr"><i class="fa fa-check"></i><b>2.1</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="exactdp.html"><a href="exactdp.html#infinite-horizon-lqr"><i class="fa fa-check"></i><b>2.1.1</b> Infinite-Horizon LQR</a></li>
<li class="chapter" data-level="2.1.2" data-path="exactdp.html"><a href="exactdp.html#lqr-with-constraints"><i class="fa fa-check"></i><b>2.1.2</b> LQR with Constraints</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="exactdp.html"><a href="exactdp.html#mdp-exact-dp"><i class="fa fa-check"></i><b>2.2</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="exactdp.html"><a href="exactdp.html#bellman-optimality-equations"><i class="fa fa-check"></i><b>2.2.1</b> Bellman Optimality Equations</a></li>
<li class="chapter" data-level="2.2.2" data-path="exactdp.html"><a href="exactdp.html#value-iteration"><i class="fa fa-check"></i><b>2.2.2</b> Value Iteration</a></li>
<li class="chapter" data-level="2.2.3" data-path="exactdp.html"><a href="exactdp.html#value-iteration-with-barycentric-interpolation"><i class="fa fa-check"></i><b>2.2.3</b> Value Iteration with Barycentric Interpolation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="approximatedp.html"><a href="approximatedp.html"><i class="fa fa-check"></i><b>3</b> Approximate Optimal Control</a>
<ul>
<li class="chapter" data-level="3.1" data-path="approximatedp.html"><a href="approximatedp.html#fitted-value-iteration"><i class="fa fa-check"></i><b>3.1</b> Fitted Value Iteration</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="approximatedp.html"><a href="approximatedp.html#linear-features"><i class="fa fa-check"></i><b>3.1.1</b> Linear Features</a></li>
<li class="chapter" data-level="3.1.2" data-path="approximatedp.html"><a href="approximatedp.html#neural-network-features"><i class="fa fa-check"></i><b>3.1.2</b> Neural Network Features</a></li>
<li class="chapter" data-level="3.1.3" data-path="approximatedp.html"><a href="approximatedp.html#fitted-q-value-iteration"><i class="fa fa-check"></i><b>3.1.3</b> Fitted Q-value Iteration</a></li>
<li class="chapter" data-level="3.1.4" data-path="approximatedp.html"><a href="approximatedp.html#deep-q-network"><i class="fa fa-check"></i><b>3.1.4</b> Deep Q Network</a></li>
<li class="chapter" data-level="3.1.5" data-path="approximatedp.html"><a href="approximatedp.html#deep-shallow"><i class="fa fa-check"></i><b>3.1.5</b> Deep + Shallow</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="approximatedp.html"><a href="approximatedp.html#trajectory-optimization"><i class="fa fa-check"></i><b>3.2</b> Trajectory Optimization</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="approximatedp.html"><a href="approximatedp.html#direct-single-shooting"><i class="fa fa-check"></i><b>3.2.1</b> Direct Single Shooting</a></li>
<li class="chapter" data-level="3.2.2" data-path="approximatedp.html"><a href="approximatedp.html#direct-multiple-shooting"><i class="fa fa-check"></i><b>3.2.2</b> Direct Multiple Shooting</a></li>
<li class="chapter" data-level="3.2.3" data-path="approximatedp.html"><a href="approximatedp.html#direct-collocation"><i class="fa fa-check"></i><b>3.2.3</b> Direct Collocation</a></li>
<li class="chapter" data-level="3.2.4" data-path="approximatedp.html"><a href="approximatedp.html#direct-orthogonal-collocation"><i class="fa fa-check"></i><b>3.2.4</b> Direct Orthogonal Collocation</a></li>
<li class="chapter" data-level="3.2.5" data-path="approximatedp.html"><a href="approximatedp.html#failure-of-open-loop-control"><i class="fa fa-check"></i><b>3.2.5</b> Failure of Open-Loop Control</a></li>
<li class="chapter" data-level="3.2.6" data-path="approximatedp.html"><a href="approximatedp.html#lqr-trajectory-tracking"><i class="fa fa-check"></i><b>3.2.6</b> LQR Trajectory Tracking</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="approximatedp.html"><a href="approximatedp.html#model-predictive-control"><i class="fa fa-check"></i><b>3.3</b> Model Predictive Control</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="approximatedp.html"><a href="approximatedp.html#turn-trajectory-optimization-into-feedback-control"><i class="fa fa-check"></i><b>3.3.1</b> Turn Trajectory Optimization into Feedback Control</a></li>
<li class="chapter" data-level="3.3.2" data-path="approximatedp.html"><a href="approximatedp.html#controllability-reachability-and-invariance"><i class="fa fa-check"></i><b>3.3.2</b> Controllability, Reachability, and Invariance</a></li>
<li class="chapter" data-level="3.3.3" data-path="approximatedp.html"><a href="approximatedp.html#basic-formulation-for-linear-systems"><i class="fa fa-check"></i><b>3.3.3</b> Basic Formulation for Linear Systems</a></li>
<li class="chapter" data-level="3.3.4" data-path="approximatedp.html"><a href="approximatedp.html#persistent-feasibility"><i class="fa fa-check"></i><b>3.3.4</b> Persistent Feasibility</a></li>
<li class="chapter" data-level="3.3.5" data-path="approximatedp.html"><a href="approximatedp.html#mpc-stability"><i class="fa fa-check"></i><b>3.3.5</b> Stability</a></li>
<li class="chapter" data-level="3.3.6" data-path="approximatedp.html"><a href="approximatedp.html#explicit-mpc"><i class="fa fa-check"></i><b>3.3.6</b> Explicit MPC</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="approximatedp.html"><a href="approximatedp.html#policy-gradient"><i class="fa fa-check"></i><b>3.4</b> Policy Gradient</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stability.html"><a href="stability.html"><i class="fa fa-check"></i><b>4</b> Stability Analysis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stability.html"><a href="stability.html#autonomous-systems"><i class="fa fa-check"></i><b>4.1</b> Autonomous Systems</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="stability.html"><a href="stability.html#concepts-of-stability"><i class="fa fa-check"></i><b>4.1.1</b> Concepts of Stability</a></li>
<li class="chapter" data-level="4.1.2" data-path="stability.html"><a href="stability.html#stability-by-linearization"><i class="fa fa-check"></i><b>4.1.2</b> Stability by Linearization</a></li>
<li class="chapter" data-level="4.1.3" data-path="stability.html"><a href="stability.html#lyapunov-analysis"><i class="fa fa-check"></i><b>4.1.3</b> Lyapunov Analysis</a></li>
<li class="chapter" data-level="4.1.4" data-path="stability.html"><a href="stability.html#invariant-set-theorem"><i class="fa fa-check"></i><b>4.1.4</b> Invariant Set Theorem</a></li>
<li class="chapter" data-level="4.1.5" data-path="stability.html"><a href="stability.html#computing-lyapunov-certificates"><i class="fa fa-check"></i><b>4.1.5</b> Computing Lyapunov Certificates</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="stability.html"><a href="stability.html#controlled-systems"><i class="fa fa-check"></i><b>4.2</b> Controlled Systems</a></li>
<li class="chapter" data-level="4.3" data-path="stability.html"><a href="stability.html#non-autonomous-systems"><i class="fa fa-check"></i><b>4.3</b> Non-autonomous Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="psets.html"><a href="psets.html"><i class="fa fa-check"></i><b>5</b> Problem Sets</a></li>
<li class="chapter" data-level="" data-path="acknowledgement.html"><a href="acknowledgement.html"><i class="fa fa-check"></i>Acknowledgement</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra and Differential Equations</a>
<ul>
<li class="chapter" data-level="A.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#linear-algebra"><i class="fa fa-check"></i><b>A.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#matrix-exponential"><i class="fa fa-check"></i><b>A.1.1</b> Matrix Exponential</a></li>
<li class="chapter" data-level="A.1.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#gradients"><i class="fa fa-check"></i><b>A.1.2</b> Gradients</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#solving-an-ordinary-differential-equation"><i class="fa fa-check"></i><b>A.2</b> Solving an Ordinary Differential Equation</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#separation-of-variables"><i class="fa fa-check"></i><b>A.2.1</b> Separation of Variables</a></li>
<li class="chapter" data-level="A.2.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#first-order-linear-ode"><i class="fa fa-check"></i><b>A.2.2</b> First-order Linear ODE</a></li>
<li class="chapter" data-level="A.2.3" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#gronwall-inequality"><i class="fa fa-check"></i><b>A.2.3</b> Gronwall Inequality</a></li>
<li class="chapter" data-level="A.2.4" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#matlab"><i class="fa fa-check"></i><b>A.2.4</b> Matlab</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>B</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>B.1</b> Theory</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>B.1.1</b> Sets</a></li>
<li class="chapter" data-level="B.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>B.1.2</b> Convex function</a></li>
<li class="chapter" data-level="B.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>B.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="B.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>B.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>B.2</b> Practice</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>B.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="B.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>B.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="B.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>B.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="B.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>B.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="B.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>B.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="B.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>B.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="B.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>B.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>C</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="C.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>C.1</b> Stability</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>C.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="C.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>C.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="C.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis-1"><i class="fa fa-check"></i><b>C.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>C.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>C.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="C.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>C.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="C.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>C.2.3</b> Duality</a></li>
<li class="chapter" data-level="C.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>C.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>C.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>C.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="C.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>C.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html"><i class="fa fa-check"></i><b>D</b> Algebraic Techniques and Sum-of-Squares</a>
<ul>
<li class="chapter" data-level="D.1" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#algebra"><i class="fa fa-check"></i><b>D.1</b> Algebra</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#polynomials"><i class="fa fa-check"></i><b>D.1.1</b> Polynomials</a></li>
<li class="chapter" data-level="D.1.2" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#representation-of-nonnegative-polynomial-univariate-case"><i class="fa fa-check"></i><b>D.1.2</b> Representation of nonnegative polynomial: Univariate case</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="the-kalman-yakubovich-lemma.html"><a href="the-kalman-yakubovich-lemma.html"><i class="fa fa-check"></i><b>E</b> The Kalman-Yakubovich Lemma</a></li>
<li class="chapter" data-level="F" data-path="feedbacklinearization.html"><a href="feedbacklinearization.html"><i class="fa fa-check"></i><b>F</b> Feedback Linearization</a></li>
<li class="chapter" data-level="G" data-path="slidingcontrol.html"><a href="slidingcontrol.html"><i class="fa fa-check"></i><b>G</b> Sliding Control</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="formulation" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> The Optimal Control Formulation<a href="formulation.html#formulation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="the-basic-problem" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> The Basic Problem<a href="formulation.html#the-basic-problem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a discrete-time dynamical system
<span class="math display" id="eq:discrete-time-dynamics">\[\begin{equation}
x_{k+1} = f_k (x_k, u_k, w_k), \quad k =0,1,\dots,N-1
\tag{1.1}
\end{equation}\]</span>
where</p>
<ul>
<li><p><span class="math inline">\(x_k \in \mathbb{X} \subseteq \mathbb{R}^n\)</span> is the <em>state</em> of the system,</p></li>
<li><p><span class="math inline">\(u_k \in \mathbb{U} \subseteq \mathbb{R}^m\)</span> is the <em>control</em> we wish to design,</p></li>
<li><p><span class="math inline">\(w_k \in \mathbb{W} \subseteq \mathbb{R}^p\)</span> a random <em>disturbance</em> or noise (e.g., due to unmodelled dynamics) which is described by a probability distribution <span class="math inline">\(P_k(\cdot \mid x_k, u_k)\)</span> that may depend on <span class="math inline">\(x_k\)</span> and <span class="math inline">\(u_k\)</span> but not on prior disturbances <span class="math inline">\(w_0,\dots,w_{k-1}\)</span>,</p></li>
<li><p><span class="math inline">\(k\)</span> indexes the discrete time,</p></li>
<li><p><span class="math inline">\(N\)</span> denotes the horizon,</p></li>
<li><p><span class="math inline">\(f_k\)</span> models the transition function of the system (typically <span class="math inline">\(f_k \equiv f\)</span> is time-invariant, especially for robotics systems; we use <span class="math inline">\(f_k\)</span> here to keep full generality).</p></li>
</ul>
<div class="remark">
<p><span id="unlabeled-div-1" class="remark"><em>Remark</em> (Deterministic v.s. Stochastic). </span>When <span class="math inline">\(w_k \equiv 0\)</span> for all <span class="math inline">\(k\)</span>, we say the system <a href="formulation.html#eq:discrete-time-dynamics">(1.1)</a> is <em>deterministic</em>; otherwise we say the system is <em>stochastic</em>. In the following we will deal with the stochastic case, but most of the methodology should carry over to the deterministic setup.</p>
</div>
<p>We consider the class of <em>controllers</em> (also called <em>policies</em>) that consist of a sequence of functions
<span class="math display">\[
\pi = \{ \mu_0,\dots,\mu_{N-1} \},
\]</span>
where <span class="math inline">\(\mu_k (x_k) \in \mathbb{U}\)</span> for all <span class="math inline">\(x_k\)</span>, i.e., <span class="math inline">\(\mu_k\)</span> is a <em>feedback</em> controller that maps the state to an admissible control. Given an initial state <span class="math inline">\(x_0\)</span> and an admissible policy <span class="math inline">\(\pi\)</span>, the state <em>trajectory</em> of the system is a sequence of random variables that evolve according to
<span class="math display" id="eq:closed-loop-state-trajectory">\[\begin{equation}
x_{k+1} = f_k(x_k,\mu_k(x_k),w_k), \quad k=0,\dots,N-1
\tag{1.2}
\end{equation}\]</span>
where the randomness comes from the disturbance <span class="math inline">\(w_k\)</span>.</p>
<p>We assume the state-control trajectory <span class="math inline">\(\{u_k\}_{k=0}^{N-1}\)</span> and <span class="math inline">\(\{x_k \}_{k=0}^{N}\)</span> induce an <em>additive cost</em>
<span class="math display" id="eq:additive-cost">\[\begin{equation}
g_N(x_N) + \sum_{k=0}^{N-1} g_k(x_k,u_k)
\tag{1.3}
\end{equation}\]</span>
where <span class="math inline">\(g_k,k=0,\dots,N\)</span> are some user-designed functions.</p>
<p>With <a href="formulation.html#eq:closed-loop-state-trajectory">(1.2)</a> and <a href="formulation.html#eq:additive-cost">(1.3)</a>, for any admissible policy <span class="math inline">\(\pi\)</span>, we denote its induced <em>expected cost</em> with initial state <span class="math inline">\(x_0\)</span> as
<span class="math display" id="eq:expected-cost">\[\begin{equation}
J_\pi (x_0) = \mathbb{E} \left\{ g_N(x_N) + \sum_{k=0}^{N-1} g_k (x_k, \mu_k(x_k))  \right\},
\tag{1.4}
\end{equation}\]</span>
where the expectation is taken over the randomness of <span class="math inline">\(w_k\)</span>.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:basicproblem" class="definition"><strong>Definition 1.1  (Discrete-time, Finite-horizon Optimal Control) </strong></span>Find the best admissible controller that minimizes the expected cost in <a href="formulation.html#eq:expected-cost">(1.4)</a>
<span class="math display">\[\begin{equation}
\pi^\star \in \arg\min_{\pi \in \Pi} J_\pi(x_0),
\end{equation}\]</span>
where <span class="math inline">\(\Pi\)</span> is the set of all admissible controllers.
The cost attained by the optimal controller, i.e., <span class="math inline">\(J^\star = J_{\pi^\star}(x_0)\)</span> is called the optimal <em>cost-to-go</em>, or the optimal <em>value function</em>.</p>
</div>
</div>
<div class="remark">
<p><span id="unlabeled-div-2" class="remark"><em>Remark</em> (Open-loop v.s. Closed-loop). </span>An important feature of the basic problem in Definition <a href="formulation.html#def:basicproblem">1.1</a> is that the problem seeks <em>feedback policies</em>, instead of numerical values of the controls, i.e., <span class="math inline">\(u_k = \mu_k(x_k)\)</span> is in general a function of the state <span class="math inline">\(x_k\)</span>. In other words, the controls are executed sequentially, one at a time after observing the state at each time. This is called closed-loop control, and is in general better than open-loop control
<span class="math display">\[
\min_{u_0,\dots,u_{N-1}} \mathbb{E} \left\{ g_N(x_N) + \sum_{k=0}^{N-1} g_k (x_k, u_k)  \right\}
\]</span>
where all the controls are planned at <span class="math inline">\(k=0\)</span>. Intuitively, a closed-loop policy is able to utilize the extra information received at each timestep (i.e., it observes <span class="math inline">\(x_{k+1}\)</span> and hence also observes the disturbance <span class="math inline">\(w_k\)</span>) to obtain a lower cost than an open-loop controller. Example 1.2.1 in <span class="citation">(<a href="#ref-bertsekas12book-dpocI">Bertsekas 2012</a>)</span> gives a concrete application where a closed-loop policy attains a lower cost than an open-loop policy.</p>
<p>In deterministic control (i.e., when <span class="math inline">\(w_k \equiv 0,\forall k\)</span>), however, a closed-loop policy has no advantage over an open-loop controller. This is obvious because at <span class="math inline">\(k=0\)</span>, even the open-loop controller predicts perfectly the consequences of all its actions and there is no extra information to be observed at later time steps. In fact, even in stochastic problems, a closed-loop policy may not be advantageous, see Exercise 1.27 in <span class="citation">(<a href="#ref-bertsekas12book-dpocI">Bertsekas 2012</a>)</span>.</p>
</div>
</div>
<div id="dynamic-programming-and-principle-of-optimality" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Dynamic Programming and Principle of Optimality<a href="formulation.html#dynamic-programming-and-principle-of-optimality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now introduce a general and powerful algorithm, namely <em>dynamic programming</em> (DP), for solving the optimal control problem <a href="formulation.html#def:basicproblem">1.1</a>. The DP algorithm builds upon a quite simple intuition called the <em>Bellman principle of optimality</em>.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:bellmanoptimality" class="theorem"><strong>Theorem 1.1  (Bellman Principle of Optimality) </strong></span>Let <span class="math inline">\(\pi^\star = \{ \mu_0^\star,\mu_1^\star,\dots,\mu_{N-1}^\star \}\)</span> be an optimal policy for the optimal control problem <a href="formulation.html#def:basicproblem">1.1</a>. Assume that when using <span class="math inline">\(\pi^\star\)</span>, a given state <span class="math inline">\(x_i\)</span> occurs at timestep <span class="math inline">\(i\)</span> with positive probability (i.e., <span class="math inline">\(x_i\)</span> is reachable at time <span class="math inline">\(i\)</span>).</p>
<p>Now consider the following subproblem where we are at <span class="math inline">\(x_i\)</span> at time <span class="math inline">\(i\)</span> and wish to minimize the cost-to-go from time <span class="math inline">\(i\)</span> to time <span class="math inline">\(N\)</span>
<span class="math display">\[
\min_{\mu_i,\dots,\mu_{N-1}} \mathbb{E} \left\{ g_N(x_N) + \sum_{k=i}^{N-1} g_k (x_k, \mu_k(x_k)) \right\}.
\]</span>
Then the truncated policy <span class="math inline">\(\{\mu^\star_i,\mu^\star_{i+1},\dots, \mu^\star_{N-1}\}\)</span> must be optimal for the subproblem.</p>
</div>
</div>
<p>Theorem <a href="formulation.html#thm:bellmanoptimality">1.1</a> can be proved intuitively by contradiction: if the truncated policy <span class="math inline">\(\{\mu^\star_i,\mu^\star_{i+1},\dots, \mu^\star_{N-1}\}\)</span> is not optimal for the subproblem, say there exists a different policy <span class="math inline">\(\{\mu_i&#39;,\mu_{i+1}&#39;,\dots, \mu_{N-1}&#39;\}\)</span> that attains a lower cost for the subproblem starting at <span class="math inline">\(x_i\)</span> at time <span class="math inline">\(i\)</span>. Then the combined policy <span class="math inline">\(\{\mu_0^\star,\dots,\mu^\star_{i-1},\mu_i&#39;,\dots,\mu_{N-1}&#39;\}\)</span> must attain a lower cost for the original optimal control problem <a href="formulation.html#def:basicproblem">1.1</a> due to the additive cost structure, contradicting the optimality of <span class="math inline">\(\pi^\star\)</span>.</p>
<p>The Bellman principle of optimality is more than just a principle, it is also an algorithm. It suggests that, to build an optimal policy, one can start by solving the last-stage subproblem to obtain <span class="math inline">\(\{\mu^\star_{N-1} \}\)</span>, and then proceed to solve the subproblem containing the last two stages to obtain <span class="math inline">\(\{ \mu^\star_{N-2},\mu^\star_{N-1} \}\)</span>. The recursion continues until optimal policies at all stages are computed. The following theorem formalizes this concept.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:dynamicprogramming" class="theorem"><strong>Theorem 1.2  (Dynamic Programming) </strong></span>The optimal value function <span class="math inline">\(J^\star(x_0)\)</span> of the optimal control problem <a href="formulation.html#def:basicproblem">1.1</a> (starting from any given initial condition <span class="math inline">\(x_0\)</span>) is equal to <span class="math inline">\(J_0(x_0)\)</span>, which can be computed backwards and recursively as
<span class="math display" id="eq:dpbackwardrecursion">\[\begin{align}
J_N(x_N) &amp;= g_N(x_N) \\
J_k(x_k) &amp;= \min_{u_k \in \mathbb{U}} \displaystyle \mathbb{E}_{w_k \sim P_k(\cdot \mid x_k, u_k)} \displaystyle \left\{ g_k(x_k,u_k) + J_{k+1}(f_k(x_k,u_k,w_k) ) \right\}, \ k=N-1,\dots,1,0.
\tag{1.5}
\end{align}\]</span>
Moreover, if <span class="math inline">\(u_k^\star = \mu_k^\star(x_k)\)</span> is a minimizer of <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a> for every <span class="math inline">\(x_k\)</span>, then the policy <span class="math inline">\(\pi^\star = \{\mu_0^\star,\dots,\mu_{N-1}^\star \}\)</span> is optimal.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>For any admissible policy <span class="math inline">\(\pi = \{ \mu_0,\dots,\mu_{N-1} \}\)</span>, denote <span class="math inline">\(\pi^k = \{ \mu_k,\dots,\mu_{N-1} \}\)</span> the last-<span class="math inline">\((N-k)\)</span>-stage truncated policy. Consider the subproblem consisting of the last <span class="math inline">\(N-k\)</span> stages starting from <span class="math inline">\(x_k\)</span>, and let <span class="math inline">\(J^\star_k(x_k)\)</span> be its optimal cost-to-go. Mathematically, this is
<span class="math display" id="eq:dptheoremdefineJkstar">\[\begin{equation}
J^\star_{k}(x_k) = \min_{\pi^k} \mathbb{E}_{w_k,\dots,w_{N-1}} \left\{ g_N(x_N) + \sum_{i=k}^{N-1} g_i (x_i,\mu_i(x_i)) \right\}, \quad k=0,1,\dots,N-1.
\tag{1.6}
\end{equation}\]</span>
We define <span class="math inline">\(J^\star_N(x_N) = g(x_N)\)</span> for <span class="math inline">\(k=N\)</span>.</p>
<p>Our goal is to prove the <span class="math inline">\(J_k(x_k)\)</span> computed by dynamic programming from <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a> is equal to <span class="math inline">\(J^\star_k (x_k)\)</span> for all <span class="math inline">\(k=0,\dots,N\)</span>. We will prove this by induction.</p>
<p>Firstly, we already have <span class="math inline">\(J^\star_N(x_N) = J_N(x_N) = g(x_N)\)</span>, so <span class="math inline">\(k=N\)</span> holds automatically.</p>
<p>Now we assume <span class="math inline">\(J^\star_{k+1}(x_{k+1}) = J_{k+1}(x_{k+1})\)</span> for all <span class="math inline">\(x_{k+1}\)</span>, and we wish to induce <span class="math inline">\(J^\star_{k}(x_{k}) = J_{k}(x_{k})\)</span>. To show this, we write
<span class="math display" id="eq:dpproof-8" id="eq:dpproof-7" id="eq:dpproof-6" id="eq:dpproof-5" id="eq:dpproof-4" id="eq:dpproof-3" id="eq:dpproof-2" id="eq:dpproof-1">\[\begin{align}
\hspace{-16mm} J^\star_{k}(x_k) &amp;= \min_{\pi^k} \mathbb{E}_{w_k,\dots,w_{N-1}} \left\{ g_N(x_N) + \sum_{i=k}^{N-1} g_i (x_i,\mu_i(x_i)) \right\} \tag{1.7}\\
&amp;= \min_{\mu_k,\pi^{k+1}} \mathbb{E}_{w_k,\dots,w_{N-1}} \left\{ g_k(x_k,\mu_k(x_k)) + g_N(x_N) + \sum_{i=k+1}^{N-1} g_i(x_i,\mu_i(x_i))  \right\}
\tag{1.8}\\
&amp;= \min_{\mu_k} \left[ \min_{\pi^{k+1}} \mathbb{E}_{w_k,\dots,w_{N-1}} \left\{ g_k(x_k,\mu_k(x_k)) + g_N(x_N) + \sum_{i=k+1}^{N-1} g_i(x_i,\mu_i(x_i))  \right\}\right] \tag{1.9}\\
&amp;= \min_{\mu_k} \mathbb{E}_{w_k} \left\{ g_k(x_k,\mu_k(x_k)) + \min_{\pi^{k+1}} \left[ \mathbb{E}_{w_{k+1},\dots,w_{N-1}} \left\{ g_N(x_N) + \sum_{i=k+1}^{N-1} g_i(x_i,\mu_i(x_i))  \right\}  \right]    \right\} \tag{1.10}\\
&amp;= \min_{\mu_k} \mathbb{E}_{w_k} \left\{ g_k(x_k,\mu_k(x_k)) + J^\star_{k+1}(f_k(x_k,\mu_k(x_k),w_k)) \right\} \tag{1.11}\\
&amp;= \min_{\mu_k} \mathbb{E}_{w_k} \left\{ g_k(x_k,\mu_k(x_k)) + J_{k+1}(f_k(x_k,\mu_k(x_k),w_k)) \right\} \tag{1.12}\\
&amp;= \min_{u_k \in \mathbb{U}} \mathbb{E}_{w_k} \left\{ g_k(x_k,\mu_k(x_k)) + J_{k+1}(f_k(x_k,\mu_k(x_k),w_k)) \right\} \tag{1.13}\\
&amp;= J_k(x_k), \tag{1.14}
\end{align}\]</span>
where <a href="formulation.html#eq:dpproof-1">(1.7)</a> follows from definition <a href="formulation.html#eq:dptheoremdefineJkstar">(1.6)</a>; <a href="formulation.html#eq:dpproof-2">(1.8)</a> expands <span class="math inline">\(\pi^k = \{ \mu_k, \pi^{k+1}\}\)</span> and <span class="math inline">\(\sum_{i=k}^{N-1} g_i = g_k + \sum_{i=k+1}^{N-1}\)</span>; <a href="formulation.html#eq:dpproof-3">(1.9)</a> writes the joint minimization over <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\pi^{k+1}\)</span> as equivalently first minimizing over <span class="math inline">\(\pi^{k+1}\)</span> and then minimizing over <span class="math inline">\(\mu_k\)</span>; <a href="formulation.html#eq:dpproof-4">(1.10)</a> is the key step and holds because <span class="math inline">\(g_k\)</span> and <span class="math inline">\(w_k\)</span> depend only on <span class="math inline">\(\mu_k\)</span> but not on <span class="math inline">\(\pi^{k+1}\)</span>; <a href="formulation.html#eq:dpproof-5">(1.11)</a> follows again from definition <a href="formulation.html#eq:dptheoremdefineJkstar">(1.6)</a> with <span class="math inline">\(k\)</span> replaced by <span class="math inline">\(k+1\)</span>; <a href="formulation.html#eq:dpproof-6">(1.12)</a> results from the induction assumption; <a href="formulation.html#eq:dpproof-7">(1.13)</a> clearly holds because any <span class="math inline">\(\mu_k(x_k)\)</span> belongs to <span class="math inline">\(\mathbb{U}\)</span> and any element in <span class="math inline">\(\mathbb{U}\)</span> can be chosen by a feedback controller <span class="math inline">\(\mu_k\)</span>; and lastly <a href="formulation.html#eq:dpproof-8">(1.14)</a> follows from the dynamic programming algorithm <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a>.</p>
<p>By induction, this shows that <span class="math inline">\(J^\star_k(x_k) = J_k(x_k)\)</span> for all <span class="math inline">\(k=0,\dots,N\)</span>.</p>
</div>
</div>
<p>The careful reader, especially from a robotics background, may soon become disappointed when seeing the DP algorithm <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a> because it is rather conceptual than practical. To see this, we only need to run DP for <span class="math inline">\(k=N-1\)</span>:
<span class="math display" id="eq:dptryN-1">\[\begin{equation}
J_{N-1}(x_{N-1}) = \min_{u_{N-1} \in \mathbb{U}} \mathbb{E}_{w_{N-1}} \left\{ g_{N-1}(x_{N-1},u_{N-1}) + J_N(f_{N-1}(x_{N-1},u_{N-1},w_{N-1})) \right\}.
\tag{1.15}
\end{equation}\]</span></p>
<p>Two challenges immediately show up:</p>
<ul>
<li><p>How to perform the minimization over <span class="math inline">\(u_{N-1}\)</span> when <span class="math inline">\(\mathbb{U}\)</span> is a continuous constraint set? Even if we assume <span class="math inline">\(g_{N-1}\)</span> is convex<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> in <span class="math inline">\(u_{N-1}\)</span>, <span class="math inline">\(J_N\)</span> is convex in <span class="math inline">\(x_{N}\)</span>, and the dynamics <span class="math inline">\(f_{N-1}\)</span> is also convex in <span class="math inline">\(u_{N-1}\)</span> (so that the optimization <a href="formulation.html#eq:dptryN-1">(1.15)</a> is convex), we may be able to solve the minimization <em>numerically</em> for each <span class="math inline">\(x_{N-1}\)</span> using a convex optimization solver, but rarely will we be able to find an analytical policy <span class="math inline">\(\mu_{N-1}^\star\)</span> such that <span class="math inline">\(u_{N-1}^\star = \mu_{N-1}^\star (x_{N-1})\)</span> for every <span class="math inline">\(x_{N-1}\)</span> (i.e., the optimal policy <span class="math inline">\(\mu_{N-1}^\star\)</span> is implict but not explict).</p></li>
<li><p>Suppose we can find an anlytical optimal policy <span class="math inline">\(\mu_{N-1}^\star\)</span>, say <span class="math inline">\(\mu_{N-1}^\star = K x_{N-1}\)</span> a linear policy, how will plugging <span class="math inline">\(\mu_{N-1}^\star\)</span> into <a href="formulation.html#eq:dptryN-1">(1.15)</a> affect the complexity of <span class="math inline">\(J_{N-1}(x_{N-1})\)</span>? One can see that even if <span class="math inline">\(\mu_{N-1}^\star\)</span> is linear in <span class="math inline">\(x_{N-1}\)</span>, <span class="math inline">\(J_{N-1}\)</span> may be highly nonlinear in <span class="math inline">\(x_{N-1}\)</span> due to the composition with <span class="math inline">\(g_{N-1}\)</span>, <span class="math inline">\(f_{N-1}\)</span> and <span class="math inline">\(J_N\)</span>. If <span class="math inline">\(J_{N-1}(x_{N-1})\)</span> becomes too complex, then clearly it becomes more challenging to perform <a href="formulation.html#eq:dptryN-1">(1.15)</a> for the next step <span class="math inline">\(k=N-2\)</span>.</p></li>
</ul>
<p>Due to these challenges, only in a very limited amount of cases will we be able to perform <em>exact dynamic programming</em>. For example, when the state space <span class="math inline">\(\mathbb{X}\)</span> and control space <span class="math inline">\(\mathbb{U}\)</span> are discrete, we can design efficient algorithms for exact DP. For another example, when the dynamics <span class="math inline">\(f_k\)</span> is linear and the cost <span class="math inline">\(g_k\)</span> is quadratic, we will also be able to compute <span class="math inline">\(J_k(x_k)\)</span> in closed form (though this sounds a bit surprising!). We will study these problems in more details in Chapter <a href="exactdp.html#exactdp">2</a>.</p>
<p>For general optimal control problems with continuous state space and control space (and most problems we care about in robotics), unfortunately, we will have to resort to <em>approximate dynamic programming</em>, basically variations of the DP algorithm <a href="formulation.html#eq:dpbackwardrecursion">(1.5)</a> where approximate value functions <span class="math inline">\(J_k(x_k)\)</span> and/or control policies <span class="math inline">\(\mu_k(x_k)\)</span> are used (e.g., with neural networks and machine learning).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> We will introduce several popular approximation schemes in Chapter <a href="approximatedp.html#approximatedp">3</a>. We will see that, although exact DP is not possible anymore, the Bellman principle of optimality still remains one of the most important guidelines for designing approximation algorithms. Efficient algorithms for approximate dynamic programming, preferrably with performance guarantees, still remain an active area of research.</p>
</div>
<div id="infinite-horizon" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Infinite-horizon Formulation<a href="formulation.html#infinite-horizon" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we are focusing on problems with a finite horizon <span class="math inline">\(N\)</span>, what if the horizon <span class="math inline">\(N\)</span> tends to infinity?</p>
<p>In particular, consider the controller <span class="math inline">\(\pi\)</span> now contains an infinite sequence of functions
<span class="math display">\[
\pi = \{ \mu_0,\dots \}
\]</span>
and let us try to find the best policy that minimizes the cost-to-go starting from <span class="math inline">\(x_0\)</span> subject to the same dynamics as in <a href="formulation.html#eq:discrete-time-dynamics">(1.1)</a> (with <span class="math inline">\(N\)</span> tends to infinity and <span class="math inline">\(f_k \equiv f\)</span>)
<span class="math display" id="eq:cost-to-go-infinite">\[\begin{equation}
J_{\pi}(x_0) = \mathbb{E} \left\{ \sum_{k=0}^{\infty} g(x_k, \mu_k(x_k)) \right\}
\tag{1.16},
\end{equation}\]</span>
where the expectation is taken over the (infinite number of) disturbances <span class="math inline">\(\{w_0,\dots \}\)</span>.</p>
<p>We can write <a href="formulation.html#eq:cost-to-go-infinite">(1.16)</a> equivalently as
<span class="math display">\[
J_{\pi}(x_0) = \lim_{N \rightarrow \infty} J_\pi^N(x_0),
\]</span>
where, with a slight abuse of notation, <span class="math inline">\(J_\pi^N(x_0)\)</span> is <a href="formulation.html#eq:expected-cost">(1.4)</a> with <span class="math inline">\(g_N(x_N)\)</span> set to zero.</p>
<p>Now we invoke the dynamic programming algorithm in Theorem <a href="formulation.html#thm:dynamicprogramming">1.2</a>. We will first set <span class="math inline">\(J_N(x_N) = g_N(x_N)=0\)</span>, and then compute backwards in time
<span class="math display">\[
J_k(x_k) = \min_{u_k \in \mathbb{U}} \mathbb{E}_{w_k} \left\{ g(x_k,u_k) + J_{k+1}(f(x_k,u_k,w_k)) \right\}, \quad k=N-1,\dots,0.
\]</span>
To make our presentation easier later, the above DP iterations are equivalent to
<span class="math display" id="eq:dp-infinite-reversed">\[\begin{align}
J_0(x_0) &amp;= 0 \\
J_{k+1}(x_{k+1}) &amp;= \min_{u_k \in \mathbb{U}} \mathbb{E}_{w_k} \left\{ g(x_k,u_k) + J_k(f(x_k,u_k,w_k))  \right\}, \quad k=0,\dots,N, \tag{1.17}
\end{align}\]</span>
where I have done nothing but reversed the time indexing.</p>
<p>Observe that when <span class="math inline">\(N \rightarrow \infty\)</span>, <a href="formulation.html#eq:dp-infinite-reversed">(1.17)</a> performs the recursion an infinite number of times.</p>
<p>We may want to conjecture three natural consequences of the infinite-horizon solution:</p>
<ol style="list-style-type: decimal">
<li><p>The optimal infinite-horizon cost is the limit of the corresponding <span class="math inline">\(N\)</span>-stage optimal cost as <span class="math inline">\(N \rightarrow \infty\)</span>, i.e.,
<span class="math display">\[
J^\star(x) = \lim_{N \rightarrow \infty} J_N(x_N),
\]</span>
where <span class="math inline">\(J_N(x_N)\)</span> is computed from DP <a href="formulation.html#eq:dp-infinite-reversed">(1.17)</a>.</p></li>
<li><p>Bacause <span class="math inline">\(J^\star\)</span> is the result of DP <a href="formulation.html#eq:dp-infinite-reversed">(1.17)</a> when <span class="math inline">\(N\)</span> tends to infinity, if the DP algorithm converges to <span class="math inline">\(J^\star\)</span>, then <span class="math inline">\(J^\star\)</span> should satisfy
<span class="math display" id="eq:bellman-optimality-equation-infinite-horizon">\[\begin{equation}
J^\star(x) = \min_{u \in \mathbb{U}} \mathbb{E}_w \left\{ g(x,u) + J^\star(f(x,u,w)) \right\}, \quad \forall x
\tag{1.18}
\end{equation}\]</span>
Note that <a href="formulation.html#eq:bellman-optimality-equation-infinite-horizon">(1.18)</a> is an <em>equation</em> that <span class="math inline">\(J^\star(x)\)</span> should satisfy for all <span class="math inline">\(x\)</span>. In fact, this is called the <em>Bellman Optimality Equation</em>.</p></li>
<li><p>If <span class="math inline">\(\mu(x)\)</span> satisfies the Bellman equation <a href="formulation.html#eq:bellman-optimality-equation-infinite-horizon">(1.18)</a>, i.e., <span class="math inline">\(u = \mu(x)\)</span> minimizes the right-hand side of <a href="formulation.html#eq:bellman-optimality-equation-infinite-horizon">(1.18)</a> for any <span class="math inline">\(x\)</span>, then the policy <span class="math inline">\(\pi = \{\mu,\mu,\dots \}\)</span> should be optimal. This is saying, the optimal policy is time-invariant.</p></li>
</ol>
<p>In fact, all of our conjectures above are true, for most infinite-horizon problems. For example, in Chapter <a href="exactdp.html#mdp-exact-dp">2.2</a>, we will investigate the Markov Decision Process (MDP) formulation, under which the above conjectures all hold. However, one should know that there also exist many infinite-horizon problems where our conjectures will fail, and there are many mathematical subtleties in rigorously proving the conjectures.</p>
<p>The reader should see why it can be more convenient to study the infinite-horizon formulation: (i) the optimal cost-to-go is only a function of the state <span class="math inline">\(x\)</span>, but not a function of timestep <span class="math inline">\(k\)</span>; (ii) the optimal policy is time-invariant and easier to implement.</p>
<p><strong>Value Iteration</strong>. The Bellman optimality equation <a href="formulation.html#eq:bellman-optimality-equation-infinite-horizon">(1.18)</a> also suggests a natural algorithm for computing <span class="math inline">\(J^\star(x)\)</span>. We start with <span class="math inline">\(J(x)\)</span> being all zero, and then iteratively update <span class="math inline">\(J(x)\)</span> by performing the right-hand side of <a href="formulation.html#eq:bellman-optimality-equation-infinite-horizon">(1.18)</a>. This is the famous <em>value iteration</em> algorithm. We will study it in Chapter <a href="exactdp.html#mdp-exact-dp">2.2</a>.</p>
<p>As practitioners, we may simply execute the dynamic programming (value iteration) algoithm without carefully checking if our problem satisfies the assumptions. If the algorithm converges, oftentimes the problem indeed satisfies the assumptions. Otherwise, the algorithm may fail to converge, as we will see in Example <a href="exactdp.html#exm:pendulumvalueiterationbarycentric">2.3</a>.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bertsekas12book-dpocI" class="csl-entry">
———. 2012. <em>Dynamic Programming and Optimal Control: Volume i</em>. Vol. 1. Athena scientific.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>You may want to read Appendix <a href="appconvex.html#appconvex">B</a> if this is your first time seeing “convex” things.<a href="formulation.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Another possible solution is to discretize continuous states and controls. However, when the dimension of state and control is high, discretization becomes too expensive in terms of memory and computational complexity.<a href="formulation.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exactdp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/01-formulation.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
