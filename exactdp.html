<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Exact Dynamic Programming | Optimal Control and Estimation</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Estimation." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Exact Dynamic Programming | Optimal Control and Estimation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Estimation." />
  <meta name="github-repo" content="hankyang94/OptimalControlEstimation" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Exact Dynamic Programming | Optimal Control and Estimation" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Estimation." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-03-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="formulation.html"/>
<link rel="next" href="approximatedp.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Estimation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="formulation.html"><a href="formulation.html"><i class="fa fa-check"></i><b>1</b> The Optimal Control Formulation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="formulation.html"><a href="formulation.html#the-basic-problem"><i class="fa fa-check"></i><b>1.1</b> The Basic Problem</a></li>
<li class="chapter" data-level="1.2" data-path="formulation.html"><a href="formulation.html#dynamic-programming-and-principle-of-optimality"><i class="fa fa-check"></i><b>1.2</b> Dynamic Programming and Principle of Optimality</a></li>
<li class="chapter" data-level="1.3" data-path="formulation.html"><a href="formulation.html#infinite-horizon"><i class="fa fa-check"></i><b>1.3</b> Infinite-horizon Formulation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exactdp.html"><a href="exactdp.html"><i class="fa fa-check"></i><b>2</b> Exact Dynamic Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path="exactdp.html"><a href="exactdp.html#lqr"><i class="fa fa-check"></i><b>2.1</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="exactdp.html"><a href="exactdp.html#infinite-horizon-lqr"><i class="fa fa-check"></i><b>2.1.1</b> Infinite-Horizon LQR</a></li>
<li class="chapter" data-level="2.1.2" data-path="exactdp.html"><a href="exactdp.html#lqr-with-constraints"><i class="fa fa-check"></i><b>2.1.2</b> LQR with Constraints</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="exactdp.html"><a href="exactdp.html#mdp-exact-dp"><i class="fa fa-check"></i><b>2.2</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="exactdp.html"><a href="exactdp.html#bellman-optimality-equations"><i class="fa fa-check"></i><b>2.2.1</b> Bellman Optimality Equations</a></li>
<li class="chapter" data-level="2.2.2" data-path="exactdp.html"><a href="exactdp.html#value-iteration"><i class="fa fa-check"></i><b>2.2.2</b> Value Iteration</a></li>
<li class="chapter" data-level="2.2.3" data-path="exactdp.html"><a href="exactdp.html#value-iteration-with-barycentric-interpolation"><i class="fa fa-check"></i><b>2.2.3</b> Value Iteration with Barycentric Interpolation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="approximatedp.html"><a href="approximatedp.html"><i class="fa fa-check"></i><b>3</b> Approximate Optimal Control</a>
<ul>
<li class="chapter" data-level="3.1" data-path="approximatedp.html"><a href="approximatedp.html#fitted-value-iteration"><i class="fa fa-check"></i><b>3.1</b> Fitted Value Iteration</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="approximatedp.html"><a href="approximatedp.html#linear-features"><i class="fa fa-check"></i><b>3.1.1</b> Linear Features</a></li>
<li class="chapter" data-level="3.1.2" data-path="approximatedp.html"><a href="approximatedp.html#neural-network-features"><i class="fa fa-check"></i><b>3.1.2</b> Neural Network Features</a></li>
<li class="chapter" data-level="3.1.3" data-path="approximatedp.html"><a href="approximatedp.html#fitted-q-value-iteration"><i class="fa fa-check"></i><b>3.1.3</b> Fitted Q-value Iteration</a></li>
<li class="chapter" data-level="3.1.4" data-path="approximatedp.html"><a href="approximatedp.html#deep-q-network"><i class="fa fa-check"></i><b>3.1.4</b> Deep Q Network</a></li>
<li class="chapter" data-level="3.1.5" data-path="approximatedp.html"><a href="approximatedp.html#deep-shallow"><i class="fa fa-check"></i><b>3.1.5</b> Deep + Shallow</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="approximatedp.html"><a href="approximatedp.html#trajectory-optimization"><i class="fa fa-check"></i><b>3.2</b> Trajectory Optimization</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="approximatedp.html"><a href="approximatedp.html#direct-single-shooting"><i class="fa fa-check"></i><b>3.2.1</b> Direct Single Shooting</a></li>
<li class="chapter" data-level="3.2.2" data-path="approximatedp.html"><a href="approximatedp.html#direct-multiple-shooting"><i class="fa fa-check"></i><b>3.2.2</b> Direct Multiple Shooting</a></li>
<li class="chapter" data-level="3.2.3" data-path="approximatedp.html"><a href="approximatedp.html#direct-collocation"><i class="fa fa-check"></i><b>3.2.3</b> Direct Collocation</a></li>
<li class="chapter" data-level="3.2.4" data-path="approximatedp.html"><a href="approximatedp.html#direct-orthogonal-collocation"><i class="fa fa-check"></i><b>3.2.4</b> Direct Orthogonal Collocation</a></li>
<li class="chapter" data-level="3.2.5" data-path="approximatedp.html"><a href="approximatedp.html#failure-of-open-loop-control"><i class="fa fa-check"></i><b>3.2.5</b> Failure of Open-Loop Control</a></li>
<li class="chapter" data-level="3.2.6" data-path="approximatedp.html"><a href="approximatedp.html#lqr-trajectory-tracking"><i class="fa fa-check"></i><b>3.2.6</b> LQR Trajectory Tracking</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="approximatedp.html"><a href="approximatedp.html#model-predictive-control"><i class="fa fa-check"></i><b>3.3</b> Model Predictive Control</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="approximatedp.html"><a href="approximatedp.html#turn-trajectory-optimization-into-feedback-control"><i class="fa fa-check"></i><b>3.3.1</b> Turn Trajectory Optimization into Feedback Control</a></li>
<li class="chapter" data-level="3.3.2" data-path="approximatedp.html"><a href="approximatedp.html#controllability-reachability-and-invariance"><i class="fa fa-check"></i><b>3.3.2</b> Controllability, Reachability, and Invariance</a></li>
<li class="chapter" data-level="3.3.3" data-path="approximatedp.html"><a href="approximatedp.html#basic-formulation-for-linear-systems"><i class="fa fa-check"></i><b>3.3.3</b> Basic Formulation for Linear Systems</a></li>
<li class="chapter" data-level="3.3.4" data-path="approximatedp.html"><a href="approximatedp.html#persistent-feasibility"><i class="fa fa-check"></i><b>3.3.4</b> Persistent Feasibility</a></li>
<li class="chapter" data-level="3.3.5" data-path="approximatedp.html"><a href="approximatedp.html#mpc-stability"><i class="fa fa-check"></i><b>3.3.5</b> Stability</a></li>
<li class="chapter" data-level="3.3.6" data-path="approximatedp.html"><a href="approximatedp.html#explicit-mpc"><i class="fa fa-check"></i><b>3.3.6</b> Explicit MPC</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="approximatedp.html"><a href="approximatedp.html#policy-gradient"><i class="fa fa-check"></i><b>3.4</b> Policy Gradient</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html"><i class="fa fa-check"></i><b>4</b> Continuous-time Optimal Control</a>
<ul>
<li class="chapter" data-level="4.1" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#the-basic-problem-1"><i class="fa fa-check"></i><b>4.1</b> The Basic Problem</a></li>
<li class="chapter" data-level="4.2" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#the-hamilton-jacobi-bellman-equation"><i class="fa fa-check"></i><b>4.2</b> The Hamilton-Jacobi-Bellman Equation</a></li>
<li class="chapter" data-level="4.3" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#linear-quadratic-regulator"><i class="fa fa-check"></i><b>4.3</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#lqr-trajectory-tracking-1"><i class="fa fa-check"></i><b>4.3.1</b> LQR Trajectory Tracking</a></li>
<li class="chapter" data-level="4.3.2" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#lqr-trajectory-stabilization"><i class="fa fa-check"></i><b>4.3.2</b> LQR Trajectory Stabilization</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#the-pontryagin-minimum-principle"><i class="fa fa-check"></i><b>4.4</b> The Pontryagin Minimum Principle</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#numerical-solution-of-the-tpbvp"><i class="fa fa-check"></i><b>4.4.1</b> Numerical Solution of the TPBVP</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#infinite-horizon-problems"><i class="fa fa-check"></i><b>4.5</b> Infinite-Horizon Problems</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#continuous-time-infinite-horizon-lqr"><i class="fa fa-check"></i><b>4.5.1</b> Infinite-Horizon LQR</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="continuous-time-optimal-control.html"><a href="continuous-time-optimal-control.html#viscosity-solution"><i class="fa fa-check"></i><b>4.6</b> Viscosity Solution</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stability.html"><a href="stability.html"><i class="fa fa-check"></i><b>5</b> Stability Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="stability.html"><a href="stability.html#autonomous-systems"><i class="fa fa-check"></i><b>5.1</b> Autonomous Systems</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="stability.html"><a href="stability.html#concepts-of-stability"><i class="fa fa-check"></i><b>5.1.1</b> Concepts of Stability</a></li>
<li class="chapter" data-level="5.1.2" data-path="stability.html"><a href="stability.html#stability-by-linearization"><i class="fa fa-check"></i><b>5.1.2</b> Stability by Linearization</a></li>
<li class="chapter" data-level="5.1.3" data-path="stability.html"><a href="stability.html#lyapunov-analysis"><i class="fa fa-check"></i><b>5.1.3</b> Lyapunov Analysis</a></li>
<li class="chapter" data-level="5.1.4" data-path="stability.html"><a href="stability.html#invariant-set-theorem"><i class="fa fa-check"></i><b>5.1.4</b> Invariant Set Theorem</a></li>
<li class="chapter" data-level="5.1.5" data-path="stability.html"><a href="stability.html#computing-lyapunov-certificates"><i class="fa fa-check"></i><b>5.1.5</b> Computing Lyapunov Certificates</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="stability.html"><a href="stability.html#controlled-systems"><i class="fa fa-check"></i><b>5.2</b> Controlled Systems</a></li>
<li class="chapter" data-level="5.3" data-path="stability.html"><a href="stability.html#non-autonomous-systems"><i class="fa fa-check"></i><b>5.3</b> Non-autonomous Systems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="output-feedback.html"><a href="output-feedback.html"><i class="fa fa-check"></i><b>6</b> Output Feedback</a>
<ul>
<li class="chapter" data-level="6.1" data-path="output-feedback.html"><a href="output-feedback.html#least-squares-estimation"><i class="fa fa-check"></i><b>6.1</b> Least-Squares Estimation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="output-feedback.html"><a href="output-feedback.html#linear-least-squares-estimation"><i class="fa fa-check"></i><b>6.1.1</b> Linear Least-Squares Estimation</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="output-feedback.html"><a href="output-feedback.html#kalman-filter"><i class="fa fa-check"></i><b>6.2</b> Kalman Filter</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="output-feedback.html"><a href="output-feedback.html#steady-state-kalman-filter"><i class="fa fa-check"></i><b>6.2.1</b> Steady-State Kalman Filter</a></li>
<li class="chapter" data-level="6.2.2" data-path="output-feedback.html"><a href="output-feedback.html#continuous-time-kalman-filter"><i class="fa fa-check"></i><b>6.2.2</b> Continuous-time Kalman Filter</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="output-feedback.html"><a href="output-feedback.html#linear-quadratic-gaussian-control"><i class="fa fa-check"></i><b>6.3</b> Linear Quadratic Gaussian Control</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="output-feedback.html"><a href="output-feedback.html#steady-state-lqg"><i class="fa fa-check"></i><b>6.3.1</b> Steady-state LQG</a></li>
<li class="chapter" data-level="6.3.2" data-path="output-feedback.html"><a href="output-feedback.html#continuous-time-lqg"><i class="fa fa-check"></i><b>6.3.2</b> Continuous-time LQG</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="output-feedback.html"><a href="output-feedback.html#nonlinear-filtering"><i class="fa fa-check"></i><b>6.4</b> Nonlinear Filtering</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="output-feedback.html"><a href="output-feedback.html#extended-kalman-filter"><i class="fa fa-check"></i><b>6.4.1</b> Extended Kalman Filter</a></li>
<li class="chapter" data-level="6.4.2" data-path="output-feedback.html"><a href="output-feedback.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>6.4.2</b> Unscented Kalman Filter</a></li>
<li class="chapter" data-level="6.4.3" data-path="output-feedback.html"><a href="output-feedback.html#particle-filter"><i class="fa fa-check"></i><b>6.4.3</b> Particle Filter</a></li>
<li class="chapter" data-level="6.4.4" data-path="output-feedback.html"><a href="output-feedback.html#feedback-particle-filter"><i class="fa fa-check"></i><b>6.4.4</b> Feedback Particle Filter</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="output-feedback.html"><a href="output-feedback.html#state-observer"><i class="fa fa-check"></i><b>6.5</b> State Observer</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="output-feedback.html"><a href="output-feedback.html#general-design-strategy"><i class="fa fa-check"></i><b>6.5.1</b> General Design Strategy</a></li>
<li class="chapter" data-level="6.5.2" data-path="output-feedback.html"><a href="output-feedback.html#luenberger-template"><i class="fa fa-check"></i><b>6.5.2</b> Luenberger Template</a></li>
<li class="chapter" data-level="6.5.3" data-path="output-feedback.html"><a href="output-feedback.html#state-affine-template"><i class="fa fa-check"></i><b>6.5.3</b> State-affine Template</a></li>
<li class="chapter" data-level="6.5.4" data-path="output-feedback.html"><a href="output-feedback.html#kazantzis-kravaris-luenberger-kkl-template"><i class="fa fa-check"></i><b>6.5.4</b> Kazantzis-Kravaris-Luenberger (KKL) Template</a></li>
<li class="chapter" data-level="6.5.5" data-path="output-feedback.html"><a href="output-feedback.html#triangular-template"><i class="fa fa-check"></i><b>6.5.5</b> Triangular Template</a></li>
<li class="chapter" data-level="6.5.6" data-path="output-feedback.html"><a href="output-feedback.html#design-with-convex-optimization"><i class="fa fa-check"></i><b>6.5.6</b> Design with Convex Optimization</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="output-feedback.html"><a href="output-feedback.html#observer-feedback"><i class="fa fa-check"></i><b>6.6</b> Observer Feedback</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="geometric-vision.html"><a href="geometric-vision.html"><i class="fa fa-check"></i><b>7</b> Geometric Vision</a>
<ul>
<li class="chapter" data-level="7.1" data-path="geometric-vision.html"><a href="geometric-vision.html#d-rotations-and-poses"><i class="fa fa-check"></i><b>7.1</b> 3D Rotations and Poses</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="geometric-vision.html"><a href="geometric-vision.html#rotation-matrices"><i class="fa fa-check"></i><b>7.1.1</b> Rotation matrices</a></li>
<li class="chapter" data-level="7.1.2" data-path="geometric-vision.html"><a href="geometric-vision.html#coordinate-frame"><i class="fa fa-check"></i><b>7.1.2</b> Coordinate Frame</a></li>
<li class="chapter" data-level="7.1.3" data-path="geometric-vision.html"><a href="geometric-vision.html#representations-of-the-rotations"><i class="fa fa-check"></i><b>7.1.3</b> Representations of the rotations</a></li>
<li class="chapter" data-level="7.1.4" data-path="geometric-vision.html"><a href="geometric-vision.html#miscellaneous-topics-on-rotations"><i class="fa fa-check"></i><b>7.1.4</b> Miscellaneous topics on rotations</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="geometric-vision.html"><a href="geometric-vision.html#the-pinhole-camera-model"><i class="fa fa-check"></i><b>7.2</b> The Pinhole Camera Model</a></li>
<li class="chapter" data-level="7.3" data-path="geometric-vision.html"><a href="geometric-vision.html#camera-pose-estimation"><i class="fa fa-check"></i><b>7.3</b> Camera Pose Estimation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="geometric-vision.html"><a href="geometric-vision.html#the-p3p-problem"><i class="fa fa-check"></i><b>7.3.1</b> The P3P Problem</a></li>
<li class="chapter" data-level="7.3.2" data-path="geometric-vision.html"><a href="geometric-vision.html#the-pnp-problem"><i class="fa fa-check"></i><b>7.3.2</b> The PnP Problem</a></li>
<li class="chapter" data-level="7.3.3" data-path="geometric-vision.html"><a href="geometric-vision.html#global-optimality"><i class="fa fa-check"></i><b>7.3.3</b> Global Optimality</a></li>
<li class="chapter" data-level="7.3.4" data-path="geometric-vision.html"><a href="geometric-vision.html#handling-outliers"><i class="fa fa-check"></i><b>7.3.4</b> Handling Outliers</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="geometric-vision.html"><a href="geometric-vision.html#point-cloud-registration"><i class="fa fa-check"></i><b>7.4</b> Point Cloud Registration</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="adaptivecontrol.html"><a href="adaptivecontrol.html"><i class="fa fa-check"></i><b>8</b> Adaptive Control</a>
<ul>
<li class="chapter" data-level="8.1" data-path="adaptivecontrol.html"><a href="adaptivecontrol.html#model-reference-adaptive-control"><i class="fa fa-check"></i><b>8.1</b> Model-Reference Adaptive Control</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="adaptivecontrol.html"><a href="adaptivecontrol.html#first-order-systems"><i class="fa fa-check"></i><b>8.1.1</b> First-Order Systems</a></li>
<li class="chapter" data-level="8.1.2" data-path="adaptivecontrol.html"><a href="adaptivecontrol.html#high-order-systems"><i class="fa fa-check"></i><b>8.1.2</b> High-Order Systems</a></li>
<li class="chapter" data-level="8.1.3" data-path="adaptivecontrol.html"><a href="adaptivecontrol.html#robotic-manipulator"><i class="fa fa-check"></i><b>8.1.3</b> Robotic Manipulator</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="adaptivecontrol.html"><a href="adaptivecontrol.html#certainty-equivalent-adaptive-control"><i class="fa fa-check"></i><b>8.2</b> Certainty-Equivalent Adaptive Control</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="psets.html"><a href="psets.html"><i class="fa fa-check"></i><b>9</b> Problem Sets</a></li>
<li class="chapter" data-level="" data-path="acknowledgement.html"><a href="acknowledgement.html"><i class="fa fa-check"></i>Acknowledgement</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra and Differential Equations</a>
<ul>
<li class="chapter" data-level="A.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#linear-algebra"><i class="fa fa-check"></i><b>A.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#matrix-exponential"><i class="fa fa-check"></i><b>A.1.1</b> Matrix Exponential</a></li>
<li class="chapter" data-level="A.1.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#gradients"><i class="fa fa-check"></i><b>A.1.2</b> Gradients</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#solving-an-ordinary-differential-equation"><i class="fa fa-check"></i><b>A.2</b> Solving an Ordinary Differential Equation</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#separation-of-variables"><i class="fa fa-check"></i><b>A.2.1</b> Separation of Variables</a></li>
<li class="chapter" data-level="A.2.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#first-order-linear-ode"><i class="fa fa-check"></i><b>A.2.2</b> First-order Linear ODE</a></li>
<li class="chapter" data-level="A.2.3" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#gronwall-inequality"><i class="fa fa-check"></i><b>A.2.3</b> Gronwall Inequality</a></li>
<li class="chapter" data-level="A.2.4" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#matlab"><i class="fa fa-check"></i><b>A.2.4</b> Matlab</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>B</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>B.1</b> Theory</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>B.1.1</b> Sets</a></li>
<li class="chapter" data-level="B.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>B.1.2</b> Convex function</a></li>
<li class="chapter" data-level="B.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>B.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="B.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>B.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>B.2</b> Practice</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>B.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="B.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>B.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="B.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>B.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="B.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>B.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="B.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>B.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="B.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>B.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="B.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>B.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>C</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="C.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>C.1</b> Stability</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>C.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="C.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>C.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="C.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis-1"><i class="fa fa-check"></i><b>C.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>C.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>C.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="C.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>C.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="C.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>C.2.3</b> Duality</a></li>
<li class="chapter" data-level="C.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>C.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>C.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>C.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="C.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>C.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html"><i class="fa fa-check"></i><b>D</b> Algebraic Techniques and Sum-of-Squares</a>
<ul>
<li class="chapter" data-level="D.1" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#algebra"><i class="fa fa-check"></i><b>D.1</b> Algebra</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#polynomials"><i class="fa fa-check"></i><b>D.1.1</b> Polynomials</a></li>
<li class="chapter" data-level="D.1.2" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#representation-of-nonnegative-polynomial-univariate-case"><i class="fa fa-check"></i><b>D.1.2</b> Representation of nonnegative polynomial: Univariate case</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="the-kalman-yakubovich-lemma.html"><a href="the-kalman-yakubovich-lemma.html"><i class="fa fa-check"></i><b>E</b> The Kalman-Yakubovich Lemma</a></li>
<li class="chapter" data-level="F" data-path="feedbacklinearization.html"><a href="feedbacklinearization.html"><i class="fa fa-check"></i><b>F</b> Feedback Linearization</a></li>
<li class="chapter" data-level="G" data-path="slidingcontrol.html"><a href="slidingcontrol.html"><i class="fa fa-check"></i><b>G</b> Sliding Control</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Estimation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="exactdp" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Exact Dynamic Programming<a href="exactdp.html#exactdp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="formulation.html#formulation">1</a>, we introduced the basic formulation of the finite-horizon and discrete-time optimal control problem, presented the Bellman principle of optimality, and derived the dynamic programming (DP) algorithm. We mentioned that, despite being a general-purpose algorithm, it can be difficult to implement DP exactly in practical applications.</p>
<p>In this Chapter, we will introduce two problem setups where DP can in fact be implemented exactly.</p>
<div id="lqr" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Linear Quadratic Regulator<a href="exactdp.html#lqr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a linear discrete-time dynamical system
<span class="math display" id="eq:lqr-linear-system">\[\begin{equation}
x_{k+1} = A_k x_k + B_k u_k + w_k, \quad k=0,1,\dots,N-1,
\tag{2.1}
\end{equation}\]</span>
where <span class="math inline">\(x_k \in \mathbb{R}^n\)</span> the state, <span class="math inline">\(u_k \in \mathbb{R}^m\)</span> the control, <span class="math inline">\(w_k \in \mathbb{R}^n\)</span> the independent, zero-mean disturbance with given probability distribution that does not depend on <span class="math inline">\(x_k,u_k\)</span>, and <span class="math inline">\(A_k \in \mathbb{R}^{n \times n}, B_k \in \mathbb{R}^{n \times m}\)</span> are known matrices determining the transition dynamics.</p>
<p>We want to solve the following optimal control problem
<span class="math display" id="eq:lqr-formulation">\[\begin{equation}
\min_{\mu_0,\dots,\mu_{N-1}} \mathbb{E} \left\{ x_N^T Q_N x_N + \sum_{k=0}^{N-1} \left( x_k^T Q_k x_k + u_k^T R_k u_k \right) \right\},
\tag{2.2}
\end{equation}\]</span>
where the expectation is taken over the randomness in <span class="math inline">\(w_0,\dots,w_{N-1}\)</span>. In <a href="exactdp.html#eq:lqr-formulation">(2.2)</a>, <span class="math inline">\(\{Q_k \}_{k=0}^N\)</span> are positive semidefinite matrices, and <span class="math inline">\(\{ R_k \}_{k=0}^{N-1}\)</span> are positive definite matrices. The formulation <a href="exactdp.html#eq:lqr-formulation">(2.2)</a> is typically known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to “regulate” the system around the origin <span class="math inline">\(x=0\)</span>.</p>
<p>We will now show that the DP algorithm in Theorem <a href="formulation.html#thm:dynamicprogramming">1.2</a> can be exactly implemented for LQR.</p>
<p>The DP algorithm computes the optimal cost-to-go backwards in time.
The terminal cost is
<span class="math display">\[
J_N(x_N) = x_N^T Q_N x_N
\]</span>
by definition.</p>
<p>The optimal cost-to-go at time <span class="math inline">\(N-1\)</span> is equal to
<span class="math display" id="eq:lqr-cost-N-1">\[\begin{equation}
\begin{split}
J_{N-1}(x_{N-1}) = \min_{u_{N-1}} \mathbb{E}_{w_{N-1}} \{ x_{N-1}^T Q_{N-1} x_{N-1} + u_{N-1}^T R_{N-1} u_{N-1} + \\ \Vert \underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \Vert^2_{Q_N} \}
\end{split}
\tag{2.3}
\end{equation}\]</span>
where <span class="math inline">\(\Vert v \Vert_Q^2 = v^T Q v\)</span> for <span class="math inline">\(Q \succeq 0\)</span>. Now observe that the objective in <a href="exactdp.html#eq:lqr-cost-N-1">(2.3)</a> is
<span class="math display">\[\begin{equation}
\begin{split}
x_{N-1}^T Q_{N-1} x_{N-1} + u_{N-1}^T R_{N-1} u_{N-1} + \Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \Vert_{Q_N}^2 + \\
\mathbb{E}_{w_{N-1}} \left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^T Q_{N-1} w_{N-1} \right] + \\
\mathbb{E}_{w_{N-1}} \left[ w_{N-1}^T Q_N w_{N-1} \right]
\end{split}
\end{equation}\]</span>
where the second line is zero due to <span class="math inline">\(\mathbb{E}(w_{N-1}) = 0\)</span> and the third line is a constant with respect to <span class="math inline">\(u_{N-1}\)</span>. Consequently, the optimal control <span class="math inline">\(u_{N-1}^\star\)</span> can be computed by setting the derivative of the objective with respect to <span class="math inline">\(u_{N-1}\)</span> equal to zero
<span class="math display" id="eq:optimal-u-N-1">\[\begin{equation}
u_{N-1}^\star = - \left[ \left( R_{N-1} + B_{N-1}^T Q_N B_{N-1} \right)^{-1} B_{N-1}^T Q_N A_{N-1} \right] x_{N-1}.
\tag{2.4}
\end{equation}\]</span>
Plugging the optimal controller <span class="math inline">\(u^\star_{N-1}\)</span> back to the objective of <a href="exactdp.html#eq:lqr-cost-N-1">(2.3)</a> leads to
<span class="math display" id="eq:optimal-cost-N-1">\[\begin{equation}
J_{N-1}(x_{N-1}) = x_{N-1}^T S_{N-1} x_{N-1} + \mathbb{E} \left[ w_{N-1}^T Q_N w_{N-1} \right],
\tag{2.5}
\end{equation}\]</span>
with
<span class="math display">\[
S_{N-1} = Q_{N-1} + A_{N-1}^T \left[ Q_N - Q_N B_{N-1} \left( R_{N-1} + B_{N-1}^T Q_N B_{N-1} \right)^{-1} B_{N-1}^T Q_N \right] A_{N-1}.
\]</span>
We note that <span class="math inline">\(S_{N-1}\)</span> is positive semidefinite (this is an exercise for you to convince yourself).</p>
<p>Now we realize that something surprising and nice has happened.</p>
<ol style="list-style-type: decimal">
<li><p>The optimal controller <span class="math inline">\(u^{\star}_{N-1}\)</span> in <a href="exactdp.html#eq:optimal-u-N-1">(2.4)</a> is a linear feedback policy of the state <span class="math inline">\(x_{N-1}\)</span>, and</p></li>
<li><p>The optimal cost-to-go <span class="math inline">\(J_{N-1}(x_{N-1})\)</span> in <a href="exactdp.html#eq:optimal-cost-N-1">(2.5)</a> is quadratic in <span class="math inline">\(x_{N-1}\)</span>, just the same as <span class="math inline">\(J_{N}(x_N)\)</span>.</p></li>
</ol>
<p>This implies that, if we continue to compute the optimal cost-to-go at time <span class="math inline">\(N-2\)</span>, we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is,</p>
<blockquote>
<p>The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time.</p>
</blockquote>
<p>We summarize the solution for the LQR problem <a href="exactdp.html#eq:lqr-formulation">(2.2)</a> as follows.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:discretetimefinitehorizonlqrsolution" class="proposition"><strong>Proposition 2.1  (Solution of Discrete-Time Finite-Horizon LQR) </strong></span>The optimal controller for the LQR problem <a href="exactdp.html#eq:lqr-formulation">(2.2)</a> is a linear state-feedback policy
<span class="math display" id="eq:lqr-solution-control">\[\begin{equation}
\mu_k^\star(x_k) = - K_k x_k, \quad k=0,\dots,N-1.
\tag{2.6}
\end{equation}\]</span>
The gain matrix <span class="math inline">\(K_k\)</span> can be computed as
<span class="math display">\[
K_k = \left( R_k + B_k^T S_{k+1} B_k  \right)^{-1} B_k^T S_{k+1} A_k,
\]</span>
where the matrix <span class="math inline">\(S_k\)</span> satisfies the following backwards recursion
<span class="math display" id="eq:finite-discrete-lqr-riccati">\[\begin{equation}
\hspace{-6mm}
\begin{split}
S_N &amp;= Q_N \\
S_k &amp;= Q_k + A_k^T \left[ S_{k+1} - S_{k+1}B_k \left( R_k + B_k^T S_{k+1} B_k  \right)^{-1}  B_k^T S_{k+1}  \right] A_k, k=N-1,\dots,0.
\end{split}
\tag{2.7}
\end{equation}\]</span>
The optimal cost-to-go is given by
<span class="math display">\[
J_0(x_0) = x_0^T S_0 x_0 + \sum_{k=0}^{N-1} \mathbb{E} \left[ w_k^T S_{k+1} w_k\right].
\]</span>
The recursion <a href="exactdp.html#eq:finite-discrete-lqr-riccati">(2.7)</a> is called the <em>discrete-time Riccati equation</em>.</p>
</div>
</div>
<p>Proposition <a href="exactdp.html#prp:discretetimefinitehorizonlqrsolution">2.1</a> states that, to evaluate the optimal policy <a href="exactdp.html#eq:lqr-solution-control">(2.6)</a>, one can first run the backwards Riccati equation <a href="exactdp.html#eq:finite-discrete-lqr-riccati">(2.7)</a> to compute all the positive definite matrices <span class="math inline">\(S_k\)</span>, and then compute the gain matrices <span class="math inline">\(K_k\)</span>. For systems of reasonable dimensions, evalutating the matrix inversion in <a href="exactdp.html#eq:finite-discrete-lqr-riccati">(2.7)</a> should be fairly efficient.</p>
<div id="infinite-horizon-lqr" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Infinite-Horizon LQR<a href="exactdp.html#infinite-horizon-lqr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In many robotics applications, it is often more useful to study the infinite-horizon LQR problem
<span class="math display" id="eq:infinite-horizon-lqr-system" id="eq:infinite-horizon-lqr-cost">\[\begin{align}
\min_{u_k} &amp; \quad  \sum_{k=0}^{\infty} \left( x_k^T Q x_k + u_k^T R u_k \right) \tag{2.8} \\
\text{subject to} &amp; \quad x_{k+1} = A x_k + B u_k, \quad k=0,\dots,\infty, \tag{2.9}
\end{align}\]</span>
where <span class="math inline">\(Q \succeq 0\)</span>, <span class="math inline">\(R \succ 0\)</span>, and <span class="math inline">\(A,B\)</span> are constant matrices. The reason for studying the formulation <a href="exactdp.html#eq:infinite-horizon-lqr-cost">(2.8)</a> is twofold. First, for nonlinear systems, we often linearize the nonlinear dynamics around an (equilibrium) point we care about, leading to constant <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> matrices. Second, we care more about the <em>asymptotic</em> effect of our controller than its behavior in a fixed number of steps. We will soon see an example of this formulation for balancing a simple pendulum.</p>
<p>The infinite-horizon formulation is essentially the finite-horizon formulation <a href="exactdp.html#eq:lqr-formulation">(2.2)</a> with <span class="math inline">\(N \rightarrow \infty\)</span>. Based on our intuition in deriving the finite-horizon LQR solution, we may want to hypothesize that the optimal cost-to-go is a quadratic function
<span class="math display" id="eq:infinite-horizon-lqr-optimal-cost">\[\begin{equation}
J_{k}(x_{k}) = x_{k}^T S x_{k}, k=0,\dots,\infty
\tag{2.10}
\end{equation}\]</span>
for some positive definite matrix <span class="math inline">\(S\)</span>, and proceed to invoke the DP algorithm. Notice that we hypothesize the matrix <span class="math inline">\(S\)</span> is in fact <em>stationary</em>, i.e., it does not change with respect to time. This hypothesis makes sense because the <span class="math inline">\(A,B,Q,R\)</span> matrices are stationary in the formulation <a href="exactdp.html#eq:infinite-horizon-lqr-cost">(2.8)</a>. Invoking the DP algorithm we have
<span class="math display" id="eq:infinite-horizon-lqr-invoke-dp">\[\begin{equation}
x_k^T S x_k = J_k(x_k) = \min_{u_k} \left\{ x_k^T Q x_k + u_k^T R u_k + \Vert \underbrace{A x_k + B u_k}_{x_{k+1}} \Vert_S^2  \right\}.
\tag{2.11}
\end{equation}\]</span>
The minimization over <span class="math inline">\(u_k\)</span> in <a href="exactdp.html#eq:infinite-horizon-lqr-invoke-dp">(2.11)</a> can again be solved in closed-form by setting the gradient of the objective with respect to <span class="math inline">\(u_k\)</span> to be zero
<span class="math display" id="eq:infinite-horizon-lqr-control">\[\begin{equation}
u_k^\star = - \underbrace{\left[ \left( R + B^T S B \right)^{-1} B^T S A \right]}_{K} x_k.
\tag{2.12}
\end{equation}\]</span>
Plugging the optimal <span class="math inline">\(u_k^\star\)</span> back into <a href="exactdp.html#eq:infinite-horizon-lqr-invoke-dp">(2.11)</a>, we see that the matrix <span class="math inline">\(S\)</span> has to satisfy the following equation
<span class="math display" id="eq:algebraic-riccati">\[\begin{equation}
S = Q + A^T \left[  S - SB \left( R + B^T S B  \right)^{-1} B^T S \right] A.
\tag{2.13}
\end{equation}\]</span>
Equation <a href="exactdp.html#eq:algebraic-riccati">(2.13)</a> is the famous <em>algebraic Riccati equation</em>.</p>
<p>Let’s zoom out to see what we have done. We started with a hypothetical optimal cost-to-go <a href="exactdp.html#eq:infinite-horizon-lqr-optimal-cost">(2.10)</a> that is stationary, and invoked the DP algorithm in <a href="exactdp.html#eq:infinite-horizon-lqr-invoke-dp">(2.11)</a>, which led us to the algebraic Riccati equation <a href="exactdp.html#eq:algebraic-riccati">(2.13)</a>. Therefore, if there actually exists a solution to the algebraic Riccati equation <a href="exactdp.html#eq:algebraic-riccati">(2.13)</a>, then the linear controller <a href="exactdp.html#eq:infinite-horizon-lqr-control">(2.12)</a> is indeed optimal (by the optimality of DP)!</p>
<p>So the question boils down to if the algebraic Riccati equation has a solution <span class="math inline">\(S\)</span> that is positive definite? The following proposition gives an answer.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:infinitehorizonlqrsolution" class="proposition"><strong>Proposition 2.2  (Solution of Discrete-Time Infinite-Horizon LQR) </strong></span>Consider a linear system
<span class="math display">\[
x_{k+1} = A x_k + B u_k,
\]</span>
with <span class="math inline">\((A,B)\)</span> controllable (see Appendix <a href="app-lti-system-theory.html#app-lti-controllable-observable">C.2</a>). Let <span class="math inline">\(Q \succeq 0\)</span> in <a href="exactdp.html#eq:infinite-horizon-lqr-cost">(2.8)</a> be such that <span class="math inline">\(Q\)</span> can be written as <span class="math inline">\(Q = C^T C\)</span> with <span class="math inline">\((A,C)\)</span> observable.</p>
<p>Then the optimal controller for the infinite-horizon LQR problem <a href="exactdp.html#eq:infinite-horizon-lqr-cost">(2.8)</a> is a stationary linear policy
<span class="math display">\[
\mu^\star (x) = - K x,
\]</span>
with
<span class="math display">\[
K = \left( R + B^T S B \right)^{-1} B^T S A.
\]</span>
The matrix <span class="math inline">\(S\)</span> is the unique positive definite matrix that satisfies the algebraic Riccati equation
<span class="math display">\[
S = Q + A^T \left[  S - SB \left( R + B^T S B  \right)^{-1} B^T S \right] A.
\]</span></p>
<p>Moreover, the closed-loop system
<span class="math display">\[
x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k
\]</span>
is stable, i.e., the eigenvalues of the matrix <span class="math inline">\(A - BK\)</span> are strictly within the unit circle (see Appendix <a href="app-lti-system-theory.html#app-lti-stability-dt">C.1.2</a>).</p>
</div>
</div>
<p>A rigorous proof of Proposition <a href="exactdp.html#prp:infinitehorizonlqrsolution">2.2</a> is available in Proposition 3.1.1 of <span class="citation">(<a href="#ref-bertsekas12book-dpocI">Bertsekas 2012</a>)</span>. The proof basically studies the limit of the discrete-time Riccati equation <a href="exactdp.html#eq:finite-discrete-lqr-riccati">(2.7)</a> when <span class="math inline">\(N \rightarrow \infty\)</span>. Indeed, the algebraic Riccati equation <a href="exactdp.html#eq:algebraic-riccati">(2.13)</a> is the limit of the discrete-time Riccati equation <a href="exactdp.html#eq:finite-discrete-lqr-riccati">(2.7)</a> when <span class="math inline">\(N \rightarrow \infty\)</span>. The assumptions of <span class="math inline">\((A,B)\)</span> being controllable and <span class="math inline">\((A,C)\)</span> being observable can be relaxted to <span class="math inline">\((A,B)\)</span> being stabilizable and <span class="math inline">\((A,C)\)</span> being detectable (for definitions of stabilizability and detectability, see Appendix <a href="app-lti-system-theory.html#app-lti-system-theory">C</a>).</p>
<p>We have not discussed how to solve the algebraic Riccati equation <a href="exactdp.html#eq:finite-discrete-lqr-riccati">(2.7)</a>. It is clear that <a href="exactdp.html#eq:finite-discrete-lqr-riccati">(2.7)</a> is not a linear system of equations in <span class="math inline">\(S\)</span>. In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see <span class="citation">(<a href="#ref-arnold84ieee-generalized">Arnold and Laub 1984</a>)</span>. Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab <a href="https://www.mathworks.com/help/control/ref/dlqr.html"><code>dlqr</code></a> function computes the <span class="math inline">\(K\)</span> and <span class="math inline">\(S\)</span> matrices from <span class="math inline">\(A,B,Q,R\)</span>.</p>
<p>Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:lqr-pendulum-stabilization" class="example"><strong>Example 2.1  (Pendulum Stabilization by LQR) </strong></span>Consider the simple pendulum in Fig. <a href="exactdp.html#fig:pendulum-drawing">2.1</a> with dynamics
<span class="math display" id="eq:lqr-pendulum-dynamics">\[\begin{equation}
x = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}, \quad
\dot{x} = f(x,u) = \begin{bmatrix}
\dot{\theta} \\
-\frac{1}{ml^2}(b \dot{\theta} + mgl \sin \theta) + \frac{1}{ml^2} u
\end{bmatrix}
\tag{2.14}
\end{equation}\]</span>
where <span class="math inline">\(m\)</span> is the mass of the pendulum, <span class="math inline">\(l\)</span> is the length of the pole, <span class="math inline">\(g\)</span> is the gravitational constant, <span class="math inline">\(b\)</span> is the damping ratio, and <span class="math inline">\(u\)</span> is the torque applied to the pendulum.</p>
<p>We are interested in applying the LQR controller to balance the pendulum in the upright position <span class="math inline">\(x_d = [\pi,0]^T\)</span> with a zero velocity.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-drawing"></span>
<img src="images/pendulum-drawing.png" alt="A Simple Pendulum." width="40%" />
<p class="caption">
Figure 2.1: A Simple Pendulum.
</p>
</div>
<p>Let us first shift the dynamics so that “<span class="math inline">\(0\)</span>” is the upright position. This can be done by defining a new variable <span class="math inline">\(z = x - x_d = [\theta - \pi, \dot{\theta}]^T\)</span>, which leads to
<span class="math display" id="eq:pendulum-dynamics-z-coordinate">\[\begin{equation}
\dot{z} = \dot{x} = f(x,u) = f(z + x_d,u) = \begin{bmatrix}
z_2 \\
\frac{1}{ml^2} \left( u - b z_2 + mgl \sin z_1  \right)
\end{bmatrix} = f&#39;(z,u).
\tag{2.15}
\end{equation}\]</span>
We then linearize the nonlinear dynamics <span class="math inline">\(\dot{z} = f&#39;(z,u)\)</span> at the point <span class="math inline">\(z^\star = 0, u^\star = 0\)</span>:
<span class="math display">\[\begin{align}
\dot{z} &amp; \approx f&#39;(z^\star,u^\star) + \left( \frac{\partial f&#39;}{\partial z} \right)_{z^\star,u^\star} (z - z^\star) + \left( \frac{\partial f&#39;}{\partial u} \right)_{z^\star,u^\star} (u - u^\star) \\
&amp; = \begin{bmatrix}
0 &amp; 1 \\
\frac{g}{l} \cos z_1 &amp; - \frac{b}{ml^2}
\end{bmatrix}_{z^\star, u^\star} z +
\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix} u \\
&amp; = \underbrace{\begin{bmatrix}
0 &amp; 1 \\
\frac{g}{l} &amp; - \frac{b}{ml^2}
\end{bmatrix}}_{A_c} z  +
\underbrace{\begin{bmatrix}
0 \\
\frac{1}{ml^2}
\end{bmatrix}}_{B_c} u.
\end{align}\]</span>
Finally, we convert the continuous-time dynamics to discrete time with a fixed discretization <span class="math inline">\(h\)</span>
<span class="math display">\[
z_{k+1} = \dot{z}_k \cdot h + z_k = \underbrace{(h \cdot A_c + I )}_{A} z_k + \underbrace{(h \cdot B_c)}_{B} u_k.
\]</span></p>
<p>We are now ready to implement the LQR controller. In the formulation <a href="exactdp.html#eq:infinite-horizon-lqr-cost">(2.8)</a>, we choose <span class="math inline">\(Q = I\)</span>, <span class="math inline">\(R = I\)</span>, and solve the gain matrix <span class="math inline">\(K\)</span> using the Matlab <code>dlqr</code> function.</p>
<p>Fig. <a href="exactdp.html#fig:pendulum-stabilization-sim">2.2</a> shows the simulation result for <span class="math inline">\(m=1,l=1,b=0.1\)</span>, <span class="math inline">\(g = 9.8\)</span>, and <span class="math inline">\(h = 0.01\)</span>, with an initial condition <span class="math inline">\(z^0 = [0.1,0.1]^T\)</span>. We can see that the LQR controller successfully stabilizes the pendulum at <span class="math inline">\(z^\star\)</span>, the upright position.</p>
<p>You can play with the Matlab code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/pendulum_stabilization_lqr.m">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-stabilization-sim"></span>
<img src="images/pendulum-stabilization-lqr.png" alt="LQR stabilization of a simple pendulum." width="60%" />
<p class="caption">
Figure 2.2: LQR stabilization of a simple pendulum.
</p>
</div>
</div>
</div>
<!-- ### LQR as Convex Optimization
Consider the following problem
\begin{equation}
\min_{K} \sum_{k=0}^{\infty} x_k^T Q x_k + u_k^T R u_k, \quad \text{subject to } x_{k+1} = A x_k + B u_k, \quad u_k = - K x_k.
(\#eq:dt-lqr-opt)
\end{equation}
Write the objective of \@ref(eq:dt-lqr-opt) as
$$
J(K) = \sum_{k=0}^{\infty} x_k^T (Q + K^T R K) x_k.
$$
Use the fact that
$$
x_{k+1} = (A - BK) x_k = (A - BK)^k x_0,
$$
we have 
\begin{align}
J(K) &= \sum_{k=0}^{\infty} x_0^T ((A - BK)^T)^k (Q + K^T R K) (A - BK)^k x_0 \\
&= \text{tr}\left[ (Q + K^T R K) X \right]
\end{align}
with 
$$
X = \sum_{k=0}^{\infty} (A - BK)^k x_0 x_0^T ((A - BK)^T)^k.
$$
One can show that $X$ is the solution to the following Lyapunov equation
$$
(A - BK) X (A - BK)^T - X + x_0 x_0^T = 0.
$$
Therefore, the original optimization problem becomes
\begin{align}
\min_{K, X} & \quad \text{tr}(QX) + \text{tr}(K^T R K X) \\
\text{subject to} & \quad (A - BK) X (A - BK)^T - X + x_0 x_0^T = 0 \\
& \quad X \succeq 0
(\#eq:dt-lqr-opt-matrix)
\end{align} -->
</div>
<div id="lqr-with-constraints" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> LQR with Constraints<a href="exactdp.html#lqr-with-constraints" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s explore LQR with constraints in Exercise <a href="psets.html#exr:lqrconstraints">9.2</a></p>
</div>
</div>
<div id="mdp-exact-dp" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Markov Decision Process<a href="exactdp.html#mdp-exact-dp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Section <a href="exactdp.html#lqr">2.1</a>, we see that linear dynamics and quadratic costs leads to exact dynamic programming. We now introduce another setup where the number of states and controls is finite (as opposed to the LQR case where <span class="math inline">\(x_k\)</span> and <span class="math inline">\(u_k\)</span> live in continuous spaces). We will see that we can execute DP exactly in this setup as well.</p>
<p>Optimal control in the case of finite states and controls is typically introduced in the framework of a <em>Markov Decision Process</em> (MDP, which is common in Reinforcement Learning).
There are many variations of a MDP, and here we only focus on the discounted infinite-horizon MDP. For a more complete treatment of MDPs, I suggest checking out <a href="https://shamulent.github.io/CS_Stat184_Fall23.html">this course at Harvard</a>.</p>
<p>Formally, a discounted infinite-horizon MDP <span class="math inline">\(\mathcal{M} = (\mathbb{X},\mathbb{U},P,g,\gamma,\sigma)\)</span> is specified by</p>
<ul>
<li><p>a state space <span class="math inline">\(\mathbb{X}\)</span> that is finite with size <span class="math inline">\(|\mathbb{X}|\)</span></p></li>
<li><p>a control space <span class="math inline">\(\mathbb{U}\)</span> that is finite with size <span class="math inline">\(|\mathbb{U}|\)</span></p></li>
<li><p>a transition function <span class="math inline">\(P: \mathbb{X} \times \mathbb{U} \rightarrow \Delta(\mathbb{X})\)</span>, where <span class="math inline">\(\Delta(\mathbb{X})\)</span> is the space of probability distributions over <span class="math inline">\(\mathbb{X}\)</span>; specifically, <span class="math inline">\(P(x&#39; \mid x, u)\)</span> is the probability of transitioning into state <span class="math inline">\(x&#39;\)</span> from state <span class="math inline">\(x\)</span> using control <span class="math inline">\(u\)</span>. If the system is deterministic, then <span class="math inline">\(P(x&#39; \mid x, u)\)</span> is nonzero only for a single next state <span class="math inline">\(x&#39;\)</span></p></li>
<li><p>a cost function <span class="math inline">\(g: \mathbb{X} \times \mathbb{U} \rightarrow [0,1]\)</span>; <span class="math inline">\(g(x,u)\)</span> is the cost of taking the control <span class="math inline">\(u\)</span> at state <span class="math inline">\(x\)</span></p></li>
<li><p>a discount factor <span class="math inline">\(\gamma \in [0,1)\)</span></p></li>
<li><p>an initial state distribution <span class="math inline">\(\sigma \in \Delta(\mathbb{X})\)</span> that specifies how the initial state <span class="math inline">\(x_0\)</span> is generated; in many cases we will assume <span class="math inline">\(x_0\)</span> is fixed and <span class="math inline">\(\sigma\)</span> is a distribution supported only on <span class="math inline">\(x_0\)</span>.</p></li>
</ul>
<p>In an MDP, the system starts at some state <span class="math inline">\(x_0 \sim \sigma\)</span>. At each step <span class="math inline">\(k = 0,1,2,\dots\)</span>, the system decides a control <span class="math inline">\(u_k \in \mathbb{U}\)</span> and incurs a cost <span class="math inline">\(g(s_k,u_k)\)</span>. The control <span class="math inline">\(u_k\)</span> brings the system into a new state <span class="math inline">\(x_{k+1} \sim P(\cdot \mid x_k, u_k)\)</span>, at which the controller decides a new control <span class="math inline">\(u_{k+1}\)</span>. This process continues forever.</p>
<p><strong>Controller (policy)</strong>. In general, a time-varying controller <span class="math inline">\(\pi = (\pi_0,\dots,\pi_k,\dots)\)</span> is a mapping from all previous states and controls to a distribution over current controls. The mapping <span class="math inline">\(\pi_k\)</span> at timestep <span class="math inline">\(k\)</span> is
<span class="math display">\[
\pi_k: (x_0,u_0,x_1,u_1,\dots,x_k) \mapsto u_k \sim q_k \in \Delta(\mathbb{U}).
\]</span>
Note that <span class="math inline">\(u_k\)</span> can be randomized and it is drawn from a distribution <span class="math inline">\(q_k\)</span> supported on the set of controls <span class="math inline">\(\mathbb{U}\)</span>.
A <em>stationary</em> controller (policy) <span class="math inline">\(\pi: \mathbb{X} \rightarrow \Delta(\mathbb{U})\)</span> specifies a decison-making strategy that is purely based on the current state <span class="math inline">\(x_k\)</span>. A <em>deterministic</em> and stationary controller <span class="math inline">\(\pi: \mathbb{X} \rightarrow \mathbb{U}\)</span> excutes a deterministic control <span class="math inline">\(u_k\)</span> at each step.</p>
<p><strong>Cost-to-go and <span class="math inline">\(Q\)</span>-value</strong>. Given a controller <span class="math inline">\(\pi\)</span> and an initial state <span class="math inline">\(x_0\)</span>, we associate with it the following discounted infinite-horizon cost
<span class="math display" id="eq:mdp-cost-policy">\[\begin{equation}
J_\pi(x_0) = \mathbb{E} \left\{ \sum_{k=0}^{\infty} \gamma^k g(x_k, u_k^{\pi}) \right\},
\tag{2.16}
\end{equation}\]</span>
where the expectation is taken over the randomness of the transition <span class="math inline">\(P\)</span> and the controller <span class="math inline">\(\pi\)</span>. Note that we have used <span class="math inline">\(u^{\pi}_k\)</span> to denote the control at step <span class="math inline">\(k\)</span> by following the controller <span class="math inline">\(\pi\)</span>. Similarly, we define the <span class="math inline">\(Q\)</span>-value function as
<span class="math display" id="eq:mdp-Q-value">\[\begin{equation}
Q_\pi(x_0,u_0) = \mathbb{E} \left\{  g(x_0,u_0) + \sum_{k=1}^{\infty} \gamma^k g(x_k,u_k^{\pi}) \right\}.
\tag{2.17}
\end{equation}\]</span>
The difference between <span class="math inline">\(Q_\pi(x_0,u_0)\)</span> and <span class="math inline">\(J_\pi(x_0)\)</span> is that at step zero, <span class="math inline">\(J_\pi(x_0)\)</span> follows the controller <span class="math inline">\(\pi\)</span> while <span class="math inline">\(Q_\pi(x_0,u_0)\)</span> assumes the control <span class="math inline">\(u_0\)</span> is given. By the assumption that <span class="math inline">\(g(x_k,u_k) \in [0,1]\)</span>, we have
<span class="math display">\[
0 \leq J_\pi(x_0), Q_\pi(x_0,u_0) \leq \sum_{k=0}^{\infty} \gamma^k = \frac{1}{1-\gamma}, \quad \forall \pi.
\]</span></p>
<p>Our goal is to find the best controller that minimizes the cost function
<span class="math display" id="eq:mdp-objective">\[\begin{equation}
\pi^\star \in \arg\min_{\pi \in \Pi} J_\pi(x_0)
\tag{2.18}
\end{equation}\]</span>
for a given initial state <span class="math inline">\(x_0\)</span>, where <span class="math inline">\(\Pi\)</span> is the space of all non-stationary and randomized controllers.</p>
<p>A remarkable property of MDPs is that there exists an optimal controller that is stationary and deterministic.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:optimalmdppolicystationary" class="theorem"><strong>Theorem 2.1  (Deterministic and Stationary Optimal Policy) </strong></span>Let <span class="math inline">\(\Pi\)</span> be the space of all non-stationary and randomized policies. Define
<span class="math display">\[
J^\star_\pi(x) = \min_{\pi \in \Pi} J_\pi(x), \quad Q^\star_\pi(x,u) = \min_{\pi \in \Pi } Q_\pi(x,u).
\]</span>
There exists a deterministic and stationary policy <span class="math inline">\(\pi^\star\)</span> such that for all <span class="math inline">\(x \in \mathbb{X}\)</span> and <span class="math inline">\(u \in \mathbb{U}\)</span>,
<span class="math display">\[
J_{\pi^\star}(x) = J^\star(x), \quad Q_{\pi^\star}(x,u) = Q^\star(x,u).
\]</span>
We call such a policy <span class="math inline">\(\pi\)</span> an optimal policy.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>See Theorem 1.7 in <span class="citation">(<a href="#ref-agarwal22book-reinforcement">Agarwal et al. 2022</a>)</span>.</p>
</div>
</div>
<p>This Theorem shows that we can restrict ourselves to stationary and deterministic policies without losing performance.</p>
<p>In the next, we show how to characterize the optimal policy and value function.</p>
<div id="bellman-optimality-equations" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Bellman Optimality Equations<a href="exactdp.html#bellman-optimality-equations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now restrict ourselves to stationary policies. We first introduce the Bellman Consistency Equations for stationary policies.</p>
<div class="theorembox">
<div class="lemma">
<p><span id="lem:mdpbellmanconsistency" class="lemma"><strong>Lemma 2.1  (Bellman Consistency Equations) </strong></span>Let <span class="math inline">\(\pi\)</span> be a stationary policy. Then <span class="math inline">\(J_\pi\)</span> and <span class="math inline">\(Q_\pi\)</span> satisfy the following Bellman consistency equations
<span class="math display" id="eq:bellman-consistency-2" id="eq:bellman-consistency-1">\[\begin{align}
J_\pi(x) &amp;= \mathbb{E}_{u \sim \pi(\cdot \mid x)} Q_\pi(x,u), \tag{2.19} \\
Q_\pi(x,u) &amp;=g(x,u) + \gamma \mathbb{E}_{x&#39; \sim P(\cdot \mid x,u)} J_\pi(x&#39;). \tag{2.20}
\end{align}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>By the definition of the cost-to-go function in <a href="exactdp.html#eq:mdp-cost-policy">(2.16)</a>, we have
<span class="math display">\[
J_\pi(x_0) = \mathbb{E}\left\{ \sum_{k=0}^{\infty} \gamma^k g(x_k, \pi(x_k))  \right\} = \mathbb{E}_{u_0 \sim \pi(\cdot \mid x_0)} \underbrace{\mathbb{E} \left\{ g(x_0,u_0) + \sum_{k=1}^{\infty} \gamma^k g(x_k,\pi(x_k)) \right\}}_{Q_\pi(x_0,u_0)}.
\]</span>
<!-- By the definition of the $Q$-value function \@ref(eq:mdp-Q-value), we have
$$
Q_\pi(x_0,\pi(x_0)) = \mathbb{E} \left\{ g(x_0,\pi(x_0)) + \sum_{k=1}^{\infty} \gamma^k g(x_k,\pi(x_k)) \right\} = \mathbb{E} \left\{ \sum_{k=0}^{\infty} \gamma^k g(x_k, \pi(x_k)) \right\} = J_\pi(x_0).
$$ -->
The above equation holds for any <span class="math inline">\(x_0\)</span>, proving <a href="exactdp.html#eq:bellman-consistency-1">(2.19)</a>.</p>
<p>To show <a href="exactdp.html#eq:bellman-consistency-2">(2.20)</a>, we recall the definition of the <span class="math inline">\(Q\)</span>-value function <a href="exactdp.html#eq:mdp-Q-value">(2.17)</a>
<span class="math display" id="eq:bellman-constency-proof-1">\[\begin{align}
Q_\pi(x_0,u_0) &amp; = \mathbb{E} \left\{ g(x_0,u_0) + \sum_{k=1}^{\infty} \gamma^k g(x_k,\pi(x_k)) \right\}  \\
&amp;=  g(x_0,u_0) + \gamma \mathbb{E} \left\{ \sum_{k=1}^{\infty} \gamma^{k-1} g(x_k,\pi(x_k)) \right\} \tag{2.21}
\end{align}\]</span>
Now observe that the expectation of the second term in <a href="exactdp.html#eq:bellman-constency-proof-1">(2.21)</a> is taken over both the randomness of <span class="math inline">\(x_1\)</span> and the randomness of the policy after <span class="math inline">\(x_1\)</span> is reached. Therefore,
<span class="math display">\[
\mathbb{E} \left\{ \sum_{k=1}^{\infty} \gamma^{k-1} g(x_k,\pi(x_k)) \right\} = \mathbb{E}_{x_1 \sim P(\cdot \mid x_0,u_0)} \underbrace{\left\{ \mathbb{E} \left\{ \sum_{k=1}^{\infty} \gamma^{k-1} g(x_1,\pi(x_1)) \right\} \right\}}_{J_\pi(x_1)}.
\]</span>
Plugging the above equation back to <a href="exactdp.html#eq:bellman-constency-proof-1">(2.21)</a>, we obtain the desired result in <a href="exactdp.html#eq:bellman-consistency-2">(2.20)</a>.</p>
</div>
</div>
<p><strong>Matrix Representation</strong>. It is useful to think of <span class="math inline">\(P,g,J_\pi,Q_\pi\)</span> as matrices. In particular, the transition function <span class="math inline">\(P\)</span> can be considered as a matrix of dimension <span class="math inline">\(|\mathbb{X}||\mathbb{U}| \times \mathbb{X}\)</span>, where
<span class="math display">\[
P_{(x,u),x&#39;} = P(x&#39; \mid x,u)
\]</span>
is the entry of <span class="math inline">\(P\)</span> at the row <span class="math inline">\((x,u)\)</span> (there are <span class="math inline">\(|\mathbb{X}||\mathbb{U}|\)</span> such rows) and column <span class="math inline">\(x&#39;\)</span> (there are <span class="math inline">\(|\mathbb{X}|\)</span> such columns). The running cost <span class="math inline">\(g\)</span> is vector of <span class="math inline">\(|\mathbb{X}||\mathbb{U}|\)</span> entries. The cost-to-go <span class="math inline">\(J_\pi(x)\)</span> is a vector of <span class="math inline">\(|\mathbb{X}|\)</span> entries. The <span class="math inline">\(Q\)</span>-value function <span class="math inline">\(Q_\pi(x,u)\)</span> is a vector of <span class="math inline">\(|\mathbb{X}||\mathbb{U}|\)</span> entries. We also introduce <span class="math inline">\(P^{\pi}\)</span> with dimension <span class="math inline">\(|\mathbb{X}||\mathbb{U}| \times |\mathbb{X}||\mathbb{U}|\)</span> as the transition matrix induced by a stationary policy <span class="math inline">\(\pi\)</span>. In particular,
<span class="math display">\[
P^\pi_{(x,u),(x&#39;,u&#39;)} = P(x&#39; \mid x,u) \pi(u&#39;\mid x&#39;).
\]</span>
In words, <span class="math inline">\(P^\pi_{(x,u),(x&#39;,u&#39;)}\)</span> is the probability that <span class="math inline">\((x&#39;,u&#39;)\)</span> follows <span class="math inline">\((x,u)\)</span>.</p>
<p>With the matrix representation, we can compactly write the Bellman consistency equation <a href="exactdp.html#eq:bellman-consistency-2">(2.20)</a> as
<span class="math display" id="eq:bellman-consistency-2-matrix">\[\begin{equation}
Q_\pi = g + \gamma P J_\pi.
\tag{2.22}
\end{equation}\]</span>
We can also combine <a href="exactdp.html#eq:bellman-consistency-1">(2.19)</a> and <a href="exactdp.html#eq:bellman-consistency-2">(2.20)</a> together and write
<span class="math display">\[
Q_\pi(x,u) = g(x,u) + \gamma \mathbb{E}_{x&#39; \sim P(\cdot \mid x,u)} \left\{ \mathbb{E}_{u&#39; \sim \pi(\cdot \mid x&#39;)} Q_\pi(x&#39;,u&#39;) \right\} = g(x,u) + \gamma \mathbb{E}_{(x&#39;,u&#39;) \sim P^\pi(\cdot \mid (x,u))} Q_\pi(x&#39;,u&#39;),
\]</span>
which, using matrix representation, becomes
<span class="math display" id="eq:bellman-consistency-2-matrix-2">\[\begin{equation}
Q_\pi = g + \gamma P^\pi Q_\pi.
\tag{2.23}
\end{equation}\]</span>
Equation <a href="exactdp.html#eq:bellman-consistency-2-matrix-2">(2.23)</a> immediately yields
<span class="math display" id="eq:bellman-consistency-Qpi-linearsystem">\[\begin{equation}
Q_\pi = (I - \gamma P^\pi)^{-1} g,
\tag{2.24}
\end{equation}\]</span>
that is, the <span class="math inline">\(Q\)</span>-value function associated with a stationary policy <span class="math inline">\(\pi\)</span> can be simply computed from solving a linear system as in <a href="exactdp.html#eq:bellman-consistency-Qpi-linearsystem">(2.24)</a>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>Lemma <a href="exactdp.html#lem:mdpbellmanconsistency">2.1</a>, together with the equivalent matrix equations <a href="exactdp.html#eq:bellman-consistency-2-matrix">(2.22)</a> and <a href="exactdp.html#eq:bellman-consistency-2-matrix-2">(2.23)</a>, provide the conditions that <span class="math inline">\(J_\pi\)</span> and <span class="math inline">\(Q_\pi\)</span>, induced by any stationary policy <span class="math inline">\(\pi\)</span>, need to satisfy. In the next, we describe the conditions that characterize the optimal policy.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:mdpbellmanoptimality" class="theorem"><strong>Theorem 2.2  (Bellman Optimality Equations) </strong></span>A vector <span class="math inline">\(Q \in \mathbb{R}^{|\mathbb{X}||\mathbb{U}|}\)</span> is said to satisfy the Bellman optimality equation if
<span class="math display" id="eq:bellman-optimality-equation">\[\begin{equation}
Q(x,u) = g(x,u) + \gamma \mathbb{E}_{x&#39; \sim P(\cdot \mid x,u)} \left\{ \min_{u&#39; \in \mathbb{U}} Q(x&#39;,u&#39;) \right\}, \quad \forall (x,u) \in \mathbb{X} \times \mathbb{U}.
\tag{2.25}
\end{equation}\]</span></p>
<p>A vector <span class="math inline">\(Q^\star\)</span> is the optimal <span class="math inline">\(Q\)</span>-value function if and only if it satisfies <a href="exactdp.html#eq:bellman-optimality-equation">(2.25)</a>. Moreover, the deterministic policy defined by
<span class="math display">\[
\pi^\star(x) \in \arg\min_{u \in \mathbb{U}} Q^\star(x,u)
\]</span>
with ties broken arbitrarily is an optimal policy.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>See Theorem 1.8 in <span class="citation">(<a href="#ref-agarwal22book-reinforcement">Agarwal et al. 2022</a>)</span>.</p>
</div>
</div>
<p>We now make a few definitions to interpret Theorem <a href="exactdp.html#thm:mdpbellmanoptimality">2.2</a>. For any vector <span class="math inline">\(Q \in \mathbb{R}^{|\mathbb{X}||\mathbb{U}|}\)</span>, define the greedy policy as
<span class="math display" id="eq:def-pi-Q">\[\begin{equation}
\pi_{Q}(x) \in \arg\min_{u \in \mathbb{U}} Q(x,u)
\tag{2.26}
\end{equation}\]</span>
with ties broken arbitrarily. With this notation, by Theorem <a href="exactdp.html#thm:mdpbellmanoptimality">2.2</a>, the optimal policy is
<span class="math display">\[
\pi^\star = \pi_{Q^\star},
\]</span>
where <span class="math inline">\(Q^\star\)</span> is the optimal <span class="math inline">\(Q\)</span>-value function. Similarly, let us define
<span class="math display">\[
J_Q(x) = \min_{u \in \mathbb{U}} Q(x,u).
\]</span>
Note that <span class="math inline">\(J_Q\)</span> has dimension <span class="math inline">\(|\mathbb{X}|\)</span>. With these notations, the <em>Bellman optimality operator</em> is defined as
<span class="math display" id="eq:bellman-optimality-operator">\[\begin{equation}
\mathcal{T}Q = g + \gamma P J_Q,
\tag{2.27}
\end{equation}\]</span>
which is nothing but a matrix representation of the right-hand side of <a href="exactdp.html#eq:bellman-optimality-equation">(2.25)</a>. This allows us to concisely write the Bellman optimality equation <a href="exactdp.html#eq:bellman-optimality-equation">(2.25)</a> as
<span class="math display" id="eq:mdp-fixed-point">\[\begin{equation}
Q = \mathcal{T}Q.
\tag{2.28}
\end{equation}\]</span>
Therefore, an equivalent way to interpret Theorem <a href="exactdp.html#thm:mdpbellmanoptimality">2.2</a> is that <span class="math inline">\(Q = Q^\star\)</span> if and only if <span class="math inline">\(Q\)</span> is a fixed point to the Bellman optimality operator <span class="math inline">\(\mathcal{T}\)</span>.</p>
</div>
<div id="value-iteration" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Value Iteration<a href="exactdp.html#value-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Intepreting the optimal <span class="math inline">\(Q\)</span>-value function as the fixed point to the Bellman optimality operator <a href="exactdp.html#eq:mdp-fixed-point">(2.28)</a> leads us to a natural algorithm for solving the optimal control problem.</p>
<p>We start with <span class="math inline">\(Q^{(0)}\)</span> being an all-zero vector and then at iteration <span class="math inline">\(t\)</span>, we perform
<span class="math display">\[
Q^{(t+1)} \leftarrow \mathcal{T} Q^{(t)},
\]</span>
with <span class="math inline">\(\mathcal{T}\)</span> defined in <a href="exactdp.html#eq:bellman-optimality-operator">(2.27)</a>. Let us observe the simplicity of this algorithm: at each iteration, one only needs to perform <span class="math inline">\(\min_{u \in \mathbb{U}} Q^{(t)}(x,u)\)</span>, which is very efficient when <span class="math inline">\(|\mathbb{U}|\)</span> is not too large.</p>
<p>The next theorem states this simple algorithm converges to the optimal value function.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:valueiteration" class="theorem"><strong>Theorem 2.3  (Value Iteration) </strong></span>Set <span class="math inline">\(Q^{(0)} = 0\)</span>. For <span class="math inline">\(t=0,\dots\)</span>, perform
<span class="math display">\[
Q^{(t+1)} \leftarrow \mathcal{T} Q^{(t)}.
\]</span>
Let <span class="math inline">\(\pi^{(k)} = \pi_{Q^{(k)}}\)</span> (see the definition in <a href="exactdp.html#eq:def-pi-Q">(2.26)</a>). For <span class="math inline">\(t \geq \frac{\log \frac{2}{(1-\gamma)^2 \epsilon}}{1-\gamma}\)</span>, we have
<span class="math display">\[
J^{\pi^{(t)}} \leq J^\star + \epsilon \mathbb{1},
\]</span>
where <span class="math inline">\(\mathbb{1}\)</span> is the all-ones vector.</p>
</div>
</div>
<p>Essentially, the value function obtained from value iteration converges to the optimal cost-to-go.</p>
<p>Let us use an example to appreciate this algorithm.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:gridworld" class="example"><strong>Example 2.2  (Shortest Path in Grid World) </strong></span>Consider the following <span class="math inline">\(10 \times 10\)</span> grid world, where the top-right cell is the goal location, and the dark blue colored cells are obstacles.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grid-world"></span>
<img src="images/grid-world-obstacle.png" alt="Grid World with Obstacles." width="50%" />
<p class="caption">
Figure 2.3: Grid World with Obstacles.
</p>
</div>
<p>We want to find the shortest path from a given cell to the target cell, while not hitting obstacles.</p>
<p>To do so, we set the state space of the system as
<span class="math display">\[
\mathbb{X} = \left\{ \begin{bmatrix} r \\ c \end{bmatrix} \middle\vert r,c \in \{ 1,\dots,10 \} \right\}
\]</span>
where <span class="math inline">\(r\)</span> is the row index (from top to bottom) and <span class="math inline">\(c\)</span> is the column index (from left to right). The control space is moving left, right, up, down, or do nothing:
<span class="math display">\[
\mathbb{U} = \left\{
    \begin{bmatrix} 1 \\ 0 \end{bmatrix},
    \begin{bmatrix} -1 \\ 0 \end{bmatrix},
    \begin{bmatrix} 0 \\ 1 \end{bmatrix},
    \begin{bmatrix} 0 \\ -1 \end{bmatrix},
    \begin{bmatrix} 0 \\ 0 \end{bmatrix}
    \right\}.
\]</span>
The system dynamics is deterministic
<span class="math display">\[
x&#39; = \begin{cases}
x + u &amp; \text{if } x + u \text{ is inside the grid} \\
x &amp; \text{otherwise}
\end{cases}.
\]</span>
We then design the following running cost function <span class="math inline">\(g\)</span>
<span class="math display">\[
g(x,u) = \begin{cases}
0 &amp; \text{if } x = [1,10]^T \text{ is the target} \\
20 &amp; \text{if } x \text{ is an obstacle} \\
1 &amp; \text{otherwise}
\end{cases}.
\]</span>
Note that <span class="math inline">\(g(x,u)\)</span> defined above does not even satisfy <span class="math inline">\(g \in [0,1]\)</span>. We then use value iteration to solve the optimal control problem with <span class="math inline">\(\gamma = 1\)</span>
<span class="math display">\[
J(x_0) = \min_{\pi} \sum_{k=0}^{\infty} g(x_k,\pi(x_k)).
\]</span></p>
<p>The <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/grid_world_value_iteration.m">Matlab script</a> of value iteration converges in <span class="math inline">\(27\)</span> iterations, and we obtain the optimal cost-to-go in Fig. <a href="exactdp.html#fig:grid-world-solution">2.4</a>.</p>
<p>Starting from the cell <span class="math inline">\([8,5]^T\)</span>, the red line in Fig. <a href="exactdp.html#fig:grid-world-solution">2.4</a> plots the optimal trajectory that clearly avoids the obstacles.</p>
<p>Feel free to play with the size of the grid and the number of obstacles.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grid-world-solution"></span>
<img src="images/grid-world-solution.png" alt="Optimal cost-to-go and an optimal trajectory." width="50%" />
<p class="caption">
Figure 2.4: Optimal cost-to-go and an optimal trajectory.
</p>
</div>
</div>
</div>
<p>Example <a href="exactdp.html#exm:gridworld">2.2</a> shows the simplicity and power of value iteration. However, the states and controls in the grid world are naturally discrete and finite. Is it possible to apply value iteration to optimal control problems where the states and controls live in continuous spaces?</p>
</div>
<div id="value-iteration-with-barycentric-interpolation" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Value Iteration with Barycentric Interpolation<a href="exactdp.html#value-iteration-with-barycentric-interpolation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us consider the discrete-time dynamics
<span class="math display">\[
x_{k+1} = f(x_k, u_k)
\]</span>
where both <span class="math inline">\(x_k\)</span> and <span class="math inline">\(u_k\)</span> live in a continuous space, say <span class="math inline">\(\mathbb{R}^{n}\)</span> and <span class="math inline">\(\mathbb{R}^m\)</span>, respectively.</p>
<p>A natural idea to apply value iteration is to discretize the state space and control space. For example, suppose <span class="math inline">\(x \in \mathbb{R}^2\)</span> and we have discretized <span class="math inline">\(\mathbb{R}^2\)</span> using <span class="math inline">\(N\)</span> points
<span class="math display">\[
\mathcal{S} = \{s_1,\dots,s_N\}
\]</span>
that lie on a 2D grid, as shown in Fig. <a href="exactdp.html#fig:barycentric-interpolation">2.5</a>. Assume <span class="math inline">\(x_k \in \mathcal{S}\)</span> lies on the mesh grid, the next state <span class="math inline">\(x_{k+1} = f(x_k,u_k)\)</span> will, however, most likely not lie exactly on one of the grid points.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:barycentric-interpolation"></span>
<img src="images/barycentric-coordinate.png" alt="Barycentric Interpolation." width="50%" />
<p class="caption">
Figure 2.5: Barycentric Interpolation.
</p>
</div>
<p>Nevertheless, <span class="math inline">\(x_{k+1}\)</span> will lie inside a triangle with vertices <span class="math inline">\(s_p, s_q, s_m\)</span>. We will now try to write <span class="math inline">\(x_{k+1}\)</span> using the vertices, that is, to find three numbers <span class="math inline">\(\lambda_p, \lambda_q, \lambda_m\)</span> such that
<span class="math display">\[
\lambda_p, \lambda_q, \lambda_m \geq 0, \quad \lambda_p + \lambda_q + \lambda_m = 1, \quad x_{k+1} = \lambda_p s_p + \lambda_q s_q + \lambda_m s_m.
\]</span>
<span class="math inline">\(\lambda_p,\lambda_q,\lambda_m\)</span> are called the barycentric coordinates of <span class="math inline">\(x_{k+1}\)</span> in the triangle formed by <span class="math inline">\(s_p,s_q,s_m\)</span>. With the barycentric coordinates, we will assign the transition matrix
<span class="math display">\[
P(x_{k+1}=s_p \mid x_k,u_k) = \lambda_p, \quad P(x_{k+1}=s_q \mid x_k,u_k) = \lambda_q, \quad P(x_{k+1}=s_m \mid x_k,u_k) = \lambda_m.
\]</span></p>
<p>Let us apply value iteration with barycentric interpolation to the simple pendulum.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:pendulumvalueiterationbarycentric" class="example"><strong>Example 2.3  (Value Iteration with Barycentric Interpolation on A Simple Pendulum) </strong></span>Consider the continuous-time pendulum dynamics in <a href="exactdp.html#eq:pendulum-dynamics-z-coordinate">(2.15)</a> that is already shifted such that <span class="math inline">\(z=0\)</span> corresponds to the upright position. With time discretization <span class="math inline">\(h\)</span>, we can write the discrete-time dynamics as
<span class="math display">\[
z_{k+1} = \dot{z}_k \cdot h + z_k = f&#39;(z_k,u_k) \cdot h + z_k.
\]</span>
We are interested in solving the optimal control problem
<span class="math display">\[
J(z_0) = \min_{u_k} \left\{ \sum_{k=0}^{\infty} \gamma^k g(x_k,u_k) \right\},
\]</span>
where the running cost is simply
<span class="math display">\[
g(x_k,u_k) = x_k^T x_k + u_k^2.
\]</span></p>
<p>We will use the parameters <span class="math inline">\(m=1,g=9.8,l=1,b=0.1\)</span>, and assume the control is bounded in <span class="math inline">\([-4.9,4.9]\)</span>.</p>
<p>We want to compute the optimal cost-to-go in the range <span class="math inline">\(z_1 \in [-\pi,\pi]\)</span> and <span class="math inline">\(z_2 \in [-\pi,\pi]\)</span>. We discretize both <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> using <span class="math inline">\(N\)</span> points, leading to <span class="math inline">\(N^2\)</span> points in the state space. We also discretize <span class="math inline">\(u\)</span> using <span class="math inline">\(N\)</span> points.</p>
<p>Applying value iteration with <span class="math inline">\(\gamma=0.9\)</span> and <span class="math inline">\(N=50\)</span>, we obtain the optimal cost-to-go in Fig. <a href="exactdp.html#fig:pendulum-VI-J-9">2.6</a>. The value iteration converges in <span class="math inline">\(277\)</span> iterations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-VI-J-9"></span>
<img src="images/pendulum-VI-J-0.9-1.png" alt="Optimal cost-to-go with discount factor 0.9." width="50%" /><img src="images/pendulum-VI-J-0.9-2.png" alt="Optimal cost-to-go with discount factor 0.9." width="50%" />
<p class="caption">
Figure 2.6: Optimal cost-to-go with discount factor 0.9.
</p>
</div>
<p>Applying value iteration with <span class="math inline">\(\gamma=0.99\)</span> and <span class="math inline">\(N=50\)</span>, we obtain the optimal cost-to-go in Fig. <a href="exactdp.html#fig:pendulum-VI-J-99">2.7</a>. The value iteration converges in <span class="math inline">\(2910\)</span> iterations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-VI-J-99"></span>
<img src="images/pendulum-VI-J-0.99-1.png" alt="Optimal cost-to-go with discount factor 0.99." width="50%" /><img src="images/pendulum-VI-J-0.99-2.png" alt="Optimal cost-to-go with discount factor 0.99." width="50%" />
<p class="caption">
Figure 2.7: Optimal cost-to-go with discount factor 0.99.
</p>
</div>
<p>Applying value iteration with <span class="math inline">\(\gamma=0.999\)</span> and <span class="math inline">\(N=50\)</span>, we obtain the optimal cost-to-go in Fig. <a href="exactdp.html#fig:pendulum-VI-J-999">2.8</a>. The value iteration converges in <span class="math inline">\(28850\)</span> iterations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-VI-J-999"></span>
<img src="images/pendulum-VI-J-0.999-1.png" alt="Optimal cost-to-go with discount factor 0.999." width="50%" /><img src="images/pendulum-VI-J-0.999-2.png" alt="Optimal cost-to-go with discount factor 0.999." width="50%" />
<p class="caption">
Figure 2.8: Optimal cost-to-go with discount factor 0.999.
</p>
</div>
<p>You can find the Matlab code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/pendulum_value_iteration_barycentric.m">here</a>.</p>
</div>
</div>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-agarwal22book-reinforcement" class="csl-entry">
Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. <span>“Reinforcement Learning: Theory and Algorithms.”</span> <em>CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep</em> 32.
</div>
<div id="ref-arnold84ieee-generalized" class="csl-entry">
Arnold, William F, and Alan J Laub. 1984. <span>“Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.”</span> <em>Proceedings of the IEEE</em> 72 (12): 1746–54.
</div>
<div id="ref-bertsekas12book-dpocI" class="csl-entry">
———. 2012. <em>Dynamic Programming and Optimal Control: Volume i</em>. Vol. 1. Athena scientific.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>One can show that the matrix <span class="math inline">\(I - \gamma P^\pi\)</span> is indeed invertible, see Corollary 1.5 in <span class="citation">(<a href="#ref-agarwal22book-reinforcement">Agarwal et al. 2022</a>)</span>.<a href="exactdp.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="formulation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="approximatedp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlEstimation/blob/main/02-exact-dp.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-estimation.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
