% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage[pagebackref=false,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{tcolorbox}
\usepackage{color}
\usepackage{framed}
\setlength{\fboxsep}{.8em}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

% \newcommand{\calM}{\mathcal{M}}


\newtcolorbox{examplebox}{
  colback=green,
  colframe=orange,
  coltext=black,
  boxsep=5pt,
  arc=4pt}

\newtcolorbox{theorembox}{
  colback=green,
  colframe=green,
  coltext=black,
  boxsep=5pt,
  arc=4pt}

\newtcolorbox{definitionbox}{
colback=white,
colframe=green,
coltext=black,
boxsep=5pt,
arc=4pt}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Optimal Control and Reinforcement Learning},
  pdfauthor={Heng Yang},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Optimal Control and Reinforcement Learning}
\author{Heng Yang}
\date{2025-10-20}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

This is the textbook for Harvard ES/AM 158: Introduction to Optimal Control and Reinforcement Learning.

\section*{Feedback}\label{feedback}
\addcontentsline{toc}{section}{Feedback}

I would like to invite you to provide feedback to the textbook via inline comments with Hypothesis:

\begin{itemize}
\item
  Go to \href{https://hypothes.is}{Hypothesis} and create an account
\item
  Install the \href{https://chrome.google.com/webstore/detail/hypothesis-web-pdf-annota/bjfhmglciegochdpefhhlphglcehbmek}{Chrome extension of Hypothesis}
\item
  Provide public comments to textbook contents and I will try to address them
\end{itemize}

\section*{Offerings}\label{offerings}
\addcontentsline{toc}{section}{Offerings}

\subsubsection*{2025 Fall}\label{fall}
\addcontentsline{toc}{subsubsection}{2025 Fall}

\textbf{Time}: Mon/Wed 2:15 - 3:30pm

\textbf{Location}: SEC 1.413

\textbf{Instructor}: Heng Yang

\textbf{Teaching Fellow}: Haoyu Han, Han Qi

\href{https://docs.google.com/document/d/1dIRYQZZJDx8K2q1TrodDDLg-bKJWWmj7o7yzOGlIs7o/edit?usp=sharing}{{[}Syllabus{]}}, \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-PSET}{{[}Problem Sets{]}}, \href{https://canvas.harvard.edu/courses/153422}{{[}Canvas{]}}

\subsubsection*{2023 Fall}\label{fall-1}
\addcontentsline{toc}{subsubsection}{2023 Fall}

The course was previously offered as \href{https://hankyang.seas.harvard.edu/OptimalControlEstimation/}{Introduction to Optimal Control and Estimation}.

Starting Fall 2025, contents about reinforcement learning have been added to the course.

\chapter{Markov Decision Process}\label{mdp}

Optimal control (OC) and reinforcement learning (RL) address the problem of making \textbf{optimal decisions} in the presence of a \textbf{dynamic environment}.

\begin{itemize}
\tightlist
\item
  In \textbf{optimal control}, this dynamic environment is often referred to as a \emph{plant} or a \emph{dynamical system}.\\
\item
  In \textbf{reinforcement learning}, it is modeled as a \emph{Markov decision process} (MDP).
\end{itemize}

The goal in both fields is to evaluate and design decision-making strategies that optimize long-term performance:

\begin{itemize}
\tightlist
\item
  \textbf{RL} typically frames this as maximizing a long-term \emph{reward}.\\
\item
  \textbf{OC} often formulates it as minimizing a long-term \emph{cost}.
\end{itemize}

The emphasis on \textbf{long-term} evaluation is crucial. Because the environment evolves over time, decisions that appear beneficial in the short term may lead to poor long-term outcomes and thus be suboptimal.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

With this motivation, we now formalize the framework of Markov Decision Processes (MDPs), which are discrete-time stochastic dynamical systems.

\section{Finite-Horizon MDP}\label{FiniteHorizonMDP}

We begin with finite-horizon MDPs and introduce infinite-horizon MDPs in the following section. An abstract definition of the finite-horizon case will be presented first, followed by illustrative examples.

A finite-horizon MDP is given by the following tuple:
\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, T),
\]
where

\begin{itemize}
\tightlist
\item
  \(\mathcal{S}\): state space (set of all possible states)
\item
  \(\mathcal{A}\): action space (set of all possible actions)
\item
  \(P(s' \mid s, a)\): probability of transitioning to state \(s'\) from state \(s\) under action \(a\) (i.e., dynamics)
\item
  \(R(s,a)\): reward of taking action \(a\) in state \(s\)
\item
  \(T\): horizon, a positive integer
\end{itemize}

For now, let us assume both the state space and the action space are discrete and have a finite number of elements. In particular, denote the number of elements in \(\mathcal{S}\) as \(|\mathcal{S}|\), and the number of elements in \(\mathcal{A}\) as \(|\mathcal{A}|\). This is also referred to as a \emph{tabular MDP}.

\textbf{Policy}. Decision-making in MDPs is represented by policies. A policy is a function that, given any state, outputs a distribution of actions: \(\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})\). That is, \(\pi(a \mid s)\) returns the probability of taking action \(a\) in state \(s\). In finite-horizon MDPs, we consider a tuple of policies:
\begin{equation}
\pi = (\pi_0, \dots, \pi_t, \dots, \pi_{T-1}),
\label{eq:policy-tuple}
\end{equation}
where each \(\pi_t\) denotes the policy at step \(t \in [0,T-1]\).

\textbf{Trajectory and Return}. Given an initial state \(s_0 \in \mathcal{S}\) and a policy \(\pi\), the MDP will evolve as

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start at state \(s_0\)
\item
  Take action \(a_0 \sim \pi_0(a \mid s_0)\) following policy \(\pi_0\)
\item
  Collect reward \(r_0 = R(s_0, a_0)\) (assume \(R\) is deterministic)
\item
  Transition to state \(s_1 \sim P(s' \mid s_0, a_0)\) following the dynamics
\item
  Go to step 2 and continue until reaching state \(s_T\)
\end{enumerate}

This evolution generates a trajectory of states, actions, and rewards:
\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T).
\]
The cumulative reward of this trajectory is \(g_0 = \sum_{t=0}^{T-1} r_t\), which is called the \emph{return} of the trajectory. Clearly, \(g_0\) is a random variable due to the stochasticity of both the policy and the dynamics. Similarly, if the state at time \(t\) is \(s_t\), we denote:
\[
g_t = r_t + \dots + r_{T-1}
\]
as the return of the policy starting at \(s_t\).

\subsection{Value Functions}\label{FiniteHorizonMDP-Value}

\textbf{State-Value Function}. Given a policy \(\pi\) as in \eqref{eq:policy-tuple}, which states are preferable at time \(t\)? The (time-indexed) state-value function assigns to each \(s\in\mathcal{S}\) the expected return from \(t\) onward when starting in \(s\) and following \(\pi\) thereafter. Formally, define
\begin{equation}
V_t^\pi(s) := \mathbb{E} \left[g_t \mid s_t=s\right]
= \mathbb{E} \left[\sum_{i=t}^{T-1} R(s_i,a_i) \middle| s_t=s,a_i\sim \pi_i(\cdot\mid s_i), s_{i+1}\sim P(\cdot\mid s_i,a_i)\right].
\label{eq:FiniteHorizonMDP-state-value}
\end{equation}
The expectation is over the randomness induced by both the policy and the dynamics. Thus, if \(V_t^\pi(s_1)>V_t^\pi(s_2)\), then at time \(t\) under policy \(\pi\) it is better in expectation to be in \(s_1\) than in \(s_2\) because the former yields a larger expected return.

\(V^{\pi}_t(s)\): given policy \(\pi\), how good is it to start in state \(s\) at time \(t\)?

\textbf{Action-Value Function}. Similarly, the action-value function assigns to each state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) the expected return obtained by starting in state \(s\), taking action \(a\) first, and then following policy \(\pi\) thereafter:
\begin{equation}
\begin{split}
Q_t^\pi(s,a) := & \mathbb{E} \left[R(s,a) + g_{t+1} \mid s_{t+1} \sim P(\cdot \mid s,a)\right] \\
= & \mathbb{E} \left[R(s,a) + \sum_{i=t+1}^{T-1} R(s_i, a_i) \middle| s_{t+1} \sim P(\cdot \mid s,a) \right].
\end{split}
\label{eq:FiniteHorizonMDP-action-value}
\end{equation}
The key distinction is that the action-value function evaluates the return when the first action may deviate from policy \(\pi\), whereas the state-value function assumes strict adherence to \(\pi\). This flexibility makes the action-value function central to improving \(\pi\), since it reveals whether alternative actions can yield higher returns.

\(Q^{\pi}_t(s,a)\): At time \(t\), how good is it to take action \(a\) in state \(s\), then follow the policy \(\pi\)?

It is easy to verify that the state-value function and the action-value function satisfy:
\begin{align}
V_t^{\pi}(s) & = \sum_{a \in \mathcal{A}} \pi_t(a \mid s) Q_t^{\pi}(s,a), \label{eq:FiniteHorizonMDP-state-value-from-action-value} \\
Q_t^{\pi}(s,a) & = R(s,a) + \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^{\pi}_{t+1} (s'). \label{eq:FiniteHorizonMDP-action-value-from-state-value}
\end{align}

From these two equations, we can derive the Bellman Consistency equations.

\begin{proposition}[Bellman Consistency (Finite Horizon)]
\protect\hypertarget{prp:BellmanConsistency}{}\label{prp:BellmanConsistency}The state-value function \(V^{\pi}_t(\cdot)\) in \eqref{eq:FiniteHorizonMDP-state-value} satisfies the following recursion:
\begin{equation}
\begin{split}
V^{\pi}_t(s) & = \sum_{a \in \mathcal{A}} \pi_t(a\mid s) \left( R(s,a) + \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^{\pi}_{t+1} (s') \right) \\
    & =: \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s' \sim P(\cdot \mid s, a)} [V^{\pi}_{t+1}(s')] \right].
\end{split}
\label{eq:BellmanConsistency-State-Value}
\end{equation}

Similarly, the action-value function \(Q^{\pi}_t(s,a)\) in \eqref{eq:FiniteHorizonMDP-action-value} satisfies the following recursion:
\begin{equation}
\begin{split}
Q^{\pi}_t (s, a) & = R(s,a) + \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left( \sum_{a' \in \mathcal{A}} \pi_{t+1}(a' \mid s') Q^{\pi}_{t+1}(s', a')\right) \\
& =: R(s, a) + \mathbb{E}_{s' \sim P(\cdot \mid s, a)} \left[\mathbb{E}_{a' \sim \pi_{t+1}(\cdot \mid s')} [Q^{\pi}_{t+1}(s', a')] \right].
\end{split}
\label{eq:BellmanConsistency-Action-Value}
\end{equation}
\end{proposition}

\subsection{Policy Evaluation}\label{policy-evaluation}

The Bellman consistency result in Proposition \ref{prp:BellmanConsistency} is fundamental because it directly yields an algorithm for evaluating a given policy \(\pi\)---that is, for computing its state-value and action-value functions---provided the transition dynamics of the MDP are known.

Policy evaluation for the state-value function proceeds as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Initialization:} set \(V^{\pi}_T(s) = 0\) for all \(s \in \mathcal{S}\).\\
\item
  \textbf{Backward recursion:} for \(t = T-1, T-2, \dots, 0\), update each \(s \in \mathcal{S}\) by
  \[
  V^{\pi}_{t}(s) = \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s' \sim P(\cdot \mid s, a)} \big[ V^{\pi}_{t+1}(s') \big] \right].
  \]
\end{itemize}

Similarly, policy evaluation for the action-value function is given by:

\begin{itemize}
\tightlist
\item
  \textbf{Initialization:} set \(Q^{\pi}_T(s,a) = 0\) for all \(s \in \mathcal{S}, a \in \mathcal{A}\).\\
\item
  \textbf{Backward recursion:} for \(t = T-1, T-2, \dots, 0\), update each \((s,a) \in \mathcal{S}\times\mathcal{A}\) by
  \[
  Q^{\pi}_t(s,a) = R(s, a) + \mathbb{E}_{s' \sim P(\cdot \mid s, a)} \left[ \mathbb{E}_{a' \sim \pi_{t+1}(\cdot \mid s')} \big[ Q^{\pi}_{t+1}(s', a') \big] \right].
  \]
\end{itemize}

The essential feature of this algorithm is its backward-in-time recursion: the value functions are first set at the terminal horizon \(T\), and then propagated backward step by step through the Bellman consistency equations.

\begin{example}[MDP, Transition Graph, and Policy Evaluation]
\protect\hypertarget{exm:MDPExampleGraph}{}\label{exm:MDPExampleGraph}It is often useful to visualize small MDPs as transition graphs, where states are represented by nodes and actions are represented by directed edges connecting those nodes.

As a simple illustrative example, consider a robot navigating on a two-state grid. At each step, the robot can either Stay in its current state or Move to the other state. This finite-horizon MDP is fully specified by the tuple of states, actions, transition dynamics, rewards, and horizon:

\begin{itemize}
\item
  States: \(\mathcal{S} = \{\alpha, \beta \}\)
\item
  Actions: \(\mathcal{A} = \{\text{Move} , \text{Stay} \}\)
\item
  Transition dynamics: we can specify the transition dynamics in the following table
\end{itemize}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2000}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2286}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3429}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2286}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
State \(s\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Action \(a\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Next State \(s'\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Probability \(P(s' \mid s, a)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\alpha\) & Stay & \(\alpha\) & 1 \\
\(\alpha\) & Move & \(\beta\) & 1 \\
\(\beta\) & Stay & \(\beta\) & 1 \\
\(\beta\) & Move & \(\alpha\) & 1 \\
\end{longtable}

\begin{itemize}
\item
  Reward: \(R(s,a)=1\) if \(a = \text{Move}\) and \(R(s,a)=0\) if \(a = \text{Stay}\)
\item
  Horizon: \(T=2\).
\end{itemize}

This MDP can be represented by the transition graph in Fig. \ref{fig:mdp-robot-transition-graph}. Note that for this MDP, the transition dynamics is deterministic. We will see a stochastic MDP soon.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/MDP/robot-mdp-graph} 

}

\caption{A Simple Transition Graph.}\label{fig:mdp-robot-transition-graph}
\end{figure}

At time \(t=0\), if the robot starts at \(s_0 = \alpha\), first chooses action \(a_0 = \text{Move}\), and then chooses action \(a_1 = \text{Stay}\), the resulting trajectory is
\[
\tau = (\alpha, \text{Move}, +1, \beta, \text{Stay}, 0,  \beta).
\]
The return of this trajectory is:
\[
g_0 = +1 + 0 = +1.
\]

\textbf{Policy Evaluation}. Given a policy
\begin{equation}
\pi = (\pi_0, \pi_1), \quad \pi_0(a \mid s) = \begin{cases}
0.5 & a = \text{Move} \\
0.5 & a = \text{Stay}
\end{cases}, 
\quad 
\pi_1( a \mid s) = \begin{cases}
0.8 & a = \text{Move} \\
0.2 & a = \text{Stay}
\end{cases}.
\end{equation}
We can use the Bellman consistency equations to compute the state-value function. We first initialize:
\[
V^{\pi}_2 = \begin{bmatrix}
0 \\ 0
\end{bmatrix},
\]
where the first row contains the value at \(s = \alpha\) and the second row contains the value at \(s = \beta\).
We then perform the backward recursion for \(t=1\). For \(s = \alpha\), we have
\begin{equation}
V^{\pi}_1(\alpha) = \begin{bmatrix}
\pi_1(\text{Move} \mid \alpha) \\
\pi_1(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_2(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_2(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8 
\end{equation}
For \(s = \beta\), we have
\begin{equation}
V^{\pi}_1(\beta) = \begin{bmatrix}
\pi_1(\text{Move} \mid \beta) \\
\pi_1(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_2(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_2(\beta)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8.
\end{equation}
Therefore, we have
\[
V^{\pi}_1 = \begin{bmatrix}
0.8 \\ 0.8
\end{bmatrix}.
\]
We then proceed to the backward recursion for \(t=0\):
\begin{align}
V_0^{\pi}(\alpha) & = \begin{bmatrix}
\pi_0(\text{Move} \mid \alpha) \\
\pi_0(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_1(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_1(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3. \\
V_0^{\pi}(\beta) & = \begin{bmatrix}
\pi_0(\text{Move} \mid \beta) \\
\pi_0(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_0(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_0(\beta)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3.
\end{align}
Therefore, the state-value function at \(t=0\) is
\[
V^{\pi}_0 = \begin{bmatrix}
1.3 \\ 1.3
\end{bmatrix}.
\]
You are encouraged to carry out the similar calculations for the action-value function.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The toy example was small enough to carry out policy evaluation by hand; in realistic MDPs, we will need the help from computers.

Consider now an MDP whose transition graph is shown in Fig. \ref{fig:mdp-hangover-transition-graph}. This example is adapted from \href{https://github.com/upb-lea/reinforcement_learning_course_materials/blob/master/exercises/solutions/ex02/Ex2.ipynb}{here}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/MDP/hangover-graph} 

}

\caption{Hangover Transition Graph.}\label{fig:mdp-hangover-transition-graph}
\end{figure}

This MDP has six states:
\[
\mathcal{S} = \{\text{Hangover}, \text{Sleep}, \text{More Sleep}, \text{Visit Lecture}, \text{Study}, \text{Pass Exam} \},
\]
and two actions:
\[
\mathcal{A} = \{\text{Lazy}, \text{Productive} \}.
\]
The stochastic transition dynamics are labeled in the transition graph. For example, at state ``Hangover'', taking action ``Productive'' will lead to state ``Visit Lecture'' with probability \(0.3\) and state ``Hangover'' with probability \(0.7\).
The rewards of the MDP are defined as:
\[
R(s,a) = \begin{cases}
+1 & s = \text{Pass Exam} \\
-1 & \text{otherwise}.
\end{cases}.
\]

\textbf{Policy Evaluation}. Consider a time-invariant random policy
\[
\pi = \{\pi_0,\dots,\pi_{T-1} \}, \quad \pi_t(a \mid s) = \begin{cases}
\alpha & a = \text{Lazy} \\
1 - \alpha & a = \text{Productive}
\end{cases},
\]
that takes ``Lazy'' with probability \(\alpha\) and ``Productive'' with probability \(1-\alpha\).

The following Python code performs policy evaluation for this MDP, with \(T=10\) and \(\alpha = 0.4\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Finite{-}horizon policy evaluation for the Hangover MDP}

\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ defaultdict}
\ImportTok{from}\NormalTok{ typing }\ImportTok{import}\NormalTok{ Dict, List, Tuple}

\NormalTok{State }\OperatorTok{=} \BuiltInTok{str}
\NormalTok{Action }\OperatorTok{=} \BuiltInTok{str}

\CommentTok{\# {-}{-}{-} MDP spec {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{S: List[State] }\OperatorTok{=}\NormalTok{ [}
    \StringTok{"Hangover"}\NormalTok{, }\StringTok{"Sleep"}\NormalTok{, }\StringTok{"More Sleep"}\NormalTok{, }\StringTok{"Visit Lecture"}\NormalTok{, }\StringTok{"Study"}\NormalTok{, }\StringTok{"Pass Exam"}
\NormalTok{]}
\NormalTok{A: List[Action] }\OperatorTok{=}\NormalTok{ [}\StringTok{"Lazy"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{]}

\CommentTok{\# P[s, a] {-}\textgreater{} list of (s\_next, prob)}
\NormalTok{P: Dict[Tuple[State, Action], List[Tuple[State, }\BuiltInTok{float}\NormalTok{]]] }\OperatorTok{=}\NormalTok{ \{}
    \CommentTok{\# Hangover}
\NormalTok{    (}\StringTok{"Hangover"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):       [(}\StringTok{"Sleep"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Hangover"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{): [(}\StringTok{"Visit Lecture"}\NormalTok{, }\FloatTok{0.3}\NormalTok{), (}\StringTok{"Hangover"}\NormalTok{, }\FloatTok{0.7}\NormalTok{)],}

    \CommentTok{\# Sleep}
\NormalTok{    (}\StringTok{"Sleep"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):          [(}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Sleep"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{):    [(}\StringTok{"Visit Lecture"}\NormalTok{, }\FloatTok{0.6}\NormalTok{), (}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{0.4}\NormalTok{)],}

    \CommentTok{\# More Sleep}
\NormalTok{    (}\StringTok{"More Sleep"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):       [(}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"More Sleep"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{): [(}\StringTok{"Study"}\NormalTok{, }\FloatTok{0.5}\NormalTok{), (}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{0.5}\NormalTok{)],}

    \CommentTok{\# Visit Lecture}
\NormalTok{    (}\StringTok{"Visit Lecture"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):       [(}\StringTok{"Study"}\NormalTok{, }\FloatTok{0.8}\NormalTok{), (}\StringTok{"Pass Exam"}\NormalTok{, }\FloatTok{0.2}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Visit Lecture"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{): [(}\StringTok{"Study"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}

    \CommentTok{\# Study}
\NormalTok{    (}\StringTok{"Study"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):         [(}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Study"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{):   [(}\StringTok{"Pass Exam"}\NormalTok{, }\FloatTok{0.9}\NormalTok{), (}\StringTok{"Study"}\NormalTok{, }\FloatTok{0.1}\NormalTok{)],}

    \CommentTok{\# Pass Exam (absorbing)}
\NormalTok{    (}\StringTok{"Pass Exam"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):       [(}\StringTok{"Pass Exam"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Pass Exam"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{): [(}\StringTok{"Pass Exam"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ R(s: State, a: Action) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{float}\NormalTok{:}
    \CommentTok{"""Reward: +1 in Pass Exam, {-}1 otherwise."""}
    \ControlFlowTok{return} \FloatTok{1.0} \ControlFlowTok{if}\NormalTok{ s }\OperatorTok{==} \StringTok{"Pass Exam"} \ControlFlowTok{else} \OperatorTok{{-}}\FloatTok{1.0}

\CommentTok{\# {-}{-}{-} Policy: time{-}invariant, state{-}independent {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\KeywordTok{def}\NormalTok{ pi(a: Action, s: State, alpha: }\BuiltInTok{float}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{float}\NormalTok{:}
    \CommentTok{"""pi(a|s): Lazy with prob alpha, Productive with prob 1{-}alpha."""}
    \ControlFlowTok{return}\NormalTok{ alpha }\ControlFlowTok{if}\NormalTok{ a }\OperatorTok{==} \StringTok{"Lazy"} \ControlFlowTok{else}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ alpha)}

\CommentTok{\# {-}{-}{-} Policy evaluation {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\KeywordTok{def}\NormalTok{ policy\_evaluation(T: }\BuiltInTok{int}\NormalTok{, alpha: }\BuiltInTok{float}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Compute \{V\_t(s)\} and \{Q\_t(s,a)\} for t=0..T with terminal condition V\_T = Q\_T = 0.}
\CommentTok{    Returns:}
\CommentTok{        V: Dict[int, Dict[State, float]]}
\CommentTok{        Q: Dict[int, Dict[Tuple[State, Action], float]]}
\CommentTok{    """}
    \ControlFlowTok{assert}\NormalTok{ T }\OperatorTok{\textgreater{}=} \DecValTok{0}
    \CommentTok{\# sanity: probabilities sum to 1 for each (s,a)}
    \ControlFlowTok{for}\NormalTok{ key, rows }\KeywordTok{in}\NormalTok{ P.items():}
\NormalTok{        total }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(p }\ControlFlowTok{for}\NormalTok{ \_, p }\KeywordTok{in}\NormalTok{ rows)}
        \ControlFlowTok{if} \BuiltInTok{abs}\NormalTok{(total }\OperatorTok{{-}} \FloatTok{1.0}\NormalTok{) }\OperatorTok{\textgreater{}} \FloatTok{1e{-}9}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\SpecialStringTok{f"Probabilities for }\SpecialCharTok{\{}\NormalTok{key}\SpecialCharTok{\}}\SpecialStringTok{ sum to }\SpecialCharTok{\{}\NormalTok{total}\SpecialCharTok{\}}\SpecialStringTok{, not 1."}\NormalTok{)}

\NormalTok{    V: Dict[}\BuiltInTok{int}\NormalTok{, Dict[State, }\BuiltInTok{float}\NormalTok{]] }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{dict}\NormalTok{)}
\NormalTok{    Q: Dict[}\BuiltInTok{int}\NormalTok{, Dict[Tuple[State, Action], }\BuiltInTok{float}\NormalTok{]] }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{dict}\NormalTok{)}

    \CommentTok{\# Terminal boundary}
    \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ S:}
\NormalTok{        V[T][s] }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ A:}
\NormalTok{            Q[T][(s, a)] }\OperatorTok{=} \FloatTok{0.0}

    \CommentTok{\# Backward recursion}
    \ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(T }\OperatorTok{{-}} \DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ S:}
            \CommentTok{\# First compute Q\_t(s,a)}
            \ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ A:}
\NormalTok{                exp\_next }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(p }\OperatorTok{*}\NormalTok{ V[t }\OperatorTok{+} \DecValTok{1}\NormalTok{][s\_next] }\ControlFlowTok{for}\NormalTok{ s\_next, p }\KeywordTok{in}\NormalTok{ P[(s, a)])}
\NormalTok{                Q[t][(s, a)] }\OperatorTok{=}\NormalTok{ R(s, a) }\OperatorTok{+}\NormalTok{ exp\_next}
            \CommentTok{\# Then V\_t(s) = E\_\{a\textasciitilde{}pi\}[Q\_t(s,a)]}
\NormalTok{            V[t][s] }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(pi(a, s, alpha) }\OperatorTok{*}\NormalTok{ Q[t][(s, a)] }\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ A)}

    \ControlFlowTok{return}\NormalTok{ V, Q}

\CommentTok{\# {-}{-}{-} Example run {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\ControlFlowTok{if} \VariableTok{\_\_name\_\_} \OperatorTok{==} \StringTok{"\_\_main\_\_"}\NormalTok{:}
\NormalTok{    T }\OperatorTok{=} \DecValTok{10}        \CommentTok{\# horizon}
\NormalTok{    alpha }\OperatorTok{=} \FloatTok{0.4}  \CommentTok{\# probability of choosing Lazy}
\NormalTok{    V, Q }\OperatorTok{=}\NormalTok{ policy\_evaluation(T}\OperatorTok{=}\NormalTok{T, alpha}\OperatorTok{=}\NormalTok{alpha)}

    \CommentTok{\# Print V\_0}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"V\_0(s) with T=}\SpecialCharTok{\{}\NormalTok{T}\SpecialCharTok{\}}\SpecialStringTok{, alpha=}\SpecialCharTok{\{}\NormalTok{alpha}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ S:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  }\SpecialCharTok{\{}\NormalTok{s}\SpecialCharTok{:13s\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{V[}\DecValTok{0}\NormalTok{][s]}\SpecialCharTok{: .3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The code returns the following state values at \(t=0\):
\begin{equation}
V^{\pi}_0 = \begin{bmatrix}
-3.582 \\ -2.306 \\ -2.180 \\ 1.757 \\ 2.939 \\ 10
\end{bmatrix},
\label{eq:HangoverRandomValueFunction}
\end{equation}
where the ordering of the states follows that defined in \(\mathcal{S}\).

You can find the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/hangover_policy_evaluation.py}{here}.
\end{example}

\subsection{Principle of Optimality}\label{optimality}

Every policy \(\pi\) induces a value function \(V_0^{\pi}\) that can be evaluated by policy evaluation (assuming the transition dynamics are known). The goal of reinforcement learning is to find an optimal policy that maximizes the value function with respect to a given initial state distribution:
\begin{equation}
V^\star_0 = \max_{\pi}\; \mathbb{E}_{s_0 \sim \mu(\cdot)} \big[ V_0^{\pi}(s_0) \big],
\label{eq:FiniteHorizonMDPRLProblem}
\end{equation}
where we have used the superscript ``\(\star\)'' to denote the optimality of the value function. \(V^\star_0\) is often known as the \emph{optimal value function}.

At first glance, \eqref{eq:FiniteHorizonMDPRLProblem} appears daunting: a naive approach would enumerate all stochastic policies \(\pi\), evaluate their value functions, and select the best. A central result in reinforcement learning and optimal control---rooted in the principle of optimality---is that the \emph{optimal} value functions satisfy a Bellman-style recursion, analogous to Proposition \ref{prp:BellmanConsistency}. This Bellman optimality recursion enables backward computation of the optimal value functions without enumerating policies.

\begin{theorem}[Bellman Optimality (Finite Horizon, State-Value)]
\protect\hypertarget{thm:FiniteHorizonMDPBellmanOptimality}{}\label{thm:FiniteHorizonMDPBellmanOptimality}Consider a finite-horizon MDP \(\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,T)\) with finite state and action sets and bounded rewards.
Define the optimal value functions \(\{V_t^\star\}_{t=0}^{T}\) by the following \emph{Bellman optimality} recursion
\begin{equation}
\begin{split}
V_T^\star(s)& \equiv 0, \\
V_t^\star(s)& = \max_{a\in\mathcal{A}}\Big\{ R(s,a)\;+\;\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^\star(s')\Big\},\ t=T-1,\ldots,0.
\end{split}
\label{eq:FiniteHorizonMDPBellmanOptimality}
\end{equation}
Then, the optimal value functions are optimal in the sense of \emph{statewise dominance}:
\begin{equation}
V_t^{\star}(s)\;\ge\; V_t^{\pi}(s)
\quad\text{for all policies }\pi,\; s\in\mathcal{S},\; t=0,\ldots,T.
\label{eq:FiniteHorizonMDPStatewiseDominance}
\end{equation}

Moreover, the deterministic policy
\(\pi^\star=(\pi^\star_0,\ldots,\pi^\star_{T-1})\) with
\begin{equation}
\begin{split}
\pi_t^\star(s)\;\in\;\arg\max_{a\in\mathcal{A}}
\Big\{ R(s,a)\;+\;\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^\star(s')\Big\}, \\
\text{for any } s\in\mathcal{S},\; t=0,\dots,T-1
\end{split}
\label{eq:FiniteHorizonMDPOptimalPolicy}
\end{equation}
is optimal, where ties can be broken by any fixed rule.
\end{theorem}

\begin{proof}
We first show that the value functions defined by the Bellman optimality recursion \eqref{eq:FiniteHorizonMDPBellmanOptimality} are \emph{optimal} in the sense that they dominate the value functions of any other policy. The proof proceeds by backward induction.

\textbf{Base case (\(t=T\)).}
For every \(s\in\mathcal{S}\),
\[
V^\star_T(s)\;=\;0\;=\;V_T^{\pi}(s),
\]
so \(V^\star_T(s)\ge V_T^{\pi}(s)\) holds trivially.

\textbf{Inductive step.}
Assume \(V^\star_{t+1}(s)\ge V^{\pi}_{t+1}(s)\) for all \(s\in\mathcal{S}\). Then, for any \(s\in\mathcal{S}\),
\begin{align*}
V_t^{\pi}(s)
&= \sum_{a\in\mathcal{A}} \pi_t(a\mid s)\!\left(R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^{\pi}(s')\right) \\
&\le \sum_{a\in\mathcal{A}} \pi_t(a\mid s)\!\left(R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^{\star}(s')\right) \\
&\le \max_{a\in\mathcal{A}} \left(R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^{\star}(s')\right)
\;=\; V_t^\star(s),
\end{align*}
where the first inequality uses the induction hypothesis and the second uses that an expectation is bounded above by a maximum. Hence \(V_t^\star(s)\ge V_t^{\pi}(s)\) for all \(s\), completing the induction. Therefore, \(\{V_t^\star\}_{t=0}^T\) dominates the value functions attainable by any policy.

Next, we show that \(\{V_t^\star\}\) is \emph{attainable} by some policy. Since \(\mathcal{A}\) is finite (tabular setting), the maximizer in the Bellman optimality operator exists for every \((t,s)\); thus we can define a (deterministic) greedy policy
\[
\pi_t^\star(s)\;\in\;\arg\max_{a\in\mathcal{A}}
\Big\{ R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^\star(s') \Big\}.
\]
A simple backward induction then shows \(V_t^{\pi^\star}(s)=V_t^\star(s)\) for all \(t\) and \(s\): at \(t=T\) both are \(0\), and if \(V_{t+1}^{\pi^\star}=V_{t+1}^\star\), then by construction of \(\pi_t^\star\) the Bellman equality yields \(V_t^{\pi^\star}=V_t^\star\). Consequently, the optimal value functions are achieved by the greedy (deterministic) policy \(\pi^\star\).
\end{proof}

\begin{corollary}[Bellman Optimality (Finite Horizon, Action-Value)]
\protect\hypertarget{cor:FiniteHorizonMDPBellmanOptimalityActionValue}{}\label{cor:FiniteHorizonMDPBellmanOptimalityActionValue}Given the optimal (state-)value functions \(V^{\star}_{t},t=0,\dots,T\), define the optimal action-value function
\begin{equation}
Q_t^\star(s,a)\;=\;R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^\star(s'), \quad t=0,\dots,T-1.
\label{eq:FiniteHorizonMDPOptimalActionValue}
\end{equation}

Then we have
\begin{equation}
V_t^\star(s)=\max_{a\in\mathcal{A}} Q_t^\star(s,a),\qquad
\pi_t^\star(s)\in\arg\max_{a\in\mathcal{A}} Q_t^\star(s,a).
\label{eq:FiniteHorizonMDPStateValueActionValue}
\end{equation}

The optimal action-value functions satisfy:
\begin{equation}
\begin{split}
Q_T^\star(s,a) & \equiv 0,\\
Q_t^\star(s,a)
& = R(s,a) \;+\; \mathbb{E}_{s'\sim P(\cdot\mid s,a)}
\!\left[ \max_{a'\in\mathcal{A}} Q_{t+1}^\star(s',a') \right],
\quad t=T-1,\ldots,0.
\end{split}
\label{eq:FiniteHorizonMDPBellmanRecursionActionValue}
\end{equation}
\end{corollary}

\subsection{Dynamic Programming}\label{dp}

The principle of optimality in Theorem \ref{thm:FiniteHorizonMDPBellmanOptimality} yields a constructive procedure to compute the optimal value functions and an associated deterministic optimal policy. This backward-induction procedure is the \emph{dynamic programming} (DP) algorithm.

\textbf{Dynamic programming (finite horizon).}

\begin{itemize}
\item
  \textbf{Initialization.} Set \(V_T^\star(s) = 0\) for all \(s \in \mathcal{S}\).
\item
  \textbf{Backward recursion.} For \(t = T-1, T-2, \dots, 0\):

  \begin{itemize}
  \item
    \emph{Optimal value:} for each \(s \in \mathcal{S}\),
    \[
    V_t^\star(s)
    = \max_{a \in \mathcal{A}}
    \left\{
    R(s,a) + \mathbb{E}_{s' \sim P(\cdot \mid s,a)} \big[ V_{t+1}^\star(s') \big]
    \right\}.
    \]
  \item
    \emph{Greedy policy (deterministic):} for each \(s \in \mathcal{S}\),
    \[
    \pi_t^\star(s) \in \arg\max_{a \in \mathcal{A}}
    \left\{
    R(s,a) + \mathbb{E}_{s' \sim P(\cdot \mid s,a)} \big[ V_{t+1}^\star(s') \big]
    \right\}.
    \]
  \end{itemize}
\end{itemize}

\begin{exercise}
How does dynamic programming look like when applied to the action-value function?
\end{exercise}

\begin{exercise}
What is the computational complexity of dynamic programming?
\end{exercise}

Let us try dynamic programming for the Hangover MDP presented before.

\begin{example}[Dynamic Programming for Hangover MDP]
\protect\hypertarget{exm:HangoverDynamicProgramming}{}\label{exm:HangoverDynamicProgramming}Consider the Hangover MDP defined by the transition graph shown in Fig. \ref{fig:mdp-hangover-transition-graph}. With slight modification to the policy evaluation code, we can find the optimal value functions and optimal policies.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Dynamic programming (finite{-}horizon optimal control) for the Hangover MDP}

\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ defaultdict}
\ImportTok{from}\NormalTok{ typing }\ImportTok{import}\NormalTok{ Dict, List, Tuple}

\NormalTok{State }\OperatorTok{=} \BuiltInTok{str}
\NormalTok{Action }\OperatorTok{=} \BuiltInTok{str}

\CommentTok{\# {-}{-}{-} MDP spec {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{S: List[State] }\OperatorTok{=}\NormalTok{ [}
    \StringTok{"Hangover"}\NormalTok{, }\StringTok{"Sleep"}\NormalTok{, }\StringTok{"More Sleep"}\NormalTok{, }\StringTok{"Visit Lecture"}\NormalTok{, }\StringTok{"Study"}\NormalTok{, }\StringTok{"Pass Exam"}
\NormalTok{]}
\NormalTok{A: List[Action] }\OperatorTok{=}\NormalTok{ [}\StringTok{"Lazy"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{]}

\CommentTok{\# P[s, a] {-}\textgreater{} list of (s\_next, prob)}
\NormalTok{P: Dict[Tuple[State, Action], List[Tuple[State, }\BuiltInTok{float}\NormalTok{]]] }\OperatorTok{=}\NormalTok{ \{}
    \CommentTok{\# Hangover}
\NormalTok{    (}\StringTok{"Hangover"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):       [(}\StringTok{"Sleep"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Hangover"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{): [(}\StringTok{"Visit Lecture"}\NormalTok{, }\FloatTok{0.3}\NormalTok{), (}\StringTok{"Hangover"}\NormalTok{, }\FloatTok{0.7}\NormalTok{)],}

    \CommentTok{\# Sleep}
\NormalTok{    (}\StringTok{"Sleep"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):          [(}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Sleep"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{):    [(}\StringTok{"Visit Lecture"}\NormalTok{, }\FloatTok{0.6}\NormalTok{), (}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{0.4}\NormalTok{)],}

    \CommentTok{\# More Sleep}
\NormalTok{    (}\StringTok{"More Sleep"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):       [(}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"More Sleep"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{): [(}\StringTok{"Study"}\NormalTok{, }\FloatTok{0.5}\NormalTok{), (}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{0.5}\NormalTok{)],}

    \CommentTok{\# Visit Lecture}
\NormalTok{    (}\StringTok{"Visit Lecture"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):       [(}\StringTok{"Study"}\NormalTok{, }\FloatTok{0.8}\NormalTok{), (}\StringTok{"Pass Exam"}\NormalTok{, }\FloatTok{0.2}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Visit Lecture"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{): [(}\StringTok{"Study"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}

    \CommentTok{\# Study}
\NormalTok{    (}\StringTok{"Study"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):         [(}\StringTok{"More Sleep"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Study"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{):   [(}\StringTok{"Pass Exam"}\NormalTok{, }\FloatTok{0.9}\NormalTok{), (}\StringTok{"Study"}\NormalTok{, }\FloatTok{0.1}\NormalTok{)],}

    \CommentTok{\# Pass Exam (absorbing)}
\NormalTok{    (}\StringTok{"Pass Exam"}\NormalTok{, }\StringTok{"Lazy"}\NormalTok{):       [(}\StringTok{"Pass Exam"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{    (}\StringTok{"Pass Exam"}\NormalTok{, }\StringTok{"Productive"}\NormalTok{): [(}\StringTok{"Pass Exam"}\NormalTok{, }\FloatTok{1.0}\NormalTok{)],}
\NormalTok{\}}

\KeywordTok{def}\NormalTok{ R(s: State, a: Action) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{float}\NormalTok{:}
    \CommentTok{"""Reward: +1 in Pass Exam, {-}1 otherwise."""}
    \ControlFlowTok{return} \FloatTok{1.0} \ControlFlowTok{if}\NormalTok{ s }\OperatorTok{==} \StringTok{"Pass Exam"} \ControlFlowTok{else} \OperatorTok{{-}}\FloatTok{1.0}

\CommentTok{\# {-}{-}{-} Dynamic programming (Bellman optimality) {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\KeywordTok{def}\NormalTok{ dynamic\_programming(T: }\BuiltInTok{int}\NormalTok{):}
    \CommentTok{"""}
\CommentTok{    Compute optimal finite{-}horizon tables:}
\CommentTok{      {-} V[t][s] = V\_t\^{}*(s)}
\CommentTok{      {-} Q[t][(s,a)] = Q\_t\^{}*(s,a)}
\CommentTok{      {-} PI[t][s] = optimal action at (t,s)}
\CommentTok{    with terminal condition V\_T\^{}* = 0.}
\CommentTok{    """}
    \ControlFlowTok{assert}\NormalTok{ T }\OperatorTok{\textgreater{}=} \DecValTok{0}

    \CommentTok{\# sanity: probabilities sum to 1 for each (s,a)}
    \ControlFlowTok{for}\NormalTok{ key, rows }\KeywordTok{in}\NormalTok{ P.items():}
\NormalTok{        total }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(p }\ControlFlowTok{for}\NormalTok{ \_, p }\KeywordTok{in}\NormalTok{ rows)}
        \ControlFlowTok{if} \BuiltInTok{abs}\NormalTok{(total }\OperatorTok{{-}} \FloatTok{1.0}\NormalTok{) }\OperatorTok{\textgreater{}} \FloatTok{1e{-}9}\NormalTok{:}
            \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\SpecialStringTok{f"Probabilities for }\SpecialCharTok{\{}\NormalTok{key}\SpecialCharTok{\}}\SpecialStringTok{ sum to }\SpecialCharTok{\{}\NormalTok{total}\SpecialCharTok{\}}\SpecialStringTok{, not 1."}\NormalTok{)}

\NormalTok{    V: Dict[}\BuiltInTok{int}\NormalTok{, Dict[State, }\BuiltInTok{float}\NormalTok{]] }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{dict}\NormalTok{)}
\NormalTok{    Q: Dict[}\BuiltInTok{int}\NormalTok{, Dict[Tuple[State, Action], }\BuiltInTok{float}\NormalTok{]] }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{dict}\NormalTok{)}
\NormalTok{    PI: Dict[}\BuiltInTok{int}\NormalTok{, Dict[State, Action]] }\OperatorTok{=}\NormalTok{ defaultdict(}\BuiltInTok{dict}\NormalTok{)}

    \CommentTok{\# Terminal boundary}
    \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ S:}
\NormalTok{        V[T][s] }\OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ A:}
\NormalTok{            Q[T][(s, a)] }\OperatorTok{=} \FloatTok{0.0}

    \CommentTok{\# Backward recursion (Bellman optimality)}
    \ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(T }\OperatorTok{{-}} \DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ S:}
            \CommentTok{\# compute Q*\_t(s,a)}
            \ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ A:}
\NormalTok{                exp\_next }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(p }\OperatorTok{*}\NormalTok{ V[t }\OperatorTok{+} \DecValTok{1}\NormalTok{][s\_next] }\ControlFlowTok{for}\NormalTok{ s\_next, p }\KeywordTok{in}\NormalTok{ P[(s, a)])}
\NormalTok{                Q[t][(s, a)] }\OperatorTok{=}\NormalTok{ R(s, a) }\OperatorTok{+}\NormalTok{ exp\_next}

            \CommentTok{\# greedy action and optimal value}
            \CommentTok{\# tie{-}breaking is deterministic by the order in A}
\NormalTok{            best\_a }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(A, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ a: Q[t][(s, a)])}
\NormalTok{            PI[t][s] }\OperatorTok{=}\NormalTok{ best\_a}
\NormalTok{            V[t][s] }\OperatorTok{=}\NormalTok{ Q[t][(s, best\_a)]}

    \ControlFlowTok{return}\NormalTok{ V, Q, PI}

\CommentTok{\# {-}{-}{-} Example run {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\ControlFlowTok{if} \VariableTok{\_\_name\_\_} \OperatorTok{==} \StringTok{"\_\_main\_\_"}\NormalTok{:}
\NormalTok{    T }\OperatorTok{=} \DecValTok{10}  \CommentTok{\# horizon}
\NormalTok{    V, Q, PI }\OperatorTok{=}\NormalTok{ dynamic\_programming(T}\OperatorTok{=}\NormalTok{T)}

    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Optimal V\_0(s) with T=}\SpecialCharTok{\{}\NormalTok{T}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ S:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  }\SpecialCharTok{\{}\NormalTok{s}\SpecialCharTok{:13s\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{V[}\DecValTok{0}\NormalTok{][s]}\SpecialCharTok{: .3f\}}\SpecialStringTok{"}\NormalTok{)}

    \BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Greedy policy at t=0:"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ S:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  }\SpecialCharTok{\{}\NormalTok{s}\SpecialCharTok{:13s\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{PI[}\DecValTok{0}\NormalTok{][s]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

    \BuiltInTok{print}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{Action value at t=0:"}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ s }\KeywordTok{in}\NormalTok{ S:}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"  }\SpecialCharTok{\{}\NormalTok{s}\SpecialCharTok{:13s\}}\SpecialStringTok{: }\SpecialCharTok{\{}\NormalTok{Q[}\DecValTok{0}\NormalTok{][s, A[}\DecValTok{0}\NormalTok{]]}\SpecialCharTok{: .3f\}}\SpecialStringTok{, }\SpecialCharTok{\{}\NormalTok{Q[}\DecValTok{0}\NormalTok{][s, A[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{: .3f\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The optimal value function at \(t=0\) is:
\begin{equation}
V^\star_0 = \begin{bmatrix}
1.259 \\
3.251 \\
3.787 \\
6.222 \\
7.778 \\
10
\end{bmatrix}.
\label{eq:HangoverOptimalValueFunction}
\end{equation}
Clearly, the optimal value function dominates the value function shown in \eqref{eq:HangoverRandomValueFunction} of the random policy at every state.

The optimal actions at \(t=0\) are:
\begin{equation}
\begin{split}
\text{Hangover} & : \text{Lazy} \\
\text{Sleep}        & : \text{Productive} \\
\text{More Sleep}   & : \text{Productive} \\
\text{Visit Lecture} & : \text{Lazy} \\
\text{Study}      & : \text{Productive} \\
\text{Pass Exam}    & : \text{Lazy}
\end{split}.
\end{equation}

You can play with the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/hangover_dynamic_programming.py}{here}.
\end{example}

\section{Infinite-Horizon MDP}\label{InfiniteHorizonMDP}

In a finite-horizon MDP, the horizon \(T\) must be specified in advance in order to carry out policy evaluation and dynamic programming. The finite horizon naturally provides a terminal condition, which serves as the boundary condition that allows backward recursion to proceed.

In many practical applications, however, the horizon \(T\) is not well defined or is difficult to determine. In such cases, it is often more natural and convenient to adopt the infinite-horizon MDP formulation.

An infinite-horizon MDP is given by the following tuple:
\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma),
\]
where \(\mathcal{S}\), \(\mathcal{A}\), \(P\), and \(R\) are the same as defined before in a finite-horizon MDP. We still restrict ourselves to the tabular MDP setup where \(\mathcal{S}\) and \(\mathcal{A}\) both have a finite number of elements.

The key difference between the finite-horizon and infinite-horizon formulations is that the fixed horizon \(T\) is replaced by a \textbf{discount factor} \(\gamma \in [0,1)\). This discount factor weights future rewards less heavily than immediate rewards, as we will see shortly.

\textbf{Stationary Policy.} In an infinite-horizon MDP, we focus on \emph{stationary} policies \(\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})\), where \(\pi(a \mid s)\) denotes the probability of taking action \(a\) in state \(s\).

In contrast, in a finite-horizon MDP we considered a tuple of \(T\) policies (see \eqref{eq:policy-tuple}), where each \(\pi_t\) could vary with time (i.e., policies were non-stationary).

Intuitively, in the infinite-horizon setting, it suffices to consider stationary policies because the decision-making problem at time \(t\) is equivalent to the problem at time \(t + k\) for any \(k \in \mathbb{N}\), as both face the same infinite horizon.

\textbf{Trajectory and Return}. Given an initial state \(s_0 \in \mathcal{S}\) and a stationary policy \(\pi\), the MDP will evolve as

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start at state \(s_0\)
\item
  Take action \(a_0 \sim \pi(\cdot \mid s_0)\) following policy \(\pi\)
\item
  Collect reward \(r_0 = R(s_0, a_0)\)
\item
  Transition to state \(s_1 \sim P(s' \mid s_0, a_0)\) following the dynamics
\item
  Go to step 2 and continue forever
\end{enumerate}

This process generates a trajectory of states, actions, and rewards:
\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots).
\]

The return of a trajectory is defined as
\[
g_0 = r_0 + \gamma r_1 + \gamma^2 r_2 + \dots = \sum_{t=0}^{\infty} \gamma^t r_t.
\]

Here, the discount factor \(\gamma\) plays a key role: it progressively reduces the weight of rewards received further in the future, making them less influential as \(t\) increases.

\subsection{Value Functions}\label{value-functions}

Similar to the case of finite-horizon MDP, we can define the state-value function and the action-value function associated with a policy \(\pi\).

\textbf{State-Value Function}. The value of a state \(s \in \mathcal{S}\) under policy \(\pi\) is the expected discounted return obtained when starting from \(s\) at time \(0\):
\begin{equation}
V^{\pi}(s) := \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \,\middle|\, s_0 = s, a_t \sim \pi(\cdot \mid s_t), s_{t+1} \sim P(\cdot \mid s_t, a_t) \right].
\label{eq:InfiniteHorizonMDPStateValue}
\end{equation}

\textbf{Action-Value Function}. The value of a state-action pair \((s,a) \in \mathcal{S} \times \mathcal{A}\) under policy \(\pi\) is the expected discounted return obtained by first taking action \(a\) in state \(s\), and then following policy \(\pi\) thereafter:
\begin{equation}
Q^{\pi}(s,a) := \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \,\middle|\, s_0 = s, a_0 = a, a_t \sim \pi(\cdot \mid s_t), s_{t+1} \sim P(\cdot \mid s_t, a_t) \right].
\label{eq:InfiniteHorizonMDPActionValue}
\end{equation}

Note that a nice feature of having a discount factor \(\gamma \in [0,1)\) is that both the state-value and the action-value functions are guaranteed to be bounded even if the horizon is unbounded (assuming the reward function is bounded).

We can verify the state-value function and the action value function satisfy the following relationship:
\begin{align}
V^{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \mid s) Q^{\pi}(s,a) \label{eq:InfiniteHorizonMDPStateValueActionValueRelation-1}\\
Q^{\pi}(s,a) & = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^{\pi}(s'). \label{eq:InfiniteHorizonMDPStateValueActionValueRelation-2}
\end{align}

Combining these two equations, we arrive at the Bellman consistency result for infinite-horizon MDP.

\begin{proposition}[Bellman Consistency (Infinite Horizon)]
\protect\hypertarget{prp:BellmanConsistencyInfiniteHorizon}{}\label{prp:BellmanConsistencyInfiniteHorizon}The state-value function \(V^{\pi}\) in \eqref{eq:InfiniteHorizonMDPStateValue} satisfies the following recursion:
\begin{equation}
\begin{split}
V^{\pi} (s) & = \sum_{a \in \mathcal{A}} \pi (a\mid s) \left( R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^{\pi} (s') \right) \\
    & =: \mathbb{E}_{a \sim \pi(\cdot \mid s)} \left[ R(s, a) + \gamma \mathbb{E}_{s' \sim P(\cdot \mid s, a)} [V^{\pi}(s')] \right].
\end{split}
\label{eq:BellmanConsistency-InfiniteHorizon-State-Value}
\end{equation}

Similarly, the action-value function \(Q^{\pi}(s,a)\) in \eqref{eq:InfiniteHorizonMDPActionValue} satisfies the following recursion:
\begin{equation}
\begin{split}
Q^{\pi} (s, a) & = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left( \sum_{a' \in \mathcal{A}} \pi(a' \mid s') Q^{\pi}(s', a')\right) \\
& =: R(s, a) + \gamma \mathbb{E}_{s' \sim P(\cdot \mid s, a)} \left[\mathbb{E}_{a' \sim \pi(\cdot \mid s')} [Q^{\pi}(s', a')] \right].
\end{split}
\label{eq:BellmanConsistency-InfiniteHorizon-Action-Value}
\end{equation}
\end{proposition}

\subsection{Policy Evaluation}\label{policy-evaluation-1}

Given a policy \(\pi\), how can we compute its associated state-value and action-value functions?

\begin{itemize}
\item
  \textbf{Finite-horizon case.} We initialize the terminal value function \(V_T^{\pi}(s) = 0\) for every \(s \in \mathcal{S}\), and then apply the Bellman Consistency result (Proposition \ref{prp:BellmanConsistency}) to perform backward recursion.
\item
  \textbf{Infinite-horizon case.} The Bellman Consistency result (Proposition \ref{prp:BellmanConsistencyInfiniteHorizon}) takes a different form and does not provide the same simple recipe for backward recursion.
\end{itemize}

\textbf{System of Linear Equations.} A closer look at the Bellman Consistency equation \eqref{eq:BellmanConsistency-InfiniteHorizon-State-Value} for the state-value function shows that it defines a square system of linear equations. Specifically, the value function \(V^{\pi}\) can be represented as a vector with \(|\mathcal{S}|\) variables, and \eqref{eq:BellmanConsistency-InfiniteHorizon-State-Value} provides \(|\mathcal{S}|\) linear equations over these variables.\\
Thus, one way to compute the state-value function is to set up this linear system and solve it. However, doing so typically requires matrix inversion or factorization, which can be computationally expensive.

The same reasoning applies to the action-value function \(Q^{\pi}\), which can be represented as a vector of \(|\mathcal{S}||\mathcal{A}|\) variables constrained by \(|\mathcal{S}||\mathcal{A}|\) linear equations.

The following proposition states that, instead of solving a linear system of equations, one can use a globally convergent iterative scheme, one that is very much like the policy evaluation algorithm for the finite-horizon MDP, to evaluate the state-value function associated with a policy \(\pi\).

\begin{proposition}[Policy Evaluation (Infinite Horizon, State-Value)]
\protect\hypertarget{prp:PolicyEvaluationInfiniteHorizonStateValue}{}\label{prp:PolicyEvaluationInfiniteHorizonStateValue}Consider an infinite-horizon MDP \(\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,\gamma)\). Fix a policy \(\pi\) and consider the iterative scheme for the state-value function:
\begin{equation}
V_{k+1}(s) \;\; \gets \;\; \sum_{a \in \mathcal{A}} \pi(a \mid s) 
\left[ R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a) V_k(s') \right],
\quad \forall s \in \mathcal{S}.
\label{eq:PolicyEvaluationInfiniteHorizonStateValue}
\end{equation}

Then, starting from any initialization \(V_0 \in \mathbb{R}^{|\mathcal{S}|}\), the sequence \(\{V_k\}\) converges to the unique fixed point \(V^{\pi}\), the state-value function associated with policy \(\pi\).
\end{proposition}

\begin{proof}
To prove the convergence of the policy evaluation algorithm, we shall introduce the notion of a Bellman operator.

\textbf{Bellman Operator}. Any value function \(V(s)\) can be interpreted as a vector in \(\mathbb{R}^{|\mathcal{S}|}\) (recall we are in the tabular MDP case). Given any value function \(V \in \mathbb{R}^{|\mathcal{S}|}\), and a policy \(\pi\), define the Bellman operator associated with \(\pi\) as \(T^{\pi}: \mathbb{R}^{|\mathcal{S}|} \mapsto \mathbb{R}^{|\mathcal{S}|}\):
\begin{equation}
(T^{\pi} V)(s) := \sum_{a \in \mathcal{A}} \pi(a \mid s) 
\left[ R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a) V(s') \right].
\label{eq:BellmanOperatorPolicy}
\end{equation}

We claim that \(T^{\pi}\) has two important properties.

\begin{itemize}
\item
  \textbf{Monotonicity}. If \(V \leq W\) (i.e., \(V(s) \leq W(s)\) for any \(s \in \mathcal{S}\)), then \(T^{\pi} V \leq T^{\pi}W\). To see this, observe that
  \begin{align*}
  (T^{\pi}V)(s) - (T^\pi W)(s) &= \sum_{a} \pi(a \mid s) \left(\gamma \sum_{s'} P(s' \mid s, a) (V(s') - W(s')) \right) \\
  & = \gamma \mathbb{E}_{a \sim \pi(\cdot \mid s), s' \sim P(\cdot \mid s,a)}[V(s') - W(s')].
  \end{align*}
  Therefore, if \(V(s') - W(s') \leq 0\) for any \(s' \in \mathcal{S}\), then \(T^{\pi}V \leq T^{\pi} W\).
\item
  \textbf{\(\gamma\)-Contraction}. For any value function \(V \in \mathbb{R}^{|\mathcal{S}|}\), define the \(\ell_{\infty}\) norm (sup norm) as
  \[
  \Vert V \Vert_{\infty} = \max_{s \in \mathcal{S}} |V(s)|.
  \]
\end{itemize}

We claim that the Bellman operator \(T^{\pi}\) is a \(\gamma\)-contraction in the sup norm, i.e.,
\begin{equation}
\Vert T^\pi V - T^\pi W \Vert_{\infty} \leq \gamma \Vert V - W \Vert_{\infty}, \quad \forall V, W \in \mathbb{R}^{|\mathcal{S}|}.
\label{eq:gamma-contraction-Bellman-operator}
\end{equation}
To prove this, observe that for any \(s \in \mathcal{S}\), we have:
\begin{align*}
|(T^\pi V)(s) - (T^\pi W)(s)|
&= \left| \sum_a \pi(a|s)\,\gamma \sum_{s'} P(s'|s,a)\big(V(s') - W(s')\big) \right| \\
&\le \gamma \sum_a \pi(a|s)\sum_{s'} P(s'|s,a)\,|V(s') - W(s')| \\
&\le \gamma \|V - W\|_\infty \sum_a \pi(a|s)\sum_{s'} P(s'|s,a) \\
&= \gamma \|V - W\|_\infty.
\end{align*}

Taking the maximum over \(s\) gives
\[
\|T^\pi V - T^\pi W\|_\infty \le \gamma \|V - W\|_\infty,
\]
so \(T^\pi\) is a \(\gamma\)-contraction in the sup norm.

With the Bellman operator defined, we observe that the value function of \(\pi\), denoted \(V^{\pi}\) in \eqref{eq:BellmanConsistency-InfiniteHorizon-State-Value}, is a \textbf{fixed point} of \(T^{\pi}\). That is to say \(V^{\pi}\) satisfies:
\[
T^{\pi} V^{\pi} = V^{\pi}.
\]
In other words, \(V^{\pi}\) is fixed (remains unchanged) under the Bellman operator.

Since \(T^{\pi}\) is a \(\gamma\)-contraction, by the Banach Fixed-Point Theorem, we know that there exists a unique fixed point to \(T^{\pi}\), which is \(V^{\pi}\). Moreover, since
\[
\Vert V_{k} - V^{\pi} \Vert_{\infty} = \Vert T^{\pi} V_{k-1} - T^{\pi} V^{\pi} \Vert_{\infty} \leq \gamma \Vert V_{k-1} - V^{\pi} \Vert_{\infty},
\]
we can deduce the rate of convergence
\[
\Vert V_{k} - V^{\pi} \Vert_{\infty} \leq \gamma^{k} \Vert V_0 - V^{\pi} \Vert_{\infty}.
\]
Therefore, policy evaluation globally converges from any initialization \(V_0\) at a linear rate of \(\gamma\).
\end{proof}

We have a similar policy evaluation algorithm for the action-value function.

\begin{proposition}[Policy Evaluation (Infinite Horizon, Action-Value)]
\protect\hypertarget{prp:PolicyEvaluationInfiniteHorizonActionValue}{}\label{prp:PolicyEvaluationInfiniteHorizonActionValue}Fix a policy \(\pi\). Consider the iterative scheme on \(Q:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\):
\begin{equation}
\begin{split}
Q_{k+1}(s,a) \;\gets\; R(s,a)
\;+\; \gamma \sum_{s'\in\mathcal{S}} P(s'\mid s,a)\!\left(\sum_{a'\in\mathcal{A}} \pi(a'\mid s')\, Q_k(s',a')\right), 
\\
\forall (s,a)\in\mathcal{S}\times\mathcal{A}.
\end{split}
\label{eq:PolicyEvaluationInfiniteHorizonActionValue}
\end{equation}
Then, for any initialization \(Q_0\), the sequence \(\{Q_k\}\) converges to the unique fixed point \(Q^{\pi}\), the action-value function associated with policy \(\pi\).
\end{proposition}

\begin{proof}
Define the Bellman operator on action-values
\[
(T^{\pi}Q)(s,a) := R(s,a) + \gamma \sum_{s'} P(s'\mid s,a)\Big(\sum_{a'} \pi(a'\mid s')\, Q(s',a')\Big).
\]
\(T^{\pi}\) is a \(\gamma\)-contraction in the sup-norm on \(\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\); hence by the Banach fixed-point theorem, global convergence holds regardless of initialization.
\end{proof}

Let us apply policy evaluation to an infinite-horizon MDP.

\begin{example}[Policy Evaluation for Inverted Pendulum]
\protect\hypertarget{exm:InfiniteHorizonMDPPolicyEvaluation}{}\label{exm:InfiniteHorizonMDPPolicyEvaluation}\leavevmode

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{images/MDP/pendulum-drawing} 

}

\caption{Inverted Pendulum.}\label{fig:mdp-pendulum-illustration}
\end{figure}

We consider the inverted pendulum with state \(s=(\theta, \dot\theta)\) and action (torque) \(a = u\), as visualized in Fig. \ref{fig:mdp-pendulum-illustration}. Our goal is to swing up the pendulum from any initial state to the upright position \(s = (0,0)\).

\textbf{Continuous-Time Dynamics}. The continuous-time dynamics of the inverted pendulum is
\[
\ddot{\theta} \;=\; \frac{g}{l}\sin(\theta) \;+\; \frac{1}{ml^2}u \;-\; c\,\dot{\theta},
\]
where \(m > 0\) is the mass of the pendulum, \(l > 0\) is the length of the pole, \(c > 0\) is the damping coefficient, and \(g\) is the gravitational constant.

\textbf{Discretization (Euler)}. With timestep \(\Delta t\), we obtain the following discrete-time dynamics:
\begin{equation}
\begin{split}
\theta_{k+1} &= \theta_k + \Delta t \, \dot{\theta}_k, \\
\dot{\theta}_{k+1} &= \dot{\theta}_k + \Delta t
\Big(\tfrac{g}{l}\sin(\theta_k) + \tfrac{1}{ml^2}u_k - c\,\dot{\theta}_k\Big).
\end{split}
\label{eq:PendulumDynamicsDiscrete}
\end{equation}

We wrap angles to \([-\pi,\pi]\) via \(\operatorname{wrap}(\theta)=\mathrm{atan2}(\sin\theta,\cos\theta)\).

\textbf{Tabular MDP}. We convert the discrete-time dynamics into a tabular MDP.

\begin{itemize}
\tightlist
\item
  \textbf{State grid.} \(\theta \in [-\pi,\pi]\), \(\dot\theta \in [-\pi,\pi]\) on uniform grids:
  \[
  \mathcal{S}=\{\;(\theta_i,\dot\theta_j)\;:\; i=1,\dots,N_\theta,\; j=1, \dots, N_{\dot\theta}\;\}.
  \]
\item
  \textbf{Action grid.} \(u \in [-mgl/2, mgl/2]\) on \(N_u\) uniform points:
  \[
  \mathcal{A}=\{u_\ell:\ell=1,\dots,N_u\}.
  \]
\item
  \textbf{Stochastic transition kernel (nearest-3 interpolation).} From a grid point \(s=(\theta_i,\dot\theta_j)\) and an action \(u_\ell\),
  compute the next continuous state \(s^+ = (\theta^+,\dot\theta^+)\) via the discrete-time dynamics in \eqref{eq:PendulumDynamicsDiscrete}.
  If \(s^+\notin\mathcal{S}\), choose the three closest grid states
  \(\{s^{(1)},s^{(2)},s^{(3)}\}\) by Euclidean distance in \((\theta,\dot\theta)\) and assign
  probabilities
  \[
  p_r \propto \frac{1}{\|s^+ - s^{(r)}\|_2 + \varepsilon},\quad r=1,2,3,
  \qquad \sum_r p_r=1,
  \]
  so nearer grid points receive higher probability (use a small \(\varepsilon>0\) to avoid division by zero).
\item
  \textbf{Reward.} A quadratic shaping penalty around the upright equilibrium:
  \[
  R(s,a) = -\Big(\theta^2 + 0.1\,\dot\theta^2 + 0.01\,u^2\Big).
  \]
\item
  \textbf{Discount.} \(\gamma \in [0,1)\). We obtain a discounted, infinite-horizon, \textbf{tabular} MDP.
\end{itemize}

\textbf{Policy}. For policy evaluation, consider \(\pi(a\mid s)\) be uniform over the discretized actions, i.e., a random policy.

\textbf{Policy Evaluation}. The following python script performs policy evaluation.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# {-}{-}{-}{-}{-} Physical \& MDP parameters {-}{-}{-}{-}{-}}
\NormalTok{g, l, m, c }\OperatorTok{=} \FloatTok{9.81}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.1}
\NormalTok{dt }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{gamma }\OperatorTok{=} \FloatTok{0.97}
\NormalTok{eps }\OperatorTok{=} \FloatTok{1e{-}8}

\CommentTok{\# Grids}
\NormalTok{N\_theta }\OperatorTok{=} \DecValTok{41}
\NormalTok{N\_thetadot }\OperatorTok{=} \DecValTok{41}
\NormalTok{N\_u }\OperatorTok{=} \DecValTok{21}

\NormalTok{theta\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\NormalTok{np.pi, np.pi, N\_theta)}
\NormalTok{thetadot\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\NormalTok{np.pi, np.pi, N\_thetadot)}
\NormalTok{u\_max }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ m }\OperatorTok{*}\NormalTok{ g }\OperatorTok{*}\NormalTok{ l}
\NormalTok{u\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\NormalTok{u\_max, u\_max, N\_u)}

\CommentTok{\# Helpers to index/unwrap}
\KeywordTok{def}\NormalTok{ wrap\_angle(x):}
    \ControlFlowTok{return}\NormalTok{ np.arctan2(np.sin(x), np.cos(x))}

\KeywordTok{def}\NormalTok{ state\_index(i, j):}
    \ControlFlowTok{return}\NormalTok{ i }\OperatorTok{*}\NormalTok{ N\_thetadot }\OperatorTok{+}\NormalTok{ j}

\KeywordTok{def}\NormalTok{ index\_to\_state(idx):}
\NormalTok{    i }\OperatorTok{=}\NormalTok{ idx }\OperatorTok{//}\NormalTok{ N\_thetadot}
\NormalTok{    j }\OperatorTok{=}\NormalTok{ idx }\OperatorTok{\%}\NormalTok{ N\_thetadot}
    \ControlFlowTok{return}\NormalTok{ theta\_grid[i], thetadot\_grid[j]}

\NormalTok{S }\OperatorTok{=}\NormalTok{ N\_theta }\OperatorTok{*}\NormalTok{ N\_thetadot}
\NormalTok{A }\OperatorTok{=}\NormalTok{ N\_u}

\CommentTok{\# {-}{-}{-}{-}{-} Dynamics step (continuous {-}\textgreater{} one Euler step) {-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ step\_euler(theta, thetadot, u):}
\NormalTok{    theta\_next }\OperatorTok{=}\NormalTok{ wrap\_angle(theta }\OperatorTok{+}\NormalTok{ dt }\OperatorTok{*}\NormalTok{ thetadot)}
\NormalTok{    thetadot\_next }\OperatorTok{=}\NormalTok{ thetadot }\OperatorTok{+}\NormalTok{ dt }\OperatorTok{*}\NormalTok{ ((g}\OperatorTok{/}\NormalTok{l) }\OperatorTok{*}\NormalTok{ np.sin(theta) }\OperatorTok{+}\NormalTok{ (}\DecValTok{1}\OperatorTok{/}\NormalTok{(m}\OperatorTok{*}\NormalTok{l}\OperatorTok{*}\NormalTok{l))}\OperatorTok{*}\NormalTok{u }\OperatorTok{{-}}\NormalTok{ c}\OperatorTok{*}\NormalTok{thetadot)}
    \CommentTok{\# clip angular velocity to grid range (bounded MDP)}
\NormalTok{    thetadot\_next }\OperatorTok{=}\NormalTok{ np.clip(thetadot\_next, thetadot\_grid[}\DecValTok{0}\NormalTok{], thetadot\_grid[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
    \ControlFlowTok{return}\NormalTok{ theta\_next, thetadot\_next}

\CommentTok{\# {-}{-}{-}{-}{-} Find 3 nearest grid states and probability weights (inverse{-}distance) {-}{-}{-}{-}{-}}
\CommentTok{\# Pre{-}compute all grid points for fast nearest neighbor search}
\NormalTok{grid\_pts }\OperatorTok{=}\NormalTok{ np.stack(np.meshgrid(theta\_grid, thetadot\_grid, indexing}\OperatorTok{=}\StringTok{\textquotesingle{}ij\textquotesingle{}}\NormalTok{), axis}\OperatorTok{={-}}\DecValTok{1}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\KeywordTok{def}\NormalTok{ nearest3\_probs(theta\_next, thetadot\_next):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ np.array([theta\_next, thetadot\_next])}
\NormalTok{    dists }\OperatorTok{=}\NormalTok{ np.linalg.norm(grid\_pts }\OperatorTok{{-}}\NormalTok{ x[}\VariableTok{None}\NormalTok{, :], axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    nn\_idx }\OperatorTok{=}\NormalTok{ np.argpartition(dists, }\DecValTok{3}\NormalTok{)[:}\DecValTok{3}\NormalTok{]  }\CommentTok{\# three smallest (unordered)}
    \CommentTok{\# sort those 3 by distance for stability}
\NormalTok{    nn\_idx }\OperatorTok{=}\NormalTok{ nn\_idx[np.argsort(dists[nn\_idx])]}
\NormalTok{    d }\OperatorTok{=}\NormalTok{ dists[nn\_idx]}
\NormalTok{    w }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (d }\OperatorTok{+}\NormalTok{ eps)}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ w }\OperatorTok{/}\NormalTok{ w.}\BuiltInTok{sum}\NormalTok{()}
    \ControlFlowTok{return}\NormalTok{ nn\_idx.astype(}\BuiltInTok{int}\NormalTok{), p}

\CommentTok{\# {-}{-}{-}{-}{-} Reward {-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ reward(theta, thetadot, u):}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{(theta}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \FloatTok{0.1}\OperatorTok{*}\NormalTok{thetadot}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \FloatTok{0.01}\OperatorTok{*}\NormalTok{u}\OperatorTok{**}\DecValTok{2}\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-} Build tabular MDP: R[s,a] and sparse P[s,a,3] {-}{-}{-}{-}{-}}
\NormalTok{R }\OperatorTok{=}\NormalTok{ np.zeros((S, A))}
\NormalTok{NS\_idx }\OperatorTok{=}\NormalTok{ np.zeros((S, A, }\DecValTok{3}\NormalTok{), dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)   }\CommentTok{\# next{-}state indices (3 nearest)}
\NormalTok{NS\_prob }\OperatorTok{=}\NormalTok{ np.zeros((S, A, }\DecValTok{3}\NormalTok{))            }\CommentTok{\# their probabilities}

\ControlFlowTok{for}\NormalTok{ i, th }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(theta\_grid):}
    \ControlFlowTok{for}\NormalTok{ j, thd }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(thetadot\_grid):}
\NormalTok{        s }\OperatorTok{=}\NormalTok{ state\_index(i, j)}
        \ControlFlowTok{for}\NormalTok{ a, u }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(u\_grid):}
            \CommentTok{\# reward at current (s,a)}
\NormalTok{            R[s, a] }\OperatorTok{=}\NormalTok{ reward(th, thd, u)}
            \CommentTok{\# next continuous state}
\NormalTok{            th\_n, thd\_n }\OperatorTok{=}\NormalTok{ step\_euler(th, thd, u)}
            \CommentTok{\# map to 3 nearest grid states}
\NormalTok{            nn\_idx, p }\OperatorTok{=}\NormalTok{ nearest3\_probs(th\_n, thd\_n)}
\NormalTok{            NS\_idx[s, a, :] }\OperatorTok{=}\NormalTok{ nn\_idx}
\NormalTok{            NS\_prob[s, a, :] }\OperatorTok{=}\NormalTok{ p}

\CommentTok{\# {-}{-}{-}{-}{-} Fixed policy: uniform over actions {-}{-}{-}{-}{-}}
\NormalTok{Pi }\OperatorTok{=}\NormalTok{ np.full((S, A), }\FloatTok{1.0} \OperatorTok{/}\NormalTok{ A)}

\CommentTok{\# {-}{-}{-}{-}{-} Iterative policy evaluation {-}{-}{-}{-}{-}}
\NormalTok{V }\OperatorTok{=}\NormalTok{ np.zeros(S)  }\CommentTok{\# initialization (any vector works)}
\NormalTok{tol }\OperatorTok{=} \FloatTok{1e{-}6}
\NormalTok{max\_iters }\OperatorTok{=} \DecValTok{10000}

\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(max\_iters):}
\NormalTok{    V\_new }\OperatorTok{=}\NormalTok{ np.zeros\_like(V)}
    \CommentTok{\# Compute Bellman update: V\_\{k+1\}(s) = sum\_a Pi(s,a)[ R(s,a) + gamma * sum\_j P(s,a,j) V\_k(ns\_j) ]}
    \CommentTok{\# First, expected next V for each (s,a)}
\NormalTok{    EV\_next }\OperatorTok{=}\NormalTok{ (NS\_prob }\OperatorTok{*}\NormalTok{ V[NS\_idx]).}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{2}\NormalTok{)  }\CommentTok{\# shape: (S, A)}
    \CommentTok{\# Then expectation over actions under Pi}
\NormalTok{    V\_new }\OperatorTok{=}\NormalTok{ (Pi }\OperatorTok{*}\NormalTok{ (R }\OperatorTok{+}\NormalTok{ gamma }\OperatorTok{*}\NormalTok{ EV\_next)).}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)  }\CommentTok{\# shape: (S,)}
    \CommentTok{\# Check convergence}
    \ControlFlowTok{if}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(V\_new }\OperatorTok{{-}}\NormalTok{ V)) }\OperatorTok{\textless{}}\NormalTok{ tol:}
\NormalTok{        V }\OperatorTok{=}\NormalTok{ V\_new}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Converged in }\SpecialCharTok{\{}\NormalTok{k}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{ iterations (sup{-}norm change \textless{} }\SpecialCharTok{\{}\NormalTok{tol}\SpecialCharTok{\}}\SpecialStringTok{)."}\NormalTok{)}
        \ControlFlowTok{break}
\NormalTok{    V }\OperatorTok{=}\NormalTok{ V\_new}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Reached max\_iters=}\SpecialCharTok{\{}\NormalTok{max\_iters}\SpecialCharTok{\}}\SpecialStringTok{ without meeting tolerance }\SpecialCharTok{\{}\NormalTok{tol}\SpecialCharTok{\}}\SpecialStringTok{."}\NormalTok{)}

\NormalTok{V\_grid }\OperatorTok{=}\NormalTok{ V.reshape(N\_theta, N\_thetadot)}

\CommentTok{\# V\_grid: shape (N\_theta, N\_thetadot)}
\CommentTok{\# theta\_grid, thetadot\_grid already defined}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{), dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{)}
\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(}
\NormalTok{    V\_grid,}
\NormalTok{    origin}\OperatorTok{=}\StringTok{"lower"}\NormalTok{,}
\NormalTok{    extent}\OperatorTok{=}\NormalTok{[thetadot\_grid.}\BuiltInTok{min}\NormalTok{(), thetadot\_grid.}\BuiltInTok{max}\NormalTok{(),}
\NormalTok{            theta\_grid.}\BuiltInTok{min}\NormalTok{(), theta\_grid.}\BuiltInTok{max}\NormalTok{()],}
\NormalTok{    aspect}\OperatorTok{=}\StringTok{"auto"}\NormalTok{,}
\NormalTok{    cmap}\OperatorTok{=}\StringTok{"viridis"}  \CommentTok{\# any matplotlib colormap, e.g., "plasma", "inferno"}
\NormalTok{)}
\NormalTok{cbar }\OperatorTok{=}\NormalTok{ fig.colorbar(im, ax}\OperatorTok{=}\NormalTok{ax)}
\NormalTok{cbar.set\_label(}\VerbatimStringTok{r"$V\^{}\textbackslash{}pi(\textbackslash{}theta,\textbackslash{}dot\{\textbackslash{}theta\})$"}\NormalTok{)}

\NormalTok{ax.set\_xlabel(}\VerbatimStringTok{r"$\textbackslash{}dot\{\textbackslash{}theta\}$"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\VerbatimStringTok{r"$\textbackslash{}theta$"}\NormalTok{)}
\NormalTok{ax.set\_title(}\VerbatimStringTok{r"State{-}value $V\^{}\textbackslash{}pi$ (tabular policy evaluation)"}\NormalTok{)}

\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

Running the code, it shows that policy evaluation converges in 518 iterations under tolerance \(10^{-6}\).

Fig. \ref{fig:mdp-pendulum-value-function-policy-evaluation} plots the value function over the state grid.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/MDP/pendulum_policy_evaluation} 

}

\caption{Value Function from Policy Evaluation.}\label{fig:mdp-pendulum-value-function-policy-evaluation}
\end{figure}

You can play with the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_policy_evaluation.py}{here}.

\end{example}

\subsection{Principle of Optimality}\label{principle-of-optimality}

In an infinite-horizon MDP, our goal is to find the optimal policy that maximizes the expected long-term discounted return:
\[
V^\star := \max_{\pi} \mathbb{E}_{s \sim \mu(\cdot)} [V^\pi(s)],
\]
where \(\mu\) is a given initial distribution. We call \(V^\star\) the optimal value function.

Given a policy \(\pi\) and its associated value function \(V^\pi\), how do we know if the policy is already optimal?

\begin{theorem}[Bellman Optimality (Infinite Horizon)]
\protect\hypertarget{thm:BellmanOptimalityInfiniteHorizon}{}\label{thm:BellmanOptimalityInfiniteHorizon}For an infinite-horizon MDP with discount factor \(\gamma \in [0,1)\),
the optimal state-value function \(V^\star(s)\)
satisfies the Bellman optimality equation
\begin{equation}
V^\star(s) \;=\; \max_{a \in \mathcal{A}}
\Big[\, R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a)\, V^\star(s') \,\Big].
\label{eq:BellmanOptimalityInfiniteHorizonStateValue}
\end{equation}

Define the optimal action-value function as
\begin{equation}
Q^\star(s,a) = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^\star(s').
\label{eq:InfiniteHorizonOptimalActionValue}
\end{equation}
We have that \(Q^\star(s,a)\) satisfies
\begin{equation}
Q^\star(s,a) \;=\; R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a)\, \left[\max_{a' \in \mathcal{A}} Q^\star(s',a') \right].
\label{eq:BellmanOptimalityInfiniteHorizonActionValue}
\end{equation}

Moreover, any greedy policy with respect to \(V^\star\) (equivalently, to \(Q^\star\)) is optimal:
\begin{equation}
\begin{split}
\pi^\star(s) & \in \arg\max_{a \in \mathcal{A}}
\Big[\, R(s,a) + \gamma \sum_{s'} P(s' \mid s,a)\, V^\star(s') \,\Big]
\quad\Longleftrightarrow\quad \\
\pi^\star(s) & \in \arg\max_{a \in \mathcal{A}} Q^\star(s,a).
\end{split}
\label{eq:InfiniteHorizonOptimalPolicy}
\end{equation}
\end{theorem}

\begin{proof}
We will first show that \(V^\star\) has statewise dominance over all other policies, and then show that \(V^\star\) can be attained by the greedy policy.

\textbf{Claim.} For any discounted MDP with \(\gamma \in [0,1)\) and any policy \(\pi\),
\[
V^\star(s) \;\ge\; V^{\pi}(s)\qquad \forall s\in\mathcal{S},
\]
where \(V^\star\) is the unique solution of the Bellman \textbf{optimality} equation and \(V^\pi\) solves the Bellman \textbf{consistency} equation for \(\pi\).

\textbf{Proof via Bellman Operators}.
Define the Bellman operators
\[
(T^\pi V)(s) := \sum_{a}\pi(a\mid s)\Big[ R(s,a)+\gamma \sum_{s'} P(s'\mid s,a)V(s') \Big],
\]
\[
(T^\star V)(s) := \max_{a}\Big[ R(s,a)+\gamma \sum_{s'} P(s'\mid s,a)V(s') \Big].
\]

Key facts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (\textbf{Monotonicity}) If \(V \ge W\) componentwise, then \(T^\pi V \ge T^\pi W\) and \(T^\star V \ge T^\star W\).
\item
  (\textbf{Dominance of \(T^*\)}) For any \(V\) and any \(\pi\),
  \[
  T^\star V \;\ge\; T^\pi V
  \]
  because the max over actions is at least the \(\pi\)-weighted average.
\item
  (\textbf{Fixed points}) \(V^\pi = T^\pi V^\pi\) and \(V^\star = T^\star V^\star\).
\item
  (\textbf{Contraction}) Each \(T^\pi\) and \(T^\star\) is a \(\gamma\)-contraction in the sup-norm; hence their fixed points are unique.
\end{enumerate}

Now start from \(V^\pi\). Using (2),
\[
V^\pi = T^\pi V^\pi \;\le\; T^\star V^\pi.
\]
Applying \(T^\star\) repeatedly and using (1),
\[
V^\pi \;\le\; T^\star V^\pi \;\le\; (T^\star)^2 V^\pi \;\le\; \cdots
\]
The sequence \((T^\star)^k V^\pi\) converges (by contraction) to the unique fixed point of \(T^\star\), namely \(V^\star\). Taking limits preserves the inequality, yielding \(V^\pi \le V^\star\) statewise.
\end{proof}

The Bellman optimality condition tells us, if a policy \(\pi\) is already greedy with respect to its value function \(V^\pi\), then \(\pi\) is the optimal policy and \(V^\pi\) is the optimal value function.

In the next, we introduce two algorithms that can guarantee finding the optimal policy and the optimal value function.

The first algorithm, policy iteration (PI), iterates over the space of policies; while the second algorithm, value iteration (VI), iterates over the space of value functions.

\subsection{Policy Improvement}\label{policy-improvement}

The policy evaluation algorithm enables us to compute the value functions associated with a given policy \(\pi\). The next result, known as the \emph{Policy Improvement Lemma}, shows that once we have \(V^{\pi}\), constructing a greedy policy with respect to \(V^{\pi}\) guarantees performance that is at least as good as \(\pi\), and strictly better in some states unless \(\pi\) is already greedy with respect to \(V^{\pi}\).

\begin{lemma}[Policy Improvement]
\protect\hypertarget{lem:InfiniteHorizonPolicyImprovement}{}\label{lem:InfiniteHorizonPolicyImprovement}Let \(\pi\) be any policy and let \(V^{\pi}\) be its state-value function.\\
Define a new policy \(\pi'\) such that for each state \(s\),
\[
\pi'(s) \in \arg\max_{a \in \mathcal{A}}
\Big[ R(s,a) + \gamma \sum_{s'} P(s' \mid s,a) V^{\pi}(s') \Big].
\]

Then for all states \(s \in \mathcal{S}\),
\[
V^{\pi'}(s) \;\ge\; V^{\pi}(s).
\]
Moreover, the inequality is strict for some state \(s\) unless \(\pi\) is already greedy with respect to \(V^\pi\) (which implies optimality).
\end{lemma}

\begin{proof}
Let \(V^{\pi}\) be the value function of a policy \(\pi\), and define a new (possibly stochastic) policy \(\pi'\) that is greedy w.r.t. \(V^{\pi}\):
\[
\pi'(\cdot \mid s) \in \arg\max_{\mu \in \Delta(\mathcal{A})}
\sum_{a}\mu(a)\Big[ R(s,a) + \gamma \sum_{s'} P(s'\mid s,a)\, V^{\pi}(s')\Big].
\]
Define the Bellman operators
\begin{align*}
(T^{\pi}V)(s) & := \sum_a \pi(a\mid s)\Big[R(s,a)+\gamma\sum_{s'}P(s'\mid s,a)V(s')\Big],\\
(T^{\pi'}V)(s) & := \sum_a \pi'(a\mid s)\Big[\cdots\Big].
\end{align*}

\textbf{Step 1: One-step improvement at \(V^{\pi}\)}.
By greediness of \(\pi'\) w.r.t. \(V^{\pi}\),
\[
(T^{\pi'} V^{\pi})(s)
= \max_{\mu}\sum_a \mu(a)\Big[R(s,a)+\gamma\sum_{s'}P(s'\mid s,a)V^{\pi}(s')\Big]
\;\;\ge\;\; (T^{\pi} V^{\pi})(s) = V^{\pi}(s),
\]
for all \(s\). Hence
\begin{equation}
T^{\pi'} V^{\pi} \;\ge\; V^{\pi}\quad\text{(componentwise).}
\label{eq:ProofPolicyImprovementStepOne}
\end{equation}

\textbf{Step 2: Monotonicity + contraction yield global improvement}.
The operator \(T^{\pi'}\) is \textbf{monotone} (order-preserving) and a \textbf{\(\gamma\)-contraction} in the sup-norm.\\
Apply \(T^{\pi'}\) repeatedly to both sides of \eqref{eq:ProofPolicyImprovementStepOne}:
\[
(T^{\pi'})^k V^{\pi} \;\ge\; (T^{\pi'})^{k-1} V^{\pi} \;\ge\; \cdots \;\ge\; V^{\pi},\qquad k=1,2,\dots
\]
By contraction, \((T^{\pi'})^k V^{\pi} \to V^{\pi'}\), the unique fixed point of \(T^{\pi'}\).\\
Taking limits preserves the inequality, so
\[
V^{\pi'} \;\ge\; V^{\pi}\quad\text{statewise.}
\]

\textbf{Strict improvement condition}.
If there exists a state \(s\) such that
\[
(T^{\pi'} V^{\pi})(s) \;>\; V^{\pi}(s),
\]
then by monotonicity we have a strict increase at that state after one iteration, and the limit remains strictly larger at that state (or at any state that can reach it with positive probability under \(\pi'\)).\\
This happens precisely when \(\pi'\) selects, with positive probability, an action \(a\) for which
\[
Q^{\pi}(s,a)=R(s,a) + \gamma \sum_{s'} P(s'\mid s,a)\, V^{\pi}(s') \;>\; V^{\pi}(s),
\]
i.e., when \(\pi\) was not already greedy (optimal) at \(s\).
\end{proof}

\subsection{Policy Iteration}\label{policy-iteration}

The policy improvement lemma and the principle of optimality, combined together, leads to the first algorithm that guarantees convergence to an optimal policy. This algorithm is called policy iteration.

\begin{theorem}[Convergence of Policy Iteration]
\protect\hypertarget{thm:PolicyIterationConvergence}{}\label{thm:PolicyIterationConvergence}

Consider a discounted MDP with finite state and action sets and \(\gamma\in[0,1)\). Let \(\{\pi_k\}_{k\ge0}\) be the sequence produced by \emph{Policy Iteration (PI)}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Policy evaluation:} compute \(V^{\pi_k}\) such that \(V^{\pi_k}=T^{\pi_k}V^{\pi_k}\).
\item
  \textbf{Policy improvement:} choose \(\pi_{k+1}\) greedy w.r.t. \(V^{\pi_k}\):
  \[
  \pi_{k+1}(s) \in \arg\max_{a}\Big[ R(s,a)+\gamma\sum_{s'}P(s'|s,a)\,V^{\pi_k}(s')\Big].
  \]
\end{enumerate}

Then:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \(V^{\pi_{k+1}} \ge V^{\pi_k}\) componentwise, and the inequality is strict for some state unless \(\pi_{k+1}=\pi_k\).
\item
  If \(\pi_{k+1}=\pi_k\), then \(V^{\pi_k}\) satisfies the Bellman optimality equation; hence \(\pi_k\) is optimal and \(V^{\pi_k}=V^*\).
\item
  Because the number of stationary policies is finite, PI terminates in finitely many iterations at an optimal policy \(\pi^*\) with value \(V^*\).
\item
  \(\Vert V^{\pi_{k+1}} - V^\star \Vert_{\infty} \leq \gamma \Vert V^{\pi_k} -  V^\star \Vert_{\infty}\), for any \(k\) (i.e., contraction).
\end{enumerate}

\end{theorem}

\begin{proof}
By the policy improvement lemma, we have
\[
V^{\pi_{k+1}} \geq V^{\pi_k}.
\]
By monotonicity of the Bellman operator \(T^{\pi_{k+1}}\), we have
\[
V^{\pi_{k+1}} = T^{\pi_{k+1}} V^{\pi_{k+1}} \geq  T^{\pi_{k+1}} V^{\pi_k}.
\]
By definition of the Bellman optimality operator, we have
\[
T^{\pi_{k+1}} V^{\pi_k} = T^\star V^{\pi_k}.
\]
Therefore,
\[
0 \geq V^{\pi_{k+1}} - V^\star \geq T^{\pi_{k+1}} V^{\pi_k} - V^\star = T^\star V^{\pi_k} - T^\star V^\star
\]
As a result,
\[
\Vert V^{\pi_{k+1}} - V^\star \Vert_{\infty} \leq \Vert T^\star V^{\pi_k} - T^\star V^\star \Vert_{\infty} \leq \gamma \Vert V^{\pi_k} -  V^\star \Vert_{\infty}.
\]
This proves the contraction result (d).
\end{proof}

Let us apply Policy Iteration to the inverted pendulum problem.

\begin{example}[Policy Iteration for Inverted Pendulum]
\protect\hypertarget{exm:InvertedPendulumPolicyIteration}{}\label{exm:InvertedPendulumPolicyIteration}

The following code performs policy iteration for the inverted pendulum problem.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# {-}{-}{-}{-}{-} Physical \& MDP parameters {-}{-}{-}{-}{-}}
\NormalTok{g, l, m, c }\OperatorTok{=} \FloatTok{9.81}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.1}
\NormalTok{dt }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{gamma }\OperatorTok{=} \FloatTok{0.97}
\NormalTok{eps }\OperatorTok{=} \FloatTok{1e{-}8}

\CommentTok{\# Grids}
\NormalTok{N\_theta }\OperatorTok{=} \DecValTok{101}
\NormalTok{N\_thetadot }\OperatorTok{=} \DecValTok{101}
\NormalTok{N\_u }\OperatorTok{=} \DecValTok{51}

\NormalTok{theta\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\OperatorTok{*}\NormalTok{np.pi, }\FloatTok{1.5}\OperatorTok{*}\NormalTok{np.pi, N\_theta)}
\NormalTok{thetadot\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\OperatorTok{*}\NormalTok{np.pi, }\FloatTok{1.5}\OperatorTok{*}\NormalTok{np.pi, N\_thetadot)}
\NormalTok{u\_max }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ m }\OperatorTok{*}\NormalTok{ g }\OperatorTok{*}\NormalTok{ l}
\NormalTok{u\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\NormalTok{u\_max, u\_max, N\_u)}

\CommentTok{\# Helpers to index/unwrap}
\KeywordTok{def}\NormalTok{ wrap\_angle(x):}
    \ControlFlowTok{return}\NormalTok{ np.arctan2(np.sin(x), np.cos(x))}

\KeywordTok{def}\NormalTok{ state\_index(i, j):}
    \ControlFlowTok{return}\NormalTok{ i }\OperatorTok{*}\NormalTok{ N\_thetadot }\OperatorTok{+}\NormalTok{ j}

\KeywordTok{def}\NormalTok{ index\_to\_state(idx):}
\NormalTok{    i }\OperatorTok{=}\NormalTok{ idx }\OperatorTok{//}\NormalTok{ N\_thetadot}
\NormalTok{    j }\OperatorTok{=}\NormalTok{ idx }\OperatorTok{\%}\NormalTok{ N\_thetadot}
    \ControlFlowTok{return}\NormalTok{ theta\_grid[i], thetadot\_grid[j]}

\NormalTok{S }\OperatorTok{=}\NormalTok{ N\_theta }\OperatorTok{*}\NormalTok{ N\_thetadot}
\NormalTok{A }\OperatorTok{=}\NormalTok{ N\_u}

\CommentTok{\# {-}{-}{-}{-}{-} Dynamics step (continuous {-}\textgreater{} one Euler step) {-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ step\_euler(theta, thetadot, u):}
\NormalTok{    theta\_next }\OperatorTok{=}\NormalTok{ wrap\_angle(theta }\OperatorTok{+}\NormalTok{ dt }\OperatorTok{*}\NormalTok{ thetadot)}
\NormalTok{    thetadot\_next }\OperatorTok{=}\NormalTok{ thetadot }\OperatorTok{+}\NormalTok{ dt }\OperatorTok{*}\NormalTok{ ((g}\OperatorTok{/}\NormalTok{l) }\OperatorTok{*}\NormalTok{ np.sin(theta) }\OperatorTok{+}\NormalTok{ (}\DecValTok{1}\OperatorTok{/}\NormalTok{(m}\OperatorTok{*}\NormalTok{l}\OperatorTok{*}\NormalTok{l))}\OperatorTok{*}\NormalTok{u }\OperatorTok{{-}}\NormalTok{ c}\OperatorTok{*}\NormalTok{thetadot)}
    \CommentTok{\# clip angular velocity to grid range (bounded MDP)}
\NormalTok{    thetadot\_next }\OperatorTok{=}\NormalTok{ np.clip(thetadot\_next, thetadot\_grid[}\DecValTok{0}\NormalTok{], thetadot\_grid[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
    \ControlFlowTok{return}\NormalTok{ theta\_next, thetadot\_next}

\CommentTok{\# {-}{-}{-}{-}{-} Find 3 nearest grid states and probability weights (inverse{-}distance) {-}{-}{-}{-}{-}}
\NormalTok{grid\_pts }\OperatorTok{=}\NormalTok{ np.stack(np.meshgrid(theta\_grid, thetadot\_grid, indexing}\OperatorTok{=}\StringTok{\textquotesingle{}ij\textquotesingle{}}\NormalTok{), axis}\OperatorTok{={-}}\DecValTok{1}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\KeywordTok{def}\NormalTok{ nearest3\_probs(theta\_next, thetadot\_next):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ np.array([theta\_next, thetadot\_next])}
\NormalTok{    dists }\OperatorTok{=}\NormalTok{ np.linalg.norm(grid\_pts }\OperatorTok{{-}}\NormalTok{ x[}\VariableTok{None}\NormalTok{, :], axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    nn\_idx }\OperatorTok{=}\NormalTok{ np.argpartition(dists, }\DecValTok{3}\NormalTok{)[:}\DecValTok{3}\NormalTok{]      }\CommentTok{\# three smallest (unordered)}
\NormalTok{    nn\_idx }\OperatorTok{=}\NormalTok{ nn\_idx[np.argsort(dists[nn\_idx])]  }\CommentTok{\# sort those 3 by distance}
\NormalTok{    d }\OperatorTok{=}\NormalTok{ dists[nn\_idx]}
\NormalTok{    w }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (d }\OperatorTok{+}\NormalTok{ eps)}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ w }\OperatorTok{/}\NormalTok{ w.}\BuiltInTok{sum}\NormalTok{()}
    \ControlFlowTok{return}\NormalTok{ nn\_idx.astype(}\BuiltInTok{int}\NormalTok{), p}

\CommentTok{\# {-}{-}{-}{-}{-} Reward {-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ reward(theta, thetadot, u):}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{(theta}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \FloatTok{0.1}\OperatorTok{*}\NormalTok{thetadot}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \FloatTok{0.01}\OperatorTok{*}\NormalTok{u}\OperatorTok{**}\DecValTok{2}\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-} Build tabular MDP: R[s,a] and sparse P[s,a,3] {-}{-}{-}{-}{-}}
\NormalTok{R }\OperatorTok{=}\NormalTok{ np.zeros((S, A))}
\NormalTok{NS\_idx }\OperatorTok{=}\NormalTok{ np.zeros((S, A, }\DecValTok{3}\NormalTok{), dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)   }\CommentTok{\# next{-}state indices (3 nearest)}
\NormalTok{NS\_prob }\OperatorTok{=}\NormalTok{ np.zeros((S, A, }\DecValTok{3}\NormalTok{))             }\CommentTok{\# their probabilities}

\ControlFlowTok{for}\NormalTok{ i, th }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(theta\_grid):}
    \ControlFlowTok{for}\NormalTok{ j, thd }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(thetadot\_grid):}
\NormalTok{        s }\OperatorTok{=}\NormalTok{ state\_index(i, j)}
        \ControlFlowTok{for}\NormalTok{ a, u }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(u\_grid):}
            \CommentTok{\# reward at current (s,a)}
\NormalTok{            R[s, a] }\OperatorTok{=}\NormalTok{ reward(th, thd, u)}
            \CommentTok{\# next continuous state}
\NormalTok{            th\_n, thd\_n }\OperatorTok{=}\NormalTok{ step\_euler(th, thd, u)}
            \CommentTok{\# map to 3 nearest grid states}
\NormalTok{            nn\_idx, p }\OperatorTok{=}\NormalTok{ nearest3\_probs(th\_n, thd\_n)}
\NormalTok{            NS\_idx[s, a, :] }\OperatorTok{=}\NormalTok{ nn\_idx}
\NormalTok{            NS\_prob[s, a, :] }\OperatorTok{=}\NormalTok{ p}

\CommentTok{\# =======================}
\CommentTok{\#     POLICY ITERATION}
\CommentTok{\# =======================}

\CommentTok{\# Represent policy as a deterministic action index per state: pi[s] in \{0..A{-}1\}}
\CommentTok{\# Start from uniform{-}random policy (deterministic tie{-}breaker: middle action)}
\NormalTok{pi }\OperatorTok{=}\NormalTok{ np.full(S, A }\OperatorTok{//} \DecValTok{2}\NormalTok{, dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)}

\KeywordTok{def}\NormalTok{ policy\_evaluation(pi, V\_init}\OperatorTok{=}\VariableTok{None}\NormalTok{, tol}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{, max\_iters}\OperatorTok{=}\DecValTok{10000}\NormalTok{):}
    \CommentTok{"""Iterative policy evaluation for deterministic pi (action index per state)."""}
\NormalTok{    V }\OperatorTok{=}\NormalTok{ np.zeros(S) }\ControlFlowTok{if}\NormalTok{ V\_init }\KeywordTok{is} \VariableTok{None} \ControlFlowTok{else}\NormalTok{ V\_init.copy()}
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(max\_iters):}
        \CommentTok{\# For each state s, use chosen action a = pi[s]}
\NormalTok{        a }\OperatorTok{=}\NormalTok{ pi  }\CommentTok{\# shape (S,)}
        \CommentTok{\# Expected next value under chosen action}
\NormalTok{        EV\_next }\OperatorTok{=}\NormalTok{ (NS\_prob[np.arange(S), a] }\OperatorTok{*}\NormalTok{ V[NS\_idx[np.arange(S), a]]).}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)  }\CommentTok{\# (S,)}
\NormalTok{        V\_new }\OperatorTok{=}\NormalTok{ R[np.arange(S), a] }\OperatorTok{+}\NormalTok{ gamma }\OperatorTok{*}\NormalTok{ EV\_next}
        \ControlFlowTok{if}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(V\_new }\OperatorTok{{-}}\NormalTok{ V)) }\OperatorTok{\textless{}}\NormalTok{ tol:}
            \CommentTok{\# print(f"Policy evaluation converged in \{k+1\} iterations.")}
            \ControlFlowTok{return}\NormalTok{ V\_new}
\NormalTok{        V }\OperatorTok{=}\NormalTok{ V\_new}
    \CommentTok{\# print("Policy evaluation reached max\_iters without meeting tolerance.")}
    \ControlFlowTok{return}\NormalTok{ V}

\KeywordTok{def}\NormalTok{ policy\_improvement(V, pi\_old}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
    \CommentTok{"""Greedy improvement: pi\textquotesingle{}(s) = argmax\_a [ R(s,a) + gamma * E[V(s\textquotesingle{})] ]."""}
    \CommentTok{\# Compute Q(s,a) = R + gamma * sum\_j P(s,a,j) V(ns\_j)}
\NormalTok{    EV\_next }\OperatorTok{=}\NormalTok{ (NS\_prob }\OperatorTok{*}\NormalTok{ V[NS\_idx]).}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{2}\NormalTok{)      }\CommentTok{\# (S, A)}
\NormalTok{    Q }\OperatorTok{=}\NormalTok{ R }\OperatorTok{+}\NormalTok{ gamma }\OperatorTok{*}\NormalTok{ EV\_next                           }\CommentTok{\# (S, A)}
\NormalTok{    pi\_new }\OperatorTok{=}\NormalTok{ np.argmax(Q, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{)         }\CommentTok{\# greedy deterministic policy}
\NormalTok{    stable }\OperatorTok{=}\NormalTok{ (pi\_old }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{) }\KeywordTok{and}\NormalTok{ np.array\_equal(pi\_new, pi\_old)}
    \ControlFlowTok{return}\NormalTok{ pi\_new, stable}

\CommentTok{\# Main PI loop}
\NormalTok{max\_pi\_iters }\OperatorTok{=} \DecValTok{100}
\NormalTok{V }\OperatorTok{=}\NormalTok{ np.zeros(S)}
\ControlFlowTok{for}\NormalTok{ it }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(max\_pi\_iters):}
    \CommentTok{\# Policy evaluation}
\NormalTok{    V }\OperatorTok{=}\NormalTok{ policy\_evaluation(pi, V\_init}\OperatorTok{=}\NormalTok{V, tol}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{, max\_iters}\OperatorTok{=}\DecValTok{10000}\NormalTok{)}
    \CommentTok{\# Policy improvement}
\NormalTok{    pi\_new, stable }\OperatorTok{=}\NormalTok{ policy\_improvement(V, pi\_old}\OperatorTok{=}\NormalTok{pi)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"[PI] Iter }\SpecialCharTok{\{}\NormalTok{it}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{: policy changed = }\SpecialCharTok{\{}\KeywordTok{not}\NormalTok{ stable}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{    pi }\OperatorTok{=}\NormalTok{ pi\_new}
    \ControlFlowTok{if}\NormalTok{ stable:}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"Policy iteration converged: policy stable."}\NormalTok{)}
        \ControlFlowTok{break}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Reached max\_pi\_iters without policy stability (may still be near{-}optimal)."}\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-} Visualization {-}{-}{-}{-}{-}}
\NormalTok{V\_grid }\OperatorTok{=}\NormalTok{ V.reshape(N\_theta, N\_thetadot)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{), dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{)}
\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(}
\NormalTok{    V\_grid,}
\NormalTok{    origin}\OperatorTok{=}\StringTok{"lower"}\NormalTok{,}
\NormalTok{    extent}\OperatorTok{=}\NormalTok{[thetadot\_grid.}\BuiltInTok{min}\NormalTok{(), thetadot\_grid.}\BuiltInTok{max}\NormalTok{(),}
\NormalTok{            theta\_grid.}\BuiltInTok{min}\NormalTok{(), theta\_grid.}\BuiltInTok{max}\NormalTok{()],}
\NormalTok{    aspect}\OperatorTok{=}\StringTok{"auto"}\NormalTok{,}
\NormalTok{    cmap}\OperatorTok{=}\StringTok{"viridis"}
\NormalTok{)}
\NormalTok{cbar }\OperatorTok{=}\NormalTok{ fig.colorbar(im, ax}\OperatorTok{=}\NormalTok{ax)}
\NormalTok{cbar.set\_label(}\VerbatimStringTok{r"$V\^{}\{\textbackslash{}pi\}(\textbackslash{}theta,\textbackslash{}dot\{\textbackslash{}theta\})$ (final PI)"}\NormalTok{)}

\NormalTok{ax.set\_xlabel(}\VerbatimStringTok{r"$\textbackslash{}dot\{\textbackslash{}theta\}$"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\VerbatimStringTok{r"$\textbackslash{}theta$"}\NormalTok{)}
\NormalTok{ax.set\_title(}\VerbatimStringTok{r"State{-}value $V$ after Policy Iteration"}\NormalTok{)}

\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}

\CommentTok{\# Visualize the greedy action *value* (torque)}
\NormalTok{pi\_grid }\OperatorTok{=}\NormalTok{ pi.reshape(N\_theta, N\_thetadot)          }\CommentTok{\# action indices}
\NormalTok{action\_values }\OperatorTok{=}\NormalTok{ u\_grid[pi\_grid]                    }\CommentTok{\# map indices {-}\textgreater{} torques}

\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{), dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{)}
\NormalTok{im }\OperatorTok{=}\NormalTok{ plt.imshow(action\_values,}
\NormalTok{           origin}\OperatorTok{=}\StringTok{"lower"}\NormalTok{,}
\NormalTok{           extent}\OperatorTok{=}\NormalTok{[thetadot\_grid.}\BuiltInTok{min}\NormalTok{(), thetadot\_grid.}\BuiltInTok{max}\NormalTok{(),}
\NormalTok{                   theta\_grid.}\BuiltInTok{min}\NormalTok{(), theta\_grid.}\BuiltInTok{max}\NormalTok{()],}
\NormalTok{           aspect}\OperatorTok{=}\StringTok{"auto"}\NormalTok{, cmap}\OperatorTok{=}\StringTok{"coolwarm"}\NormalTok{)         }\CommentTok{\# diverging colormap good for ± torque}
\NormalTok{cbar }\OperatorTok{=}\NormalTok{ plt.colorbar(im)}
\NormalTok{cbar.set\_label(}\StringTok{"Greedy action value (torque)"}\NormalTok{)}

\NormalTok{plt.xlabel(}\VerbatimStringTok{r"$\textbackslash{}dot\{\textbackslash{}theta\}$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\VerbatimStringTok{r"$\textbackslash{}theta$"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Greedy policy (torque) after PI"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

Running the code produces the optimal value function shown in Fig. \ref{fig:mdp-pendulum-PI-value} and the optimal policy shown in Fig. \ref{fig:mdp-pendulum-PI-policy}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/MDP/pendulum_PI_value} 

}

\caption{Optimal Value Function after Policy Iteration}\label{fig:mdp-pendulum-PI-value}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/MDP/pendulum_PI_policy} 

}

\caption{Optimal Policy after Policy Iteration}\label{fig:mdp-pendulum-PI-policy}
\end{figure}

We can apply the optimal policy to the pendulum with an initial state of \((-\pi, 0)\) (i.e., the bottomright position). Fig. \ref{fig:mdp-pendulum-PI-rollout-trajectory} plots the rollout trajectory of \(\theta, \dot{\theta}, u\). We can see that the optimal policy is capable of performing ``bang-bang'' control to accumulate energy before swinging up.

Fig. \ref{fig:mdp-pendulum-PI-rollout-trajectory-value} overlays the trajectory on top of the optimal value function.

You can play with the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_policy_iteration.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/MDP/pendulum_optimal_trajectory} 

}

\caption{Optimal Trajectory of Pendulum Swing-Up}\label{fig:mdp-pendulum-PI-rollout-trajectory}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/MDP/pendulum_optimal_trajectory_value} 

}

\caption{Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function}\label{fig:mdp-pendulum-PI-rollout-trajectory-value}
\end{figure}

\end{example}

\subsection{Value Iteration}\label{value-iteration}

Policy iteration---as the name suggests---iterates on \emph{policies}: it alternates between
(1) \emph{policy evaluation} (computing \(V^{\pi}\) for the current policy \(\pi\)) and
(2) \emph{policy improvement} (making \(\pi\) greedy w.r.t. \(V^{\pi}\)).

An alternative, often very effective, method is \emph{value iteration}. Unlike policy iteration, value iteration does \emph{not} explicitly maintain a policy during its updates; it iterates directly on the value function toward the fixed point of the Bellman optimality* operator. Once the value function has (approximately) converged, the optimal policy is obtained by a single greedy extraction step. Note that intermediate value iterates need not correspond to the value of any actual policy.

The value iteration (VI) algorithm works as follows:

\textbf{Initialization.} Choose any \(V_0:\mathcal{S}\to\mathbb{R}\) (e.g., \(V_0 \equiv 0\)).\\
\textbf{Iteration.} For \(k=0,1,2,\dots\),
\[
V_{k+1}(s) \;\leftarrow\; \max_{a\in\mathcal{A}}
\Big[\, R(s,a) \;+\; \gamma \sum_{s'\in\mathcal{S}} P(s'\mid s,a)\; V_k(s') \,\Big],
\quad \forall s\in\mathcal{S}.
\]
\textbf{Stopping rule.} Stop when \(\lVert V_{k+1}-V_k\rVert_\infty \le \varepsilon\) (or any chosen tolerance).

\textbf{Policy extraction (greedy):}
\[
\pi_{k+1}(s) \in \arg\max_{a\in\mathcal{A}}
\Big[\, R(s,a) \;+\; \gamma \sum_{s'} P(s'\mid s,a)\; V_{k+1}(s') \,\Big].
\]

The following theorem states the convergence of value iteration.

\begin{theorem}[Convergence of Value Iteration]
\protect\hypertarget{thm:ValueIterationConvergence}{}\label{thm:ValueIterationConvergence}Let \(T^\star\) be the Bellman optimality operator,
\[
(T^\star V)(s) := \max_{a}\Big[ R(s,a) + \gamma \sum_{s'} P(s'\mid s,a)\, V(s') \Big].
\]
For \(\gamma\in[0,1)\) and finite \(\mathcal{S},\mathcal{A}\), \(T^\star\) is a \(\gamma\)-contraction in the sup-norm. Hence, for any \(V_0\),
\[
V_k \;=\; (T^\star )^k V_0 \;\xrightarrow[k\to\infty]{}\; V^*,
\]
the unique fixed point of \(T^\star\). Moreover, the greedy policy \(\pi_k\) extracted from \(V_k\) converges to an optimal policy \(\pi^\star\).

In addition, after \(k\) iterations, we have
\[
\lVert V_k - V^* \rVert_\infty \;\le\; \gamma^k \, \lVert V_0 - V^* \rVert_\infty.
\]
\end{theorem}

Finally, we apply value iteration to the inverted pendulum problem.

\begin{example}[Value Iteration for Inverted Pendulum]
\protect\hypertarget{exm:InvertedPendulumValueIteration}{}\label{exm:InvertedPendulumValueIteration}The following code performs value iteration for the inverted pendulum problem.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# {-}{-}{-}{-}{-} Physical \& MDP parameters {-}{-}{-}{-}{-}}
\NormalTok{g, l, m, c }\OperatorTok{=} \FloatTok{9.81}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, }\FloatTok{0.1}
\NormalTok{dt }\OperatorTok{=} \FloatTok{0.05}
\NormalTok{gamma }\OperatorTok{=} \FloatTok{0.97}
\NormalTok{eps }\OperatorTok{=} \FloatTok{1e{-}8}

\CommentTok{\# Grids}
\NormalTok{N\_theta }\OperatorTok{=} \DecValTok{101}
\NormalTok{N\_thetadot }\OperatorTok{=} \DecValTok{101}
\NormalTok{N\_u }\OperatorTok{=} \DecValTok{51}

\NormalTok{theta\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\OperatorTok{*}\NormalTok{np.pi, }\FloatTok{1.5}\OperatorTok{*}\NormalTok{np.pi, N\_theta)}
\NormalTok{thetadot\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\OperatorTok{*}\NormalTok{np.pi, }\FloatTok{1.5}\OperatorTok{*}\NormalTok{np.pi, N\_thetadot)}
\NormalTok{u\_max }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ m }\OperatorTok{*}\NormalTok{ g }\OperatorTok{*}\NormalTok{ l}
\NormalTok{u\_grid }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\NormalTok{u\_max, u\_max, N\_u)}

\CommentTok{\# Helpers to index/unwrap}
\KeywordTok{def}\NormalTok{ wrap\_angle(x):}
    \ControlFlowTok{return}\NormalTok{ np.arctan2(np.sin(x), np.cos(x))}

\KeywordTok{def}\NormalTok{ state\_index(i, j):}
    \ControlFlowTok{return}\NormalTok{ i }\OperatorTok{*}\NormalTok{ N\_thetadot }\OperatorTok{+}\NormalTok{ j}

\KeywordTok{def}\NormalTok{ index\_to\_state(idx):}
\NormalTok{    i }\OperatorTok{=}\NormalTok{ idx }\OperatorTok{//}\NormalTok{ N\_thetadot}
\NormalTok{    j }\OperatorTok{=}\NormalTok{ idx }\OperatorTok{\%}\NormalTok{ N\_thetadot}
    \ControlFlowTok{return}\NormalTok{ theta\_grid[i], thetadot\_grid[j]}

\NormalTok{S }\OperatorTok{=}\NormalTok{ N\_theta }\OperatorTok{*}\NormalTok{ N\_thetadot}
\NormalTok{A }\OperatorTok{=}\NormalTok{ N\_u}

\CommentTok{\# {-}{-}{-}{-}{-} Dynamics step (continuous {-}\textgreater{} one Euler step) {-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ step\_euler(theta, thetadot, u):}
\NormalTok{    theta\_next }\OperatorTok{=}\NormalTok{ wrap\_angle(theta }\OperatorTok{+}\NormalTok{ dt }\OperatorTok{*}\NormalTok{ thetadot)}
\NormalTok{    thetadot\_next }\OperatorTok{=}\NormalTok{ thetadot }\OperatorTok{+}\NormalTok{ dt }\OperatorTok{*}\NormalTok{ ((g}\OperatorTok{/}\NormalTok{l) }\OperatorTok{*}\NormalTok{ np.sin(theta) }\OperatorTok{+}\NormalTok{ (}\DecValTok{1}\OperatorTok{/}\NormalTok{(m}\OperatorTok{*}\NormalTok{l}\OperatorTok{*}\NormalTok{l))}\OperatorTok{*}\NormalTok{u }\OperatorTok{{-}}\NormalTok{ c}\OperatorTok{*}\NormalTok{thetadot)}
    \CommentTok{\# clip angular velocity to grid range (bounded MDP)}
\NormalTok{    thetadot\_next }\OperatorTok{=}\NormalTok{ np.clip(thetadot\_next, thetadot\_grid[}\DecValTok{0}\NormalTok{], thetadot\_grid[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
    \ControlFlowTok{return}\NormalTok{ theta\_next, thetadot\_next}

\CommentTok{\# {-}{-}{-}{-}{-} Find 3 nearest grid states and probability weights (inverse{-}distance) {-}{-}{-}{-}{-}}
\NormalTok{grid\_pts }\OperatorTok{=}\NormalTok{ np.stack(np.meshgrid(theta\_grid, thetadot\_grid, indexing}\OperatorTok{=}\StringTok{\textquotesingle{}ij\textquotesingle{}}\NormalTok{), axis}\OperatorTok{={-}}\DecValTok{1}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\KeywordTok{def}\NormalTok{ nearest3\_probs(theta\_next, thetadot\_next):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ np.array([theta\_next, thetadot\_next])}
\NormalTok{    dists }\OperatorTok{=}\NormalTok{ np.linalg.norm(grid\_pts }\OperatorTok{{-}}\NormalTok{ x[}\VariableTok{None}\NormalTok{, :], axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    nn\_idx }\OperatorTok{=}\NormalTok{ np.argpartition(dists, }\DecValTok{3}\NormalTok{)[:}\DecValTok{3}\NormalTok{]      }\CommentTok{\# three smallest (unordered)}
\NormalTok{    nn\_idx }\OperatorTok{=}\NormalTok{ nn\_idx[np.argsort(dists[nn\_idx])]  }\CommentTok{\# sort those 3 by distance}
\NormalTok{    d }\OperatorTok{=}\NormalTok{ dists[nn\_idx]}
\NormalTok{    w }\OperatorTok{=} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (d }\OperatorTok{+}\NormalTok{ eps)}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ w }\OperatorTok{/}\NormalTok{ w.}\BuiltInTok{sum}\NormalTok{()}
    \ControlFlowTok{return}\NormalTok{ nn\_idx.astype(}\BuiltInTok{int}\NormalTok{), p}

\CommentTok{\# {-}{-}{-}{-}{-} Reward {-}{-}{-}{-}{-}}
\KeywordTok{def}\NormalTok{ reward(theta, thetadot, u):}
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{(theta}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \FloatTok{0.1}\OperatorTok{*}\NormalTok{thetadot}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \FloatTok{0.01}\OperatorTok{*}\NormalTok{u}\OperatorTok{**}\DecValTok{2}\NormalTok{)}

\CommentTok{\# {-}{-}{-}{-}{-} Build tabular MDP: R[s,a] and sparse P[s,a,3] {-}{-}{-}{-}{-}}
\NormalTok{R }\OperatorTok{=}\NormalTok{ np.zeros((S, A))}
\NormalTok{NS\_idx }\OperatorTok{=}\NormalTok{ np.zeros((S, A, }\DecValTok{3}\NormalTok{), dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)   }\CommentTok{\# next{-}state indices (3 nearest)}
\NormalTok{NS\_prob }\OperatorTok{=}\NormalTok{ np.zeros((S, A, }\DecValTok{3}\NormalTok{))             }\CommentTok{\# their probabilities}

\ControlFlowTok{for}\NormalTok{ i, th }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(theta\_grid):}
    \ControlFlowTok{for}\NormalTok{ j, thd }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(thetadot\_grid):}
\NormalTok{        s }\OperatorTok{=}\NormalTok{ state\_index(i, j)}
        \ControlFlowTok{for}\NormalTok{ a, u }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(u\_grid):}
\NormalTok{            R[s, a] }\OperatorTok{=}\NormalTok{ reward(th, thd, u)}
\NormalTok{            th\_n, thd\_n }\OperatorTok{=}\NormalTok{ step\_euler(th, thd, u)}
\NormalTok{            nn\_idx, p }\OperatorTok{=}\NormalTok{ nearest3\_probs(th\_n, thd\_n)}
\NormalTok{            NS\_idx[s, a, :] }\OperatorTok{=}\NormalTok{ nn\_idx}
\NormalTok{            NS\_prob[s, a, :] }\OperatorTok{=}\NormalTok{ p}

\CommentTok{\# =======================}
\CommentTok{\#       VALUE ITERATION}
\CommentTok{\# =======================}

\CommentTok{\# Bellman optimality update:}
\CommentTok{\# V\_\{k+1\}(s) = max\_a [ R(s,a) + gamma * sum\_j P(s,a,j) * V\_k(ns\_j) ]}
\NormalTok{V }\OperatorTok{=}\NormalTok{ np.zeros(S)}
\NormalTok{tol }\OperatorTok{=} \FloatTok{1e{-}6}
\NormalTok{max\_vi\_iters }\OperatorTok{=} \DecValTok{1000}

\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(max\_vi\_iters):}
    \CommentTok{\# Expected next V for every (s,a), given current V\_k}
\NormalTok{    EV\_next }\OperatorTok{=}\NormalTok{ (NS\_prob }\OperatorTok{*}\NormalTok{ V[NS\_idx]).}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# shape (S, A)}
\NormalTok{    Q }\OperatorTok{=}\NormalTok{ R }\OperatorTok{+}\NormalTok{ gamma }\OperatorTok{*}\NormalTok{ EV\_next                       }\CommentTok{\# shape (S, A)}
\NormalTok{    V\_new }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(Q, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)                     }\CommentTok{\# greedy backup over actions}

\NormalTok{    delta }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{max}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(V\_new }\OperatorTok{{-}}\NormalTok{ V))}
    \CommentTok{\# Optional: a stopping rule aligned with policy loss bound could scale tol}
    \CommentTok{\# e.g., stop when delta \textless{}= tol * (1 {-} gamma) / (2 * gamma)}
    \ControlFlowTok{if}\NormalTok{ delta }\OperatorTok{\textless{}}\NormalTok{ tol:}
\NormalTok{        V }\OperatorTok{=}\NormalTok{ V\_new}
        \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Value Iteration converged in }\SpecialCharTok{\{}\NormalTok{k}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{ iterations (sup{-}norm change }\SpecialCharTok{\{}\NormalTok{delta}\SpecialCharTok{:.2e\}}\SpecialStringTok{)."}\NormalTok{)}
        \ControlFlowTok{break}
\NormalTok{    V }\OperatorTok{=}\NormalTok{ V\_new}
\ControlFlowTok{else}\NormalTok{:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Reached max\_vi\_iters=}\SpecialCharTok{\{}\NormalTok{max\_vi\_iters}\SpecialCharTok{\}}\SpecialStringTok{ (last sup{-}norm change }\SpecialCharTok{\{}\NormalTok{delta}\SpecialCharTok{:.2e\}}\SpecialStringTok{)."}\NormalTok{)}

\CommentTok{\# Greedy policy extraction from the final V}
\NormalTok{EV\_next }\OperatorTok{=}\NormalTok{ (NS\_prob }\OperatorTok{*}\NormalTok{ V[NS\_idx]).}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{2}\NormalTok{)   }\CommentTok{\# recompute with final V}
\NormalTok{Q }\OperatorTok{=}\NormalTok{ R }\OperatorTok{+}\NormalTok{ gamma }\OperatorTok{*}\NormalTok{ EV\_next}
\NormalTok{pi }\OperatorTok{=}\NormalTok{ np.argmax(Q, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)                     }\CommentTok{\# deterministic greedy policy (indices)}

\CommentTok{\# {-}{-}{-}{-}{-} Visualization: Value function {-}{-}{-}{-}{-}}
\NormalTok{V\_grid }\OperatorTok{=}\NormalTok{ V.reshape(N\_theta, N\_thetadot)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{), dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{)}
\NormalTok{im }\OperatorTok{=}\NormalTok{ ax.imshow(}
\NormalTok{    V\_grid,}
\NormalTok{    origin}\OperatorTok{=}\StringTok{"lower"}\NormalTok{,}
\NormalTok{    extent}\OperatorTok{=}\NormalTok{[thetadot\_grid.}\BuiltInTok{min}\NormalTok{(), thetadot\_grid.}\BuiltInTok{max}\NormalTok{(),}
\NormalTok{            theta\_grid.}\BuiltInTok{min}\NormalTok{(), theta\_grid.}\BuiltInTok{max}\NormalTok{()],}
\NormalTok{    aspect}\OperatorTok{=}\StringTok{"auto"}\NormalTok{,}
\NormalTok{    cmap}\OperatorTok{=}\StringTok{"viridis"}
\NormalTok{)}
\NormalTok{cbar }\OperatorTok{=}\NormalTok{ fig.colorbar(im, ax}\OperatorTok{=}\NormalTok{ax)}
\NormalTok{cbar.set\_label(}\VerbatimStringTok{r"$V\^{}*(\textbackslash{}theta,\textbackslash{}dot\{\textbackslash{}theta\})$ (Value Iteration)"}\NormalTok{)}

\NormalTok{ax.set\_xlabel(}\VerbatimStringTok{r"$\textbackslash{}dot\{\textbackslash{}theta\}$"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\VerbatimStringTok{r"$\textbackslash{}theta$"}\NormalTok{)}
\NormalTok{ax.set\_title(}\VerbatimStringTok{r"State{-}value $V$ after Value Iteration"}\NormalTok{)}

\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}

\CommentTok{\# {-}{-}{-}{-}{-} Visualization: Greedy torque field {-}{-}{-}{-}{-}}
\NormalTok{pi\_grid }\OperatorTok{=}\NormalTok{ pi.reshape(N\_theta, N\_thetadot)   }\CommentTok{\# action indices}
\NormalTok{action\_values }\OperatorTok{=}\NormalTok{ u\_grid[pi\_grid]             }\CommentTok{\# map indices {-}\textgreater{} torques}

\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{), dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{)}
\NormalTok{im }\OperatorTok{=}\NormalTok{ plt.imshow(}
\NormalTok{    action\_values,}
\NormalTok{    origin}\OperatorTok{=}\StringTok{"lower"}\NormalTok{,}
\NormalTok{    extent}\OperatorTok{=}\NormalTok{[thetadot\_grid.}\BuiltInTok{min}\NormalTok{(), thetadot\_grid.}\BuiltInTok{max}\NormalTok{(),}
\NormalTok{            theta\_grid.}\BuiltInTok{min}\NormalTok{(), theta\_grid.}\BuiltInTok{max}\NormalTok{()],}
\NormalTok{    aspect}\OperatorTok{=}\StringTok{"auto"}\NormalTok{,}
\NormalTok{    cmap}\OperatorTok{=}\StringTok{"coolwarm"}   \CommentTok{\# good for ± torque}
\NormalTok{)}
\NormalTok{cbar }\OperatorTok{=}\NormalTok{ plt.colorbar(im)}
\NormalTok{cbar.set\_label(}\StringTok{"Greedy action value (torque)"}\NormalTok{)}

\NormalTok{plt.xlabel(}\VerbatimStringTok{r"$\textbackslash{}dot\{\textbackslash{}theta\}$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\VerbatimStringTok{r"$\textbackslash{}theta$"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Greedy policy (torque) extracted from Value Iteration"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

Try it for yourself \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_value_iteration.py}{here}!

You should obtain the same results as policy iteration.
\end{example}

\chapter{Value-based Reinforcement Learning}\label{value-rl}

In Chapter \ref{mdp}, we introduced algorithms for policy evaluation, policy improvement, and computing optimal policies in the tabular setting when the model is known. These dynamic-programming methods are grounded in Bellman consistency and optimality and come with strong convergence guarantees.

A key limitation of the methods in Chapter \ref{mdp} is that they require the transition dynamics \(P(s' \mid s, a)\) to be known. While in some applications modeling the dynamics is feasible (e.g., the inverted pendulum), in many others it is costly or impractical to obtain an accurate model of the environment (e.g., a humanoid robot interacting with everyday objects).

This motivates relaxing the known-dynamics assumption and asking whether we can design algorithms that learn purely from interaction---i.e., by collecting data through environment interaction. This brings us to \textbf{model-free reinforcement learning}.

In this chapter we focus on \textbf{value-based} RL methods. The central idea is to learn the value functions---\(V(s)\) and \(Q(s,a)\)---from interaction with the environment and then leverage these estimates to derive (approximately) optimal policies. We begin with tabular methods and then move to function-approximation approaches (e.g., neural networks) for problems where a tabular representation is intractable.

\section{Tabular Methods}\label{tabular-methods}

Consider an infinite-horizon Markov decision process (MDP)\\
\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma),
\]
with a discount factor \(\gamma \in [0,1)\). We focus on the \emph{tabular setting} where both the state space \(\mathcal{S}\) and the action space \(\mathcal{A}\) are finite, with cardinalities \(|\mathcal{S}|\) and \(|\mathcal{A}|\), respectively.

A policy is a stationary stochastic mapping\\
\[
\pi: \mathcal{S} \to \Delta(\mathcal{A}),
\]
where \(\pi(a \mid s)\) denotes the probability of selecting action \(a\) in state \(s\).

Unlike in Chapter \ref{mdp}, here we do not assume knowledge of the transition dynamics \(P\) or the reward function \(R\) (other than that \(R\) is deterministic). Instead, we assume we can interact with the environment and obtain \emph{trajectories} of the form\\
\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots),
\]
by following a policy \(\pi\).

\subsection{Policy Evaluation}\label{tabular-PE}

We first consider the problem of estimating the value function of a given policy \(\pi\). Recall the definition of the state-value function associated with \(\pi\) is:
\begin{equation}
V^{\pi}(s) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s \right],
\label{eq:InfiniteHorizonStateValueRestate}
\end{equation}
where the expectation is taken over the randomness of both the policy \(\pi\) and the transition dynamics \(P\).

\subsubsection{Monte Carlo Estimation}\label{monte-carlo-estimation}

The basic idea of Monte Carlo (MC) estimation is to approximate the value function \(V^\pi\) by averaging \emph{empirical returns} observed from sampled trajectories generated under policy \(\pi\). Since the return is defined as the discounted sum of future rewards, MC methods replace the expectation in the definition of \(V^\pi\) with an average over sampled trajectories.

\textbf{Episodic Assumption.} To make Monte Carlo methods well-defined, we restrict attention to the \emph{episodic setup}, where each trajectory terminates upon reaching a terminal state (and the rewards thereafter are always zero). This ensures that the return is finite and can be computed exactly for each trajectory. Concretely, if an episode terminates at time \(T\), the return starting from time \(t\) is
\begin{equation}
g_t = \sum_{k=0}^{T-t-1} \gamma^k r_{t+k} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^{T-t-1} r_{T-1}.
\label{eq:return-MC}
\end{equation}

\textbf{Algorithmic Form.} Let \(\mathcal{D}(s)\) denote the set of all time indices at which state \(s\) is visited across sampled episodes. Then the Monte Carlo estimate of the value function is
\begin{equation}
\hat{V}(s) = \frac{1}{|\mathcal{D}(s)|} \sum_{t \in \mathcal{D}(s)} g_t.
\label{eq:StateValueMCEstimate}
\end{equation}

There are two common variants:

\begin{itemize}
\tightlist
\item
  \textbf{First-visit MC:} use only the first occurrence of \(s\) in each episode.\\
\item
  \textbf{Every-visit MC:} use all occurrences of \(s\) within an episode.
\end{itemize}

Both variants converge to the same value function in the limit of infinitely many episodes.

\textbf{Incremental Implementation.} Monte Carlo can be written as an incremental stochastic-approximation update that uses the return \(g_t\) as the \emph{target} and a \emph{diminishing step size}. Let \(N(s)\) be the number of (first- or every-) visits to state \(s\) that have been used to update \(\hat V(s)\) so far, and let \(g_t\) be the return computed at a particular visit time \(t\in\mathcal{D}(s)\). Then the MC update is
\begin{equation}
\hat V(s) \;\leftarrow\; \hat V(s) + \alpha_{N(s)}\,\big( g_t - \hat V(s) \big),
\qquad \alpha_{N(s)} > 0 \text{ diminishing.}
\label{eq:mc-incremental}
\end{equation}
A canonical choice is the \emph{sample-average} step size \(\alpha_{N(s)} = 1/N(s)\), which yields the recurrence
\begin{align}
\hat V_{N}(s) = \hat V_{N-1}(s) + \tfrac{1}{N}\big(g_t - \hat V_{N-1}(s)\big)
& = \Big(1-\tfrac{1}{N}\Big)\hat V_{N-1}(s) + \tfrac{1}{N}\, g_t \\
& = \frac{N-1}{N} \frac{1}{N-1} \sum_{i=1}^{N-1} g_{t,i} + \frac{1}{N} g_t \\
& = \frac{1}{N} \sum_{i=1}^N g_{t,i}
\end{align}
so that \(\hat V_{N}(s)\) equals the average of the \(N\) observed returns for \(s\) (i.e., Eq. \eqref{eq:StateValueMCEstimate}). In the above equation, I have used \(g_{t,i}\) to denote the \(i\)-th return before \(g_t\) was collected (and \(g_t = g_{t,N}\)). More generally, any diminishing schedule satisfying
\[
\sum_{n=1}^\infty \alpha_n = \infty, \qquad \sum_{n=1}^\infty \alpha_n^2 < \infty
\]
(e.g., \(\alpha_n = c/(n+t_0)^p\) with \(1/2 < p \le 1\)) also ensures consistency in the tabular setting. In first-visit MC, \(N(s)\) increases by one per episode at most; in every-visit MC, \(N(s)\) increases at each occurrence of \(s\) within an episode.

\textbf{Theoretical Guarantees.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unbiasedness:} For any state \(s\), the return \(g_t\) is an unbiased sample of \(V^\pi(s)\).\\
  \[
  \mathbb{E}[g_t \mid s_t = s] = V^\pi(s).
  \]
\item
  \textbf{Consistency:} By the law of large numbers, as the number of episodes grows,
  \[
  \hat{V}(s) \xrightarrow{\text{a.s.}} V^\pi(s).
  \]
\item
  \textbf{Asymptotic Normality:} The MC estimator converges at rate \(O(1/\sqrt{N})\), where \(N\) is the number of episodes used for the estimation.
\end{enumerate}

\textbf{Limitations.} Despite its conceptual simplicity, MC estimation suffers from several drawbacks:

\begin{itemize}
\item
  It requires \emph{episodes to terminate}, making it unsuitable for continuing tasks without artificial truncation.
\item
  It can only update value estimates \emph{after an episode ends}, which is data-inefficient.
\item
  While unbiased, MC estimates often have \emph{high variance}, leading to slow convergence.
\end{itemize}

These limitations motivate the study of \emph{Temporal-Difference (TD) learning}, which updates value estimates online and can handle continuing tasks.

\subsubsection{Temporal-Difference Learning}\label{temporal-difference-learning}

While Monte Carlo methods estimate value functions by averaging full returns from complete episodes, Temporal-Difference (TD) learning provides an alternative approach that updates value estimates \emph{incrementally} after each step of interaction with the environment. The key idea is to combine the sampling of Monte Carlo with the \emph{bootstrapping} of dynamic programming.

\textbf{High-Level Intuition.} TD learning avoids waiting until the end of an episode by using the Bellman consistency equation as a basis for updates. Recall that for any policy \(\pi\), the Bellman consistency equation reads:
\begin{equation}
V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} \left[ R(s,a) + \gamma \mathbb{E}_{s' \sim P(s' \mid s, a)} V(s')  \right].
\label{eq:InfiniteHorizonBellmanConsistencyRestate}
\end{equation}
At a high level, TD learning turns the expectation in Bellman equation into sampling.
At each step, it updates the current estimate of the value function toward a \emph{one-step bootstrap target}: the immediate reward plus the discounted value of the next state. This makes TD methods more data-efficient and applicable to continuing tasks without terminal states.

\textbf{Algorithmic Form.} Suppose the agent is in state \(s_t\), takes action \(a_t \sim \pi(\cdot \mid s_t)\), receives reward \(r_t\), and transitions to \(s_{t+1}\). The TD(0) update rule is
\begin{equation}
\hat{V}(s_t) \;\leftarrow\; \hat{V}(s_t) + \alpha \big[ r_t + \gamma \hat{V}(s_{t+1}) - \hat{V}(s_t) \big],
\label{eq:TDZeroUpdate}
\end{equation}
where \(\alpha \in (0,1]\) is the learning rate.

The term inside the brackets,
\begin{equation}
\delta_t = r_t + \gamma \hat{V}(s_{t+1}) - \hat{V}(s_t),
\label{eq:TDError}
\end{equation}
is called the TD error. It measures the discrepancy between the current value estimate and the bootstrap target. The algorithm updates \(\hat{V}(s_t)\) in the direction of reducing this error.

\textbf{Theoretical Guarantees.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Convergence in the Tabular Case:} If each state is visited infinitely often and the learning rate sequence satisfies
  \[
  \sum_t \alpha_t = \infty, \; \sum_t \alpha_t^2 < \infty
  \]
  then TD(0) converges almost surely to the true value function \(V^\pi\). For example, choosing \(\alpha_t = 1/(t+1)\) satisfies this condition. Section \ref{value-rl-convergence-td} provides a detailed proof of the convergence of TD learning.
\item
  \textbf{Bias--Variance Tradeoff:}

  \begin{itemize}
  \item
    The TD target uses the current estimate \(\hat{V}(s_{t+1})\) rather than the true value, which introduces \emph{bias}.
  \item
    However, it has significantly \emph{lower variance} than Monte Carlo estimates, often leading to faster convergence in practice.
  \end{itemize}

  To see this, note that for TD(0), the target is a one-step bootstrap:
  \[
  y_t = r_t + \gamma \hat{V}(s_{t+1}).
  \]
  This replaces the true value \(V^\pi(s_{t+1})\) with the \emph{current estimate} \(\hat{V}(s_{t+1})\). As a result, \(y_t\) is \emph{biased} relative to the true return. However, since it depends only on the immediate reward and the next state, the variance of \(y_t\) is \emph{much lower} than that of the Monte Carlo target.
\end{enumerate}

\textbf{Limitations.}

\begin{itemize}
\item
  TD(0) relies on bootstrapping, which introduces bias relative to Monte Carlo methods.
\item
  Convergence can be slow if the learning rate is not chosen carefully.
\end{itemize}

In summary, Temporal-Difference learning addresses the major limitations of Monte Carlo estimation: it works in \emph{continuing tasks}, updates \emph{online} at each step, and is generally more \emph{sample-efficient}. However, it trades away unbiasedness for bias--variance efficiency, motivating further extensions such as multi-step TD and TD(\(\lambda\)).

\subsubsection{Multi-Step TD Learning}\label{PE-MultiStepTD}

Monte Carlo methods use the \emph{full return} \(g_t\), while TD(0) uses a \emph{one-step bootstrap}. Multi-step TD learning generalizes these two extremes by using \(n\)-step returns as targets. In this way, multi-step TD interpolates between Monte Carlo and TD(0).

\textbf{High-Level Intuition.} The motivation is to balance the high variance of Monte Carlo with the bias of TD(0). Instead of waiting for a full return (MC) or using only one step of bootstrapping (TD(0)), multi-step TD uses partial returns spanning \(n\) steps of real rewards, followed by a bootstrap. This provides a flexible tradeoff between bias and variance.

\textbf{Algorithmic Form.} The \(n\)-step return starting from time \(t\) is defined as
\begin{equation}
g_t^{(n)} = r_t + \gamma r_{t+1} + \dots + \gamma^{n-1} r_{t+n-1} + \gamma^n \hat{V}(s_{t+n}).
\label{eq:nStepReturn}
\end{equation}

The \(n\)-step TD update is
\begin{equation}
\hat{V}(s_t) \;\leftarrow\; \hat{V}(s_t) + \alpha \big[ g_t^{(n)} - \hat{V}(s_t) \big],
\label{eq:nStepTDUpdate}
\end{equation}
where \(g_t^{(n)}\) replaces the one-step target in TD(0) \eqref{eq:TDZeroUpdate}.

\begin{itemize}
\item
  For \(n=1\): the method reduces to TD(0).
\item
  For \(n=T-t\) (the full episode length): the method reduces to Monte Carlo.
\end{itemize}

\textbf{Theoretical Guarantees.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Convergence in the Tabular Case:} With suitable learning rates and sufficient exploration, \(n\)-step TD converges to \(V^\pi\).
\item
  \textbf{Bias--Variance Tradeoff:}

  \begin{itemize}
  \item
    Larger \(n\): lower bias, higher variance (closer to Monte Carlo).
  \item
    Smaller \(n\): higher bias, lower variance (closer to TD(0)).
  \item
    Intermediate \(n\) provides a balance that often yields faster learning in practice.
  \end{itemize}
\end{enumerate}

\textbf{Limitations.}

\begin{itemize}
\item
  Choosing the right \(n\) is problem-dependent: too small and bias dominates; too large and variance grows.
\item
  Requires storing \(n\)-step reward sequences before updating, which can increase memory and computation.
\end{itemize}

In summary, multi-step TD unifies Monte Carlo and TD(0) by introducing \(n\)-step returns. It allows practitioners to \emph{tune the bias--variance tradeoff} by selecting \(n\). Later, we will see how TD(\(\lambda\)) averages over all \(n\)-step returns in a principled way, further smoothing this tradeoff.

\subsubsection{\texorpdfstring{Eligibility Traces and TD(\(\lambda\))}{Eligibility Traces and TD(\textbackslash lambda)}}\label{TDlambda}

So far, we have seen that Monte Carlo methods use \emph{full returns} \(g_t\), while TD(0) uses a \emph{one-step bootstrap}. Multi-step TD methods generalize between these two extremes by using \(n\)-step returns. However, a natural question arises: \emph{can we combine information from all possible \(n\)-step returns in a principled way?}

This motivates TD(\(\lambda\)), which blends multi-step TD methods into a single algorithm using \emph{eligibility traces}.

\textbf{High-Level Intuition.} TD(\(\lambda\)) introduces a parameter \(\lambda \in [0,1]\) that controls the weighting of \(n\)-step returns:

\begin{itemize}
\item
  \(\lambda = 0\): reduces to TD(0), relying only on one-step bootstrapping.
\item
  \(\lambda = 1\): reduces to Monte Carlo, relying on full returns.
\item
  \(0 < \lambda < 1\): interpolates smoothly between these two extremes by averaging all \(n\)-step returns with exponentially decaying weights.
\end{itemize}

Formally, the \(\lambda\)-return is
\begin{equation}
g_t^{(\lambda)} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} g_t^{(n)},
\label{eq:LambdaReturn}
\end{equation}
where \(g_t^{(n)}\) is the \(n\)-step return defined in \eqref{eq:nStepReturn}.

\begin{remark}
To make the \(\lambda\)-return well defined, we consider two cases.

\textbf{Episodic Case: Well-posed.} If an episode terminates at time \(T\), let \(N=T-t\) be the remaining steps. Then
\begin{equation}
\begin{split}
g_t^{(\lambda)}
& =
(1-\lambda)\sum_{n=1}^{N-1}\lambda^{\,n-1} \, g_t^{(n)}
\;+\;
\lambda^{\,N-1}\, g_t^{(N)}, \\
& = (1-\lambda)\sum_{n=1}^{N}\lambda^{\,n-1} \, g_t^{(n)}
\;+\;
\lambda^{N}\, g_t^{(N)},
\end{split}
\label{eq:lambda-return-episodic}
\end{equation}
where \(g_t^{(n)}\) is the \(n\)-step return (Eq. \eqref{eq:nStepReturn}) and \(g_t^{(N)}\) is the \emph{full} Monte Carlo return (Eq. \eqref{eq:return-MC}).

This expression is well-defined for all \(\lambda\in[0,1]\). Note that the weights form a convex combination:
\[
(1-\lambda)\sum_{n=1}^{N-1}\lambda^{n-1} + \lambda^{N-1}
= 1-\lambda^{N-1}+\lambda^{N-1} = 1.
\]

\textbf{Continuing Case: Limit.} Taking \(\lambda\uparrow 1\) in \eqref{eq:lambda-return-episodic} gives
\[
\lim_{\lambda\uparrow 1} g_t^{(\lambda)}
= g_t^{(N)} = g_t,
\]
so the \(\lambda\)-return \emph{reduces to the Monte Carlo return} at \(\lambda=1\). For continuing tasks (no terminal \(T\)), \(\lambda=1\) is conventionally defined by this same limiting argument, yielding the infinite-horizon discounted return when \(\gamma<1\).
\end{remark}

\textbf{Eligibility Traces.} Naively computing \(g_t^{(\lambda)}\) would require storing and combining infinitely many \(n\)-step returns, which is impractical. Instead, TD(\(\lambda\)) uses eligibility traces to implement this efficiently online.

An eligibility trace is a temporary record that tracks how much each state is ``eligible'' for updates based on how recently and frequently it has been visited. Specifically, for each state \(s\), we maintain a trace \(z_t(s)\) that evolves as
\begin{equation}
z_t(s) = \gamma \lambda z_{t-1}(s) + \mathbf{1}\{s_t = s\},
\label{eq:EligibilityTrace}
\end{equation}
where \(\mathbf{1}\{s_t = s\}\) is an indicator that equals 1 if state \(s\) is visited at time \(t\), and 0 otherwise.

\textbf{TD(\(\lambda\)) Update Rule.} At each time step \(t\), we compute the TD error
\[
\delta_t = r_t + \gamma \hat{V}(s_{t+1}) - \hat{V}(s_t),
\]
as in \eqref{eq:TDError}. Then, for each state \(s\), we update
\begin{equation}
\hat{V}(s) \;\leftarrow\; \hat{V}(s) + \alpha \, \delta_t \, z_t(s).
\label{eq:TDLambdaUpdate}
\end{equation}

Thus, all states with nonzero eligibility traces are updated simultaneously, with the magnitude of the update determined by both the TD error and the eligibility trace. See Proposition \ref{prp:ForwardBackwardEquivalence} below for a justification.

\textbf{Theoretical Guarantees.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In the tabular case, TD(\(\lambda\)) converges almost surely to the true value function \(V^\pi\) under the usual stochastic approximation conditions (sufficient exploration, decaying step sizes).
\item
  The parameter \(\lambda\) directly controls the bias--variance tradeoff:

  \begin{itemize}
  \item
    Smaller \(\lambda\): more bootstrapping, more bias but lower variance.
  \item
    Larger \(\lambda\): less bootstrapping, less bias but higher variance.
  \end{itemize}
\item
  TD(\(\lambda\)) can be shown to converge to the fixed point of the \(\lambda\)-operator, which is itself a contraction mapping.
\end{enumerate}

In summary, eligibility traces provide an elegant mechanism to combine the advantages of Monte Carlo and TD learning. TD(\(\lambda\)) introduces a spectrum of algorithms: at one end TD(0), at the other Monte Carlo, and in between a family of methods balancing bias and variance. In practice, intermediate values such as \(\lambda \approx 0.9\) often work well.

\begin{proposition}[Forward–Backward Equivalence]
\protect\hypertarget{prp:ForwardBackwardEquivalence}{}\label{prp:ForwardBackwardEquivalence}Consider one episode \(s_0,a_0,r_0,\ldots,s_T\) with \(\hat V(s_T)=0\). Let the \textbf{forward view} apply updates at the end of the episode:
\[
\hat V(s_t) \leftarrow \hat V(s_t) + \alpha \big[g_t^{(\lambda)}-\hat V(s_t)\big],
\quad t=0,\ldots,T-1,
\]
where \(g_t^{(\lambda)}\) is the \(\lambda\)-return in \eqref{eq:LambdaReturn} with the \(n\)-step returns \(g_t^{(n)}\) from \eqref{eq:nStepReturn}, and where \(\hat V\) is kept fixed while computing all \(g_t^{(\lambda)}\).

Let the \textbf{backward view} run through the episode once, using the TD error \(\delta_t\) from \eqref{eq:TDError} and eligibility traces \(z_t(s)\) from \eqref{eq:EligibilityTrace}, and then apply the cumulative update
\[
\Delta_{\text{back}} \hat V(s) \;=\; \alpha \sum_{t=0}^{T-1} \delta_t\, z_t(s).
\]

Then, for every state \(s\),
\[
\Delta_{\text{back}} \hat V(s)
\;=\;
\alpha \sum_{t:\, s_t=s}\big[g_t^{(\lambda)}-\hat V(s_t)\big],
\]
i.e., the net parameter change produced by \eqref{eq:TDLambdaUpdate} equals that of the \(\lambda\)-return updates.
\end{proposition}

\begin{proof}
Fix a state \(s\). Using \eqref{eq:EligibilityTrace},
\[
z_t(s)=\sum_{k=0}^{t}(\gamma\lambda)^{\,t-k}\,\mathbf{1}\{s_k=s\}.
\]
Hence
\[
\sum_{t=0}^{T-1}\delta_t z_t(s)
=\sum_{t=0}^{T-1}\delta_t \sum_{k=0}^{t}(\gamma\lambda)^{\,t-k}\mathbf{1}\{s_k=s\}
=\sum_{k:\,s_k=s}\; \sum_{t=k}^{T-1} (\gamma\lambda)^{\,t-k}\delta_t .
\tag{1}
\]

Write \(\delta_t=r_t+\gamma\hat V(s_{t+1})-\hat V(s_t)\) and split the inner sum:
\[
\sum_{t=k}^{T-1} (\gamma\lambda)^{t-k}\delta_t
= \underbrace{\sum_{t=k}^{T-1} \gamma^{t-k}\lambda^{t-k} r_t}_{\text{(A)}}
+ \underbrace{\sum_{t=k}^{T-1}\gamma^{t-k}\lambda^{t-k}(\gamma\hat V(s_{t+1})-\hat V(s_t))}_{\text{(B)}}.
\]

Term (B) telescopes. Shifting index in the first part of (B),
\[
\sum_{t=k}^{T-1}\gamma^{t-k}\lambda^{t-k}\gamma \hat V(s_{t+1})
= \sum_{t=k+1}^{T}\gamma^{t-k}\lambda^{t-1-k}\hat V(s_t).
\]
Therefore
\[
\text{(B)}=
-\hat V(s_k) 
+ \sum_{t=k+1}^{T-1}\gamma^{t-k}\lambda^{t-1-k}(1-\lambda)\hat V(s_t)
+ \underbrace{\gamma^{T-k}\lambda^{T-1-k}\hat V(s_T)}_{=\,0}.
\tag{2}
\]

Combining (A) and (2), and reindexing with \(n=t-k\),
\[
\sum_{t=k}^{T-1} (\gamma\lambda)^{t-k}\delta_t
= -\hat V(s_k)
+ \sum_{n=0}^{T-1-k}\gamma^{n}\lambda^{n} r_{k+n}
+ (1-\lambda)\sum_{n=1}^{T-1-k}\gamma^{n}\lambda^{n-1}\hat V(s_{k+n}).
\tag{3}
\]

On the other hand, expanding the \(\lambda\)-return \eqref{eq:LambdaReturn},
\[
\begin{aligned}
g_k^{(\lambda)}
&=(1-\lambda)\sum_{n=1}^{T-k}\lambda^{n-1}
\Bigg(\sum_{m=0}^{n-1}\gamma^{m} r_{k+m} + \gamma^{n}\hat V(s_{k+n})\Bigg) + \lambda^{T-k} g_k^{(T-k)}\\
&= \sum_{n=0}^{T-1-k}\gamma^{n}\lambda^{n} r_{k+n}
+ (1-\lambda)\sum_{n=1}^{T-1-k}\gamma^{n}\lambda^{n-1}\hat V(s_{k+n}),
\end{aligned}
\tag{4}
\]
where we used that \(\hat V(s_T)=0\). Comparing (3) and (4) yields
\[
\sum_{t=k}^{T-1} (\gamma\lambda)^{t-k}\delta_t
= g_k^{(\lambda)} - \hat V(s_k).
\tag{5}
\]

Substituting (5) into (1) and multiplying by \(\alpha\) completes the proof.
\end{proof}

\begin{example}[Policy Evaluation (MC and TD Family)]
\protect\hypertarget{exm:PolicyEvaluationRandomWalk}{}\label{exm:PolicyEvaluationRandomWalk}We consider the classic random-walk MDP with terminal states:

\begin{itemize}
\tightlist
\item
  \textbf{States:} \(\{0,1,2,3,4,5,6\}\), where \(0\) and \(6\) are terminal; nonterminal states are \(1{:}5\).
\item
  \textbf{Actions:} \(\{-1,+1\}\) (``Left''/``Right'').
\item
  \textbf{Dynamics:} From a nonterminal state \(s\in\{1,\dots,5\}\), action \(-1\) moves to \(s-1\), and action \(+1\) moves to \(s+1\).
\item
  \textbf{Rewards:} Transitioning into state \(6\) yields reward \(+1\); all other transitions yield \(0\).
\item
  \textbf{Discount:} \(\gamma=1\) (episodic task). Episodes start at state \(s_0=3\) and terminate upon reaching \(\{0,6\}\).
\end{itemize}

We evaluate the \emph{equiprobable policy} \(\pi\) that chooses Left/Right with probability \(1/2\) each at every nonterminal state. Under this policy, the true state-value function on nonterminal states \(s\in\{1,\dots,5\}\) is
\begin{equation}
V^\pi(s) \;=\; \frac{s}{6}.
\label{eq:trueV-rw}
\end{equation}

We compare four \emph{tabular policy-evaluation} methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Monte Carlo (MC), first-visit} --- using full returns as target.
\item
  \textbf{TD(0)} --- one-step bootstrap.
\item
  \textbf{\(n\)-step TD} --- here we use \(n=3\) (intermediate between MC and TD(0)).
\item
  \textbf{TD(\(\lambda\))} --- accumulating eligibility traces (we illustrate with \(\lambda=0.9\)).
\end{enumerate}

All methods estimate \(V^\pi\) from trajectories generated by \(\pi\).

\textbf{Error Metric.}
We report the \emph{mean-squared error (MSE)} over nonterminal states after each episode:
\begin{equation}
\mathrm{MSE}_t \;=\; \frac{1}{5}\sum_{s=1}^{5}\big(\hat V_t(s)-V^\pi(s)\big)^2,
\label{eq:mse-metric}
\end{equation}
where \(V^\pi\) is given by \eqref{eq:trueV-rw}. Curves are averaged over multiple random seeds.

\textbf{Fixed Step Sizes.}
We first use a fixed step size \(\alpha=0.1\) for all methods. Fig. \ref{fig:policy-evaluation-random-walk-fixed-step-size} shows the trajectories of MSE versus number of episodes. We can see that, when using a constant step size, these methods do not converge to exactly the true value function, but to a small neighborhood. In addition, if the algorithm initially decays very fast, then the final variance is larger. For example, MC initially decays very fast, but has a higher variance, whereas TD(0) initially decays slower, but has a lower final variance. This agrees with the theoretical analysis in \citep{kearns2000bias}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/Value-RL/mc_td_comparison_constant_step} 

}

\caption{Policy Evaluation, MC versus TD Family, Fixed Step Size}\label{fig:policy-evaluation-random-walk-fixed-step-size}
\end{figure}

\textbf{Diminishing Step Sizes.}
We then use a diminishing step size for the TD family:
\begin{equation}
\alpha_t(s) \;=\; \frac{c}{\big(N_t(s)+t_0\big)^p}, 
\qquad \tfrac{1}{2} < p \le 1,
\label{eq:per-state-decay}
\end{equation}
where \(N_t(s)\) counts how many times \(V(s)\) has been updated up to time \(t\). A common choice is \(p=1\) with moderate \(c>0\) and \(t_0>0\).

Fig. \ref{fig:policy-evaluation-random-walk-diminishing-step-size} shows the MSE versus episodes for MC, TD(0), 3-step TD, and TD(\(\lambda\)) under the diminishing step-size. Observe that all algorithms converge to the true value function under the diminishing step size schedule.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/Value-RL/mc_td_comparison_diminishing_step} 

}

\caption{Policy Evaluation, MC versus TD Family, Diminishing Step Size}\label{fig:policy-evaluation-random-walk-diminishing-step-size}
\end{figure}

You are encouraged to play with the parameters of these algorithms in the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/policy_evaluation_mc_td.py}{here}.
\end{example}

\subsection{Convergence Proof of TD Learning}\label{value-rl-convergence-td}

\textbf{Setup.} Consider a tabular MDP with finite state space \(\mathcal{S}\) and action space \(\mathcal{A}\), and a discount factor \(\gamma \in [0,1)\). Assume the reward function is bounded, for example, \(R(s,a) \in [0,1]\) for any \((s,a) \in \mathcal{S} \times \mathcal{A}\). Let \(\pi\) be a stochastic policy and \(V^\pi\) be the true value function associated with \(\pi\), the target we wish to estimate from interaction data. Denote
\[
\mathcal{F}_t = \sigma(s_0,a_0,r_0,\dots,s_{t-1},a_{t-1},r_{t-1}),
\]
as the \(\sigma\) algebra of all state-action-reward information up to time \(t-1\).

\textbf{TD(0) Update.} We maintain a tabular estimate \(V_t\) of the true value \(V^\pi\). On visiting \(s_t\) and observing \((s_t, a_t, r_t, s_{t+1})\), the TD(0) algorithm performs
\begin{equation}
V_{t+1}(s_t) = V_t(s_t) + \alpha_t(s_t) \delta_t,
\label{eq:TD0update-proof}
\end{equation}
where \(\delta_t\) is the TD error
\[
\delta_t = r_t + \gamma V_{t}(s_{t+1}) - V_t(s_t).
\]
The update \eqref{eq:TD0update-proof} only changes the value at \(s_t\), leaving the value at other states unchanged.

\textbf{Robbins--Monro Step Size.} We assume the step size \(\alpha\) satisfy the Robbins--Monro condition. That is, for any \(s \in \mathcal{S}\):
\[
\alpha_t(s) >0, \quad \sum_{t: s_t = s} \alpha_t(s) = \infty, \quad \sum_{t: s_t = s} \alpha^2_t(s) < \infty.
\]

\textbf{Stationary Distribution.}
Assume the Markov chain over \(\mathcal{S}\) induced by \(\pi\) is ergodic, then a unique stationary state distribution \(\mu^\pi\) exists and satisfy:
\begin{equation}
\mu^\pi(s') = \sum_{s \in \mathcal{S}} \mu^\pi(s) \left( \sum_{a \in \mathcal{A}}  \pi(a \mid s) P(s' \mid s, a) \right), \quad \forall s' \in \mathcal{S}.
\label{eq:StationaryDistribution-Tabular}
\end{equation}
If we denote
\begin{equation}
P^\pi(s' \mid s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) P(s' \mid s,a),
\label{eq:TabularStateOnlyTransition}
\end{equation}
as the \(\pi\)-induced state-only transition dynamics, then condition \eqref{eq:StationaryDistribution-Tabular} is equivalent to
\begin{equation}
\mu^\pi(s') = \sum_{s \in \mathcal{S}} \mu^\pi(s) P^\pi (s' \mid s), \quad \forall s' \in \mathcal{S}.
\label{eq:StationaryDistribution-Tabular-State-Only}
\end{equation}

See \eqref{eq:StationaryDistribution-definition} for a generalization to continuous MDP.

\textbf{Bellman Operator.} For any \(V:\mathcal S\to\mathbb R\), define the Bellman operator associated with \(\pi\) as \(T^\pi V:\mathcal S\to\mathbb R\) by
\begin{equation}
(T^\pi V)(s)\;=\;\sum_{a\in\mathcal A}\pi(a\mid s)\sum_{s'\in\mathcal S} P(s'\mid s,a)\,\Big( R(s,a)+\gamma\,V(s')\Big).
\label{eq:BellmanOperatorRestate-TDConvergenceProof}
\end{equation}
We know that the operator \(T^\pi\) is a \(\gamma\)-contraction in \(\|\cdot\|_\infty\). Hence it has a unique fixed point \(V^\pi\) satisfying \(V^\pi=T^\pi V^\pi\).

The following theorem states the almost sure convergence of TD learning iterates to the true value function.

\begin{theorem}[TD(0) Convergence (Tabular)]
\protect\hypertarget{thm:TD0ConvergenceTabular}{}\label{thm:TD0ConvergenceTabular}Under the tabular MDP setup and assumptions above, the TD(0) iterates \(V_t\) generated by \eqref{eq:TD0update-proof} converge almost surely to \(V^\pi\).
\end{theorem}

To prove this theorem, we need the following two lemmas.

\begin{lemma}[Robbins-Siegmund Lemma]
\protect\hypertarget{lem:RobbinsSiegmund}{}\label{lem:RobbinsSiegmund}Let \((X_t)_{t\ge 0}\) be nonnegative and adapted to \((\mathcal F_t)\). Suppose there exist nonnegative \((\beta_t),(\gamma_t),(\xi_t)\) with \(\sum_t \gamma_t<\infty\) and \(\sum_t \xi_t<\infty\) such that
\[
\mathbb E[X_{t+1}\mid \mathcal F_t]\;\le\;(1+\gamma_t)X_t\;-\;\beta_t\;+\;\xi_t\qquad\text{almost surely}
\]
Then \(X_t\) converges almost surely to a finite random variable and \(\sum_t \beta_t<\infty\) almost surely.
\end{lemma}

This lemma is from \citep{robbins1971convergence}.

\begin{lemma}
\protect\hypertarget{lem:TDConvergenceLemma}{}\label{lem:TDConvergenceLemma}Let \(\mu^\pi\) be the stationary distribution in \eqref{eq:StationaryDistribution-Tabular-State-Only}, \(D=\mathrm{diag}(\mu^\pi)\), and \(w:=V-V^\pi\). Then
\[
\langle w,\,D\,(T^\pi V - V)\rangle\;\le\;-(1-\gamma)\,\|w\|_D^2,
\]
where \(\langle x,y\rangle=x^\top y\) and \(\|w\|_D^2=\sum_s \mu^\pi(s)\,w(s)^2\).
\end{lemma}

\begin{proof}
First, for any two value functions \(V, U \in \mathbb{R}^{|\mathcal{S}|}\), we have
\[
(T^\pi V)(s) - (T^\pi U)(s) = \gamma \sum_{a} \pi(a \mid s) \sum_{s'} P(s' \mid s,a) (V(s') - U(s)').
\]
Therefore,
\[
T^\pi V - T^\pi U = \gamma \widetilde P (V - U),
\]
with
\begin{equation}
(\widetilde P u)(s):=\sum_a\pi(a\mid s)\sum_{s'}P(s'\mid s,a)\,u(s').
\label{eq:def-tilde-P}
\end{equation}
With this, we can write
\begin{equation}
\begin{split}
T^\pi V - V & = T^\pi V - V^\pi + V^\pi - V \\
& = T^\pi V - T^\pi V^\pi - ( V - V^\pi ) \\
& = \gamma \widetilde P (V - V^\pi) - ( V - V^\pi ) \\
& = (\gamma \widetilde P - I) ( V - V^\pi ) \\
& = (\gamma \widetilde P - I) w.
\end{split}
\end{equation}
Thus,
\begin{equation}
\langle w, D(T^\pi V - V)\rangle
= -\,w^\top D\,(I-\gamma \widetilde P)\,w
= -\|w\|_D^2 + \gamma\,\langle w, D\,\widetilde P w\rangle.
\label{eq:InnerProductInequality}
\end{equation}

Next, we prove \(\langle w, D\widetilde P w\rangle\le \|w\|_D^2\).

\begin{itemize}
\item
  First, we show \(\|\widetilde P w\|_D\le \|w\|_D\). For any state \(s \in \mathcal{S}\), from \eqref{eq:def-tilde-P}, we have
  \[
  (\widetilde P w)(s) = \sum_{s'} P^\pi (s' \mid s) w(s'),
  \]
  where \(P^\pi(s' \mid s)\) is the \(\pi\)-induced state-only transition in \eqref{eq:TabularStateOnlyTransition}. Since \(P^\pi(\cdot \mid s)\) is a probability distribution, and \(x \mapsto x^2\) is convex, we have
  \[
  ((\widetilde P w)(s))^2 = \left( \sum_{s'} P^\pi (s' \mid s) w(s') \right)^2 \leq \sum_{s'} P^\pi (s' \mid s) w^2(s').
  \]
  Therefore, we have
  \begin{equation}
  \begin{split}
  \Vert \widetilde P w \Vert_D^2 & = \sum_s \mu^{\pi}(s) ((\widetilde P w)(s))^2 \\
  & \leq \sum_s \mu^{\pi}(s) \left( \sum_{s'} P^\pi (s' \mid s) w^2(s') \right) \\
  & = \sum_{s'} \left( \sum_{s} \mu^\pi(s) P^\pi(s' \mid s) \right) w^2(s') \\
  & = \sum_{s'} \mu^\pi (s') w^2 (s') = \Vert w \Vert_D^2.
  \end{split}
  \end{equation}
  where the second-from-last equality holds because \(\mu^\pi\) is the stationary distribution and satisfies \eqref{eq:StationaryDistribution-Tabular-State-Only}.
\item
  Second, we write
  \[
  \langle w, D\widetilde P w\rangle = \langle D^{0.5} w, D^{0.5} \widetilde P w \rangle \leq \Vert D^{0.5} w \Vert \cdot \Vert D^{0.5} \widetilde P w \Vert = \Vert w \Vert_D \cdot \Vert \widetilde P w \Vert_D \leq \Vert w \Vert_D^2.
  \]
\end{itemize}

Plugging this back to \eqref{eq:InnerProductInequality}, we obtain
\[
\langle w, D(T^\pi V - V) \rangle \leq - \Vert w \Vert_D^2 + \gamma \Vert w \Vert_D^2,
\]
proving the desired result in the lemma.
\end{proof}

We are now ready to prove Theorem \ref{thm:TD0ConvergenceTabular}.

\begin{proof}
\textbf{Step 1 (TD as stochastic approximation).} For the TD error
\[
\delta_t=r_{t}+\gamma V_t(s_{t+1})-V_t(s_t),
\]
we have the conditional expectation
\[
\mathbb E[\delta_t\mid \mathcal F_t, s_t]
=\sum_{a}\pi(a\mid s_t)\sum_{s'}P(s'\mid s_t,a)\Big(R(s_t,a)+\gamma V_t(s')\Big)-V_t(s_t)
=\big(T^\pi V_t - V_t\big)(s_t).
\]
Define the ``noise'':
\[
\eta_{t+1}:=\delta_t-\mathbb E[\delta_t\mid \mathcal F_t,s_t].
\]
Then \(\mathbb E[\eta_{t+1}\mid \mathcal F_t,s_t]=0\) and the TD update is equivalent to
\begin{equation}
V_{t+1}(s_t)=V_t(s_t)+\alpha_t(s_t)\Big( \big(T^\pi V_t - V_t\big)(s_t) + \eta_{t+1}\Big),
\label{eq:TDStochasticApproximation}
\end{equation}
while learving all other coordinates unchanged. Because rewards are uniformly bounded, we know that \(V_t\) remains bounded. Hence, \(\mathbb E[\eta_{t+1}^2\mid \mathcal F_t, s_t]\) is uniformly bounded. Equation \eqref{eq:TDStochasticApproximation} shows that the TD update can be seen as a stochastic approximation to the Bellman operator \eqref{eq:BellmanOperatorRestate-TDConvergenceProof}.

\textbf{Step 2 (Lyapunov drift).} Let \(D=\mathrm{diag}(\mu^\pi)\), a diagonal matrix whose diagonal entries are the probabilities in \(\mu^\pi\).
Define the Lyapunov function
\[
\mathcal L(V)=\frac{1}{2} \|V-V^\pi\|_D^2=\frac{1}{2} \sum_s \mu^\pi(s)\,\big(V(s)-V^\pi(s)\big)^2.
\]
Let \(w_t:=V_t - V^\pi\). Since only the \(s_t\)-coordinate changes at time \(t\), we have
\begin{equation}
\begin{split}
\mathcal L(V_{t+1})-\mathcal L(V_t)
&=\frac{1}{2} \mu^\pi(s_t)\Big(( \underbrace{V_t(s_t)+\alpha_t\delta_t}_{V_{t+1}(s_t)} - V^\pi(s_t) )^2 -(V_t(s_t)-V^\pi(s_t))^2\Big)\\
&= \mu^\pi(s_t)\,\alpha_t\,\delta_t\,w_t(s_t)\;+\;\frac{1}{2} \mu^\pi(s_t)\,\alpha_t^2\,\delta_t^{\,2}.
\end{split}
\label{eq:DifferenceBetweenLyapunov}
\end{equation}
Define \(g_t := T^\pi V_t - V_t\). Taking conditional expectation given \(\mathcal F_t\) and i.i.d. \(s_t\sim \mu^\pi\),
\begin{equation}
\begin{split}
\mathbb E\!\left[\mathcal L(V_{t+1})-\mathcal L(V_t)\mid \mathcal F_t\right]
&= \alpha_t\,\mathbb E\!\left[\mu^\pi(s_t)\,w_t(s_t)\,g_t(s_t)\mid \mathcal F_t\right]
+ \frac{1}{2} \alpha_t^2\,\mathbb E\!\left[\mu^\pi(s_t)\,\delta_t^{\,2}\mid \mathcal F_t\right]\\
&= \alpha_t\,\sum_s \mu^\pi(s)\,w_t(s)\,g_t(s) \;+\; \frac{1}{2} \alpha_t^2\,C_t,
\end{split}
\label{eq:DifferenceBetweenLyapunov-ConditionalExpectation}
\end{equation}
where \(C_t:=\mathbb E\!\left[\mu^\pi(s_t)\,\delta_t^{\,2}\mid \mathcal F_t\right]\) is finite because rewards are bounded and \(V_t\) stays bounded. Assume \(C_t \leq C\).

At the same time, by Lemma \ref{lem:TDConvergenceLemma}
\[
\sum_s \mu^\pi(s)\,w_t(s)\,g_t(s)
=\langle w_t, D g_t\rangle
\le -(1-\gamma)\,\|w_t\|_D^2.
\]
Plugging into \eqref{eq:DifferenceBetweenLyapunov-ConditionalExpectation} yields
\begin{equation}
\mathbb E\!\left[\mathcal L(V_{t+1})\mid \mathcal F_t\right]
\;\le\;
\mathcal L(V_t)\;-\;\alpha_t\,(1-\gamma)\,\|w_t\|_D^2\;+\;\frac{1}{2} \alpha_t^2\,C.
\label{eq:TD0ConvergenceProof-RS-Form}
\end{equation}
This is in Robbins--Siegmund form with
\[
X_t:=\mathcal L(V_t),\qquad
\beta_t:=(1-\gamma)\,\alpha_t\,\|w_t\|_D^2,\qquad
\gamma_t:=0,\qquad
\xi_t:=\frac{1}{2} C\,\alpha_t^2.
\]
We have \(\sum_t \xi_t<\infty\) by \(\sum_t \alpha_t^2<\infty\). Therefore \(X_t\) converges a.s. and \(\sum_t \beta_t<\infty\) a.s., which implies \(\sum_t \alpha_t \|w_t\|_D^2<\infty\). Since \(\sum_t \alpha_t=\infty\), it must be that \(\liminf_t \|w_t\|_D=0\).\\
Finally, using \eqref{eq:TD0ConvergenceProof-RS-Form} again and the continuity of the drift, one shows that any subsequential limit of \(V_t\) must satisfy \(T^\pi V - V=0\); by uniqueness of the fixed point, the only possible limit is \(V^\pi\). Hence \(V_t\to V^\pi\) almost surely.
\end{proof}

\subsection{On-Policy Control}\label{on-policy-control}

Monte Carlo (MC) estimation and the TD family evaluate policies directly from interaction---no model required. We now turn evaluation into control via generalized policy iteration (GPI): repeatedly (i) evaluate the current policy from data and (ii) improve it by acting greedily with respect to the new estimates. We first cover on-policy control methods, which estimate and improve the same (typically \(\varepsilon\)-greedy) policy, and then off-policy methods, which learn about a target policy while behaving with a different one.

\subsubsection{Monte Carlo Control}\label{monte-carlo-control}

\textbf{High-level Intuition.}

\begin{itemize}
\item
  \textbf{Goal.} Learn an (approximately) optimal policy by alternating \emph{policy evaluation} and \emph{policy improvement} using only sampled episodes.
\item
  \textbf{Why action-values?} Estimating \(Q^\pi(s,a)\) lets us improve the policy \emph{without a model} by choosing ``\(\arg\max_a Q(s,a)\)''.
\item
  \textbf{Exploration.} Pure greedy improvement can get stuck. MC control keeps the policy \emph{\(\varepsilon\)-soft} (e.g., \(\varepsilon\)-greedy) so that every action has nonzero probability and all state-action pairs continue to be sampled. An \(\varepsilon\)-soft policy is one that never rules out any action: in every state \(s\), each action \(a\) gets at least a small fraction of probability. Formally, in the tabular setup, we have that a policy \(\pi\) is \(\varepsilon\)-soft if and only if
  \begin{equation}
  \forall s, \forall a: \quad \pi(a \mid s) \geq \frac{\varepsilon}{|\mathcal{A}(s)|}, \quad \varepsilon \in (0,1],
  \label{eq:epsilon-soft-policy}
  \end{equation}
  where \(\mathcal{A}(s)\) denotes the set of actions the agent can select at state \(s\).
\item
  \textbf{Coverage mechanisms.} Classic guarantees use either:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi})}
  \tightlist
  \item
    \textbf{Exploring starts (ES):} start each episode from a randomly chosen \((s,a)\) with nonzero probability; or\\
  \item
    \textbf{\(\varepsilon\)-soft / GLIE (Greedy in the Limit with Infinite Exploration):} use \(\varepsilon\)-greedy behavior with \(\varepsilon_t \downarrow 0\) so every \((s,a)\) is visited infinitely often while the policy becomes greedy in the limit.
  \end{enumerate}
\end{itemize}

\textbf{Algorithmic Form.}
We maintain tabular action-value estimates \(Q(s,a)\) and an \emph{\(\varepsilon\)-soft} policy \(\pi\) (\(\varepsilon\)-greedy w.r.t. \(Q\)). After each episode we update \(Q\) from \emph{empirical returns} and then improve \(\pi\).

\textbf{Return from time \(t\):}
\[
g_t = r_t + \gamma r_{t+1} + \dots + \gamma^{T-t} r_T = \sum_{k=0}^{T-t} \gamma^{k} r_{t+k}.
\]

\textbf{First-visit MC update (common choice):}
\begin{equation}
Q(s_t,a_t) \;\leftarrow\; Q(s_t,a_t) + \alpha_{N(s_t,a_t)}\!\left(g_t - Q(s_t,a_t)\right),
\label{eq:MCControl-QUpdate}
\end{equation}
applied only on the first occurrence of \((s_t,a_t)\) in the episode. \emph{Sample-average} learning uses \(\alpha_n = 1/n\) per pair; more generally, use diminishing stepsizes.

\textbf{Policy improvement (\(\varepsilon\)-greedy):}
\begin{equation}
\pi(a|s) \;=\;
\begin{cases}
1-\varepsilon + \dfrac{\varepsilon}{|\mathcal{A}(s)|}, & a \in \arg\max_{a'} Q(s,a'), \\
\dfrac{\varepsilon}{|\mathcal{A}(s)|}, & \text{otherwise}.
\end{cases}
\label{eq:MCControl-PI}
\end{equation}

\textbf{Theoretical Guarantees.}

Assume a tabular episodic MDP and \(\gamma \in [0,1)\).

\begin{itemize}
\item
  \textbf{Convergence with Exploring Starts.} If every state--action pair has nonzero probability of being the \emph{first} pair of an episode (using ES), and each \(Q(s,a)\) is updated toward the true mean return from \((s,a)\) (e.g., via sample averages), then repeated policy evaluation and greedy improvement converge with probability 1 to an optimal deterministic policy. (If one uses an \(\varepsilon\)-greedy improvement, then it converges to an optimal \(\varepsilon\)-soft policy.)
\item
  \textbf{Convergence with \(\varepsilon\)-soft GLIE behavior.} If the behavior policy is GLIE---every \((s,a)\) is visited infinitely often and \(\epsilon_t \to 0\)---and the stepsizes for each \((s,a)\) satisfy the Robbins--Monro conditions \(\sum_{t} \alpha_t(s,a) = \infty,\sum_{t} \alpha_t(s,a)^2 < \infty\), then \(Q(s,a)\) converges to \(Q^\star(s,a)\) for all pairs visited infinitely often, and the \(\varepsilon\)-greedy policy converges almost surely to an optimal policy.
\end{itemize}

\begin{remark}
\textbf{Unbiased but high-variance.} MC targets \(g_t\) are unbiased estimates of action values under the current policy, but can have high variance---especially for long horizons---so convergence can be slower than TD methods. Keeping \(\varepsilon>0\) ensures exploration but limits asymptotic optimality to the best \(\varepsilon\)-soft policy; hence \(\varepsilon_t \downarrow 0\) (GLIE) is recommended for optimality.
\end{remark}

\subsubsection{SARSA (On-Policy TD Control)}\label{sarsa-on-policy-td-control}

\textbf{High-level Intuition.}

\begin{itemize}
\tightlist
\item
  \textbf{Goal.} Turn evaluation into control by updating action values online and improving the same policy that generates data.
\item
  \textbf{Key idea.} Replace Monte Carlo returns with a bootstrapped target. After taking action \(a_t\) in state \(s_t\) and observing \(r_{t}, s_{t+1}\), sample the next action \(a_{t+1}\) from the current policy and update toward \(r_{t} + \gamma Q(s_{t+1}, a_{t+1})\).
\item
  \textbf{On-policy nature.} SARSA evaluates the behavior policy itself, typically an \(\varepsilon\)-greedy policy w.r.t. \(Q\).
\item
  \textbf{Exploration.} Use \(\varepsilon\)-soft behavior so every action keeps nonzero probability. For optimality, let \(\varepsilon_t \downarrow 0\) to obtain GLIE (Greedy in the Limit with Infinite Exploration).
\end{itemize}

\textbf{Algorithmic Form.}

Let \(Q\) be a tabular action-value function and \(\pi_t\) be \(\varepsilon_t\)-greedy w.r.t. \(Q_t\).

\textbf{TD target and error:}
\begin{equation}
y_t = r_{t} + \gamma Q(s_{t+1}, a_{t+1}), \qquad
\delta_t = y_t - Q(s_t, a_t).
\label{eq:SARSA-TDTarget}
\end{equation}

\textbf{SARSA update (one-step):}
\begin{equation}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha_t(s_t,a_t)\, \delta_t.
\label{eq:SARSA-QUpdate}
\end{equation}

\textbf{\(\varepsilon\)-greedy policy improvement:}
\begin{equation}
\pi_{t+1}(a\mid s) =
\begin{cases}
1-\varepsilon_{t+1} + \dfrac{\varepsilon_{t+1}}{|\mathcal A(s)|}, & a \in \arg\max_{a'} Q_{t+1}(s,a'),\\
\dfrac{\varepsilon_{t+1}}{|\mathcal A(s)|}, & \text{otherwise.}
\end{cases}
\label{eq:SARSA-PolicyUpdate}
\end{equation}

\textbf{Variants.}

\begin{itemize}
\item
  \textbf{Expected SARSA} replaces the sampled \(a_{t+1}\) by its expectation under \(\pi_t\) for lower variance:
  \begin{equation}
  y_t = r_{t} + \gamma \sum_a \pi_t(a\mid s_{t+1}) Q(s_{t+1}, a).
  \label{eq:ExpectedSARSA-Target}
  \end{equation}
\item
  \textbf{\(n\)-step SARSA} and \textbf{SARSA(\(\lambda\))} blend multi-step targets; these trade bias and variance similarly to MC vs TD.
\end{itemize}

\textbf{Convergence Guarantees.}

Assume a finite MDP, \(\gamma \in [0,1)\), asynchronous updates, and that each state--action pair is visited infinitely often.

\begin{itemize}
\tightlist
\item
  \textbf{GLIE convergence to optimal policy.} If the behavior is GLIE, i.e., \(\varepsilon_t \downarrow 0\) while ensuring infinite exploration, and stepsizes satisfy the Robbins--Monro conditions, then \(Q_t \to Q^\star\) almost surely and the \(\varepsilon_t\)-greedy behavior becomes greedy in the limit, yielding an optimal policy almost surely.
\end{itemize}

\subsection{Off-Policy Control}\label{off-policy-control}

Off-policy methods learn about a \emph{target} policy \(\pi\) while following a (potentially different) \emph{behavior} policy \(b\) to gather data. This decoupling is useful when:

\begin{itemize}
\item
  you want to \emph{reuse logged data} collected by some \(b\) (e.g., a rule-based controller or a past system),
\item
  you need \emph{safer exploration} by restricting behavior \(b\) while aiming to evaluate or improve a different \(\pi\),
\item
  you want to learn about the \emph{greedy} policy without executing it, which motivates algorithms like Q-learning.
\end{itemize}

In this section we first cover off-policy policy evaluation with \emph{importance sampling}, then show how it can be used to construct an off-policy \emph{Monte Carlo control} scheme in the tabular case. Finally, we present Q-learning.

\subsubsection{Importance Sampling for Policy Evaluation}\label{importance-sampling-for-policy-evaluation}

\textbf{Motivation.} Suppose we have episodes generated by a behavior policy \(b\), but we want the value of a different target policy \(\pi\). For a state value this is \(V^\pi(s) = \mathbb{E}_\pi[g_t \mid s_t=s]\), and for action values \(Q^\pi(s,a) = \mathbb{E}_\pi[g_t \mid s_t=s, a_t=a]\), where
\[
g_t = \sum_{k=0}^{T-t} \gamma^{k} r_{t+k}.
\]
Because the data come from \(b\), the naive sample average is biased. Importance sampling (IS) reweights returns so that expectations under \(b\) equal those under \(\pi\).

A basic \emph{support condition} is required:
\begin{equation}
\text{If } \pi(a\mid s) > 0 \text{ then } b(a\mid s) > 0 \quad \text{for all visited } (s,a).
\label{eq:IS-SupportCondition}
\end{equation}
This ensures that \(\pi\) is absolutely continuous with respect to \(b\) on the experienced trajectories.

\textbf{Importance Sampling (episode-wise).} Consider a trajectory starting at time \(t\):
\[
\tau_t = (s_t, a_t, r_t, s_{t+1}, a_{t+1}, \dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T).
\]
The probability of observing this trajectory conditioned on \(s_t = s\), under policy \(\pi\), is
\[
\mathbb{P}_{\pi}[\tau_t \mid s_t = s] = \pi(a_t \mid s_t) P(s_{t+1} \mid s_t, a_t) \pi(a_{t+1} \mid s_{t+1}) \cdots \pi(a_{T-1} \mid s_{T-1}) P(s_T \mid s_{T-1}, a_{T-1}).
\]
The probability of observing the same trajectory conditioned on \(s_t = s\), under policy \(b\), is
\[
\mathbb{P}_{b}[\tau_t \mid s_t = s] = b(a_t \mid s_t) P(s_{t+1} \mid s_t, a_t) b(a_{t+1} \mid s_{t+1}) \cdots b(a_{T-1} \mid s_{T-1}) P(s_T \mid s_{T-1}, a_{T-1}).
\]
Since the return \(g_t\) is a deterministic function of \(\tau_t\), i.e., applying the reward function \(R\) to state-action pairs, we have that
\begin{equation}
\begin{split}
V^\pi (s) & = \mathbb{E}_{\pi}[g_t \mid s_t = s] = \sum_{\tau_t} g_t \mathbb{P}_\pi [\tau_t \mid s_t = s] \\
& = \sum_{\tau_t} g_t \mathbb{P}_b[\tau_t \mid s_t = s] \left(\frac{\mathbb{P}_\pi [\tau_t \mid s_t = s]}{\mathbb{P}_b[\tau_t \mid s_t = s]} \right) \\
& = \sum_{\tau_t} \left( \frac{\pi(a_t \mid s_t) \pi(a_{t+1} \mid s_{t+1}) \cdots \pi(a_{T-1} \mid s_{T-1}) }{b(a_t \mid s_t)  b(a_{t+1} \mid s_{t+1}) \cdots b(a_{T-1} \mid s_{T-1})} \right) g_t \mathbb{P}_b [\tau_t \mid s_t = s]
\end{split}
\label{eq:IS-Trajectory-LikelihoodRatio}
\end{equation}
Therefore, define the \emph{likelihood ratio}
\begin{equation}
\rho_{t:T-1} = \prod_{k=t}^{T-1} \frac{\pi(a_k \mid s_k)}{b(a_k \mid s_k)},
\label{eq:IS-LikelihoodRatio}
\end{equation}
we have
\begin{equation}
V^\pi(s) = \mathbb{E}_b\left[\rho_{t:T-1} g_t \mid s_t=s\right]. 
\label{eq:IS-Value}
\end{equation}
Similarly, we have
\begin{equation}
Q^\pi(s,a) =  \mathbb{E}_b\!\left[\rho_{t:T-1} g_t \mid s_t=s, a_t=a\right].
\label{eq:IS-ActionValue}
\end{equation}
Given \(n\) episodes, the ordinary IS estimator for \(Q^\pi\) at the first visit of \((s,a)\) is
\[
\hat Q_n^{\text{IS}}(s,a) = \frac{1}{N_n(s,a)} \sum_{i=1}^n \mathbf{1}\{(s,a)\text{ visited}\}\, \rho_{t_i:T_i-1}^{(i)}\, g_{t_i}^{(i)},
\]
where \(N_n(s,a)\) counts the number of first visits of \((s,a)\). In words, to estimate the \(Q\) value of the target policy \(\pi\) using trajectories of the behavior policy \(b\), we need to reweight the return \(g_t\) by the likelihood ratio \(\rho_{t:T-1}\). Note that the likelihood ratio does not require knowledge about the transition dynamics.

\textbf{Algorithmic Form: Off-policy Monte Carlo Policy Evaluation.}

\textbf{Input:} behavior \(b\), target \(\pi\), episodes from \(b\)\\
\textbf{For each episode} \((s_0,a_0,r_0,s_1,\dots,s_{T-1},a_{T-1},r_{T-1},s_T)\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For \(t=T-1,\dots,0\) compute episode-wise likelihood ratio \(\rho_{t:T-1}\) and return \(g_t\),
\item
  For first visits of \((s_t,a_t)\), update
  \[
  Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha_{N(s_t,a_t)}\big(\rho_{t:T-1} g_t - Q(s_t,a_t)\big).
  \]
  Use sample averages \(\alpha_n=1/n\) or Robbins-Monro stepsizes.
\end{enumerate}

\textbf{Guarantees.} Under the support condition and finite variance assumptions, ordinary IS is \emph{unbiased} and converges almost surely to \(Q^\pi\).

\subsubsection{Off-Policy Monte Carlo Control}\label{off-policy-monte-carlo-control}

\textbf{High-level Intuition.} We wish to improve a target policy \(\pi\) toward optimality while behaving with a different exploratory policy \(b\). We evaluate \(Q^\pi\) off-policy using IS on data from \(b\), then set \(\pi\) greedy with respect to the updated \(Q\). Keep \(b\) sufficiently exploratory (for coverage), for example \(\varepsilon\)-greedy with a fixed \(\varepsilon>0\) or a GLIE schedule.

\textbf{Algorithmic Form.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialize \(Q(s,a)\) arbitrarily. Set target \(\pi\) to be greedy w.r.t. \(Q\). Choose an exploratory behavior \(b\) that ensures coverage, e.g., \(\varepsilon\)-greedy w.r.t. \(Q\) with \(\varepsilon>0\).
\item
  Loop over iterations \(i=0,1,2,\dots\):

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Data collection under \(b\): generate a batch of episodes using \(b\).
  \item
    Off-policy evaluation of \(\pi\): for each episode, compute IS targets for first visits of \((s_t,a_t)\) and update \(Q\) using either ordinary IS
  \item
    Policy improvement: set for all states
    \[
    \pi_{i+1}(s) \in \arg\max_{a} Q(s,a).
    \]
  \item
    Optionally update \(b\) to remain exploratory, for example \(b\) \(\leftarrow\) \(\varepsilon\)-greedy w.r.t. \(Q\) with a chosen \(\varepsilon\) or a GLIE decay.
  \end{enumerate}
\end{enumerate}

\textbf{Convergence Guarantees.}

\begin{itemize}
\item
  \textbf{Evaluation step:} With the support condition and appropriate stepsizes, off-policy MC prediction converges almost surely to \(Q^\pi\) when using ordinary IS.
\item
  \textbf{Control in the batch GPI limit:} If each evaluation step produces estimates that converge to the exact \(Q^{\pi_i}\) before improvement, then by the policy improvement theorem the sequence of greedy target policies \(\pi_i\) converges to an optimal policy in finite MDPs.
\end{itemize}

\begin{remark}
\textbf{Choice of \(b\).} A common and simple choice is an \(\varepsilon\)-greedy behavior \(b\) w.r.t. current \(Q\) that maintains \(\varepsilon>0\) for coverage or uses GLIE so that \(\varepsilon_t \downarrow 0\) while all pairs are still visited infinitely often.
\end{remark}

\subsubsection{Q-Learning}\label{q-learning}

\textbf{High-Level Intuition.}

\begin{itemize}
\item
  \textbf{What it learns.} Q-Learning seeks the fixed point of the Bellman optimality operator
  \[
  (\mathcal T^\star Q)(s,a) = \mathbb E\big[ r_{t} + \gamma \max_{a'} Q(s_{t+1}, a') \mid s_t=s, a_t=a \big],
  \]
  whose unique fixed point is \(Q^\star\). Because \(\mathcal T^\star\) is a \(\gamma\)-contraction in \(\|\cdot\|_\infty\), repeatedly applying it converges to \(Q^\star\) in the tabular case.
\item
  \textbf{Why off-policy.} We can behave with any sufficiently exploratory policy \(b\) (e.g., \(\varepsilon\)-greedy w.r.t. current \(Q\)) but learn from the greedy target \(\max_{a'} Q(s',a')\). No importance sampling is needed.
\end{itemize}

\textbf{Algorithmic Form.} Let \(Q\) be a tabular action-value function. At each step observe a transition \((s_t, a_t, r_{t}, s_{t+1})\) generated by a behavior policy \(b_t\) (typically \(\varepsilon_t\)-greedy w.r.t. \(Q_t\)).

\begin{itemize}
\item
  \textbf{Target and TD error}
  \[
  y_t = r_{t} + \gamma \max_{a'} Q(s_{t+1}, a'), \qquad
  \delta_t = y_t - Q(s_t, a_t).
  \]
\item
  \textbf{Update}
  \[
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha_t(s_t,a_t)\, \delta_t.
  \]
\item
  \textbf{Behavior (exploration)}
  Use \(\varepsilon_t\)-greedy with \(\varepsilon_t\) decaying (GLIE) or any scheme that ensures each \((s,a)\) is updated infinitely often.
\end{itemize}

\textbf{Convergence.} In a finite MDP with \(\gamma \in [0,1)\), if each \((s,a)\) is updated infinitely often (sufficient exploration) and stepsizes satisfy Robbins-Monro conditions, then Q-Learning converges to \(Q^\star\) with probability 1.

\subsubsection{Double Q-Learning}\label{double-q-learning}

\textbf{Motivation.} Max operators tend to be optimistically biased when action values are noisy. Consider an example where in state \(s\) one can take two actions \(1\) and \(2\). The estimated Q function \(\hat{Q}(s, \cdot)\) has two values \(+1\) and \(-1\) with equal probability. In this case we have \(Q(s,1) = Q(s,2) = \mathbb{E}[\hat{Q}(s,\cdot)] = 0\). Therefore, \(\max Q(s,a) = 0\). However, the noisy estimated \(\hat{Q}(s,\cdot)\) has four outcomes with equal probabilities:
\[
(+1,-1), (+1,+1), (-1, +1), (-1,-1).
\]
Therefore, we have
\[
\mathbb{E}[\max_a \hat{Q}(s,a)] = \frac{1}{4} (1 + 1 + 1 -1) = 1/2 > \max_a Q(s,a),
\]
which overestimates the max \(Q\) value. In general, we have
\[
\mathbb{E}[\max_a \hat{Q}(s,a)] \geq \max_a \mathbb{E} [\hat{Q}(s,a)] = \max_a Q(s,a),
\]
where the estimates \(\hat{Q}\) are noisy (try to prove this on your own).
In Q-Learning the target
\[
y_t = r_{t} + \gamma \max_{a'} Q(s_{t+1}, a')
\]
can therefore overestimate action values and slow learning or push policies toward risky actions.

Double Q-Learning reduces this bias by decoupling selection from evaluation: maintain two independent estimators \(Q^A\) and \(Q^B\). Use one to select the greedy action and the other to evaluate it, and alternate which table you update. This weakens the statistical coupling that creates overestimation.

\textbf{Algorithmic Form.} Keep two tables \(Q^A, Q^B\). Use an \(\varepsilon\)-greedy behavior policy with respect to a combined estimate, e.g., \(Q^{\text{avg}}= \tfrac12(Q^A+Q^B)\) or \(Q^A+Q^B\).

At each step observe \((s_t, a_t, r_{t}, s_{t+1})\). With probability \(1/2\) update \(Q^A\), else update \(Q^B\).

\begin{itemize}
\item
  \textbf{Update \(Q^A\):}
  \[
  a^\star = \arg\max_{a'} Q^A(s_{t+1}, a'),\qquad
  y_t = r_{t} + \gamma\, Q^B(s_{t+1}, a^\star),
  \]
  \[
  Q^A(s_t, a_t) \leftarrow Q^A(s_t, a_t) + \alpha_t(s_t,a_t)\big[y_t - Q^A(s_t, a_t)\big].
  \]
\item
  \textbf{Update \(Q^B\):}
  \[
  a^\star = \arg\max_{a'} Q^B(s_{t+1}, a'),\qquad
  y_t = r_{t} + \gamma\, Q^A(s_{t+1}, a^\star),
  \]
  \[
  Q^B(s_t, a_t) \leftarrow Q^B(s_t, a_t) + \alpha_t(s_t,a_t)\big[y_t - Q^B(s_t, a_t)\big].
  \]
\item
  \textbf{Behavior policy (\(\varepsilon\)-greedy):} choose \(a_t \sim \varepsilon\)-greedy with respect to \(Q^{\text{avg}}(s_t,\cdot)\).
  A GLIE schedule \(\varepsilon_t \downarrow 0\) is standard.
\item
  \textbf{Acting and planning:} for greedy actions or plotting a single estimate, use \(Q^{\text{avg}} = \tfrac12(Q^A+Q^B)\).
\end{itemize}

\textbf{Convergence.}

\begin{itemize}
\tightlist
\item
  \textbf{Tabular setting.} In a finite MDP with \(\gamma \in [0,1)\), bounded rewards, sufficient exploration so that every \((s,a)\) is updated infinitely often, and Robbins--Monro stepsizes for each pair. Double Q-Learning converges with probability 1 to \(Q^\star\).
\end{itemize}

\begin{example}[Value-based RL for Grid World]
\protect\hypertarget{exm:GridWorldMCControl}{}\label{exm:GridWorldMCControl}

Consider the following \(5 \times 5\) grid with \((0,4)\) being the goal and the terminal state. At every state, the agent can take four actions: left, right, up, and down. There is a wall in the gray area shown in Fig. \ref{fig:grid-world}. Upon hitting the wall, the agent stays in the original cell. Every action incurs a reward of \(-1\). Once the agent arrives at the goal state, reward stays at 0.

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{images/Value-RL/grid-world} 

}

\caption{Grid World}\label{fig:grid-world}
\end{figure}

We run Generalized Policy Iteration (GPI) with Monte Carlo (on-policy), SARSA, Expected SARSA, Q-Learning, and Double Q-Learning on this problem with diminishing learning rates.

Fig. \ref{fig:grid-world-Q-convergence} plots the error between the estimated Q values (of different algorithms) and the ground-truth optimal Q value (obtained from value iteration with known transition dynamics). Except Monte Carlo control which converges slowly, the other methods converge fast.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/Value-RL/Q-Value-Convergence} 

}

\caption{Convergence of Estimated Q Values.}\label{fig:grid-world-Q-convergence}
\end{figure}

From the final estimated Q value, we can extract a greedy policy, visualized below.

You can play with the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/GPI_GridWorld.py}{here}.

\begin{verbatim}
MC Control:
 > > > > G
^ # ^ ^ ^
v # ^ ^ ^
v # > ^ ^
> > > > ^
SARSA:
 > > > > G
^ # > > ^
^ # ^ ^ ^
^ # ^ ^ ^
> > ^ ^ ^
Expected SARSA:
 > > > > G
^ # > > ^
^ # ^ ^ ^
^ # > > ^
> > ^ ^ ^
Q-Learning:
 > > > > G
^ # ^ ^ ^
^ # ^ ^ ^
^ # ^ ^ ^
> > ^ ^ ^
Double Q-Learning:
 > > > > G
^ # > ^ ^
^ # > ^ ^
^ # ^ ^ ^
> > > > ^
\end{verbatim}

\end{example}

\section{Function Approximation}\label{function-approximation}

Many reinforcement learning problems have continuous state spaces--think of mechanical systems like robot arms, legged locomotion, drones, and autonomous vehicles. In these domains the state \(s\) (e.g., joint angles/velocities, poses) lives in \(\mathbb{R}^n\), which makes a tabular representation of the value functions impossible. In this case, we must approximate values with parameterized functions.

\subsection{Basics of Continuous MDP}\label{basics-of-continuous-mdp}

In a continuous MDP, at least one of the state space or the action space is a continuous space. Suppose \(\mathcal{S} \subseteq \mathbb{R}^n\) and \(\mathcal{A} \subseteq \mathbb{R}^{m}\) are both continuous spaces.

The environment kernel \(P(\cdot \mid s, a)\) is a Markov kernel from \(\mathcal{S} \times \mathcal{A}\) to \(\mathcal{S}\): for each state-action pair \((s,a)\), \(P(\cdot \mid s,a)\) is a probability measure on \(\mathcal{S}\). For each Borel set \(B \subseteq \mathcal{S}\), the map \((s,a) \mapsto P(B \mid s, a)\) is measurable. For example, \(P(\mathcal{S} \mid s, a) = 1\) for any \((s,a)\).

The policy kernel \(\pi(\cdot \mid s)\) is a stochastic kernel from \(\mathcal{S}\) to \(\mathcal{A}\): for each \(s\), \(\pi(\cdot \mid s)\) is a probability measure on \(\mathcal{A}\).

\textbf{Induced State-Transition Kernel.} For notational convenience, given a policy and the environment kernel \(P\), we define a state-only Markov kernel
\begin{equation}
P^\pi(B \mid s) := \int_{\mathcal{A}} P(B \mid s, a) \pi(da \mid s), \quad B \subseteq \mathcal{S}.
\label{eq:StateOnlyMarkovKernel}
\end{equation}
In words, \(P^\pi(B \mid s)\) measures the probability of landing at a set \(B\) starting from state \(s\), under all actions possible for the policy \(\pi\).

If densities exist, i.e., \(P(ds' \mid s, a) = p(s' \mid s, a) ds'\) and \(\pi(da \mid s) = \pi(a \mid s) da\), then,
\begin{equation}
p^\pi(s' \mid s) := \int_{\mathcal{A}} p(s' \mid s, a) \pi(a \mid s) da \quad\text{and}\quad P^{\pi}(d s' \mid s) = p^\pi(s' \mid s) ds'.
\label{eq:StateOnlyMarkovKernel-density}
\end{equation}

\textbf{Stationary State Distribution.} A probability measure \(\mu^\pi\) on \(\mathcal{S}\) is called \emph{stationary} for the state-transition kernel \(P^\pi\) if and only if
\begin{equation}
\mu^{\pi}(B) = \int_{\mathcal{S}} P^\pi(B \mid s) \mu^{\pi}(ds), \quad \forall B \subseteq \mathcal{S}.
\label{eq:StationaryDistribution-definition}
\end{equation}
If a density \(\mu^\pi(s)\) exists, then the above equation is the followng condition
\begin{equation}
\mu^{\pi}(s') = \int_{\mathcal{S}} p^\pi(s' \mid s) \mu^{\pi}(s) ds.
\label{eq:StationaryDistribution-definition-density}
\end{equation}
In words, the state distribution \(\mu^\pi\) does not change under the state-transition kernel \(P^\pi\) (e.g., if a state \(A\) has probability \(0.1\) of being visited at time \(t\), the probability of visiting \(A\) in the next time step remains \(0.1\), under policy \(\pi\)).
Under standard ergodicity assumptions, this stationary state distribution \(\mu^\pi\) exists and is unique (after sufficient steps, the initial state distribution does not matter and the state distribution follows \(\mu^\pi\)). Moreover, the empirical state distribution converge to \(\mu^\pi\).

\subsection{Policy Evaluation}\label{policy-evaluation-2}

For simplicity, let us first relax the state space to be a continuous space \(\mathcal{S} \subseteq \mathbb{R}^n\). We assume the action space \(\mathcal{A}\) is still finite with \(|\mathcal{A}|\) elements. We first consider the problem of policy evaluation, i.e., estimate the value functions associated with a policy \(\pi\) from interaction data with the environment.

\textbf{Bellman Consistency.} Given a policy \(\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})\), its associated state-value function \(V^\pi\) must satisfy the following Bellman Consistency equation
\begin{equation}
V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \left[ R(s, a) + \gamma \int_{\mathcal{S}} V(s') P(d s' \mid s, a)  \right].
\label{eq:BellmanConsistencyContinuousStateFiniteAction}
\end{equation}
Notice that since \(\mathcal{S}\) is a continuous space, we need to replace ``\(\sum_{s' \in \mathcal{S}}\)'' with ``\(\int_{\mathcal{S}}\)''. If \(P(d s' \mid s, a)\) has a density \(p(s' \mid s, a)\), the above Bellman consistency equation also reads
\begin{equation}
V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \left[ R(s, a) + \gamma \int_{\mathcal{S}} V(s') p(s' \mid s, a) ds'  \right].
\label{eq:BellmanConsistencyContinuousStateFiniteAction-1}
\end{equation}

\textbf{Bellman Operator.} Define the Bellman operator \(T^\pi\) acting on any bounded measurable function \(V:\mathcal{S}\to\mathbb{R}\) by
\begin{equation}
(T^\pi V)(s) = \sum_{a\in\mathcal{A}} \pi(a\mid s)\left[ R(s,a) + \gamma \int_{\mathcal{S}} V(s') P(ds'\mid s,a)\right].
\label{eq:BellmanOperator}
\end{equation}
Then \(V^\pi\) is the unique fixed point of \(T^\pi\), i.e., \(V^\pi = T^\pi V^\pi\). Moreover, when rewards are uniformly bounded and \(\gamma\in[0,1)\), \(T^\pi\) is a \(\gamma\)-contraction under the sup-norm and is monotone.

\textbf{Approximate Value Function.} In large/continuous state spaces we restrict attention to a parametric family \({V(\cdot;\theta): \theta\in\mathbb{R}^d}\) and learn \(\theta\) from data. We use \(\nabla_\theta V(s;\theta) \in \mathbb{R}^d\) to denote the gradient of \(V\) with respect to \(\theta\) at state \(s\).

A special and very important case is linear function approximation
\begin{equation}
V(s;\theta) = \theta^\top \phi(s),
\label{eq:LinearV}
\end{equation}
where \(\phi(s) = [\phi_1(s),\ldots,\phi_d(s)]^\top\) are fixed basis functions (e.g., neural network last-layer features). When \(V(s;\theta) = \theta^{\top} \phi(s)\), we have
\[
\nabla_\theta V(s;\theta) = \phi(s).
\]
When we restrict the value function to a function class (e.g., linear features or a neural network), it is generally not guaranteed that the unique fixed point of the Bellman operator \eqref{eq:BellmanOperator}, namely \(V^\pi\), belongs to that class. This misspecification (or realizability gap) means we typically cannot recover \(V^\pi\) exactly; instead, we seek its \emph{best approximation} according to a chosen criterion.

\subsubsection{Monte Carlo Estimation}\label{monte-carlo-estimation-1}

Given an episode \((s_t,a_t,r_t,\dots,s_T)\) collected by policy \(\pi\), its discounted return
\[
g_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^{T-t} r_T
\]
is an unbiased estimate of the value at \(s_t\), i.e., \(V^{\pi}(s_t)\).

Therefore, Monte Carlo estimation follows the intuitive idea to make the approximate value function \(V(\cdot, \theta)\) fit the returns from these episodes as close as possible:
\begin{equation}
\min_{\theta} \frac{1}{\mathcal{D}} \sum_{t \in \mathcal{D}} \frac{1}{2} (g_t - V(s_t, \theta))^2,
\label{eq:MCEstimationGoal}
\end{equation}
where \(\mathcal{D}\) denotes the dataset of episodes collected under policy \(\pi\). The formulation \eqref{eq:MCEstimationGoal} is a \emph{batch} formulation in the sense that it waits until all episodes are collected before performing the optimization.

In an online formulation, we can optimize after every episode the objective function
\[
\min_{\theta} \frac{1}{2} (g_t - V(s_t, \theta))^2,
\]
which leads to one step of gradient descent:
\begin{equation}
\theta \ \ \leftarrow \ \ \theta + \alpha_t (g_t - V(s_t;\theta)) \nabla_\theta V(s_t; \theta).
\label{eq:MCOneStepGD}
\end{equation}
To connect the above update back to the MC update \eqref{eq:mc-incremental} in the tabular case, we see that the term \(g_t - V(s_t;\theta\) is similar as before the difference between the target and the current estimate. However, in the case of function approximation, the error is multiplied by the gradient \(\nabla_\theta V (s_t; \theta)\).

It is worth noting that when using function approximation, the update on \(\theta\) caused by one episode \((s_t,\dots)\) will affect the values at all other states even if the policy only visited state \(s_t\).

\textbf{Convergence Guarantees.} Assume on-policy sampling under \(\pi\), bounded rewards, and step sizes \(\alpha_t\) satisfying Robbins--Monro conditions.

\begin{itemize}
\item
  For \emph{linear} \(V(s;\theta)=\theta^\top\phi(s)\) with full-rank features, i.e.,
  \[
  \mathbb{E}_{s \sim \mu^\pi} \left[ \phi(s) \phi(s)^\top \right] \succ 0,
  \]
  and \(\mathbb{E}_{s \sim \mu^\pi}\|\phi(s)\|^2<\infty\), the iterates converge almost surely to the unique global minimizer of the convex objective
  \begin{equation}
  \theta_{\text{MC}}^\star \in \arg\min_\theta \;  \frac{1}{2}\mathbb{E}_{s_t \sim \mu^\pi}\!\left[ \big(V(s_t;\theta)-V^\pi(s_t)\big)^2 \right],
  \label{eq:convergence-of-MC}
  \end{equation}
  where the expectation is with respect to stationary state distribution \(\mu^\pi\) under \(\pi\).
\item
  For \emph{nonlinear} differentiable function classes with bounded gradients, the iterates converge almost surely to a stationary point of the same objective.

  \begin{itemize}
  \tightlist
  \item
    Correction: Since Monte Carlo Estimation can be seen as performing Stochastic Gradient Descent on the objective in \eqref{eq:convergence-of-MC}, to guarantee convergence to a first-order stationary point, we need some technical conditions: (a) diminishing step sizes satisfying the Robbins-Monro condition; (b) bounded second-order moment of the stochastic gradient; and (c) \(L\)-smoothness of the objective.
  \end{itemize}
\end{itemize}

\subsubsection{Semi-Gradient TD(0)}\label{semi-gradient-td0}

We know from previous discussion that MC uses the full return \(g_t\) as the target and thus can have high variance. A straightforward idea is to replace the MC target \(g_t\) in the update \eqref{eq:MCOneStepGD} by the one-step bootstrap target
\[
r_t + \gamma V(s_{t+1};\theta),
\]
which yields the \emph{semi-gradient TD(0)} update
\begin{equation}
\theta \ \leftarrow\ \theta \;+\; \alpha_t \,\big(r_t + \gamma V(s_{t+1};\theta) - V(s_t;\theta)\big)\, \nabla_\theta V(s_t;\theta).
\label{eq:SemiGradientTD0}
\end{equation}
(At terminal \(s_{t+1}\), use \(V(s_{t+1};\theta)=0\) or equivalently set \(\gamma=0\) for that step.)

\textbf{Why call it ``semi-gradient''?} Let the TD error be
\[
\delta_t(\theta) \;:=\; r_t + \gamma V(s_{t+1};\theta) - V(s_t;\theta).
\]
Consider the per-sample squared TD error objective
\[
\min_{\theta}\; \frac{1}{2} \,\delta_t(\theta)^2.
\]
Its \textbf{true gradient} (a.k.a. the \emph{residual gradient}) is
\[
\nabla_\theta \frac{1}{2} \delta_t(\theta)^2
\;=\;
\delta_t(\theta)\,\big(\gamma \nabla_\theta V(s_{t+1};\theta) - \nabla_\theta V(s_t;\theta)\big).
\]
Thus a \textbf{true-gradient (residual-gradient) TD(0)} step would be
\begin{equation}
\theta \ \leftarrow\ \theta \;-\; \alpha_t \,\delta_t(\theta)\,\big(\gamma \nabla_\theta V(s_{t+1};\theta) - \nabla_\theta V(s_t;\theta)\big).
\label{eq:ResidualGradientTD0}
\end{equation}

By contrast, the semi-gradient TD(0) step in \eqref{eq:SemiGradientTD0} ignores the dependence of the target on \(\theta\) (i.e., it drops the \(\gamma \nabla_\theta V(s_{t+1};\theta)\) term) and treats the target \(r_t+\gamma V(s_{t+1};\theta)\) as a \emph{constant} when differentiating. Concretely,
\[
\nabla_\theta \frac{1}{2} \big( \text{target} - V(s_t;\theta)\big)^2
\;\approx\;
-\big(\text{target} - V(s_t;\theta)\big)\,\nabla_\theta V(s_t;\theta).
\]
This approximation yields the simpler update \eqref{eq:SemiGradientTD0}.

\textbf{Convergence Guarantees.} When using linear approximation, the Monte Carlo estimator converges to \(\theta^\star_{\text{MC}}\) in \eqref{eq:convergence-of-MC}. We now study what the semi-gradient TD(0) updates \eqref{eq:SemiGradientTD0} converge to.

\textbf{Projected Bellman Operator.} Fix a weighting/visitation distribution \(\mu\) on \(\mathcal S\) (e.g., the stationary distribution \(\mu^\pi\)) and the associated inner product
\[
\langle f,g\rangle_\mu := \mathbb{E}_{s\sim \mu}[\,f(s)g(s)\,], 
\qquad 
\|f\|_\mu := \sqrt{\langle f,f\rangle_\mu}.
\]
Let \(\mathcal V := \{V(s;\theta)=\theta^\top\phi(s)\;:\;\theta\in\mathbb{R}^d\}\) be the linear function class spanned by features \(\phi:\mathcal S\to\mathbb{R}^d\). The \emph{\(\mu\)-orthogonal projection} \(\Pi_\mu:\mathcal{F}\to\mathcal V\) is
\[
\Pi_\mu f \;:=\; \arg\min_{V\in\mathcal V}\, \| V - f\|_\mu .
\]
In words, given any function \(f \in \mathcal{F}: \mathcal{S} \mapsto \mathbb{R}\), \(\Pi_\mu f\) returns the closest function \(V\) to \(f\) that belongs to the subset of linearly representable functions \(\mathcal{V}\), where the ``closest'' is defined by the weighting distribution \(\mu\).
The Projected Bellman Operator is the composition
\begin{equation}
\mathcal{T}^\pi_{\!\text{proj}} \;:=\; \Pi_\mu \, T^\pi,
\qquad\text{i.e.,}\qquad
\big(\mathcal{T}^\pi_{\!\text{proj}} V\big)(\cdot) \;=\; \Pi_\mu \!\left[\, T^\pi V \,\right](\cdot).
\label{eq:ProjectedBellmanOperator}
\end{equation}

\begin{itemize}
\item
  \(T^\pi\) is the Bellman operator defined in \eqref{eq:BellmanOperator}.
\item
  \(\Pi_\mu\) projects any function onto \(\mathcal V\) using the \(\mu\)-weighted \(L^2\) norm.
\item
  In discrete \(\mathcal S\), write \(\Phi\in\mathbb{R}^{|\mathcal S|\times d}\) with rows \(\phi(s)^\top\) and \(D=\mathrm{diag}(\mu(s))\). Then
  \[
  \Pi_\mu f \;=\; \Phi\,(\Phi^\top D \Phi)^{-1}\Phi^\top D f.
  \]
\item
  \(T^\pi\) is a \(\gamma\)-contraction under \(\|\cdot\|_\mu\), and \(\Pi_\mu\) is nonexpansive under \(\|\cdot\|_\mu\), hence \(\mathcal{T}^\pi_{\!\text{proj}}\) is a \(\gamma\)-contraction:
  \[
  \|\Pi_\mu T^\pi V - \Pi_\mu T^\pi U\|_\mu \;\le\; \|T^\pi V - T^\pi U\|_\mu \;\le\; \gamma \|V-U\|_\mu.
  \]
\end{itemize}

Therefore, \eqref{eq:ProjectedBellmanOperator}, the projected Bellman equation (PBE), has a unique fixed point \(V_{\text{TD}}^\star\in\mathcal V\) satisfying
\begin{equation}
V_{\text{TD}}^\star \;=\; \Pi_\mu\, T^\pi V_{\text{TD}}^\star.
\label{eq:PBEFixedPoint}
\end{equation}

\textbf{Semi-gradient TD(0) Converges to the PBE Fixed Point (linear case).} Assume on-policy sampling under an ergodic chain, bounded second moments, Robbins--Monro stepsizes, and full-rank features under \(\mu = \mu^\pi\). In the linear case \(V(s;\theta)=\theta^\top\phi(s)\), define
\[
\delta_t(\theta) := r_t + \gamma \theta^\top \phi(s_{t+1}) - \theta^\top \phi(s_t).
\]
The semi-gradient TD(0) update \eqref{eq:SemiGradientTD0} becomes
\[
\theta \leftarrow \theta + \alpha_t\, \delta_t(\theta)\,\phi(s_t).
\]
Taking conditional expectation w.r.t. the stationary visitation (and using the Markov property) yields the mean update:
\[
\mathbb{E}\!\left[\delta_t(\theta)\,\phi(s_t)\right] \;=\; b \;-\; A\theta,
\]
with the standard TD system
\begin{equation}
A \;:=\; \mathbb{E}_{\mu}\!\big[\phi(s_t)\big(\phi(s_t)-\gamma \phi(s_{t+1})\big)^\top\big],
\qquad
b \;:=\; \mathbb{E}_{\mu}\!\big[r_{t}\,\phi(s_t)\big].
\label{eq:TDLinearSystem-again}
\end{equation}
Thus, in expectation, TD(0) performs a stochastic approximation to the ODE
\[
\dot\theta \;=\; b - A\theta,
\]
whose unique globally asymptotically stable equilibrium is
\[
\theta^\star_{\text{TD}} \;=\; A^{-1} b,
\]
provided the symmetric part of \(A\) is positive definite (guaranteed on-policy with full-rank features). Standard stochastic approximation theory then gives
\[
\theta_t \xrightarrow{\text{a.s.}} \theta^\star_{\text{TD}}.
\]
Finally, one can show the equivalence with the PBE: \(V(\cdot; \theta) \in\mathcal V\) satisfies \(V(\cdot; \theta)=\Pi_\mu T^\pi V(\cdot; \theta)\) if and only if \(A\theta \;=\; b\) (see a proof below).
Hence the almost-sure limit \(V(\cdot; \theta^\star_{\text{TD}})\) is exactly the fixed point \eqref{eq:PBEFixedPoint}.

\begin{proof}
The projected Bellman equation reads
\[
V(\cdot;\theta) = \Pi_\mu T^\pi V(\cdot; \theta).
\]
Since \(V(\cdot; \theta) = \theta^\top \phi(\cdot)\) is the orthogonal projection of \(T^\pi V(\cdot; \theta)\) onto \(\mathcal V\) weighted by \(\mu\), we have that
\[
\mathbb{E}_\mu \left[ \phi(s_t) (T^\pi V(s_t;\theta) - V(s_t;\theta)) \right] = 0 = \mathbb{E}_\mu \left[ \phi(s_t) (r_t + \gamma \theta^\top \phi(s_{t+1}) - \theta^\top \phi(s_t) )\right],
\]
which reduces to \(A \theta = b\) with \(A\) and \(b\) defined in \eqref{eq:TDLinearSystem-again}.
\end{proof}

\textbf{What does convergence to the PBE fixed point imply?}

\begin{itemize}
\item
  \textbf{Best fixed point in the feature subspace (good).} \(V_{\text{TD}}^\star\) is the unique function in \(\mathcal V\) whose Bellman update \(T^\pi V\) projects back to itself under \(\Pi_\mu\). If \(V^\pi\in\mathcal V\) (realisable case), then \(V_{\text{TD}}^\star=V^\pi\).
\item
  \textbf{Different target than least squares (mixed).} TD(0) solves the Projected Bellman Equation \eqref{eq:ProjectedBellmanOperator}; Monte Carlo least-squares solves
  \[
  \min_{V\in\mathcal V} \frac{1}{2} \|V - V^\pi\|_\mu^2.
  \]
  When \(V^\pi\notin\mathcal V\), these solutions generally differ. Either can have lower \(\mu\)-weighted prediction error depending on features and dynamics; in practice TD often wins due to lower variance and online bootstrapping.
\end{itemize}

\subsection{On-Policy Control}\label{on-policy-control-1}

\subsubsection{Semi-Gradient SARSA(0)}\label{semi-gradient-sarsa0}

\textbf{High-level Intuition.} Semi-gradient SARSA(0) is an on-policy value-based control method. It learns an action-value function \(Q(s,a;\theta)\) by bootstrapping one step ahead and using the next action actually selected by the current behavior policy (e.g., \(\varepsilon\)-greedy). Because the target uses \(Q(s_{t+1},a_{t+1};\theta)\), SARSA trades some bias for substantially lower variance than Monte Carlo, updates online from each transition, and naturally couples policy evaluation (of the current policy) with policy improvement (by making the policy greedy/soft-greedy w.r.t. the current \(Q\)).

\textbf{Algorithmic Form (On-policy, Finite \(\mathcal A\)).} Let the behavior policy at time \(t\) be \(\pi_t(\cdot\mid s)\) (e.g., \(\varepsilon_t\)-greedy w.r.t. \(Q(\cdot,\cdot;\theta_t)\)). For each step:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Given \(s_t\), pick \(a_t \sim \pi_t(\cdot \mid s_t)\); observe \(r_t\) and \(s_{t+1}\).
\item
  Pick the next action \(a_{t+1} \sim \pi_t(\cdot\mid s_{t+1})\).
\item
  Form the TD error
  \begin{equation}
  \delta_t \;=\; r_t \;+\; \gamma\, Q(s_{t+1},a_{t+1};\theta) \;-\; Q(s_t,a_t;\theta).
  \label{eq:SARSA-TDerror}
  \end{equation}
\item
  Update parameters with a semi-gradient step
  \begin{equation}
  \theta \;\leftarrow\; \theta \;+\; \alpha_t\, \delta_t \,\nabla_\theta Q(s_t,a_t;\theta).
  \label{eq:SARSA-Update}
  \end{equation}
  For terminal \(s_{t+1}\), use \(Q(s_{t+1},a_{t+1};\theta)=0\) (equivalently, set \(\gamma=0\) on terminal transitions).
\end{enumerate}

\begin{itemize}
\item
  \textbf{Linear special case.} If \(Q(s,a;\theta)=\theta^\top \phi(s,a)\), then \(\nabla_\theta Q(s_t,a_t;\theta)=\phi(s_t,a_t)\) and the update becomes
  \[
  \theta \leftarrow \theta + \alpha_t\, \delta_t\, \phi(s_t,a_t).
  \]
\item
  \textbf{Expected SARSA (variance reduction).} Replace the sample bootstrap by its expectation under \(\pi_t\):
  \begin{equation}
  \delta_t^{\text{exp}} \;=\; r_t \;+\; \gamma \sum_{a'\in\mathcal A} \pi_t(a'\mid s_{t+1})\, Q(s_{t+1},a';\theta) \;-\; Q(s_t,a_t;\theta),
  \label{eq:ExpectedSARSA}
  \end{equation}
  then update \(\theta \leftarrow \theta + \alpha_t\, \delta_t^{\text{exp}}\, \nabla_\theta Q(s_t,a_t;\theta)\).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Update the next policy to be \(\varepsilon_{t+1}\)-greedy w.r.t. the new Q value \(Q(\cdot, \cdot; \theta_{t+1})\). (\(\varepsilon_t\) follows GLIE.)
\end{enumerate}

\begin{example}[Semi-Gradient SARSA for Mountain Car]
\protect\hypertarget{exm:MountainCarSemiGradientSARSA}{}\label{exm:MountainCarSemiGradientSARSA}

Consider the Mountain Car problem from Gym illustrated in Fig. \ref{fig:Mountain-car-plot}. The state space \(\mathcal{S} \subset \mathbb{R}^2\) is continuous and contains the position of the car along the \(x\)-axis as well as the car's velocity. The action space \(\mathcal{A}\) is discrete and contains three elements: ``\(0\): Accelerate to the left'', ``\(1\): Don't accelerate'', and ``\(2\): Accelerate to the right''. The transition dynamics of the mountain car is:
\begin{equation}
\begin{split}
v_{t+1} &= v_t + (a_t - 1) F - \cos (3 p_t) g \\
p_{t+1} &= p_t + v_{t+1}
\end{split}
\end{equation}
where \((p_t, v_t)\) denotes the state at time \(t\) with position and velocity, \(a_t\) denotes the action at time \(t\), \(F=0.001\) is the force and \(g = 0.0025\) is the gravitational constant.

The goal is for the mountain car to reach the flag placed on top of the right hill as quickly as possible. Therefore, the agent is penalised with a reward of \(-1\) for each timestep. The position of the car is assigned a uniform random value in \([-0.6 , -0.4]\). The starting velocity of the car is always assigned to 0. In every episode, the agent is allowed a maximum of \(200\) steps (therefore, the worst per-episode return is \(-200\)).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/Value-RL/mountain_car} 

}

\caption{Mountain Car from Gym}\label{fig:Mountain-car-plot}
\end{figure}

\textbf{Naive Semi-Gradient SARSA.} We first apply the semi-gradient SARSA algorithm introduced above to the mountain car problem. We parameterize the action value \(Q\) as a 2-layer multi-layer perceptron (MLP). Fig. \ref{fig:Mountain-car-sarsa-episode-return} shows the average return per episode as training progreses. Clearly, the return stagnates at \(-200\) and the algorithm failed to learn. The rollout in Fig. \ref{fig:Mountain-car-sarsa-rollout} confirms that the final policy is not able to achieve the goal.

You can find code for the naive semi-gradient SARSA algorithm \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/mountaincar_sarsa.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/mountain-car_sarsa_eps3000_seed50_returns} 

}

\caption{Average return w.r.t. episode (Semi-Gradient SARSA)}\label{fig:Mountain-car-sarsa-episode-return}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/mountain-car_sarsa_eps3000_seed50_greedy-episode-0} 

}

\caption{Example rollout (Semi-Gradient SARSA)}\label{fig:Mountain-car-sarsa-rollout}
\end{figure}

\textbf{Semi-Gradient SARSA with Experience Replay.} Inspired by the technique of experience replay (ER) popularized by DQN (see \ref{DQN}), we incorporated ER into semi-gradient SARSA, which breaks its on-policy nature. Fig. \ref{fig:Mountain-car-sarsa-er-episode-return} displays the learning curve, which shows steady increase of the per-episode return. Applying the final learned policy to the mountain car yields a successful trajectory to the top of the mountain.

You can find code for the semi-gradient SARSA with experience replay algorithm \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/mountaincar_sarsa_er.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/mountain-car_sarsa+er_eps3000_seed50_returns} 

}

\caption{Average return w.r.t. episode (Semi-Gradient SARSA + Experience Replay)}\label{fig:Mountain-car-sarsa-er-episode-return}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/mountain-car_sarsa+er_eps3000_seed50_greedy-episode-0} 

}

\caption{Example rollout (Semi-Gradient SARSA + Experience Replay)}\label{fig:Mountain-car-sarsa-er-rollout}
\end{figure}

\end{example}

\subsection{Off-Policy Control}\label{off-policy-control-1}

Off-policy control seeks to learn the optimal action--value function while collecting data under a different behavior policy (e.g., an \(\varepsilon\)-soft policy). As in the tabular setting, Q-learning is the canonical off-policy control method. With function approximation, however, off-policy control becomes substantially harder than in the tabular case.

To illustrate why, we first present off-policy semi-gradient TD(0) for policy evaluation and use Baird's counterexample \citep{baird1995residual} to highlight the deadly triad: bootstrapping + function approximation + off-policy sampling can cause divergence.

We then turn to the Deep Q-Network (DQN) \citep{mnih2015human}, which stabilizes Q-learning with two key mechanisms---experience replay and a target network---leading to landmark Atari results. Finally, we connect DQN to fitted Q-iteration (FQI) \citep{riedmiller2005neural}, a batch method with theoretical guarantees, to clarify why these stabilizations work \citep{fan2020theoretical}.

\subsubsection{Off-Policy Semi-Gradient TD(0)}\label{off-policy-semi-gradient-td0}

\textbf{Setup.} We aim to estimate the state-value function of a target policy \(\pi\) using a different behavior policy \(b\). Since the state space is continuous, we employ function approximation to represent the value function as \(V(s;\theta)\) with \(\theta \in \mathbb{R}^d\). In the case of linear approximation, we have \(V(s;\theta) = \theta^\top \phi(s)\) where \(\phi(s)\) is a function that featurizes the state.

\textbf{Semi-Gradient TD(0).} Given a transition \((s_t,a_t,r_t,s_{t+1})\) collected under the behavior policy \(b\), form the TD error
\[
\delta_t = r_t + \gamma V(s_{t+1}; \theta) - V(s_t;\theta).
\]
The off-policy Semi-Gradient TD(0) update reads
\begin{equation}
\theta \quad \leftarrow \quad \theta + \alpha_t \rho_t \delta_t \nabla_\theta V(s_t; \theta),
\label{eq:FAOffPolicySemiGradientTD0}
\end{equation}
where \(\rho_t\) is the likelihood ratio as in \eqref{eq:IS-LikelihoodRatio}:
\[
\rho_t = \frac{\pi(a_t \mid s_t)}{b(a_t \mid s_t)}.
\]
This off-policy semi-gradient TD(0) update \eqref{eq:FAOffPolicySemiGradientTD0} looks perfectly reasonable. However, the following Baird's counterexample illustrates the instability of the algorithm.

\textbf{Baird's Counterexample.} Consider an MDP with 7 states containing 6 upper states and 1 lower state, as shown in Fig. \ref{fig:Baird-example} (the figure comes from \citep{sutton1998reinforcement}). There are two actions, one called ``solid'' and the other called ``dashed''. If the agent picks ``solid'' at any state, then the system transitons to the lower state with probability 1. If the agent picks ``dashed'' at any state, then the system transitions to any one of the upper states with equal probability. All rewards are zero, and the discount factor is \(\gamma=0.99\).

The target policy \(\pi\) always picks ``solid'', while the behavior policy \(b\) picks ``solid'' with probability \(1/7\) and ``dashed'' with probability \(6/7\).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Value-RL/Baird} 

}

\caption{Baird Counterexample}\label{fig:Baird-example}
\end{figure}

Consider the case of linear function approximation where \(V(s;w) = w^\top \phi(s)\) where \(w \in \mathbb{R}^8\). For the upper states, the feature \(\phi(s)\) leads to \(V(s;w) = 2 w_1 + w_8\), and for the lower state, the feature \(\phi(s)\) leads to \(V(s;w) = w_7 + 2w_8\).

This \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/BairdExample.py}{Python script} implements the off-policy semi-gradient TD(0) algorithm with importance sampling for policy evaluation.

Fig. \ref{fig:Baird-example-value} plots \(\Vert w \Vert_2\), the magnitude of \(w\), with respect to iterations. Clearly, we see the parameter \(w\) diverges under the off-policy semi-gradient TD(0) algorithm.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/Baird-w} 

}

\caption{Baird Counterexample: divergence}\label{fig:Baird-example-value}
\end{figure}

\textbf{The Deadly Triad.} Three ingredients were used together in Baird's example:

\begin{itemize}
\item
  Off-policy: a different behavior policy \(b\) is used to collect data for the evaluation of the target policy \(\pi\);
\item
  Function approximation: the value function employs function approximation;
\item
  Bootstrapping: the TD error uses the bootstrapped target ``\(r_t + \gamma V(s_{t+1}; \theta)\)'' instead of the full return as in Monte Carlo.
\end{itemize}

The ``deadly triad'' is used to illustrate that using all three ingredients together will lead to the potential divergence of policy evaluation.

\textbf{Why?} Recall that, using linear approximation, the on-policy Semi-Gradient TD(0) algorithm guarantees convergence to the unique fixed point of the projected Bellman equation (PBE) \eqref{eq:PBEFixedPoint}, restated here
\[
V^\star_{\text{TD}} = \Pi_{\mu} T^\pi V^\star_{\text{TD}},
\]
where \(\mu\) is the stationary distribution induced by the policy \(\pi\).
A central reason for the guaranteed convergence is that the projected Bellman operator
\[
\Pi_{\mu} T^\pi
\]
is a \(\gamma\)-contraction and has a unique fixed point. Therefore, the on-policy Semi-Gradient TD(0) algorithm---can be seen as a stochastic approximation of the projected Bellman operator---enjoys convergence guarantees.

However, in the off-policy case, the orthogonal projection \(\Pi_\mu\) needs to be modified as \(\Pi_{\nu}\), where \(\nu\) is the stationary distribution induced by the behavior policy \(b\). The new operator
\[
\Pi_{\nu} T^\pi
\]
is not guaranteed to be a \(\gamma\)-contraction, due to the mismatch between \(\nu\)---induced by \(b\)---and the target policy \(\pi\). Therefore, divergence can potentially happen.

\textbf{How to Fix?} Multiple algorithms have been proposed to fix the deadly triad. Notable examples include the gradient TD (GTD) family of algorithms \citep{sutton2008convergent}, \citep{sutton2009fast}, and the Emphatic TD (ETD) learning algorithm \citep{mahmood2015emphatic}. They are influential and widely cited, but they are not (yet) mainstream in deep RL practice. Their main appeal is theoretical---they provide off-policy evaluation algorithms with convergence guarantees under linear function approximation. Moreover, GTD/TDC require two-time-scale step-sizes and ETD's emphatic weights can have high variance, making them less attractive for large-scale control with neural networks. I encourage you to read the papers to understand the algorithms. However, in the next, I will explain the deep Q network (DQN) approach that is more popular in practice.

\subsubsection{Deep Q Network}\label{DQN}

We consider continuous state spaces with a finite action set, and a parametric action--value function \(Q(s,a; \theta)\).

\textbf{Naive (semi-gradient) Q-Learning with Function Approximation.} The goal is to learn \(Q^\star\) and act \(\varepsilon\)-greedily w.r.t. \(Q(\cdot,\cdot; \theta)\). The update uses a bootstrapped optimality target built from the current network.
\begin{equation}
\begin{split}
y_t & = r_t + \gamma \max_{a'} Q(s_{t+1}, a';\theta) \\
\theta & \leftarrow \theta + \alpha \,\big(y_t - Q(s_t,a_t;\theta)\big)\,\nabla_\theta Q(s_t,a_t; \theta).
\end{split}
\label{eq:naiveQLearningFA}
\end{equation}
The transitions \((s_t,a_t,r_t,s_{t+1})\) are generated using a \(\varepsilon\)-greedy policy with respect to \(Q(s,a;\theta)\).

This naive variant is \textbf{off-policy + bootstrapping + function approximation} (i.e., the deadly triad) and thus can be unstable.

\textbf{Deep Q Network (DQN) with Experience Replay (ER) and Target Network (TN).} DQN augments the above naive Q learning with two stabilizers:

\begin{itemize}
\item
  \textbf{Experience Replay (ER):} store transitions in a buffer \(\mathcal{D}\); train on i.i.d.-like mini-batches to decorrelate updates and reuse data.
\item
  \textbf{Target Network (TN):} maintain a delayed copy \(Q(\cdot,\cdot; \theta^-)\) to compute targets
  \[
  y_t = r_t + \gamma \max_{a'} Q(s_{t+1},a';\theta^-),
  \]
  keeping the target fixed for many gradient steps.
\end{itemize}

The full DQN algorithm is presented below.

\begin{itemize}
\tightlist
\item
  Initialize replay buffer \(\mathcal{D}\) with capacity \(N\)
\item
  Initialize approximate Q value \(Q(s,a;\theta)\)
\item
  Initialize target \(Q_T(s,a;\theta^-)\) with \(\theta^- = \theta\)
\item
  For episode \(=1,\dots,M\) do:

  \begin{itemize}
  \tightlist
  \item
    Initialize \(s_0\)
  \item
    For \(t=0,\dots,T\) do:

    \begin{itemize}
    \tightlist
    \item
      \(\varepsilon\)-greedy policy:

      \begin{itemize}
      \tightlist
      \item
        With probability \(\varepsilon\) select a random action \(a_t \in \mathcal{A}\)
      \item
        Otherwise select \(a_t = \arg\max_a Q(s_t,a_t;\theta)\)
      \end{itemize}
    \item
      Observe transition \(\tau_t = (s_t, a_t, r_t, s_{t+1})\)
    \item
      Put \(\tau_t\) inside replay buffer \(\mathcal{D}\)
    \item
      Sample a random minibatch of transitions \(\{(s_i, a_i, r_i, s_{i+1})\}_{i \in \mathcal{I}}\) from \(\mathcal{D}\)
    \item
      For \(i \in \mathcal{I}\) do:

      \begin{itemize}
      \tightlist
      \item
        Set target \(y_i = r_i + \gamma \max_a Q_T(s_{i+1}, a; \theta^-)\) using the target network
      \item
        Update \(\theta \leftarrow \theta + \alpha (y_i - Q(s_i,a_i;\theta)) \nabla_\theta Q(s_i, a_i; \theta)\)
      \end{itemize}
    \item
      Every \(C\) steps synchronize the target network with the \(Q\) net: \(\theta^- = \theta\)
    \end{itemize}
  \end{itemize}
\end{itemize}

Although the naive Q-leanring with function approximation can be unstable, DQN has achieved great success and is also very efficient \citep{mnih2015human}.

\textbf{Fitted Q Iteration (FQI).} To understand why DQN can achieve stabilized training compared to naive Q-learning with function approximation. It is insightful to look at the fitted Q iteration (FQI) algorithm, presented below.

\begin{itemize}
\tightlist
\item
  Initialize \(Q^{(0)} = Q(s,a;\theta_0)\)
\item
  For \(k=0,1,2,\dots,K-1\) do:

  \begin{itemize}
  \tightlist
  \item
    Sample i.i.d. transitions \(\{ (s_i,a_i,r_i,s_{i+1}) \}_{i=1}^N\) with \((s_i, a_i)\) drawn from a distribution \(\mu\)
  \item
    Compute targets \(y_i = r_i + \gamma \max_a Q^{(k)}(s_{t+1},a;\theta_k),i=1,\dots,N\)
  \item
    Update the action-value function:
    \[
    Q^{(k+1)} = Q(s,a;\theta_{k+1}), \quad \theta_{k+1} \in \arg\min_{\theta}  \frac{1}{N} \sum_{i=1}^N (y_i - Q(s_i,a_i;\theta))^2
    \]
  \end{itemize}
\end{itemize}

The FQI algorithm samples trajectories from a fixed distribution \(\mu\) and optimizes the parameter using targets generated from those trajectories. Under reasonable coverage and approximation assumptions, it converges and admits finite-sample error bounds \citep{antos2007fitted}, \citep{munos2008finite}.

\textbf{Connection to DQN.} DQN is similar to FQI in the following aspects.

\begin{itemize}
\item
  \textbf{Frozen targets:} DQN's target network \(Q(\cdot,\cdot; \theta^-)\) plays the role of \(Q^{(k-1)}\) in FQI.
\item
  \textbf{Supervised fit:} DQN's mini-batch loss minimizes \(\sum (Q(s_i,a_i; \theta) - y_i)^2\), just like FQI's regression step.
\item
  \textbf{Data usage:} FQI trains on a fixed dataset; DQN's replay buffer approximates training on an (ever-growing) \emph{quasi-fixed} dataset by repeatedly sampling past transitions.
\item
  \textbf{Iteration vs.~updates:} FQI alternates \emph{full} regressions and \emph{target recomputation}; DQN alternates \emph{many SGD steps} with \emph{periodic target updates}. In the limit of many SGD steps per target update and a large replay buffer, DQN \(\approx\) online, incremental FQI.
\end{itemize}

This perspective explains why \textbf{ER + TN} make DQN far more stable than naive Q-learning with function approximation: they make the optimization behave like a sequence of supervised fits to \textbf{fixed} targets drawn from a nearly \textbf{stationary} dataset.

\begin{example}[DQN for Mountain Car]
\protect\hypertarget{exm:MountainCarQLearningFA}{}\label{exm:MountainCarQLearningFA}

Consider again the Mountain car problem from Example \ref{exm:MountainCarSemiGradientSARSA}.

\textbf{Naive Q-Learning with Function Approximation.} As shown in Fig. \ref{fig:Mountain-car-q-episode-return} and Fig. \ref{fig:Mountain-car-q-rollout}, naive Q-leanring with function approximation fails to learn a good policy.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/mountain-car_qlearn_naive_eps2500_seed123_returns} 

}

\caption{Average return w.r.t. episode (Naive Q Learning)}\label{fig:Mountain-car-q-episode-return}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/mountain-car_qlearn_naive_eps2500_seed123_greedy-episode-0} 

}

\caption{Example rollout (Naive Q Learning)}\label{fig:Mountain-car-q-rollout}
\end{figure}

\textbf{DQN with Experience Replay and Target Network.} Adding ER and TN to Q-learning leads to steady learning (Fig. \ref{fig:Mountain-car-dqn-episode-return}) and a successful final policy (Fig. \ref{fig:Mountain-car-dqn-rollout}).

You can find code for these experiments \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/mountaincar_q_learning.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/mountain-car_dqn_eps2500_seed123_returns} 

}

\caption{Average return w.r.t. episode (DQN)}\label{fig:Mountain-car-dqn-episode-return}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Value-RL/mountain-car_dqn_eps2500_seed123_greedy-episode-0} 

}

\caption{Example rollout (DQN)}\label{fig:Mountain-car-dqn-rollout}
\end{figure}

\end{example}

\chapter{Policy Gradient Methods}\label{policy-gradient}

In Chapter \ref{value-rl}, we relaxed two key assumptions of the MDP introduced in Chapter \ref{mdp}:

\begin{itemize}
\tightlist
\item
  \textbf{Unknown dynamics}: the transition function \(P\) was no longer assumed to be known.\\
\item
  \textbf{Continuous states}: the state space \(\mathcal{S}\) was extended from finite to continuous.
\end{itemize}

When only the dynamics are unknown but the MDP remains tabular, we introduced generalized versions of policy iteration (e.g., SARSA) and value iteration (e.g., Q-learning). These algorithms can recover near-optimal value functions with strong convergence guarantees.

When both the dynamics are unknown and the state space is continuous, tabular methods become infeasible. In this setting, we employed function approximation to represent value functions, and generalized SARSA and Q-learning accordingly. We also introduced stabilization techniques such as experience replay and target networks to ensure more reliable learning.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In this chapter, we relax a third assumption: the action space \(\mathcal{A}\) is also continuous. This setting captures many important real-world systems, such as autonomous vehicles and robots. Handling continuous actions requires a departure from the value-based methods of Chapter \ref{value-rl}. The key difficulty is that even if we had access to a near-optimal action-value function \(Q(s,a)\), selecting the control action requires solving
\[
\max_a Q(s,a),
\]
which is often computationally expensive and can lead to suboptimal solutions.

To address this challenge, we introduce a new paradigm: policy gradient methods. Rather than learning value functions to derive policies indirectly, we directly optimize parameterized policies using gradient-based methods.

We begin this chapter by reviewing the fundamentals of gradient-based optimization, and then build upon them to develop algorithms for searching optimal policies via policy gradients.

\section{Gradient-based Optimization}\label{gradient-optimization}

Gradient-based optimization is the workhorse behind most modern machine learning algorithms, including policy gradient methods. The central idea is to iteratively update the parameters of a model in the direction that most improves an objective function.

\subsection{Basic Setup}\label{basic-setup}

Suppose we have a differentiable objective function \(J(\theta)\), where \(\theta \in \mathbb{R}^d\) represents the parameter vector. The goal is to find
\[
\theta^\star \in \arg\max_\theta J(\theta).
\]

The gradient of the objective with respect to the parameters,
\[
\nabla_\theta J(\theta) = 
\begin{bmatrix}
\frac{\partial J}{\partial \theta_1} &
\frac{\partial J}{\partial \theta_2} &
\cdots &
\frac{\partial J}{\partial \theta_d}
\end{bmatrix}^\top,
\]
provides the local direction of steepest ascent. Gradient-based optimization uses this direction to iteratively update the parameters. Note that modern machine learning software tools such as PyTorch allow the user to conveniently query the gradient of any function \(J\) defined by neural networks.

\subsection{Gradient Ascent and Descent}\label{gradient-ascent-and-descent}

The simplest method is \textbf{gradient ascent} (for maximization):
\[
\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k),
\]
where \(\alpha > 0\) is the learning rate.\\
For minimization, the update rule uses \textbf{gradient descent}:
\[
\theta_{k+1} = \theta_k - \alpha \nabla_\theta J(\theta_k).
\]

The choice of learning rate \(\alpha\) is critical:

\begin{itemize}
\tightlist
\item
  Too large \(\alpha\) can cause divergence.\\
\item
  Too small \(\alpha\) leads to slow convergence.
\end{itemize}

\subsubsection{Convergence Guarantees}\label{convergence-guarantees}

For convex functions \(J(\theta)\), gradient descent (or ascent) can be shown to converge to the \textbf{global optimum} under appropriate conditions on the learning rate.

For non-convex functions---which are common in reinforcement learning---gradient methods may only find so-called \textbf{first-order stationary points}, i.e., points \(\theta\) at which the gradient \(\nabla_\theta J(\theta) = 0\). Nevertheless, they remain effective in practice.

{TODO: graph different stationary points}

We now formalize the convergence speed of Gradient Descent (GD) for minimizing a smooth convex function. We switch to the minimization convention and write the objective as \(f:\mathbb{R}^d \to \mathbb{R}\) (to avoid sign confusions with \(J\) used for maximization). We assume exact gradients \(\nabla f(\theta)\) are available.

\textbf{Setup and Assumptions.}

\begin{itemize}
\item
  (\textbf{Convexity}) For all \(\theta,\vartheta\in\mathbb{R}^d\),
  \begin{equation}
  f(\vartheta) \;\ge\; f(\theta) + \nabla f(\theta)^\top(\vartheta-\theta).
  \label{eq:PG-GO-convexity}
  \end{equation}
\item
  (\textbf{\(L\)-smoothness}) The gradient is \(L\)-Lipschitz: for all \(\theta,\vartheta\),
  \begin{equation}
  \|\nabla f(\vartheta)-\nabla f(\theta)\| \;\le\; L\|\vartheta-\theta\|.
  \label{eq:PG-GO-Lsmooth}
  \end{equation}
  Equivalently (the \textbf{descent lemma}), for all \(\theta,\Delta\),
  \begin{equation}
  f(\theta+\Delta) \;\le\; f(\theta) + \nabla f(\theta)^\top \Delta + \frac{L}{2}\|\Delta\|^2.
  \label{eq:PG-GO-descent-lemma}
  \end{equation}
\end{itemize}

Consider Gradient Descent with a constant stepsize \(\alpha>0\):
\[
\theta_{k+1} \;=\; \theta_k \;-\; \alpha\, \nabla f(\theta_k).
\]

\begin{theorem}[GD on smooth convex function]
\protect\hypertarget{thm:gd-convex-smooth}{}\label{thm:gd-convex-smooth}Let \(f\) be convex and \(L\)-smooth with a minimizer
\[
\theta^\star\in\arg\min_\theta f(\theta).
\]
and the global minimum \(f^\star = f(\theta^\star)\).
If \(0<\alpha\le \frac{2}{L}\), then the GD iterates satisfy for all \(k\ge 0\):
\begin{equation}
f(\theta_k) - f^\star \leq \frac{2 (f(\theta_0) - f^\star) \Vert \theta_0 - \theta^\star \Vert^2 }{2 \Vert \theta_0 - \theta^\star \Vert^2 + k\alpha ( 2 - L \alpha) (f(\theta_0) - f^\star)}
\label{eq:GD-SmoothConvex-Value}
\end{equation}
In particular, choosing \(\alpha=\frac{1}{L}\) yields the canonical \(O(1/k)\) convergence rate in suboptimality:
\begin{equation}
f(\theta_k) - f^\star \leq \frac{2L \Vert \theta_0 - \theta^\star \Vert^2}{k+4}
\label{eq:GD-SmoothConvex-optimal-rate}
\end{equation}
\end{theorem}

\begin{proof}
See Theorem 2.1.14 and Corollary 2.1.2 in \citep{nesterov2018lectures}.
\end{proof}

\textbf{Strongly Convex Case (Linear Rate).} If, in addition, \(f\) is \(\mu\)-strongly convex (\(\mu>0\)), i.e., for all \(\theta,\vartheta\in\mathbb{R}^d\),
\begin{equation}
f(\vartheta)\;\ge\; f(\theta) + \nabla f(\theta)^\top(\vartheta-\theta) \;+\; \frac{\mu}{2}\,\|\vartheta-\theta\|^2.
\label{eq:PG-GO-strongly-convex}
\end{equation}
Then, GD with \(0<\alpha\le \frac{2}{\mu + L}\) enjoys a \textbf{linear} (geometric) rate:

\begin{theorem}[GD on smooth strongly convex function]
\protect\hypertarget{thm:gd-strongly-convex}{}\label{thm:gd-strongly-convex}If \(f\) is \(L\)-smooth and \(\mu\)-strongly convex, then for \(0<\alpha\le \frac{2}{\mu + L}\),
\begin{equation}
\Vert \theta_k - \theta^\star \Vert^2 \leq \left( 1 - \frac{2\alpha \mu L}{\mu + L} \right)^k \Vert \theta_0 - \theta^\star \Vert^2.
\label{eq:GD-Strongly-Convex-1}
\end{equation}
If \(\alpha = \frac{2}{\mu + L}\), then
\begin{equation}
\begin{split}
\Vert \theta_k - \theta^\star \Vert & \leq \left( \frac{Q_f - 1}{Q_f + 1} \right)^k \Vert \theta_0 - \theta^\star \Vert \\
f(\theta_k) - f^\star & \leq \frac{L}{2} \left( \frac{Q_f - 1}{Q_f + 1} \right)^{2k} \Vert \theta_0 - \theta^\star \Vert^2,
\end{split}
\label{eq:GD-Strongly-Convex-2}
\end{equation}
where \(Q_f = L/\mu\).
\end{theorem}

\begin{proof}
See Theorem 2.1.15 in \citep{nesterov2018lectures}.
\end{proof}

\textbf{Practical Notes.}

\begin{itemize}
\item
  The step size \(\alpha=\frac{1}{L}\) is \textbf{optimal among fixed stepsizes} for the above worst-case bounds on smooth convex \(f\).
\item
  In practice, backtracking line search or adaptive schedules can approach similar behavior without knowing \(L\).
\item
  For policy gradients (which maximize \(J\)), apply the results to \(f=-J\) and flip the update sign (gradient ascent). The smooth/convex assumptions rarely hold globally in RL, but these results calibrate expectations about step sizes and motivate variance reduction and curvature-aware methods used later.
\end{itemize}

\subsection{Stochastic Gradients}\label{stochastic-gradients}

In reinforcement learning and other large-scale machine learning problems, computing the exact gradient \(\nabla_\theta J(\theta)\) is often infeasible. Instead, we use an unbiased estimator \(\hat{\nabla}_\theta J(\theta)\) computed from a subset of data (or trajectories in RL). The update becomes
\[
\theta_{k+1} = \theta_k + \alpha \hat{\nabla}_\theta J(\theta_k).
\]

This approach, known as \textbf{stochastic gradient ascent/descent (SGD)}, trades off exactness for computational efficiency. Variance in the gradient estimates plays an important role in convergence speed and stability.

\subsubsection{Convergence Guarantees}\label{convergence-guarantees-1}

We now turn to the convergence guarantees of stochastic gradient methods, which replace exact gradients with unbiased noisy estimates. Throughout this section we consider the minimization problem \(\min_\theta f(\theta)\) and assume \(\nabla f\) is available only through a stochastic oracle.

\textbf{Setup and Assumptions.}

Let \(f:\mathbb{R}^d\!\to\!\mathbb{R}\) be differentiable. At iterate \(\theta_k\), we observe a random vector \(g_k\) such that
\[
\mathbb{E}[\,g_k \mid \theta_k\,] = \nabla f(\theta_k)
\quad\text{and}\quad
\mathbb{E}\!\left[\|g_k-\nabla f(\theta_k)\|^2 \mid \theta_k\right] \le \sigma^2.
\]
We will also use one of the following standard regularity conditions:

\begin{itemize}
\tightlist
\item
  (\textbf{Convex + \(L\)-smooth}) \(f\) is convex and the gradient is \(L\)-Lipschitz.\\
\item
  (\textbf{Strongly convex + \(L\)-smooth}) \(f\) is \(\mu\)-strongly convex and \(L\)-smooth.
\end{itemize}

We consider the SGD update
\[
\theta_{k+1} \;=\; \theta_k - \alpha_k\, g_k,
\]
and define the \textbf{averaged iterate}
\[
\bar\theta_K := \frac{1}{K+1}\sum_{k=0}^{K}\theta_k.
\]

\begin{theorem}[SGD on smooth convex function]
\protect\hypertarget{thm:sgd-convex-rate}{}\label{thm:sgd-convex-rate}

Assume \(f\) is convex and \(L\)-smooth. Suppose there exists \(G\!>\!0\) with \(\mathbb{E}\|g_k\|^2 \le G^2\) for all \(k\).

\begin{itemize}
\item
  Choose a constant stepsize \(\alpha_k = \alpha > 0\). Then for all \(K \ge 1\),
  \begin{equation}
  \mathbb{E}\big[f(\bar\theta_K)\big] - f^\star \leq \frac{\Vert \theta_0 - \theta^\star \Vert^2}{2 \alpha (K+1)} + \frac{\alpha G^2}{2}.
  \label{eq:SGD-convex-fixed-step-size}
  \end{equation}
\item
  Choose a diminishing step size \(\alpha_k = \frac{\Vert \theta_0 - \theta^\star \Vert}{G \sqrt{k+1}}\), then
  \begin{equation}
  \mathbb{E}\big[f(\bar\theta_K)\big] - f^\star \leq \frac{\Vert \theta_0 - \theta^\star \Vert G}{\sqrt{K+1}} = \mathcal{O}\left(  \frac{1}{\sqrt{K}} \right).
  \label{eq:SGD-convex-diminishing-step-size}
  \end{equation}
\end{itemize}

\end{theorem}

\begin{proof}
See this \href{https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_24.pdf}{lecture note} and \citep{garrigos2023handbook}.
\end{proof}

\textbf{Remarks.}

\begin{itemize}
\item
  The bound is on the \emph{averaged} iterate \(\bar\theta_K\) (the last iterate may be worse by constants without further assumptions).
\item
  Replacing the second-moment bound by a variance bound \(\sigma^2\) yields the same rate with \(G^2\) replaced by \(\sigma^2 + \sup_k\|\nabla f(\theta_k)\|^2\).
\item
  With a constant stepsize, SGD converges \(\mathcal{O}(1/k)\) up to a neighborhood set by the gradient noise.
\end{itemize}

The next theorem states the convergence rate of SGD for minimizing strongly convex functions.

\begin{theorem}[SGD on smooth strongly convex function]
\protect\hypertarget{thm:sgd-strong-rate}{}\label{thm:sgd-strong-rate}Assume \(f\) is \(\mu\)-strongly convex and \(L\)-smooth, and \(\mathbb{E}\!\left[\|g_k\|^2 \right]\le G^2\).\\
With stepsize \(\alpha_k = \frac{1}{\mu(k+1)}\), the SGD iterates satisfy for all \(K\!\ge\!1\),
\begin{equation}
\begin{split}
\mathbb{E}[f(\bar\theta_K)] - f^\star & \leq \frac{G^2}{2 \mu (K+1)} (1 + \log(K+1)), \\
\mathbb{E} \Vert \bar\theta_K - \theta^\star \Vert^2 & \leq \frac{Q}{K+1}, \ \ Q = \max \left( \frac{G^2}{\mu^2}, \Vert \theta_0 - \theta^\star \Vert^2 \right).
\end{split}
\label{eq:SGD-Strongly-Convex}
\end{equation}
\end{theorem}

\begin{proof}
See this \href{https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_24.pdf}{lecture note} and \citep{garrigos2023handbook}.
\end{proof}

\textbf{Practical Takeaways for Policy Gradients.}

\begin{itemize}
\item
  Use \textbf{diminishing stepsizes} for theoretical convergence (\(\alpha_k \propto 1/\sqrt{k}\) for general convex, \(\alpha_k \propto 1/k\) for strongly convex surrogates).
\item
  With \textbf{constant stepsizes}, expect fast initial progress down to a variance-limited plateau; lowering variance (e.g., via baselines/advantage estimation) is as important as tuning \(\alpha\).
\end{itemize}

{TODO: graph the different trajectories between minimizing a convex function using GD and SGD.}

\subsection{Beyond Vanilla Gradient Methods}\label{beyond-vanilla-gradient-methods}

Several refinements to basic gradient updates are widely used:

\begin{itemize}
\tightlist
\item
  \textbf{Momentum methods}: incorporate past gradients to smooth updates and accelerate convergence.
\item
  \textbf{Adaptive learning rates (Adam, RMSProp, AdaGrad)}: adjust the learning rate per parameter based on historical gradient magnitudes.
\item
  \textbf{Second-order methods}: approximate or use curvature information (the Hessian) for more informed updates, though often impractical in high dimensions.
\end{itemize}

\section{Policy Gradients}\label{policy-gradients}

Policy gradients optimize a \emph{parameterized stochastic policy} directly, without requiring an explicit action-value maximization step. They are applicable to both finite and continuous action spaces and are especially useful when actions are continuous or when ``\(\arg\max\)'' over \(Q(s,a)\) is costly or ill-posed.

\subsection{Setup}\label{setup}

We consider a Markov decision process (MDP) with (possibly continuous) state space \(\mathcal{S}\), action space \(\mathcal{A}\), unknown dynamics \(P\), reward function \(R(s,a)\), and discount factor \(\gamma\in[0,1)\). Let \(\pi_\theta(a\mid s)\) be a differentiable stochastic policy with parameters \(\theta\in\mathbb{R}^d\).

\begin{itemize}
\item
  \textbf{Trajectory.} A state-action trajectory is \(\tau=(s_0,a_0,s_1,a_1,\dots,s_{T})\) with probability density/mass
  \begin{equation}
  p_\theta(\tau) = \rho(s_0)\prod_{t=0}^{T-1} \pi_\theta(a_t\mid s_t)\,P(s_{t+1}\mid s_t,a_t),
  \label{eq:trajectory-density}
  \end{equation}
  where \(\rho\) is the initial state distribution and \(T\) is the (random or fixed) episode length.
\item
  \textbf{Return.} Define the (discounted) return
  \begin{equation}
  R(\tau) \;=\; \sum_{t=0}^{T-1}\gamma^t R(s_t,a_t), 
  \label{eq:PG-Trajectory-Return}
  \end{equation}
  and the return-to-go
  \begin{equation}
  g_t \;=\; \sum_{t'=t}^{T-1}\gamma^{t'-t} R(s_{t'},a_{t'}).
  \label{eq:PG-return-to-go}
  \end{equation}
\item
  \textbf{Optimization objective.} The goal is to maximize the expected return
  \begin{equation}
  J(\theta) \;\equiv\; \mathbb{E}_{\tau\sim p_\theta}\!\left[R(\tau)\right]
  \;=\; \mathbb{E}\!\left[\sum_{t=0}^{T-1}\gamma^t R(s_t,a_t)\right],
  \label{eq:PG-objective}
  \end{equation}
  where the expectation is taken over the randomness in (i) the initial state \(s_0 \sim \rho\), (ii) the policy \(\pi_\theta\), and (iii) the transition dynamics \(P\).
\end{itemize}

\subsubsection{Policy models}\label{policy-models}

\begin{itemize}
\item
  \textbf{Finite action spaces (\(\mathcal{A}\) discrete).} A common choice is a \textbf{softmax (categorical) policy} over a score (logit) function \(f_\theta(s,a)\):
  \begin{equation}
  \pi_\theta(a\mid s)
  \;=\;
  \frac{\exp\{f_\theta(s,a)\}}{\sum_{a'\in\mathcal{A}}\exp\{f_\theta(s,a')\}}.
  \label{eq:finite-action-policy}
  \end{equation}
  Here we use \(\exp\{f_\theta(s,a)\} = e^{f_\theta(s,a)}\) for pretty formatting. Typically \(f_\theta\) is a neural network or a linear function over features.
\item
  \textbf{Continuous action spaces (\(\mathcal{A}\subseteq\mathbb{R}^m\)).} A standard choice is a \textbf{Gaussian policy}:
  \begin{equation}
  \pi_\theta(a\mid s) \;=\; \mathcal{N}\!\big(a;\;\mu_\theta(s),\,\Sigma_\theta(s)\big),
  \label{eq:continuous-action-policy}
  \end{equation}
  where \(\mu_\theta(s)\) and (often diagonal) covariance \(\Sigma_\theta(s)\) are differentiable functions (e.g., neural networks) parameterized by \(\theta\). The policy \(\pi_\theta(a \mid s)\) samples actions from the Gaussian parameterized by \(\mu_\theta(s)\) and \(\Sigma_\theta(s)\). Other choices include squashed Gaussians (e.g., \(\tanh\)) or Beta distributions for bounded actions.
\end{itemize}

\subsection{The Policy Gradient Lemma}\label{the-policy-gradient-lemma}

With the gradient-based optimization machinery from Section \ref{gradient-optimization}, a natural strategy for the policy optimization problem in \eqref{eq:PG-objective} is gradient ascent on the objective \(J(\theta)\).
Consequently, the central task is to characterize the ascent direction, i.e., to compute \(\nabla_\theta J(\theta)\).

The policy gradient lemma, stated below, provides exactly this characterization. Crucially, it expresses \(\nabla_\theta J(\theta)\) in terms of the policy's score function \(\nabla_\theta \log \pi_\theta(a\mid s)\) and returns, without differentiating through the environment dynamics. This likelihood-ratio form makes policy optimization feasible even when the transition model is unknown or non-differentiable.

\begin{theorem}[Policy Gradient Lemma]
\protect\hypertarget{thm:policy-gradient-lemma}{}\label{thm:policy-gradient-lemma}Let \(J(\theta)=\mathbb{E}_{\tau \sim p_\theta}[R(\tau)]\) as defined in \eqref{eq:PG-objective} Then:
\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Big[R(\tau)\,\nabla_\theta \log p_\theta(\tau)\Big]
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Bigg[\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t\mid s_t)\;R(\tau)\Bigg].
\label{eq:PG-PGLemma-1}
\end{equation}
By causality (future action does not affect past reward), the full return can be replaced by return-to-go:
\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Bigg[\sum_{t=0}^{T-1} \gamma^t
\nabla_\theta \log \pi_\theta(a_t\mid s_t)\;g_t\Bigg].
\label{eq:PG-PGLemma-2}
\end{equation}
Equivalently, using value functions,
\begin{equation}
\nabla_\theta J(\theta)
\;=\;
 \frac{1}{1-\gamma} \mathbb{E}_{s\sim d_\theta,\;a\sim\pi_\theta} \Big[\nabla_\theta \log \pi_\theta(a\mid s)\,Q^{\pi_\theta}(s,a)\Big],
\label{eq:PG-PGLemma-3}
\end{equation}
where \(d_\theta\) is the (discounted) on-policy state visitation distribution for infinite-horizon MDPs:
\begin{equation}
d_\theta(s) = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t \Pr_\theta(s_t=s).
\label{eq:state-visitation-distribution}
\end{equation}
\end{theorem}

\begin{proof}
We prove the three equivalent forms step by step. Throughout, we assume \(\theta\) parameterizes only the policy \(\pi_\theta\) (not the dynamics \(P\) nor the initial distribution \(\rho\)), and that interchanging \(\nabla_\theta\) with the trajectory integral/sum is justified (e.g., bounded rewards and finite horizon or standard dominated-convergence conditions).
Let the return-to-go \(g_t\) be defined as in \eqref{eq:PG-return-to-go}.

\textbf{Step 1 (Log-derivative trick).} Write the objective as an expectation over trajectories:
\[
J(\theta) \;=\; \int R(\tau)\, p_\theta(\tau)\, d\tau.
\]
Differentiate under the integral and use
\begin{equation}
\nabla_\theta p_\theta(\tau)=p_\theta(\tau)\nabla_\theta\log p_\theta(\tau)
\label{eq:log-derivative-trick}
\end{equation}
we can write:
\[
\nabla_\theta J(\theta)
= \int R(\tau)\,\nabla_\theta p_\theta(\tau)\, d\tau
= \int R(\tau)\, p_\theta(\tau)\,\nabla_\theta \log p_\theta(\tau)\, d\tau
= \mathbb{E}_{\tau\sim p_\theta}\!\big[R(\tau)\,\nabla_\theta \log p_\theta(\tau)\big],
\]
which is \eqref{eq:PG-PGLemma-1} up to expanding \(\log p_\theta(\tau)\). To see why \eqref{eq:log-derivative-trick} is true, write
\[
\nabla_\theta \log p_\theta(\tau) = \frac{1}{p_\theta(\tau)} \nabla_\theta p_\theta(\tau),
\]
using the chain rule.

\textbf{Step 2 (Policy-only dependence).} Factor the trajectory likelihood/mass:
\[
p_\theta(\tau)
= \rho(s_0)\,\prod_{t=0}^{T-1}\pi_\theta(a_t\mid s_t)\,P(s_{t+1}\mid s_t,a_t).
\]
Since \(\rho\) and \(P\) do not depend on \(\theta\),
\[
\log p_\theta(\tau)
= \text{const} \;+\; \sum_{t=0}^{T-1}\log \pi_\theta(a_t\mid s_t)
\quad\Rightarrow\quad
\nabla_\theta \log p_\theta(\tau) \;=\; \sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t).
\]
Substitute into Step 1 to obtain the second equality in \eqref{eq:PG-PGLemma-1}:
\[
\nabla_\theta J(\theta)
= \mathbb{E}_{\tau\sim p_\theta}\!\Bigg[\sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,R(\tau)\Bigg].
\]

\textbf{Step 3 (Causality \(\Rightarrow\) return-to-go).} Expand \(R(\tau)=\sum_{t=0}^{T-1}\gamma^{t} r_{t}\) (with \(r_{t}:=R(s_{t},a_{t})\)) and swap sums:
\[
\mathbb{E} \Bigg[\sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,R(\tau)\Bigg]
=
\sum_{t=0}^{T-1}\sum_{t'=0}^{T-1}\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\gamma^{t'} r_{t'}\big].
\]
For \(t'<t\), the factor \(\gamma^{t'} r_{t'}\) is measurable w.r.t. the history \(\mathcal{F}_t=\sigma(s_0,a_0,\dots,s_t)\), while
\[
\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big|\,\mathcal{F}_t\big]
= \sum_{a} \pi_\theta(a\mid s_t)\,\nabla_\theta \log \pi_\theta(a\mid s_t) = \nabla_\theta \sum_{a}\pi_\theta(a\mid s_t) = \nabla_\theta 1 = 0,
\]
(and analogously with integrals for continuous \(\mathcal{A}\)). Hence by the tower property,
\[
\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\gamma^{t'} r_{t'}\big]=0\quad\text{for all }t'<t.
\]
Therefore only the terms with \(t'\ge t\) survive, and
\[
\nabla_\theta J(\theta)
= \sum_{t=0}^{T-1}\mathbb{E} \Big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\sum_{t'=t}^{T-1}\gamma^{t'} r_{t'}\Big]
= \mathbb{E} \Bigg[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\Bigg],
\]
which is \eqref{eq:PG-PGLemma-2}.

\textbf{Step 4 (Value-function form).} Condition on \((s_t,a_t)\) and use the definition of the action-value function:
\[
Q^{\pi_\theta}(s_t,a_t) \;\equiv\; \mathbb{E}\!\left[g_t \,\middle|\, s_t,a_t\right].
\]
Taking expectations then yields
\[
\mathbb{E} \big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\big]
= \mathbb{E} \big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,Q^{\pi_\theta}(s_t,a_t)\big].
\]
Summing over \(t\) and collecting terms with the (discounted) on-policy state visitation distribution \(d_\theta\) (for the infinite-horizon case, e.g., \(d_\theta(s)=(1-\gamma)\sum_{t=0}^\infty \gamma^t\,\Pr_\theta(s_t=s)\); for finite \(T\), use the corresponding finite-horizon weighting), we obtain
\[
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{s\sim d_\theta,\;a\sim \pi_\theta} \Big[\nabla_\theta \log \pi_\theta(a\mid s)\,Q^{\pi_\theta}(s,a)\Big],
\]
which is \eqref{eq:PG-PGLemma-3}.

\textbf{Conclusion.} Combining Steps 1--4 proves all three stated forms of the policy gradient.
\end{proof}

\subsection{REINFORCE}\label{reinforce}

The policy gradient lemma immediately gives us an algorithm. Specifically, the gradient receipe in \eqref{eq:PG-PGLemma-1} tells us that if we generate one trajectory \(\tau\) by following the policy \(\pi\), then
\begin{equation}
\widehat{\nabla_\theta J} = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \mid s_t) R(\tau)
\label{eq:PG-Estimator-1}
\end{equation}
is an unbiased estimator of the true gradient.

With this sample gradient estimator, we obtain the classical REINFORCE algorithm.

\textbf{Single-Trajectory (Naive) REINFORCE}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize \(\theta_0\) for the initial policy \(\pi_{\theta_0}(a \mid s)\)
\item
  For \(k=0,1,\dots,\) do:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Obtain a trajectory \(\tau \sim p_{\theta_k}\)
\item
  Compute the stochastic gradient \(g_k\) as in \eqref{eq:PG-Estimator-1}
\item
  Update \(\theta_{k+1} = \theta_k + \alpha_k g_k\)
\end{itemize}

To reduce variance of the gradient estimator, we can use a minibatch of trajectories. For example, given a batch of \(N\) trajectories \(\{\tau^{(i)}\}_{i=1}^N\) collected by \(\pi_\theta\), define for each timestep the return-to-go
\[
g_t^{(i)} = \sum_{t'=t}^{T^{(i)}-1} \gamma^{t'-t} R\!\left(s_{t'}^{(i)},a_{t'}^{(i)}\right).
\]
An unbiased gradient estimator, from \eqref{eq:PG-PGLemma-2} is
\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1} \gamma^t
\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big) g_t^{(i)}.
\label{eq:PG-Estimator-2}
\end{equation}

This leads to the following minibatch REINFORCE algorithm.

\textbf{Minibatch REINFORCE}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize \(\theta_0\) for the initial policy \(\pi_{\theta_0}(a \mid s)\)
\item
  For \(k=0,1,\dots,\) do:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Obtain N trajectories \(\{ \tau^{(i)} \}_{i=1}^N \sim p_{\theta_k}\)
\item
  Compute the stochastic gradient \(g_k\) as in \eqref{eq:PG-Estimator-2}
\item
  Update \(\theta_{k+1} = \theta_k + \alpha_k g_k\)
\end{itemize}

We apply both the single-trajectory (naive) REINFORCE and a minibatch variant to the CartPole-v1 balancing task. The results show that variance reduction via minibatching is crucial for stable learning and for obtaining strong policies with policy-gradient methods.

\begin{example}[REINFORCE for Cart-Pole Balancing]
\protect\hypertarget{exm:cartpole-reinforce}{}\label{exm:cartpole-reinforce}

Consider the cart-pole balancing task illustrated in Fig. \ref{fig:cart-pole-illustration}. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cart_pole} 

}

\caption{Cart Pole balance.}\label{fig:cart-pole-illustration}
\end{figure}

\textbf{State Space.} The state of the cart-pole system is denoted by \(s \in \mathcal{S} \subset \mathbb{R}^4\), containing the position and velocity of the cart, as well as the angle and angular velocity of the pole.

\textbf{Action Space.} The action space \(\mathcal{A}\) is discrete and contains two elements: pushing to the left and pushing to the right.

The dynamics of the MDP is provided by the \href{https://gymnasium.farama.org/environments/classic_control/cart_pole/}{Gym simulator} and is described in the original paper \citep{barto2012neuronlike}. At the beginning of the episode, all state variables are randomly initialized in \([-0.05,0.05]\) and the goal for the agent is to apply the actions to balance the cart-pole for as long as possible---the agent gets a reward of \(+1\) every step if (1) the pole angle remains between \(-12^\circ\) and \(+12^\circ\) and (2) the cart position remains between \(-2.4\) and \(2.4\). The maximum episode length is \(500\).

We design a policy network in the form of \eqref{eq:finite-action-policy} since the action space is finite.

\textbf{REINFORCE.} We first apply the naive REINFORCE algorithm where the gradient estimator is computed from a single trajectory as in \eqref{eq:PG-Estimator-1}. Fig. \ref{fig:cart-pole-learning-curve-reinforce} shows the learning curve, which indicates that the REINFORCE algorithm was not able to learn a good policy after 2000 episodes.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_returns_reinforce} 

}

\caption{Learning curve (Naive REINFORCE).}\label{fig:cart-pole-learning-curve-reinforce}
\end{figure}

\textbf{Minibatch REINFORCE.} We then apply the minibatch REINFORCE algorithm where the gradient estimator is computed from multiple (\(20\) in our case) trajectories as in \eqref{eq:PG-Estimator-2}. Fig. \ref{fig:cart-pole-learning-curve-minibatch-reinforce} shows the learning curve, which shows steady increase in the per-episode return that eventually gets close to the maximum per-episode return \(500\).

Fig. \ref{fig:cart-pole-policy-rollout-minibatch-reinforce} shows a rollout video of applying the policy training from minibatch REINFORCE. We can see the policy nicely balances the cart-pole system.

You can play with the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_reinforce.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_returns_minibatch_reinforce} 

}

\caption{Learning curve (Minibatch REINFORCE).}\label{fig:cart-pole-learning-curve-minibatch-reinforce}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_policy_rollout_minibatch_reinforce} 

}

\caption{Policy rollout (Minibatch REINFORCE).}\label{fig:cart-pole-policy-rollout-minibatch-reinforce}
\end{figure}

\end{example}

\subsection{Baselines and Variance Reduction}\label{baselines-and-variance-reduction}

From the REINFORCE experiments above, we have seen firsthand that \textbf{variance reduction} is critical for stable policy-gradient learning.

A natural question is: \emph{what framework can we use to systematically reduce the variance of the gradient estimator while preserving unbiasedness?}

\subsubsection{Baseline}\label{baseline}

A key device is a \textbf{baseline} \(b:\mathcal{S}\to\mathbb{R}\) added at each timestep:
\begin{equation}
\widehat{g}
\;=\;
\sum_{t=0}^{T-1} \gamma^t\,\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t - b(s_t)\big).
\label{eq:baseline-estimator}
\end{equation}

The only difference between \eqref{eq:baseline-estimator} and the original gradient estimator \eqref{eq:PG-PGLemma-2} is that the baseline \(b(s_t)\) is subtracted from the return-to-go \(g_t\). The next theorem states that any state-only baseline does not change the expectation of the gradient estimator.

\begin{theorem}[Baseline Invariance]
\protect\hypertarget{thm:baseline-invariance}{}\label{thm:baseline-invariance}Let \(b:\mathcal{S}\to\mathbb{R}\) be any function independent of the action \(a_t\). Then
\[
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]=0,
\]
and thus
\begin{equation}
\nabla_\theta J(\theta) \;=\; 
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t - b(s_t)\big)\right].
\label{eq:PG-baseline-form}
\end{equation}

Equivalently, using action-values,
\begin{equation}
\nabla_\theta J(\theta)
=
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\theta,\;a\sim\pi_\theta}
\!\Big[\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^{\pi_\theta}(s,a)-b(s)\big)\Big].
\label{eq:baseline-estimator-Qvalue}
\end{equation}
\end{theorem}

\begin{proof}
We prove (i) the baseline term has zero expectation, (ii) the baseline-subtracted estimator in \eqref{eq:PG-baseline-form} is unbiased, and (iii) the equivalent \(Q\)-value form \eqref{eq:baseline-estimator-Qvalue}.

Throughout we assume standard conditions ensuring interchange of expectation and differentiation (e.g., bounded rewards with finite horizon or discounted infinite horizon, and a differentiable policy).

\textbf{Step 1 (Score-function expectation is zero).} Fix a state \(s\in\mathcal{S}\). The \textbf{score function} integrates/sums to zero under the policy:
\begin{equation}
\begin{split}
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s)}\!\big[\nabla_\theta \log \pi_\theta(a\mid s)\big]
& =
\sum_{a\in\mathcal{A}} \pi_\theta(a\mid s)\,\nabla_\theta \log \pi_\theta(a\mid s)
=
\sum_{a\in\mathcal{A}} \nabla_\theta \pi_\theta(a\mid s) \\
& = 
\nabla_\theta \sum_{a\in\mathcal{A}} \pi_\theta(a\mid s)
=
\nabla_\theta 1
=
0,
\end{split}
\end{equation}
with the obvious replacement of sums by integrals for continuous \(\mathcal{A}\). This identity is the standard ``score has zero mean'' property.

\textbf{Step 2 (Baseline term has zero expectation).} Let \(\mathcal{F}_t := \sigma(s_0,a_0,\ldots,s_t)\) be the history up to time \(t\) and recall that \(b(s_t)\) is \textbf{independent of} \(a_t\). Using iterated expectations:
\[
\mathbb{E}\!\left[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]
=
\mathbb{E}\!\left[
\gamma^t\, b(s_t)\,
\underbrace{\mathbb{E}\!\left[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\middle|\, s_t\right]}_{=\,0~\text{by Step 1}}
\right]
= 0.
\]
Summing over \(t\) yields
\[
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]=0.
\]

\textbf{Step 3 (Unbiasedness of the baseline-subtracted estimator).} By the policy gradient lemma (likelihood-ratio form with return-to-go; see \eqref{eq:PG-PGLemma-2}),
\[
\nabla_\theta J(\theta)
=
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\right].
\]
Subtract and add the baseline term inside the expectation:
\[
  \begin{split}
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\right]
& =
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\right]
\;+\; \\
& \quad \quad \underbrace{\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]}_{=\,0~\text{by Step 2}}.
\end{split}
\]
Therefore \eqref{eq:PG-baseline-form} holds, proving that \textbf{any} state-only baseline preserves unbiasedness.

\textbf{Step 4 (Equivalent \(Q\)-value form).} Condition on \((s_t,a_t)\) and use the definition \(Q^{\pi_\theta}(s_t,a_t):=\mathbb{E}[g_t\mid s_t,a_t]\):
\[
\mathbb{E}\!\big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\big]
=
\mathbb{E}\!\Big[
\gamma^t\,
\mathbb{E}\!\big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\mid s_t\big]
\Big].
\]
Inside the inner expectation (over \(a_t\sim \pi_\theta(\cdot\mid s_t)\)) and using \(b(s_t)\)'s independence from \(a_t\),
\[
\mathbb{E}\!\big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\mid s_t\big]
=
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s_t)}\!\Big[\nabla_\theta \log \pi_\theta(a\mid s_t)\,\big(Q^{\pi_\theta}(s_t,a)-b(s_t)\big)\Big].
\]
Summing over \(t\) with discount \(\gamma^t\) and collecting terms with the (discounted) on-policy state-visitation distribution \(d_\theta\) (cf.~\eqref{eq:state-visitation-distribution}) yields the infinite-horizon identity
\[
\nabla_\theta J(\theta)
=
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\theta,\;a\sim \pi_\theta}\!
\Big[\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^{\pi_\theta}(s,a)-b(s)\big)\Big],
\]
which is \eqref{eq:baseline-estimator-Qvalue}.
\end{proof}

\subsubsection{Optimal Baseline and Advantage}\label{optimal-baseline-and-advantage}

Among all state-only baselines \(b(s)\), which one minimizes the variance of the gradient estimator?

\begin{theorem}[Variance-Minimizing Baseline (per-state)]
\protect\hypertarget{thm:variance-minimizing-baseline}{}\label{thm:variance-minimizing-baseline}For the estimator
\[
g(s,a)=\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big),
\]
the \(b(s)\) minimizing \(\operatorname{Var}[g\mid s]\) is
\[
b^\star(s)=
\frac{\mathbb{E}_{a\sim \pi_\theta}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\, Q^\pi(s,a)\right]}
{\mathbb{E}_{a\sim \pi_\theta}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\right]}.
\label{eq:best-baseline}
\]
Assuming that the norm factor \(\Vert \nabla_\theta \log \pi_\theta(a\mid s) \Vert^2\) varies slowly with \(a\), then
\[
b^\star(s) \approx V^\pi(s).
\]
\end{theorem}

\begin{proof}
Let \(s\in\mathcal{S}\) be fixed and write
\[
u(a\mid s) \;\equiv\; \nabla_\theta \log \pi_\theta(a\mid s)\in\mathbb{R}^d,
\qquad
w(a\mid s) \;\equiv\; \|u(a\mid s)\|^2 \;\ge 0.
\]
Consider the vector-valued random variable
\[
g(s,a) \;=\; u(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big),
\]
where the randomness is over \(a\sim \pi_\theta(\cdot\mid s)\).

We aim to choose \(b(s)\in\mathbb{R}\) to minimize the \textbf{conditional variance}
\[
\operatorname{Var}[g\mid s] \;=\; \mathbb{E}\!\left[\|g(s,a)-\mathbb{E}[g\mid s]\|^2 \,\middle|\, s\right].
\]
Using the identity \(\operatorname{Var}[X]=\mathbb{E}\|X\|^2-\|\mathbb{E}X\|^2\) (for vector \(X\) with Euclidean norm), we have
\[
\operatorname{Var}[g\mid s]
\;=\;
\underbrace{\mathbb{E}\!\left[\|g(s,a)\|^2 \mid s\right]}_{\text{depends on } b(s)}
\;-\;
\underbrace{\big\|\mathbb{E}[g\mid s]\big\|^2}_{\text{independent of } b(s)}.
\]
We first show that the mean term is independent of \(b(s)\). Indeed,
\[
\mathbb{E}[g\mid s]
=
\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\big[u(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)\big]
=
\mathbb{E}\!\big[u(a\mid s)\,Q^\pi(s,a)\big]
\;-\;
b(s)\,\underbrace{\mathbb{E}\!\big[u(a\mid s)\big]}_{=\,0},
\]
where \(\mathbb{E}[u(a\mid s)]=\sum_a \pi_\theta(a\mid s)\nabla_\theta\log\pi_\theta(a\mid s)=\nabla_\theta \sum_a \pi_\theta(a\mid s)=\nabla_\theta 1=0\) (replace sums by integrals in the continuous case). Therefore \(\mathbb{E}[g\mid s]\) does \textbf{not} depend on \(b(s)\).

Consequently, minimizing \(\operatorname{Var}[g\mid s]\) is equivalent to minimizing the conditional \textbf{second moment}
\[
\mathbb{E}\!\left[\|g(s,a)\|^2 \mid s\right]
=
\mathbb{E}\!\left[\|u(a\mid s)\|^2 \,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right]
=
\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right].
\]
The right-hand side is a convex quadratic in the scalar \(b(s)\). Differentiate w.r.t. \(b(s)\) and set to zero:
\[
\frac{\partial}{\partial b(s)}
\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right]
=
-2\,\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)\,\middle|\, s\right]
= 0.
\]
Hence,
\[
\mathbb{E}\!\left[w(a\mid s)\,Q^\pi(s,a)\,\middle|\, s\right]
=
b(s)\,\mathbb{E}\!\left[w(a\mid s)\,\middle|\, s\right],
\]
and provided \(\mathbb{E}[w(a\mid s)\mid s]>0\) (i.e., the Fisher information at \(s\) is non-degenerate), the unique minimizer is
\[
b^\star(s)
=
\frac{\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\, Q^\pi(s,a)\right]}
{\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\right]},
\]
which is \eqref{eq:best-baseline}. If \(\mathbb{E}[w(a\mid s)\mid s]=0\) (e.g., a locally deterministic policy), then \(g\equiv 0\) almost surely and any \(b(s)\) attains the minimum.

Finally, when the weight \(w(a\mid s)=\|\nabla_\theta \log \pi_\theta(a\mid s)\|^2\) varies slowly with \(a\) (or is approximately constant) for a fixed \(s\), the ratio simplifies to
\[
b^\star(s)\;\approx\;\frac{\mathbb{E}[c(s)\,Q^\pi(s,a)\mid s]}{\mathbb{E}[c(s)\mid s]}
\;=\;
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s)}\!\big[Q^\pi(s,a)\big]
\;=\;
V^\pi(s),
\]
so that the baseline-subtracted target becomes the \textbf{advantage}
\(A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)\).
\end{proof}

When using \(V^\pi(s)\) as the baseline, the baseline-subtracted target is called the \textbf{advantage function}
\begin{equation}
A^{\pi_\theta}(s,a)\;=\;Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s).
\label{eq:advantage-def}
\end{equation}
The corresponding minibatch gradient estimator becomes
\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1} \gamma^t\,
\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big)\,
\widehat{A}_t^{(i)},
\quad
\widehat{A}_t^{(i)} \approx g_t^{(i)} - V_\phi \big(s_t^{(i)}\big),
\label{eq:reinforce-adv-estimator}
\end{equation}
where \(V_\phi\) is a learned approximation to \(V^{\pi_\theta}\).

\subsubsection{Intuition for the Advantage}\label{intuition-for-the-advantage}

The advantage
\[
A^\pi(s,a) \;=\; Q^\pi(s,a) - V^\pi(s)
\]
measures how much \emph{better or worse} action \(a\) is at state \(s\) \emph{relative to the policy's average action quality} \(V^\pi(s)=\mathbb{E}_{a\sim\pi}[Q^\pi(s,a)\mid s]\).\\
Hence \(\mathbb{E}_{a\sim\pi}[A^\pi(s,a)\mid s]=0\): it is a \emph{relative} score.

With a value baseline, the policy-gradient update is
\[
\nabla_\theta J(\theta)
\;=\;
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\pi,\;a\sim\pi}\!\big[
\nabla_\theta \log \pi_\theta(a\mid s)\,A^\pi(s,a)
\big].
\]

\begin{itemize}
\item
  If \(A^\pi(s,a) > 0\): the term \(\nabla_\theta \log \pi_\theta(a\mid s)\,A^\pi(s,a)\) \textbf{increases} \(\log \pi_\theta(a\mid s)\) (and thus \(\pi_\theta(a\mid s)\))---the policy puts \textbf{more} probability mass on actions that outperformed its average at \(s\).
\item
  If \(A^\pi(s,a) < 0\): it \textbf{decreases} \(\log \pi_\theta(a\mid s)\)---the policy puts \textbf{less} probability mass on actions that underperformed at \(s\).
\item
  If \(A^\pi(s,a) \approx 0\): the action performed about as expected; the update at that \((s,a)\) is \textbf{negligible}.
\end{itemize}

Subtracting \(V^\pi(s)\) centers returns \emph{per state}, so the update depends only on \emph{relative} goodness. This:

\begin{itemize}
\item
  preserves unbiasedness (baseline invariance),
\item
  reduces variance (no large, shared offset),
\item
  focuses learning on which actions at \(s\) should get more/less probability.
\end{itemize}

\subsubsection{REINFORCE with a Learned Value Baseline}\label{REINFORCE-LearnedValue}

Recall that in Section \ref{function-approximation}, we have introduced multiple algorithms that can learn an approximate value function for policy evaluation. For example, we can use Monte Carlo estimation.

We now combine REINFORCE with a learned baseline \(V_\phi(s)\approx V^{\pi_\theta}(s)\), yielding a lower-variance update while keeping the estimator unbiased.

\textbf{Minibatch REINFORCE with a Learned Value Baseline}

\textbf{Inputs:} policy \(\pi_\theta(a\mid s)\), value \(V_\phi(s)\), discount \(\gamma\in[0,1)\), stepsizes \(\alpha_\theta,\alpha_\phi>0\), batch size \(N\).\\
\textbf{Convergence controls:} tolerance \(\varepsilon>0\), maximum inner steps \(K_{\max}\) (value-fit loop), optional patience \(P\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Collect trajectories.} Roll out \(N\) on-policy trajectories \(\{\tau^{(i)}\}_{i=1}^N\) using \(\pi_\theta\).\\
  For each trajectory \(i\) and timestep \(t\), record \((s_t^{(i)},a_t^{(i)},r_t^{(i)})\).
\item
  \textbf{Compute returns-to-go.} For each \(i,t\),
  \[
  g_t^{(i)} \;=\; \sum_{t'=t}^{T^{(i)}-1} \gamma^{\,t'-t}\, r_{t'}^{(i)}.
  \]
\item
  \textbf{Fit the value to convergence (critic inner loop).} Define the batch regression loss
  \[
  \mathcal{L}_V(\phi)
  \;=\;
  \frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1}
  \big(g_t^{(i)} - V_\phi(s_t^{(i)})\big)^2.
  \]
  Perform gradient steps on \(\phi\) \textbf{until convergence} on this fixed batch:
  \[
  \phi \leftarrow \phi - \alpha_\phi \,\nabla_\phi \mathcal{L}_V(\phi).
  \]
  Repeat for \(k=1,\dots,K_{\max}\) or until
  \[
  \frac{\mathcal{L}_V^{(k-1)}-\mathcal{L}_V^{(k)}}{\max\{1,|\mathcal{L}_V^{(k-1)}|\}} < \varepsilon
  \]
  for \(M\) consecutive checks.
  Denote the (approximately) converged parameters by \(\phi^\star\).
\item
  \textbf{Form (optionally standardized) advantages using the converged value.}
  \[
  \widehat{A}_t^{(i)} \;=\; g_t^{(i)} - V_{\phi^\star}\!\big(s_t^{(i)}\big),
  \qquad
  \tilde{A}_t^{(i)} \;=\; \frac{\widehat{A}_t^{(i)} - \mu_A}{\sigma_A+\delta}\ \ (\text{optional, batch-wise}),
  \]
  where \(\mu_A,\sigma_A\) are the mean and std of \(\{\widehat{A}_t^{(i)}\}\) over the \textbf{whole} batch, and \(\delta>0\) is a small constant.
\item
  \textbf{Single policy (actor) update.} Using the converged baseline, take \textbf{one} ascent step:
  \[
  \theta \;\leftarrow\; \theta
  \;+\; \alpha_\theta \cdot
  \frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1}
  \gamma^t\,\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big)\,\tilde{A}_t^{(i)}.
  \]
  \emph{(If not standardizing, use \(\widehat{A}_t^{(i)}\) in place of \(\tilde{A}_t^{(i)}\).)}
\item
  \textbf{Repeat} from Step 1 with the updated policy.
\end{enumerate}

\textbf{Notes.}

\begin{itemize}
\item
  By baseline invariance, subtracting \(V_{\phi^\star}(s)\) keeps the policy-gradient unbiased while reducing variance.
\item
  Converging the critic on each fixed batch (Steps 3--4) approximates the variance-minimizing baseline for that batch before a single actor step, often stabilizing learning in high-variance settings.
\end{itemize}

\begin{example}[REINFORCE with a Learned Value Baseline for Cart-Pole]
\protect\hypertarget{exm:cartpole-reinforce-learned-value}{}\label{exm:cartpole-reinforce-learned-value}

Consider the same cart-pole balancing task in Example \ref{exm:cartpole-reinforce}. We use minibatch REINFORCE with a learned value baseline (batch size \(50\)), the algorithm described above.

Fig. \ref{fig:cart-pole-learning-curve-minibatch-reinforce-learned-value} shows the learning curve. The algorithm is able to steadily increase the per-episode returns.

Fig. \ref{fig:cart-pole-policy-rollout-minibatch-reinforce-learned-value} shows a rollout of the system trajectory under the learned policy.

You can play with the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_reinforce_learned_value.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_returns_minibatch_reinforce_value_baseline} 

}

\caption{Learning curve (Minibatch REINFORCE with a Learned Value Baseline).}\label{fig:cart-pole-learning-curve-minibatch-reinforce-learned-value}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_policy_rollout_minibatch_reinforce_value_baseline} 

}

\caption{Policy rollout (Minibatch REINFORCE with a Learned Value Baseline).}\label{fig:cart-pole-policy-rollout-minibatch-reinforce-learned-value}
\end{figure}

\end{example}

\section{Actor--Critic Methods}\label{actorcritic-methods}

Actor--critic (AC) algorithms marry \textbf{policy gradients} (the \emph{actor}) with \textbf{value function learning} (the \emph{critic}). The critic reduces variance by supplying low-noise estimates of action quality (values or advantages), while the actor updates the policy using these estimates. In contrast to pure Monte Carlo baselines, actor--critic \textbf{bootstraps} from its own predictions, enabling online, incremental, and often more sample-efficient learning.

\subsection{Anatomy of an Actor--Critic}\label{anatomy-of-an-actorcritic}

\begin{itemize}
\tightlist
\item
  \textbf{Actor (policy):} a differentiable policy \(\pi_\theta(a\mid s)\).
\item
  \textbf{Critic (value):} an approximator for \(V_\phi(s)\), \(Q_\psi(s,a)\), or directly the advantage \(A_\eta(s,a)\).
\item
  \textbf{Update coupling:} the actor ascends a baseline-subtracted log-likelihood objective using \emph{advantage-like} targets supplied by the critic.
\end{itemize}

\subsection{On-Policy Actor--Critic with TD(0)}\label{ActorCriticTD}

We first learn a state value function \(V_\phi(s)\) with a \textbf{one-step bootstrapped} TD(0) target:
\[
\delta_t \;\equiv\; r_t + \gamma\,V_\phi(s_{t+1}) - V_\phi(s_t),
\qquad
\mathcal{L}_V(\phi) \;=\; \frac12\,\delta_t^{\,2}.
\label{eq:td0}
\]
If \(V_\phi \approx V^\pi\), then \(\mathbb{E}[\delta_t\mid s_t,a_t]\approx A^\pi(s_t,a_t)\), so \(\delta_t\) serves as a low-variance \textbf{advantage} target for the actor:
\[
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{|\mathcal{B}|}
\sum_{(s_t,a_t)\in \mathcal{B}}
\nabla_\theta \log\pi_\theta(a_t\mid s_t)\,\underbrace{\delta_t}_{\text{advantage target}}.
\label{eq:ac-actor-delta}
\]
\emph{(Practical: normalize \(\{\delta_t\}_{\mathcal{B}}\) to mean \(0\) and unit variance within a batch; clip gradients for stability.)}

\textbf{On-Policy Actor--Critic with One-Step Bootstrap (TD(0))}

\textbf{Inputs:} policy \(\pi_\theta(a\mid s)\), value \(V_\phi(s)\), discount \(\gamma\in[0,1)\), stepsizes \(\alpha_\theta,\alpha_\phi>0\), rollout length \(K\), minibatch size \(|\mathcal{B}|\).

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Collect on-policy rollouts.} Run \(\pi_\theta\) for \(K\) steps (optionally across parallel envs), storing transitions \(\{(s_t,a_t,r_t,s_{t+1}\}\).
\item
  \textbf{Compute TD errors.} For each transition, compute the TD error
  \[
  \delta_t \leftarrow r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t).
  \]
\item
  \textbf{Critic update (value).} Minimize \(\sum_{t\in\mathcal{B}} \frac12\,\delta_t^{\,2}\): perform multiple steps of
  \[
  \phi \leftarrow \phi - \alpha_\phi \,\nabla_\phi \Big(\frac{1}{|\mathcal{B}|}\sum_{t\in\mathcal{B}}\frac12\,\delta_t^{\,2}\Big).
  \]
\item
  \textbf{Actor advantages.} Set \(\widehat{A}_t \leftarrow \delta_t\) (optionally normalize over \(\mathcal{B}\)).
\item
  \textbf{Actor update (policy gradient).}
  \[
  \theta \leftarrow \theta + \alpha_\theta \,\frac{1}{|\mathcal{B}|}\sum_{t\in\mathcal{B}}
  \nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t.
  \]
\item
  \textbf{Repeat} from step 1.
\end{enumerate}

We apply the on-policy actor-critic algorithm to the cart-pole balancing task.

\begin{example}[Actor–Critic with One-Step Bootstrap for Cart-Pole]
\protect\hypertarget{exm:cartpole-actor-critic-TD}{}\label{exm:cartpole-actor-critic-TD}

Consider the same cart-pole balancing control task as before, and this time apply the on-policy actor-critic with one-step bootstrap.

Fig. \ref{fig:cart-pole-learning-curve-actor-critic-TD} shows the learning curve.

Fig. \ref{fig:cart-pole-policy-rollout-actor-critic-TD} shows an example rollout of the policy.

You can play with the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_actor-critic_on-policy_td0.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_returns_actor-critic-td} 

}

\caption{Learning curve (Actor--Critic with One-Step Bootstrap).}\label{fig:cart-pole-learning-curve-actor-critic-TD}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_policy_rollout_actor-critic-td} 

}

\caption{Policy rollout (Actor--Critic with One-Step Bootstrap).}\label{fig:cart-pole-policy-rollout-actor-critic-TD}
\end{figure}

\end{example}

\subsection{Generalized Advantage Estimation (GAE)}\label{PG-GAE}

In REINFORCE with a learned value baseline (Section \ref{REINFORCE-LearnedValue}), we used the full Monte Carlo return \(g_t\) as the target for value function approximation; while in on-policy Actor-Critic with TD(0) (Section \ref{ActorCriticTD}), we used the one-step bootstrap return \(r_t + \gamma V_\phi (s_{t+1})\) as the target for value function estimation.

Recall in policy evaluation (Section \ref{tabular-PE}), we have introduced a spectrum of methods that sit in between Monte Carlo and TD(0): they are methods that leverage the \(n\)-step bootstrap return that balance bias and variance. (Section \ref{PE-MultiStepTD} and \ref{TDlambda}).

In particular, recall the definition of an \(n\)-step bootstrap return
\begin{equation}
g^{(n)}_t = \sum_{k=0}^{n-1} \gamma^k r_{t+k} \;+\; \gamma^{n}\,V_\phi(s_{t+n}),
\label{eq:nstep-target}
\end{equation}
where \(V_\phi\) denotes the approximate value function.
The \(\lambda\)-return (with \(\lambda \in [0,1]\)) performs a convex combination of all the \(n\)-step returns
\begin{equation}
g^{(\lambda)}_t = (1-\lambda)\sum_{n=1}^{\infty} \lambda^{n-1} \, g^{(n)}_t.
\label{eq:lambda-return}
\end{equation}

The Generalized Advantage Estimation (GAE) algorithm \citep{schulman2015high} is an Actor-Critic type of policy gradient method that leverages the \(\lambda\)-return as the target for fitting the critic (i.e., the approximate value function).

\textbf{GAE-\(\lambda\) Advantage.} Start from the TD residual
\[
\delta_t \;=\; r_t + \gamma\,V_\phi(s_{t+1}) - V_\phi(s_t),
\qquad
\label{eq:td-residual}
\]
and define the GAE-\(\lambda\) advantage as the exponentially-weighted sum of future TD residuals:
\[
\widehat{A}^{(\lambda)}_t
\;=\;
\sum_{\ell=0}^{T-1-t}
(\gamma\lambda)^\ell \,\delta_{t+\ell}.
\qquad
\label{eq:gae-def}
\]
This admits an efficient backward recursion:
\[
\widehat{A}^{(\lambda)}_t
\;=\;
\delta_t \;+\; \gamma\lambda\,\widehat{A}^{(\lambda)}_{t+1},
\quad
\widehat{A}^{(\lambda)}_{T}=0
\text{ (at terminal).}
\qquad
\label{eq:gae-recursion}
\]

\textbf{From Advantage to Return.} A key identity (obtained by expanding the sum of TD residuals and grouping terms) is
\[
\sum_{\ell=0}^{\infty} (\gamma\lambda)^\ell \,\delta_{t+\ell}
\;=\;
\Big(1-\lambda\Big)\sum_{n=1}^{\infty}\lambda^{n-1}\Big(g^{(n)}_t - V_\phi(s_t)\Big).
\label{eq:gae-equals-mix}
\]
The left-hand side is the GAE-\(\lambda\) advantage, and the right-hand side is \(g^{(\lambda)}_t - V_{\phi}(s_t)\). Therefore,
\[
\widehat{A}^{(\lambda)}_t
\;=\;
g^{(\lambda)}_t \;-\; V_\phi(s_t),
\qquad\text{and hence}\qquad
g_t^{(\lambda)}
\;=\;
\widehat{A}^{(\lambda)}_t + V_\phi(s_t).
\]
In GAE, we use
\[
\widehat{V}^{\,\text{targ}}_t
\;=\;
\widehat{A}^{(\lambda)}_t \;+\; V_\phi(s_t),
\label{eq:gae-vtarget}
\]
as the target for fitting \(V_\phi\).

\textbf{GAE Policy Gradient.} The true on-policy policy gradient can be written as
\[
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}\!\left[\sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,A^\pi(s_t,a_t)\right].
\qquad
\label{eq:true-pg}
\]
An estimator remains unbiased if we replace \(A^\pi\) by any \(\widehat{A}\) satisfying
\[
\mathbb{E}\!\left[\widehat{A}_t \,\middle|\, s_t,a_t\right] \;=\; A^\pi(s_t,a_t).
\qquad
\label{eq:cond-unbiased}
\]

When the critic is exact, \(V_\phi\equiv V^\pi\), each \(n\)-step bootstrap return has expectation
\[
\mathbb{E}\!\left[g^{(n)}_t \,\middle|\, s_t,a_t\right] \;=\; Q^\pi(s_t,a_t),
\]
so by linearity and \eqref{eq:lambda-return},
\[
\mathbb{E}\!\left[g^{(\lambda)}_t \,\middle|\, s_t,a_t\right] \;=\; Q^\pi(s_t,a_t).
\]
Using \(\widehat{A}^{(\lambda)}_t = g^{(\lambda)}_t - V^\pi(s_t)\) gives
\[
\mathbb{E}\!\left[\widehat{A}^{(\lambda)}_t \,\middle|\, s_t,a_t\right]
\;=\; Q^\pi(s_t,a_t) - V^\pi(s_t)
\;=\; A^\pi(s_t,a_t),
\]
which satisfies \eqref{eq:cond-unbiased}. Plugging \(\widehat{A}^{(\lambda)}_t\) into \eqref{eq:true-pg} thus yields an unbiased policy-gradient estimator.

The pseudocode for GAE is presented below.

\textbf{On-Policy Actor--Critic with Generalized Advantage Estimation (GAE)}

\textbf{Inputs:} policy \(\pi_\theta(a\mid s)\), value \(V_\phi(s)\), discount \(\gamma\in[0,1)\), GAE parameter \(\lambda\in[0,1]\); stepsizes \(\alpha_\theta,\alpha_\phi>0\); rollout length \(T\); minibatch size \(B\).

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Collect rollouts.} Run \(\pi_\theta\) to collect \(B\) trajectories and each trajectory has \(T\) steps , storing \((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\).
\item
  \textbf{Values \& residuals.} Compute
  \[
  v_t\!\leftarrow\!V_\phi(s_t), \ \ v_{t+1}\!\leftarrow\!V_\phi(s_{t+1}),\ \ m_t\!\leftarrow\!1-\mathrm{done}_t, \ \ \delta_t \leftarrow r_t + \gamma m_t v_{t+1} - v_t.
  \]
\item
  \textbf{Backward GAE.} Set \(\widehat{A}_{T}\!\leftarrow\!0\), and for \(t=T-1\) to \(0\) do:\\
  \[
  \widehat{A}_t \leftarrow \delta_t + \gamma\lambda m_t \widehat{A}_{t+1}.
  \]
  \emph{(Optionally normalize \(\{\widehat{A}_t\}\) within the minibatch.)}
\item
  \textbf{Critic target (\(\lambda\)-return).} Set critic target
  \[
  \widehat{V}^{\,\text{targ}}_t \leftarrow \widehat{A}_t + v_t \;\;(=g^{(\lambda)}_t).
  \]
\item
  \textbf{Critic update.} Gradient descent:
  \[
  \phi \leftarrow \phi - \alpha_\phi \nabla_\phi \frac{1}{ B }\sum_{t }\big(V_\phi(s_t)-\widehat{V}^{\,\text{targ}}_t\big)^2.
  \]\\
  \emph{(Often take several critic steps here.)}
\item
  \textbf{Actor update.} Gradient ascent
  \[
  \theta \leftarrow \theta + \alpha_\theta \frac{1}{ B }\sum_{t } \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\widehat{A}_t.
  \]
\end{enumerate}

The next example applies GAE to the cart-pole balancing problem.

\begin{example}[GAE for Cart-Pole Balancing]
\protect\hypertarget{exm:cartpole-gae}{}\label{exm:cartpole-gae}

Fig. \ref{fig:cart-pole-learning-curve-gae} shows the learning curve using Actor-Critic with GAE and Fig. \ref{fig:cart-pole-policy-rollout-gae} shows a sample rollout of the trained policy.

The Python code can be found \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_actor-critic_gae.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_returns_gae} 

}

\caption{Learning curve (Actor--Critic with GAE).}\label{fig:cart-pole-learning-curve-gae}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_policy_rollout_gae} 

}

\caption{Policy rollout (Actor--Critic with GAE).}\label{fig:cart-pole-policy-rollout-gae}
\end{figure}

\end{example}

\subsection{Off-Policy Actor--Critic}\label{off-policy-actorcritic}

On-policy actor--critic discards data after a single update. \emph{Off-policy} methods decouple the behavior policy (that collects data) from the target policy (that we improve), enabling replay buffers and better sample efficiency.

\textbf{Off-Policy Policy Gradient.} When data come from a behavior policy \(b\neq \pi_\theta\), define the per-decision likelihood ratio
\[
\rho_t \;=\; \frac{\pi_\theta(a_t\mid s_t)}{b (a_t\mid s_t)}.
\qquad \label{eq:is-ratio}
\]
A basic off-policy policy gradient with an advantage target \(\widehat{A}_t\) is
\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\mathbb{E}\!\left[\rho_t\,\nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t\right].
\label{eq:offpolicy-actor}
\end{equation}
In practice we often clip the ratio to control variance:
\[
\bar\rho_t \;=\; \min\{\rho_t,\;c\}, \quad c\!\ge\!1,
\qquad
\widehat{\nabla_\theta J}
\approx
\mathbb{E}\!\left[\bar\rho_t\,\nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t\right].
\]
Clipping introduces small bias but usually reduces variance.

\textbf{Off-policy Critic.} A convenient choice is an \emph{action-value critic} \(Q_\psi(s,a)\) trained with an expected SARSA style target under the current \(\pi_\theta\):
\[
\begin{split}
y_t & = r_t \;+\; \gamma \,
\mathbb{E}_{a'\sim\pi_\theta(\cdot\mid s_{t+1})}\!\left[Q_{\bar\psi}(s_{t+1},a')\right],
\\
\psi & \leftarrow \arg\min_\psi \;\mathbb{E}\big[(Q_\psi(s_t,a_t)-y_t)^2\big],
\end{split} 
\label{eq:offpolicy-critic}
\]
where \(Q_{\bar\psi}\) is a target network used to stabilize bootstrapping (i.e., mitigate the deadly triad).
For discrete actions, the expectation is an exact sum \(\sum_{a'}\pi_\theta(a'\mid s')Q_{\bar\psi}(s',a')\); for continuous, we approximate the expectation with a few samples \(a'\!\sim\!\pi_\theta(\cdot\mid s')\).

\textbf{Advantage.} Given \(Q_\psi\), we can estimate the advantage by
\[
\widehat{A}_t
\;=\;Q_\psi(s_t,a_t) \;-\; V_\psi(s_t),
\quad
V_\psi(s)
\;\equiv\;
\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[Q_\psi(s,a)].
\qquad \label{eq:offpolicy-adv}
\]
Again, for discrete actions, we can compute \(V_\psi\) exactly; for continuous actions, we approximate using a few samples.

Pseudocode for off-policy actor-critic is presented below.

\textbf{Experience-Replay Off-Policy Actor--Critic}

\textbf{Inputs:} target policy \(\pi_\theta\), Q-critic \(Q_\psi\) (and target \(Q_{\bar\psi}\)), discount \(\gamma\), stepsizes \(\alpha_\theta,\alpha_\psi\), replay buffer \(\mathcal{D}\), IS clip \(c\ge 1\), minibatch size \(B\).

\textbf{Initialize:} \(\bar\psi\leftarrow\psi\). Behavior policy \(b\) can be \(\pi_\theta\) with exploration (e.g., \(\varepsilon\)-greedy).

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Interact \& store.} Use \(b\) to step the env and append to \(\mathcal{D}\) tuples \((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t,\;p_t^b)\), where \(p_t^b=b(a_t\mid s_t)\) (store this to compute \(\rho_t\)).
\item
  \textbf{Sample minibatch} Sample transitions \(\{(s,a,r,s',d,p^\mu)\}_{i=1}^B\) from the replay buffer \(\mathcal{D}\).
\item
  \textbf{Critic target (expected SARSA).}

  \begin{itemize}
  \tightlist
  \item
    Compute \(\pi_\theta(a'\mid s')\) and \(Q_{\bar\psi}(s',a')\).
  \item
    Set \(y \leftarrow r + \gamma(1-\mathrm{done}_t)\sum_{a'} \pi_\theta(a'\mid s')\,Q_{\bar\psi}(s',a')\). (for continuous actions: perform sample average)
  \end{itemize}
\item
  \textbf{Critic update.}
  \[
  \psi \leftarrow \psi - \alpha_\psi \nabla_\psi\frac{1}{B}\sum_{i=1}^B\big(Q_\psi(s_i,a_i)-y_i\big)^2.
  \]
  \emph{(Optionally clip gradients; perform multiple critic steps.)}
\item
  \textbf{Actor advantage.}

  \begin{itemize}
  \tightlist
  \item
    Compute \(V_\psi(s)=\sum_{a}\pi_\theta(a\mid s)\,Q_\psi(s,a)\) (or sample-average for continuous actions).\\
  \item
    Set \(\widehat{A}=Q_\psi(s,a)-V_\psi(s)\); optionally normalize \(\widehat{A}\) within the batch.
  \end{itemize}
\item
  \textbf{Importance ratios (clipped).}
  \[
  \rho \leftarrow \frac{\pi_\theta(a\mid s)}{p^b},\qquad \bar\rho \leftarrow \min\{\rho,\;c\}.
  \]
\item
  \textbf{Actor update.}
  \[
  \theta \leftarrow \theta + \alpha_\theta\,
  \frac{1}{B}\sum_{i=1}^B
  \bar\rho_i \,\nabla_\theta \log \pi_\theta(a_i\mid s_i)\,\widehat{A}_i.
  \]
\item
  \textbf{Target network (moving average).}
  \[
  \bar\psi \leftarrow \tau\,\psi + (1-\tau)\,\bar\psi.
  \]
\end{enumerate}

\textbf{Notes \& Variants.}

\begin{itemize}
\tightlist
\item
  \textbf{Unbiased vs.~biased:} Without clipping and with a correct critic/advantage, \eqref{eq:offpolicy-actor} is unbiased; clipping \(\bar\rho\) adds bias but improves variance.\\
\item
  \textbf{Critic options:} You can learn \(V_\phi\) instead of \(Q_\psi\) using off-policy TD with IS; using \(Q\) with an expected SARSA target avoids IS in the critic while keeping evaluation under \(\pi_\theta\).\\
\item
  \textbf{Behavior refresh:} Periodically update \(b\) toward \(\pi_\theta\) (reduce exploration) to keep ratios well-behaved.
\end{itemize}

The next example applies off-policy actor-critic to cart-pole balancing.

\begin{example}[Off-Policy Actor-Critic for Cart-Pole Balancing]
\protect\hypertarget{exm:cart-pole-off-policy-actor-critic}{}\label{exm:cart-pole-off-policy-actor-critic}

Fig. \ref{fig:cart-pole-learning-curve-off-policy-ac} shows the learning curve of applying off-policy actor-critic to cart-pole balancing.

Fig. \ref{fig:cart-pole-policy-rollout-off-policy-ac} shows a sample rollout of the learned policy.

The Python code can be found \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_off-policy_actor-critic.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_returns_off-policy_actor-critic} 

}

\caption{Learning curve (Off-Policy Actor--Critic).}\label{fig:cart-pole-learning-curve-off-policy-ac}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_policy_rollout_off-policy_actor-critic} 

}

\caption{Policy rollout (Off-Policy Actor--Critic).}\label{fig:cart-pole-policy-rollout-off-policy-ac}
\end{figure}

\end{example}

The next example applies off-policy actor-critic to a control problem with a continuous action space.

\begin{example}[Off-Policy Actor-Critic for Inverted Pendulum]
\protect\hypertarget{exm:pendulum-off-policy-ac}{}\label{exm:pendulum-off-policy-ac}

Consider the Inverted Pendulum problem illustrated in Fig. \ref{fig:pendulum-diagram}. The state of the pendulum is \(s = (\theta, \dot{\theta})\), or equivalently, \(s = (x, y, \dot{\theta})\) with \(x = \cos(\theta), y = \sin(\theta)\). The action space is continuous: \(\tau \in \mathcal{A} = [-2,2]\).

The dynamics of the pendulum is specified by \href{https://gymnasium.farama.org/environments/classic_control/pendulum/}{Gym}, and the reward is
\[
R(s,\tau) = -(\theta^2 + 0.1  \dot{\theta}^2 + 0.001 \tau^2).
\]
The episode truncates at \(200\) time steps.

\begin{figure}

{\centering \includegraphics[width=0.2\linewidth]{images/Policy-Gradients/pendulum} 

}

\caption{Illustration of Inverted Pendulum in Gym.}\label{fig:pendulum-diagram}
\end{figure}

Fig. \ref{fig:pendulum-learning-curve-off-policy-ac} shows the learning curve of applying off-policy actor-critic to the pendulum problem.

Fig. \ref{fig:pendulum-policy-rollout-off-policy-ac} shows a sample rollout of the learned policy.

You can find the Python code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_off-policy_actor-critic.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/pendulum_returns_off-policy_actor-critic} 

}

\caption{Learning curve (Off-Policy Actor-Critic).}\label{fig:pendulum-learning-curve-off-policy-ac}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{images/Policy-Gradients/pendulum_policy_rollout_off-policy_actor-critic} \includegraphics[width=0.4\linewidth]{images/Policy-Gradients/pendulum_policy_rollout_off-policy_actor-critic-1} \includegraphics[width=0.4\linewidth]{images/Policy-Gradients/pendulum_policy_rollout_off-policy_actor-critic-2} \includegraphics[width=0.4\linewidth]{images/Policy-Gradients/pendulum_policy_rollout_off-policy_actor-critic-3} 

}

\caption{Policy rollout (Off-Policy Actor-Critic).}\label{fig:pendulum-policy-rollout-off-policy-ac}
\end{figure}

\end{example}

\section{Advanced Policy Gradients}\label{advanced-policy-gradients}

\subsection{Revisiting Generalized Policy Iteration}\label{revisiting-generalized-policy-iteration}

Recall from Chapter \ref{value-rl} that \textbf{generalized policy iteration (GPI)} extends tabular policy iteration (with known dynamics) to unknown-dynamics settings. At a high level, GPI iterates over policies; at iteration \(k\) it performs:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Policy evaluation.} Use the current policy \(\pi_k\) to generate \(N\) trajectories and estimate either the \(Q\)-function \(\hat Q^{\pi_k}(s,a)\) or the advantage function \(\hat A^{\pi_k}(s,a)\), using function approximation. This can be done, for example, with the GAE algorithm introduced in Section \ref{PG-GAE}, and is the ``critic'' in the Actor--Critic family of methods.
\item
  \textbf{Policy improvement.} Construct a new policy \(\pi_{k+1}\) that (approximately) prefers actions deemed better by \(\hat Q^{\pi_k}\) or \(\hat A^{\pi_k}\):
  \[
  \pi_{k+1}(s) \approx \arg\max_a \hat Q^{\pi_k}(s,a) \;=\; \arg\max_a \hat A^{\pi_k}(s,a).
  \]
  In policy gradients, we approximate \(\arg\max_a \hat A^{\pi_k}(s,a)\) via gradient ascent in \(a\), i.e., using
  \begin{equation}
  \nabla_\theta J(\theta)
  = \frac{1}{1-\gamma}\,
  \mathbb{E}_{s \sim d_{\pi_k},\, a \sim \pi_k}
  \big[\nabla_\theta \log \pi_\theta(a \mid s)\, \hat A^{\pi_k}(s,a)\big].
  \label{eq:policy-gradient-advantage-gpi}
  \end{equation}
\end{enumerate}

A key observation is that we use an advantage estimate obtained from data generated by \(\pi_k\) (the old policy) to produce a new policy. In the tabular case, this improvement step guarantees monotonic improvement of \(\pi_{k+1}\) over \(\pi_k\), because the evaluation produces a value (or advantage) estimate over the entire state space. In continuous state spaces, this no longer holds: we typically can only obtain an advantage estimate that is accurate \emph{along the state--action distribution induced by \(\pi_k\)} rather than globally over \(\mathcal{S}\times\mathcal{A}\). (If, however, we use off-policy data, the expectation here can be different.)

The question ``\emph{how much better is \(\pi_{k+1}\) than \(\pi_k\)?}'' motivates a relation between the performances of two policies that explicitly accounts for distribution shift.

\subsection{Performance Difference Lemma}\label{performance-difference-lemma}

The following performance difference lemma (PDL) expresses the return gap between two policies in terms of the (old) policy's advantage and the (new) policy's state-action visitation:

\begin{theorem}[Performance Difference Lemma]
\protect\hypertarget{thm:pdl}{}\label{thm:pdl}Let \(\pi\) and \(\pi'\) be two stationary policies in a discounted MDP with \(\gamma\in[0,1)\). Then
\begin{equation}
J(\pi') - J(\pi)
\;=\;
\frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d^{\pi'},\,a\sim \pi'}\!\left[A^{\pi}(s,a)\right],
\qquad
\label{eq:PDL}
\end{equation}
where \(d^{\pi}(s)=(1-\gamma)\sum_{t=0}^\infty \gamma^t\,\Pr_{\pi}(s_t=s)\) is the (discounted) state-visitation distribution generated by policy \(\pi\) and \(A^\pi=Q^\pi-V^\pi\) is the advantage.
\end{theorem}

\textbf{Interpretation.} The performance difference lemma highlights \textbf{distribution shift}: the advantage is evaluated under policy \(\pi\), while the expectation is taken over the state--action distribution induced by \(\pi'\). In policy gradients, when performing a step using \eqref{eq:policy-gradient-advantage-gpi}, we are approximately maximizing the surrogate
\[
\mathcal{L}_{\pi}(\pi') := \frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d^{\pi},\,a\sim \pi'}[A^{\pi}(s,a)],
\]
where the state distribution is \(d_\pi\), not \(d_{\pi'}\). To guarantee improvement, we want this surrogate to reflect the true gain \(J(\pi')-J(\pi)\). The two coincide when \(d^{\pi'} \approx d^{\pi}\). Hence, \textbf{keep \(\pi'\) close to \(\pi\)} so state visitation does not change dramatically, making the surrogate reliable (to some extent, off-policy versions of actor--critic aim to achieve this). This ``stay local'' principle underpins TRPO, NPG, and PPO.

\subsection{Trust Region Constraint}\label{trust-region-constraint}

How to enforce the new policy \(\pi_{\theta_{k+1}}\) to be close to the old policy \(\pi_{\theta_{k}}\)?

\textbf{KL Divergence.} The Kullback--Leibler (KL) divergence is a type of statistical distance: a measure of how much an approximating probability distribution \(Q\) is different from a true probability distribution \(P\). Formally, let \(P\) and \(Q\) be two probability distributions supported on \(\mathcal{X}\), the KL divergence between \(P\) and \(Q\) is
\begin{equation}
D_{\mathrm{KL}}(P \Vert Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right) = \mathbb{E}_{x \sim P(x)}\left[ \log \left( \frac{P(x)}{Q(x)} \right) \right].
\label{eq:KL-divergence-def}
\end{equation}
For example, when \(P = Q\), we have \(D_{\mathrm{KL}}(P \Vert Q) = 0\). Indeed, \(D_{\mathrm{KL}}(P \Vert Q) \geq 0\) and the equality holds if and only if \(P = Q\).

\textbf{Trust Region Constraint.} We now augment the usual policy optimization problem with a trust region constraint defined by the KL divergence. In particular, we wish to improve the current policy \(\pi_{\theta_k}\) \textbf{locally} by maximizing a \textbf{surrogate advantage objective} while constraining the \textbf{expected KL divergence} from the old policy. This keeps the new policy \(\pi_{\theta}\) close to \(\pi_{\theta_k}\), so the surrogate built under \(d^{\pi_{\theta_k}}\) remains predictive of true improvement.

Formally, let \(\theta_k\) denote the current policy parameters. Define the importance ratio
\[
\rho_\theta(s,a)=\frac{\pi_\theta(a\mid s)}{\pi_{\theta_k}(a\mid s)}.
\]
We aim to maximize the on-policy surrogate
\begin{equation}
L_{\theta_k}(\theta)
\;=\;
\mathbb{E}_{s\sim d^{\pi_{\theta_k}},\,a\sim \pi_{\theta_k}}
\!\big[\rho_\theta(s,a)\,\widehat{A}^{\,\pi_{\theta_k}}(s,a)\big],
\qquad
\label{eq:trpo-surrogate}
\end{equation}
subject to an expected KL constraint measured under the old state distribution:
\begin{equation}
\bar D_{\mathrm{KL}}(\theta_k\,\|\,\theta)
\;:=\;
\mathbb{E}_{s\sim d^{\pi_{\theta_k}}}
\!\left[
D_{\mathrm{KL}}\!\big(\pi_{\theta_k}(\cdot\mid s)\,\|\,\pi_{\theta}(\cdot\mid s)\big)
\right]
\;\le\;\delta,
\qquad
\label{eq:trpo-kl}
\end{equation}
with a small radius \(\delta>0\). In summary, we are now interested in the following constrained policy optimization problem:
\begin{equation}
\begin{split}
\max_\theta & \quad L_{\theta_k}(\theta) \\
\text{subject to} & \quad \bar D_{\mathrm{KL}}(\theta_k\,\|\,\theta) \leq \delta.
\end{split}
\label{eq:constrained-policy-optimization}
\end{equation}

\subsection{Natural Policy Gradient}\label{natural-policy-gradient}

The natural policy gradient method \citep{kakade2001natural} can be seen as first performing a linear approximation to the objective of \eqref{eq:constrained-policy-optimization} and a quadratic approximation to the constraint of \eqref{eq:constrained-policy-optimization}, and then solve the resulting approximate problem in closed form.

\textbf{Leading-Order Approximation.} To maximize the surrogate \(L_{\theta_k}(\theta)\) in \eqref{eq:trpo-surrogate} subject to the KL trust-region constraint \eqref{eq:trpo-kl}, we linearize the surrogate around \(\theta_k\) and quadratically approximate the KL trust region constraint. This leads to the following convex quadratic program (QP)
\begin{equation}
\max_{\Delta\theta}\ \ g^\top \Delta\theta
\quad\text{s.t.}\quad
\frac{1}{2}\,\Delta\theta^\top F(\theta_k)\,\Delta\theta \;\le\; \delta,
\label{eq:npg-qp}
\end{equation}
where
\begin{equation}
g \;=\; \nabla_\theta L_{\theta_k}(\theta)\big|_{\theta=\theta_k}
= \mathbb{E}_{s\sim d^{\pi_{\theta_k}},\,a\sim \pi_{\theta_k}(\cdot\mid s)} \big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\,\widehat{A}(s,a)\big]
\label{eq:npg-policy-gradient}
\end{equation}
is the policy gradient, and
\begin{equation}
F(\theta_k) \;=\; \mathbb{E}_{s\sim d^{\pi_{\theta_k}},\,a\sim \pi_{\theta_k}(\cdot\mid s)} \big[\nabla_\theta \log \pi_{\theta_k}(a \mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\big]
\label{eq:npg-fisher}
\end{equation}
is the (empirical) \emph{Fisher information} of the policy under the old distribution. See a proof in Section \ref{proof-fisher}.

One can show that the QP \eqref{eq:npg-qp} has a closed-form solution:
\begin{equation}
p_{\text{NPG}} \;=\; F(\theta_k)^{-1}\, g,
\qquad
\Delta\theta_{\text{NPG}}
\;=\;
\sqrt{\frac{2\delta}{g^\top F(\theta_k)^{-1} g}}\;\;p_{\text{NPG}},
\label{eq:npg-step}
\end{equation}
where \(p_{\text{NPG}}\) is called the \emph{natural policy gradient}, for the reason that the usual policy gradient \(g\) is pre-multiplied by \(F(\theta_k)^{-1}\), which contains the second-order curvature of the KL constraint. In practice, \(p_{\text{NPG}}\) is computed with \href{https://en.wikipedia.org/wiki/Conjugate_gradient_method}{conjugate gradient (CG)} using Fisher--vector products; no matrices are formed. In \eqref{eq:npg-step},
\[
\alpha = \sqrt{\frac{2\delta}{g^\top F(\theta_k)^{-1} g}} = \sqrt{ \frac{2\delta}{p^\top_{\text{NPG}} F(\theta_k) p_{\text{NPG}} } }
\]
is often called the trust-region step size.

The following pseudocode implements NPG with GAE as the critic.

\textbf{Natural Policy Gradient (with GAE advantages)}

\textbf{Inputs:} initial policy \(\theta_0\); value/critic \(\phi_0\); discount \(\gamma\); GAE parameter \(\lambda\); KL radius \(\delta\) (or learning rate \(\eta\)); CG iterations \(K_{\mathrm{cg}}\); (optional) damping \(\xi>0\).

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Collect rollouts (on-policy).} Run \(\pi_{\theta_k}\) to obtain a batch \(\{(s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\}_{t=1}^N\); cache \(\log \pi_{\theta_k}(a_t\mid s_t)\).
\item
  \textbf{Critic / advantages (GAE).}\\
  Compute TD residuals \(\delta_t = r_t + \gamma(1-\mathrm{done}_t)V_\phi(s_{t+1}) - V_\phi(s_t)\);\\
  backward recursion \(\widehat{A}_t = \delta_t + \gamma\lambda(1-\mathrm{done}_t)\widehat{A}_{t+1}\), with \(\widehat{A}_{T}=0\);\\
  (optionally) standardize \(\widehat{A}\); set value targets \(\widehat{V}^{\,\text{targ}}_t=\widehat{A}_t+V_\phi(s_t)\).
\item
  \textbf{Value update.} Fit \(V_\phi\) by minimizing \(\sum_t (V_\phi(s_t)-\widehat{V}^{\,\text{targ}}_t)^2\) (one or several epochs).
\item
  \textbf{Surrogate gradient.}\\
  \[
  g \;=\; \frac{1}{N} \sum_{t} \nabla_\theta \log \pi_{\theta_k}(a_t \mid s_t )\,\widehat{A}_t .
  \]
\item
  \textbf{Fisher--vector product (FvP).} Define the empirical KL
  \(\bar D_{\mathrm{KL}}(\theta_k\,\|\,\theta)\). Implement \(v\mapsto Fv\) as the \textbf{Hessian--vector product} of \(\bar D_{\mathrm{KL}}\) at \(\theta_k\) (optionally use \textbf{damping} \(F \leftarrow F+\xi I\) to make sure \(F\) is positive definite).
\item
  \textbf{Conjugate gradient (CG).} Approximately solve \((F)\,p = g\) to obtain \(p_{\text{NPG}}\approx F^{-1}g\).
\item
  \textbf{Step size.}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Trust-region scaling:} set
    \(\alpha \leftarrow \sqrt{\frac{2\delta}{p^\top_{\text{NPG}} F p_{\text{NPG}}}}\) and update \(\theta_{k+1} \leftarrow \theta_k + \alpha p_{\text{NPG}}\).\\
  \item
    \textbf{Fixed-rate natural step:} choose \(\eta>0\) and set \(\theta_{k+1} \leftarrow \theta_k + \eta p_{\text{NPG}}\) (monitor empirical KL for safety).
  \end{itemize}
\end{enumerate}

\subsection{Proof of Fisher Information}\label{proof-fisher}

Let the expected KL trust-region constraint (measured under the old policy's state distribution) be
\[
\bar D_{\mathrm{KL}}(\theta_k\!\parallel\!\theta)
\;:=\;
\mathbb{E}_{s\sim d^{\pi_{\theta_k}}}
\Big[ D_{\mathrm{KL}}\!\big(\pi_{\theta_k}(\cdot\mid s)\,\|\,\pi_\theta(\cdot\mid s)\big) \Big].
\]
Write \(\theta=\theta_k+\Delta\theta\) and define, for a fixed state \(s\),
\[
f_s(\theta)
\;=\;
D_{\mathrm{KL}}\!\big(\pi_{\theta_k}(\cdot\mid s)\,\|\,\pi_\theta(\cdot\mid s)\big)
= \mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}
\big[\log \pi_{\theta_k}(a\mid s) - \log \pi_\theta(a\mid s)\big].
\]
We will show that the \emph{second-order Taylor expansion} of \(\bar D_{\mathrm{KL}}\) around \(\theta_k\) is
\[
\bar D_{\mathrm{KL}}(\theta_k\!\parallel\!\theta_k+\Delta\theta)
\;=\;
\frac{1}{2}\,\Delta\theta^\top
\underbrace{
\mathbb{E}_{s\sim d^{\pi_{\theta_k}},\,a\sim \pi_{\theta_k}(\cdot\mid s)}
\!\big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\big]
}_{F(\theta_k)\ \text{(Fisher information)}}
\,\Delta\theta \;+\; \mathcal{O}(\|\Delta\theta\|^3).
\]

\textbf{Step 1: Zeroth- and first-order terms vanish at \(\theta=\theta_k\).} For each \(s\),
\[
f_s(\theta_k) = D_{\mathrm{KL}}\!\big(\pi_{\theta_k}\,\|\,\pi_{\theta_k}\big)=0.
\]
The gradient (holding the expectation under \(\pi_{\theta_k}\)) is
\[
\nabla_\theta f_s(\theta)
= -\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}\big[\nabla_\theta \log \pi_\theta(a\mid s)\big].
\]
Evaluating at \(\theta=\theta_k\),
\[
   \begin{split}
\nabla_\theta f_s(\theta_k)
& = -\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}\big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\big] \\
& = -\sum_a \pi_{\theta_k}(a\mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)
= -\nabla_\theta \sum_a \pi_{\theta_k}(a\mid s)
= 0,
\end{split}
\]
using the normalization \(\sum_a \pi_{\theta_k}(a\mid s)=1\). Hence both the value and the first-order term are zero.

\textbf{Step 2: The Hessian equals the (per-state) Fisher information.} The Hessian of \(f_s\) is
\[
\nabla_\theta^2 f_s(\theta)
= -\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}\big[\nabla_\theta^2 \log \pi_\theta(a\mid s)\big].
\]
At \(\theta=\theta_k\), apply the \emph{information identity} (a.k.a. \href{https://math.stackexchange.com/questions/2026428/what-is-second-bartlett-identity}{Bartlett identity}):
\[
-\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}\big[\nabla_\theta^2 \log \pi_{\theta_k}(a\mid s)\big]
=
\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}
\big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\, \nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\big].
\]

\emph{Proof sketch of the identity:} start from \(\sum_a \pi_\theta(a\mid s)=1\), differentiate once to get
\(\mathbb{E}_{a\sim \pi_\theta}[\nabla\log\pi_\theta]=0\); differentiate again and use the product rule to obtain
\(\mathbb{E}_{a\sim \pi_\theta}[\nabla^2\log\pi_\theta + (\nabla\log\pi_\theta)(\nabla\log\pi_\theta)^\top]=0\).

Thus,
\[
\nabla_\theta^2 f_s(\theta_k)
=
\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}
\big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\big]
\;=:\; F_s(\theta_k).
\]

\textbf{Step 3: Second-order Taylor expansion and averaging over states.} For each \(s\),
\[
f_s(\theta_k+\Delta\theta)
= \frac{1}{2}\,\Delta\theta^\top F_s(\theta_k)\,\Delta\theta \;+\; \mathcal{O}(\|\Delta\theta\|^3).
\]
Taking expectation over \(s\sim d^{\pi_{\theta_k}}\) gives
\[
\bar D_{\mathrm{KL}}(\theta_k\!\parallel\!\theta_k+\Delta\theta)
= \mathbb{E}_{s\sim d^{\pi_{\theta_k}}}[f_s(\theta_k+\Delta\theta)]
= \frac{1}{2}\,\Delta\theta^\top
\underbrace{\mathbb{E}_{s\sim d^{\pi_{\theta_k}}}[F_s(\theta_k)]}_{F(\theta_k)}
\,\Delta\theta \;+\; \mathcal{O}(\|\Delta\theta\|^3).
\]

\textbf{Conclusion.} The Fisher information \(F(\theta_k)\) is exactly the Hessian of the expected KL at \(\theta_k\). Therefore, the KL trust-region constraint admits the quadratic local approximation
\[
\bar D_{\mathrm{KL}}(\theta_k\!\parallel\!\theta_k+\Delta\theta)\;\approx\;\frac{1}{2}\,\Delta\theta^\top F(\theta_k)\,\Delta\theta,
\]
which yields the TRPO/NPG quadratic constraint and identifies \(F(\theta_k)\) as the local metric tensor of the policy manifold.

\subsection{Trust Region Policy Optimization}\label{trust-region-policy-optimization}

The NPG algorithm presented above leverages a leading-order approximation of the KL-constrained policy optimization problem \eqref{eq:constrained-policy-optimization}.

In Trust Region Policy Optimization \citep{schulman2015trust}, we still use the leading-order approximation to obtain the natural policy gradient direction, but additionally, we perform a \emph{backtracking line search} to enforce the true (nonlinear) KL constraint and surrogate improvement.

The following pseudocode implements TRPO with GAE as the critic.

\textbf{TRPO (with GAE advantages)}

\textbf{Inputs:} initial policy \(\theta_0\); value/critic parameters \(\phi_0\); discount \(\gamma\); GAE parameter \(\lambda\); KL radius \(\delta\); CG iterations \(K_{\mathrm{cg}}\); backtrack factor \(\beta\in(0,1)\); max backtracks \(M\).

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Collect rollouts (on-policy).} Run \(\pi_{\theta_k}\) to obtain trajectories; build a batch \(\{(s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\}_{t=1}^N\).
\item
  \textbf{Critic / advantages (GAE).}\\
  Compute TD residuals \(\delta_t = r_t + \gamma(1-\mathrm{done}_t)V_\phi(s_{t+1}) - V_\phi(s_t)\);\\
  backward recursion \(\widehat{A}_t = \delta_t + \gamma\lambda(1-\mathrm{done}_t)\widehat{A}_{t+1}\), with \(\widehat{A}_{T}=0\);\\
  (optionally) standardize \(\widehat{A}\) within the batch; set value targets \(\widehat{V}^{\,\text{targ}}_t=\widehat{A}_t+V_\phi(s_t)\).
\item
  \textbf{Value function update.} Fit \(V_\phi\) by minimizing
  \(\sum_t (V_\phi(s_t)-\widehat{V}^{\,\text{targ}}_t)^2\) (one or several epochs).
\item
  \textbf{Policy gradient at \(\theta_k\).}\\
  \[
  g \;=\; \nabla_\theta L_{\theta_k}(\theta)\big|_{\theta=\theta_k}
    \approx 
    \frac{1}{N} \sum_{t} \nabla_\theta \log \pi_{\theta_k}(a_t \mid s_t )\,\widehat{A}_t .
  \]
\item
  \textbf{Fisher--vector product (FvP).} Define the Fisher information under \(\pi_{\theta_k}\):
  \[
  F(\theta_k)\;=\;\mathbb{E}\Big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\Big].
  \]
  Implement \(v\mapsto Fv\) via the \textbf{Hessian-vector product} of the empirical KL.
\item
  \textbf{Conjugate gradient (CG) solve.} Approximately solve \(F p = g\) with \(K_{\mathrm{cg}}\) CG iterations to get the natural direction \(p_{\text{NPG}}\approx F^{-1}g\).
\item
  \textbf{Compute step size for the quadratic trust region.}\\
  \[
  \alpha \;\leftarrow\; \sqrt{\frac{2\delta}{p^\top_{\text{NPG}} F p_{\text{NPG}} }}.
  \]
  Candidate update: \(\theta^\star \leftarrow \theta_k + \alpha p_{\text{NPG}}\).
\item
  \textbf{Backtracking line search (feasibility + improvement).} Repeatedly set \(\theta^\star \leftarrow \theta_k + \beta^j \alpha p_{\text{NPG}}\) for \(j=0,1,\dots,M\) until both hold on the batch:

  \begin{itemize}
  \tightlist
  \item
    \textbf{KL constraint:} \(\bar D_{\mathrm{KL}}(\theta_k\,\|\,\theta^\star) \le \delta\).
  \item
    \textbf{Surrogate improvement:} \(L_{\theta_k}(\theta^\star) \ge L_{\theta_k}(\theta_k)\).
  \end{itemize}

  Accept the first \(\theta^\star\) that satisfies both; set \(\theta_{k+1}\leftarrow \theta^\star\).
\end{enumerate}

\subsubsection{Backtracking Line Search}\label{backtracking-line-search}

\textbf{Batch-only evaluation.} During TRPO's line search you \textbf{do not collect new trajectories}. All checks are computed on the same batch gathered with the old policy \(\pi_{\theta_k}\) (i.e., under \(d^{\pi_{\theta_k}}\)).

\textbf{Given:} a candidate update \(\theta^\star = \theta_k + \beta^j \alpha p_{\text{NPG}}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Empirical KL constraint (nonlinear, ``true'' KL).} Compute the state-wise KL between the full action distributions of the old and candidate policies and average over the batch states:
  \[
  \widehat{\bar D}_{\mathrm{KL}}(\theta_k \,\|\, \theta^\star)
  \;=\;
  \frac{1}{|\mathcal{B}|}\sum_{s\in\mathcal{B}}
  D_{\mathrm{KL}}\!\big(\pi_{\theta_k}(\cdot\mid s)\,\|\,\pi_{\theta^\star}(\cdot\mid s)\big).
  \]

  \begin{itemize}
  \tightlist
  \item
    \textbf{Categorical policy:}\\
    \[
    D_{\mathrm{KL}}(\pi_{\theta_k}\,\|\,\pi_{\theta^\star})
    \;=\;
    \sum_{a} \pi_{\theta_k}(a\mid s)\,
    \Big[\log \pi_{\theta_k}(a\mid s)-\log \pi_{\theta^\star}(a\mid s)\Big].
    \]
  \item
    \textbf{Gaussian policy} (mean \(\mu(s),\) covariance \(\Sigma(s)\); use pre-squash distribution if actions are squashed):\\
    \[
    D_{\mathrm{KL}}\big(\mathcal{N}(\mu_k,\Sigma_k)\,\|\,\mathcal{N}(\mu_\star,\Sigma_\star)\big)
    = \frac{1}{2}\Big(
      \operatorname{tr}(\Sigma_\star^{-1}\Sigma_k)
      + (\mu_\star-\mu_k)^\top \Sigma_\star^{-1}(\mu_\star-\mu_k)
      - d + \log\tfrac{\det \Sigma_\star}{\det \Sigma_k}
    \Big).
    \]
    \textbf{Feasibility test:} accept if \(\widehat{\bar D}_{\mathrm{KL}}(\theta_k \,\|\, \theta^\star) \le \delta\) (cf.~\eqref{eq:trpo-kl}).
  \end{itemize}
\item
  \textbf{Surrogate improvement.} Evaluate the TRPO surrogate \(L_{\theta_k}(\theta)\) (cf.~\eqref{eq:trpo-surrogate}) on the same batch using importance ratios from \(\theta^\star\):
  \[
  \widehat{L}_{\theta_k}(\theta^\star)
  \;=\;
  \frac{1}{|\mathcal{B}|}\sum_{(s,a)\in\mathcal{B}}
  \frac{\pi_{\theta^\star}(a\mid s)}{\pi_{\theta_k}(a\mid s)}\,
  \widehat{A}^{\,\pi_{\theta_k}}(s,a),
  \qquad
  \widehat{L}_{\theta_k}(\theta_k)=\frac{1}{|\mathcal{B}|}\sum_{(s,a)}\widehat{A}^{\,\pi_{\theta_k}}(s,a).
  \]
  \textbf{Improvement test:} accept if \(\widehat{L}_{\theta_k}(\theta^\star)\ge \widehat{L}_{\theta_k}(\theta_k)\).
\item
  \textbf{Backtracking loop (on-batch).} Decrease the step by \(\beta\in(0,1)\) until both tests pass or a maximum of \(M\) backtracks is reached:
  \[
  \theta^\star \leftarrow \theta_k + \beta^j \alpha p_{\text{NPG}},\quad
  j=0,1,\dots,M.
  \]
  If successful, set \(\theta_{k+1}\leftarrow \theta^\star\); otherwise keep \(\theta_{k+1}\leftarrow \theta_k\).
\end{enumerate}

\subsection{Proximal Policy Optimization}\label{proximal-policy-optimization}

While NPG/TRPO are stable, they may be computationally heavier due to constrained solves or natural-step systems. Proximal Policy Optimization (PPO) \citep{schulman2017proximal} replaces the hard constraint with a \emph{penalized (regularized) objective} and optimizes it with standard first-order SGD:
\begin{equation}
\ell_k(\theta)
\;=\;
\mathbb{E}_{s \sim d^{\pi_{\theta_k}},a\sim \pi_{\theta_k}}
\big[\,\rho_\theta(s,a)\, \widehat{A}^{\,\pi_{\theta_k}}(s,a)\,\big]
\;-\;
\lambda\;
\mathbb{E}_{s \sim d^{\pi_{\theta_k}},a\sim \pi_{\theta_k}}
\!\left[\log\frac{\pi_{\theta_k}(a\mid s)}{\pi_\theta(a\mid s)}\right],
\label{eq:PPO-regularized}
\end{equation}
where \(\lambda > 0\) and the second term is the \emph{per-sample KL penalty} that discourages large departures from \(\pi_{\theta_k}\).
Conceptually, this is a \emph{Lagrangian relaxation} of TRPO's trust region, where the hard constraint is moved to the objective function as a soft penalty.

\subsubsection{Gradient of the KL--Regularized Surrogate}\label{gradient-of-the-klregularized-surrogate}

Treat \(\widehat{A}^{\,\pi_{\theta_k}}\) and the sampling distribution as fixed during the policy update. Using
\(\nabla_\theta \rho_\theta = \rho_\theta\,\nabla_\theta \log \pi_\theta\) and
\(\nabla_\theta \log \frac{\pi_{\theta_k}}{\pi_\theta} = - \nabla_\theta \log \pi_\theta\), the gradient of the KL-regularized objective \eqref{eq:PPO-regularized} is
\[
\nabla_\theta \ell_k(\theta)
\;=\;
\mathbb{E}_{s \sim d^{\pi_{\theta_k}}, a\sim \pi_{\theta_k}}
\!\Big[
\nabla_\theta \log \pi_\theta(a\mid s)\,
\underbrace{\big(\rho_\theta(s,a)\,\widehat{A}(s,a)-\lambda\big)}_{\text{effective advantage}}
\Big].
\qquad \label{eq:PPO-penalty-gradient}
\]
This shows the KL penalty shifts the effective advantage by \(-\lambda\).

\subsubsection{From the Lagrangian Relaxation to PPO Updates}\label{from-the-lagrangian-relaxation-to-ppo-updates}

There are two standard PPO realizations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{PPO--KL (penalty version).} Directly ascend \(\ell_k(\theta)\) with minibatch SGD:
  \[
  \theta \leftarrow \theta + \alpha\;
  \frac{1}{B}\sum_{(s,a)\in\mathcal{B}}
  \nabla_\theta \log \pi_\theta(a\mid s)\,
  \big(\rho_\theta(s,a)\,\widehat{A}(s,a)-\lambda\big).
  \]
  After each epoch, measure the empirical KL
  \(\widehat{\bar D}_{\mathrm{KL}}(\theta_k\!\parallel\!\theta)\)
  on the batch; increase \(\lambda\) if KL is too high (tighten the region), decrease \(\lambda\) if it is too low.
\item
  \textbf{PPO--Clip (clipping version).} Replace the penalty with a \emph{hard} trust region on the ratio \(\rho_\theta\). When \(\widehat{A}>0\), forbid \(\rho_\theta>1+\varepsilon\); when \(\widehat{A}<0\), forbid \(\rho_\theta<1-\varepsilon\). This yields the clipped objective
  \begin{equation}
  \hspace{-16mm}
  \ell^{\text{CLIP}}_k(\theta)
  \;=\;
  \mathbb{E}_{s \sim d^{\pi_{\theta_k}}, a \sim \pi_{\theta_k}}
  \Big[\min\!\big(\rho_\theta(s,a)\,\widehat{A}(s,a),\;
  \operatorname{clip}(\rho_\theta(s,a),\,1-\varepsilon,\,1+\varepsilon)\,\widehat{A}(s,a)\big)\Big],
  \label{eq:PPO-clip-obj}
  \end{equation}
  which is a first-order proxy to the Lagrangian/TRPO trust region: the min/clip term cancels the incentive to move \(\rho_\theta\) outside \([1-\varepsilon,1+\varepsilon]\) in directions that would further increase the objective.
\end{enumerate}

Both versions are typically combined with a value-function loss and an entropy bonus to encourage exploration:
\[
\mathcal{L}^{\text{PPO}}(\theta,\phi)
= - \ell^{\text{PG}}_k(\theta)
+ c_v\,\mathbb{E}\big[(V_\phi(s)-\widehat{V}^{\,\text{targ}})^2\big]
- c_e\,\mathbb{E}\big[\mathcal{H}(\pi_\theta(\cdot\mid s))\big],
\]
where \(\ell^{\text{PG}}_k\) is either \(\ell^{\text{CLIP}}_k\) or \(\ell_k\).

\textbf{Why PPO ``forbids'' \(\rho_\theta\) from leaving \([1-\varepsilon,\,1+\varepsilon]\).} Let \(r \equiv \rho_\theta(s,a)=\frac{\pi_\theta(a\mid s)}{\pi_{\theta_k}(a\mid s)}\) and \(\widehat{A}=\widehat{A}^{\,\pi_{\theta_k}}(s,a)\).
The PPO--Clip objective for one sample is
\[
L^{\text{CLIP}}(r,\widehat{A})
= \min\!\big(r\,\widehat{A},\ \operatorname{clip}(r,1-\varepsilon,1+\varepsilon)\,\widehat{A}\big).
\]

Let's do a case analysis, as shown in Fig. \ref{fig:ppo-clip-obj}.

\begin{itemize}
\item
  If \(\widehat{A}>0\): increasing \(r\) (i.e., increasing \(\pi_\theta(a\mid s)\)) raises the \emph{unclipped} term \(r\,\widehat{A}\).\\
  The clipped term equals \((1+\varepsilon)\widehat{A}\) whenever \(r>1+\varepsilon\).
  Hence
  \[
  L^{\text{CLIP}}(r,\widehat{A})
  =
  \begin{cases}
  r\,\widehat{A}, & r\le 1+\varepsilon,\\[4pt]
  (1+\varepsilon)\widehat{A}, & r>1+\varepsilon,
  \end{cases}
  \]
  so \(\frac{\partial L^{\text{CLIP}}}{\partial r}=\widehat{A}\) for \(r\le 1+\varepsilon\) and \textbf{\(0\)} for \(r>1+\varepsilon\).
  There is no further gain by pushing \(r\) beyond \(1+\varepsilon\); the gradient vanishes.\\
  Intuitively: don't increase an action's probability \emph{too much} even if it looks good---stay proximal.
\item
  If \(\widehat{A}<0\): decreasing \(r\) (i.e., reducing \(\pi_\theta(a\mid s)\)) lowers the unclipped term \(r\,\widehat{A}\).
  The clipped term equals \((1-\varepsilon)\widehat{A}\) whenever \(r<1-\varepsilon\).
  Thus
  \[
  L^{\text{CLIP}}(r,\widehat{A})
  =
  \begin{cases}
  r\,\widehat{A}, & r\ge 1-\varepsilon,\\[4pt]
  (1-\varepsilon)\widehat{A}, & r<1-\varepsilon,
  \end{cases}
  \]
  so \(\frac{\partial L^{\text{CLIP}}}{\partial r}=\widehat{A}(<0)\) for \(r\ge 1-\varepsilon\) and \textbf{\(0\)} for \(r<1-\varepsilon\).
  There is no incentive to shrink \(r\) below \(1-\varepsilon\); the gradient goes to zero.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/Policy-Gradients/PPO_clip} 

}

\caption{The clipped objective function in PPO (from the original PPO paper).}\label{fig:ppo-clip-obj}
\end{figure}

Therefore, the \(\min\) with a clipped ratio creates flat regions where the objective stops improving in the ``profitable'' outward direction. This removes the optimization incentive to move \(r\) outside \([1-\varepsilon,1+\varepsilon]\), implementing a per-sample trust region on the probability ratio while retaining the standard policy-gradient inside the bracket.

The following pseudocode implements PPO (clipped version) with GAE.

\textbf{Proximal Policy Optimization (PPO--Clip)}

\textbf{Inputs:} policy \(\pi_\theta\), value \(V_\phi\), discount \(\gamma\), GAE \(\lambda\), clip \(\varepsilon\), coefficients \(c_v,c_e\), learning rate \(\alpha\), epochs \(K_{\text{epoch}}\), minibatch size \(B\).

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Collect on-policy data.} Roll out \(\pi_{\theta_k}\) to get trajectories \(\{(s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\}\).\\
  Cache \(\log \pi_{\theta_k}(a_t\mid s_t)\).
\item
  \textbf{Compute GAE advantages and value targets.}\\
  \(\delta_t = r_t + \gamma(1-\mathrm{done}_t) V_\phi(s_{t+1}) - V_\phi(s_t)\)\\
  \(\widehat{A}_t = \delta_t + \gamma\lambda(1-\mathrm{done}_t)\widehat{A}_{t+1}\), with \(\widehat{A}_T=0\).\\
  \(\widehat{V}^{\,\text{targ}}_t = \widehat{A}_t + V_\phi(s_t)\).\\
  (Optionally standardize \(\{\widehat{A}_t\}\) within the batch.)
\item
  \textbf{Policy/Value optimization (multiple epochs).}\\
  For \(e=1,\dots,K_{\text{epoch}}\):

  \begin{itemize}
  \tightlist
  \item
    Split the batch into minibatches \(\mathcal{B}\) of size \(B\).\\
  \item
    For each \(\mathcal{B}\):
    \[
    \rho_\theta(s,a) = \exp\!\big(\log\pi_\theta(a\mid s)-\log\pi_{\theta_k}(a\mid s)\big),
    \]
    \[
    \ell^{\text{CLIP}}_{\mathcal{B}}(\theta) =
    \frac{1}{B}\sum_{(s,a)\in\mathcal{B}}
    \min\!\big(\rho_\theta\,\widehat{A},\;
    \operatorname{clip}(\rho_\theta,1-\varepsilon,1+\varepsilon)\,\widehat{A}\big),
    \]
    \[
    \ell^{\text{VAL}}_{\mathcal{B}}(\phi)=
    \frac{1}{B}\sum_{s\in\mathcal{B}}(V_\phi(s)-\widehat{V}^{\,\text{targ}})^2,\quad
    \mathcal{H}_{\mathcal{B}}(\theta)=\frac{1}{B}\sum_{s\in\mathcal{B}}\mathcal{H}(\pi_\theta(\cdot\mid s)).
    \]
  \item
    The total loss to be minimized is
    \[
    \mathcal{J}_{\mathcal{B}}(\theta,\phi) = - \ell^{\text{CLIP}}_{\mathcal{B}}(\theta) + c_v\,\ell^{\text{VAL}}_{\mathcal{B}}(\phi) - c_e\,\mathcal{H}_{\mathcal{B}}(\theta).
    \]
  \item
    Take an optimizer step on \(\mathcal{J}_{\mathcal{B}}\) (e.g., Adam with learning rate \(\alpha\)).
  \end{itemize}
\item
  \textbf{(Optional) Early stopping by KL.}\\
  Estimate \(\widehat{\bar D}_{\mathrm{KL}}(\theta_k\!\parallel\!\theta)\) on the whole batch; stop inner epochs early if it exceeds a threshold.
\end{enumerate}

\subsection{Soft Actor--Critic}\label{soft-actorcritic}

Standard actor--critic methods maximize expected return. Soft Actor--Critic (SAC) augments the objective with an entropy bonus that explicitly encourages exploration and robustness while remaining off-policy and sample efficient \citep{haarnoja2018soft}. We first introduce a minimal implementation of SAC for discrete actions, then present full SAC with additional techniques for continuous actions.

\subsubsection{SAC for Discrete Actions}\label{sac-for-discrete-actions}

\textbf{Entropy of A Probability Distribution.} Given a probability distribution \(P\) supported on the set \(\mathcal{X}\), the entropy of the distribution is defined as
\begin{equation}
\mathcal{H}(P) = - \sum_{x \in \mathcal{X}} P(x) \log P(x) = - \mathbb{E}_{x \sim P} \log P(x).
\label{eq:entropy}
\end{equation}
Since \(0 \leq P(x) \leq 1\) for any \(x\), it is clear that \(\mathcal{H}(P) \geq 0\) for any distribution \(P\).

Suppose the set \(\mathcal{X}\) has \(N\) elements \(x_1,\dots,x_N\), and suppose \(P(x_i) = p_i \geq 0,i=1,\dots,N\). We claim that the distribution \(P^\star\) that maximizes \(\mathcal{H}(P)\) is such that \(p^\star_i = \frac{1}{N},i=1,\dots,N\).

To show this, consider the function \(\log t\) that is concave for \(t > 0\). Using \href{https://en.wikipedia.org/wiki/Jensen's_inequality}{Jensen's inequality}, we have that
\begin{equation}
\begin{split}
\mathcal{H}(P) & = - \sum_{x} P(x) \log P(x) = \sum_{x} P(x) \log \frac{1}{P(x)} \\
& \leq \log \left( \sum_{x} P(x) \frac{1}{P(x)} \right) \\
& = \log N,
\end{split}
\end{equation}
with the equality holds if and only if \(P(x_1)=P(x_2)=\dots=P(x_N) = \frac{1}{N}\). Therefore, maximizing the entropy \(\mathcal{H}(P)\) encourages the distribution \(P\) to have a density function that spreads out evenly over the set \(\mathcal{X}\).

\textbf{Maximum-Entropy Objective.} SAC maximizes the soft objective
\begin{equation}
\begin{split}
J(\pi) &= \mathbb{E}\!\left[\sum_{t}\gamma^t\Big(R(s_t,a_t)\;+\;\alpha\,\mathcal{H}\!\left(\pi(\cdot\mid s_t)\right)\Big)\right],
\\
\mathcal{H}(\pi(\cdot\mid s)) &= -\mathbb{E}_{a\sim\pi}[\log \pi(a\mid s)],
\end{split}
\label{eq:soft-objective}
\end{equation}
where the entropy function \(\mathcal{H}(\cdot)\) encourages the policy to explore, and the temperature \(\alpha>0\) balances reward maximization against exploration.

Given a trajectory \(\tau = (s_0, a_0, r_0, s_1, a_1,\dots)\), define the soft return:
\[
g_t = \sum_{t=0} \gamma^t \left( R(s_t, a_t) - \alpha \log \pi (a_t \mid s_t) \right).
\]
This leads to the ``soft'' state value and soft action value associated with \(\pi\):
\begin{equation}
\begin{split}
V^\pi(s) & = \mathbb{E}_{a\sim\pi}\left[Q^\pi(s,a)\;-\;\alpha\log\pi(a\mid s)\right], \\
Q^\pi(s,a) & = R(s,a) + \gamma\,\mathbb{E}_{s'} \left[V^\pi(s')\right].
\end{split}
\label{eq:soft-value-functions}
\end{equation}
Combining the two equations above, we obtain a soft Bellman Consistency equation on the \(Q\) value:
\begin{equation}
Q^{\pi}(s,a) = R(s, a) + \gamma \mathbb{E}_{s'} \left[ \mathbb{E}_{a' \sim \pi} \left[ Q^\pi(s', a') - \alpha \log \pi(a' \mid s') \right] \right].
\label{eq:Bellman-Consistency-Soft-Q}
\end{equation}

\textbf{Critic Update.} For a replay sample \((s,a,r,s')\), assuming discrete actions, we can compute the target \(Q\) value following the soft Bellman Consistency equation \eqref{eq:Bellman-Consistency-Soft-Q}
\begin{equation}
y \;=\;
r + \gamma \sum_{a'} \pi_{\theta} (a' \mid s') \left( Q_{\bar{\psi}}(s', a') - \alpha \log  \pi_\theta (a' \mid s') \right)
\label{eq:sac-target}
\end{equation}
where \(Q_{\bar{\psi}}\) is the target Q network inspired by DQN to mitigate the deadly triad.
The critic loss is therefore
\begin{equation}
\mathcal{L}_Q(\psi) \;=\; \mathbb{E}_{(s,a) \sim \mathcal{D}}\big[\big(Q_{\psi}(s,a)-y\big)^2\big],
\label{eq:sac-critic-loss}
\end{equation}
where the expectation is taken over a minibatch drawn from the replay buffer.

\textbf{Actor Update.} Given the learned critic \(Q_\psi\) and replay state distribution \(s \sim \mathcal{D}\), the SAC policy improvement step chooses \(\pi_\theta\) to \textbf{minimize}, for each state, the soft advantage--regularized objective
\begin{equation}
J_\pi(\theta)
\;=\;
\mathbb{E}_{s\sim\mathcal{D}}
\left[
\sum_{a} \pi_\theta(a\mid s)\left( \alpha\,\log \pi_\theta(a\mid s) - Q_\psi(s,a)\right)
\right].
\label{eq:sac-actor-discrete}
\end{equation}
For discrete actions, the expectation over \(a\) is a finite sum---no action sampling is required.

Differentiating \eqref{eq:sac-actor-discrete} yields the policy gradient
\begin{equation}
\nabla_\theta J_\pi(\theta)
=
\mathbb{E}_{s\sim\mathcal{D}}
\left[
\sum_{a}
\nabla_\theta \pi_\theta(a\mid s)\;
\Big(\alpha\,[1+\log \pi_\theta(a\mid s)] - Q_\psi(s,a)\Big)
\right].
\label{eq:sac-actor-grad-discrete}
\end{equation}

The following pseudocode implements a basic SAC algorithm with discrete actions.

\textbf{Soft Actor--Critic (Discrete Actions, Single Q + Single Target)}

\textbf{Inputs:} replay buffer \(\mathcal{D}\); policy \(\pi_\theta(a\mid s)\) over \(K\) actions; single critic \(Q_{\psi}(s,\cdot)\) (returns a \(K\)-vector); target critic parameters \(\bar\psi\); discount \(\gamma\); temperature \(\alpha\) (learned or fixed); Polyak \(\tau\in(0,1]\); batch size \(B\); stepsizes \(\alpha_\theta,\alpha_\psi\).

\textbf{Initialize:} \(\bar\psi \leftarrow \psi\).

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Interaction.}\\
  Observe \(s_t\). Sample \(a_t \sim \pi_\theta(\cdot\mid s_t)\); step env to get \((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\); push to \(\mathcal{D}\).
\item
  \textbf{Sample minibatch.}\\
  Draw \(B\) transitions \(\{(s,a,r,s',d)\}\) from \(\mathcal{D}\).
\item
  \textbf{Target computation (single target network).}

  \begin{itemize}
  \tightlist
  \item
    Compute \(\pi_\theta(\cdot\mid s')\) and \(\log \pi_\theta(\cdot\mid s')\).
  \item
    Evaluate target critic \(Q_{\bar\psi}(s',\cdot)\).
  \item
    Soft value target:\\
    \[
    V_{\text{tgt}}(s')=\Big\langle \pi_\theta(\cdot\mid s'),\; Q_{\bar\psi}(s',\cdot)-\alpha\,\log \pi_\theta(\cdot\mid s') \Big\rangle.
    \]
  \item
    \textbf{Bellman target:}\\
    \[
    y \leftarrow r + \gamma(1-d)\,V_{\text{tgt}}(s').
    \]
    \emph{(Matches \eqref{eq:sac-target} with one target network.)}
  \end{itemize}
\item
  \textbf{Critic update.}
  Minimize the squared error (cf.~\eqref{eq:sac-critic-loss}):\\
  \[
    \psi \leftarrow \psi - \alpha_\psi \nabla_{\psi}\;\frac{1}{B}\sum\nolimits_{(s,a,r,s',d)}
    \big(Q_{\psi}(s,a)-y\big)^2.
    \]
\item
  \textbf{Actor update.} Minimize (cf.~\eqref{eq:sac-actor-discrete}):\\
  \[
    \theta \leftarrow \theta - \alpha_\theta \nabla_\theta \frac{1}{B}\sum_s
    \sum_{a} \pi_\theta(a\mid s)\,\Big(\alpha\,\log \pi_\theta(a\mid s)-Q_{\psi}(s,a)\Big).
    \]
\item
  \textbf{Target critic (Polyak).}\\
  \[
  \bar\psi \;\leftarrow\; \tau\,\psi \;+\; (1-\tau)\,\bar\psi.
  \]
\end{enumerate}

The next example applies the SAC algorithm above to the cart-pole problem.

\begin{example}[SAC for Cart-pole Balancing]
\protect\hypertarget{exm:cartpole-sac}{}\label{exm:cartpole-sac}

We use a fixed temperature \(\alpha=0.2\).

Fig. \ref{fig:cart-pole-learning-curve-sac} shows the learning curve of SAC.

Fig. \ref{fig:cart-pole-policy-rollout-sac} shows a sample rollout of the learned policy.

You can find the code \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_sac.py}{here}. Play with the temperature parameter.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole_learning_curve_sac} 

}

\caption{Learning curve (Soft Actor--Critic).}\label{fig:cart-pole-learning-curve-sac}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/cartpole-sac-rollout} 

}

\caption{Policy rollout (Soft Actor--Critic).}\label{fig:cart-pole-policy-rollout-sac}
\end{figure}

\end{example}

\subsubsection{SAC for Continuous Actions}\label{sac-for-continuous-actions}

In continuous action spaces we cannot sum over actions. SAC therefore:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  samples actions from the current policy using a \emph{reparameterization} trick (low-variance gradients), and
\item
  computes the soft Bellman target with those sampled actions and a \emph{twin-target minimum} to reduce overestimation.
\end{enumerate}

\textbf{Reparameterization (pathwise) Gradient.} Let the stochastic policy be a Gaussian in unconstrained space, squashed by \texttt{tanh} to the action bounds:
\[
u \;=\; \mu_\theta(s)\;+\;\sigma_\theta(s)\odot\varepsilon,\quad \varepsilon\sim\mathcal N(0,I),
\qquad
a \;=\; \tanh(u)\cdot a_{\text{scale}} + a_{\text{bias}},
\]
where \(\sigma_\theta(s)\) outputs per-dimension standard deviation.
This gives a differentiable map \(a=f_\theta(s,\varepsilon)\). Expectations over \(a\sim \pi_\theta(\cdot\mid s)\) are then written as expectations over \(\varepsilon\), so gradients can flow through \(f_\theta\) (the \emph{pathwise derivative}). The correct log-density under the squashed policy uses change-of-variables:
\[
\log\pi_\theta(a\mid s)
\;=\;
\log\mathcal N\!\big(u;\mu_\theta(s),\sigma_\theta^2(s)\big)
\;-\;\sum_i \log\!\big(1-\tanh^2(u_i)\big)
\;+\;\text{constant}.
\]
The intuition here is that the \texttt{tanh} function is a nonlinear transformation that distorts the original Gaussian distribution. This ``tanh correction'' is crucial for stable training.

\textbf{Critic Update.} Maintain two critics \(Q_{\psi_1},Q_{\psi_2}\) and their target copies
\(Q_{\bar\psi_1},Q_{\bar\psi_2}\).
For a replay minibatch \((s,a,r,s',d)\), form the target by drawing a next action from the current policy:
\begin{equation}
a' \sim \pi_\theta(\cdot\mid s'),\qquad
y \;=\; r \;+\; \gamma(1-d)\,\Big(
\min\nolimits_{j=1,2} Q_{\bar\psi_j}(s',a') \;-\; \alpha \log\pi_\theta(a'\mid s')
\Big).
\label{eq:cont-target}
\end{equation}
Each critic minimizes the squared error to \(y\) (with stop-grad on \(y\)):
\[
\mathcal L_Q(\psi_j) \;=\; \mathbb{E}\big[(Q_{\psi_j}(s,a)-y)^2\big],\quad j=1,2,
\]
where the expectation is taken over the distribution in the replay buffer.

\textbf{Actor Update.} The actor minimizes the soft objective under the replay state distribution:
\begin{equation}
J_\pi(\theta)
\;=\;
\mathbb{E}_{s\sim\mathcal D,\,\varepsilon}\left[
\alpha\,\log\pi_\theta \big(f_\theta(s,\varepsilon)\mid s\big)
-\min\nolimits_{j=1,2} Q_{\psi_j} \big(s,f_\theta(s,\varepsilon)\big)
\right].
\label{eq:cont-actor}
\end{equation}
By reparameterization, the gradient flows through both the explicit \(\log\pi_\theta\) term and the path \(a=f_\theta(s,\varepsilon)\). Particularly, denote \(Q_\psi(\cdot, \cdot) = \min_{j=1,2} Q_{\psi_j} (\cdot, \cdot)\), we have that
\[
\nabla_\theta J_\pi(\theta) = \mathbb{E}_{s,\varepsilon} \left[ \alpha \nabla_\theta \log \pi_\theta (a \mid s) + \left( \alpha \nabla_a \log \pi_\theta(a \mid s) - \nabla_a Q_\psi (s,a) \right) \nabla_\theta f_\theta (s, \varepsilon) \right]_{a=f_\theta(s,\varepsilon)}.
\]
In code, you typically just write the loss
\[
\mathbb{E}_{s, \varepsilon}\left[ \alpha \log \pi_\theta (a \mid s) - Q_\psi (s, a) \right], \quad a = f_\theta(s, \varepsilon),
\]
and autodiff will automatically compute the correct gradient.

\textbf{Tuning \(\alpha\) (Temperature).} \(\alpha\) trades off reward pursuit vs.~policy entropy. A fixed \(\alpha\) is problem-dependent. SAC treats \(\alpha\) as a dual variable to enforce a target entropy \(\bar{\mathcal H}\) (often \(-\text{dim}(\mathcal A)\)):
\[
J(\alpha)\;=\;\mathbb{E}_{s\sim\mathcal D,\,a\sim\pi_\theta}\!\Big[-\alpha\big(\log \pi_\theta(a\mid s)+\bar{\mathcal H}\big)\Big],
\quad
\log\alpha \leftarrow \log\alpha - \alpha_\alpha \nabla_{\log\alpha} J(\alpha).
\]
This adapts exploration automatically across tasks and training phases.

\textbf{Soft Actor--Critic (Continuous Actions, Twin Critics + Twin Targets)}

\textbf{Inputs:} replay buffer \(\mathcal{D}\); policy \(\pi_\theta(a\mid s)\) reparameterized by \(a=f_\theta(s,\varepsilon)\) with tanh-squashed Gaussian; twin critics \(Q_{\psi_1},Q_{\psi_2}\); twin target critics with params \(\bar\psi_1,\bar\psi_2\); discount \(\gamma\); temperature \(\alpha\) (learned or fixed); Polyak \(\tau\in(0,1]\); batch size \(B\); stepsizes \(\alpha_\theta,\alpha_\psi,\alpha_\alpha\).

\textbf{Initialize:} \(\bar\psi_j \leftarrow \psi_j\) for \(j\in\{1,2\}\).

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Interaction.}\\
  Observe \(s_t\). Sample \(\varepsilon_t\sim\mathcal N(0,I)\), set \(a_t=f_\theta(s_t,\varepsilon_t)\); step env to get \((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\); push to \(\mathcal{D}\).
\item
  \textbf{Sample minibatch.}\\
  Draw \(B\) transitions \(\{(s,a,r,s',d)\}\) from \(\mathcal{D}\).
\item
  \textbf{Target computation (twin targets, reparameterized next action).}

  \begin{itemize}
  \tightlist
  \item
    Sample \(\varepsilon'\sim\mathcal N(0,I)\), set \(a'=f_\theta(s',\varepsilon')\).
  \item
    Compute \(\log\pi_\theta(a'\mid s')\) with \textbf{tanh correction}.
  \item
    Evaluate target critics \(Q_{\bar\psi_1}(s',a')\), \(Q_{\bar\psi_2}(s',a')\); let \(Q_{\min}(s',a')=\min\{Q_{\bar\psi_1},Q_{\bar\psi_2}\}\).
  \item
    \textbf{Bellman target:}\\
    \[
    y \leftarrow r + \gamma(1-d)\,\big(Q_{\min}(s',a') - \alpha\,\log\pi_\theta(a'\mid s')\big).
    \]
    \emph{(Stop gradient through \(y\).)}
  \end{itemize}
\item
  \textbf{Critic updates (both heads).}\\
  \[
  \psi_j \leftarrow \psi_j - \alpha_\psi \nabla_{\psi_j}\;\frac{1}{B}\sum (Q_{\psi_j}(s,a)-y)^2,\quad j=1,2.
  \]
\item
  \textbf{Actor update (reparameterized).}

  \begin{itemize}
  \tightlist
  \item
    For each \(s\) in the batch, sample \(\varepsilon\), set \(a=f_\theta(s,\varepsilon)\).
  \item
    \textbf{Actor objective:}\\
    \[
    J_\pi(\theta)=\frac{1}{B}\sum_s \Big(\alpha\,\log\pi_\theta(a\mid s)-\min\nolimits_j Q_{\psi_j}(s,a)\Big).
    \]
  \item
    Update:
    \[
    \theta \leftarrow \theta - \alpha_\theta \nabla_\theta J_\pi(\theta).
    \]
  \end{itemize}
\item
  \textbf{Temperature (optional).}\\
  With target entropy \(\bar{\mathcal H}\) and parameter \(\log\alpha\):
  \[
  J(\alpha)=\frac{1}{B}\sum_s \big[-\alpha(\log\pi_\theta(a\mid s)+\bar{\mathcal H})\big],\quad
  \log\alpha \leftarrow \log\alpha - \alpha_\alpha \nabla_{\log\alpha} J(\alpha),\quad
  \alpha\leftarrow e^{\log\alpha}.
  \]
\item
  \textbf{Target critics (Polyak).} For \(j=1,2\):
  \[
  \bar\psi_j \;\leftarrow\; \tau\,\psi_j \;+\; (1-\tau)\,\bar\psi_j.
  \]
\end{enumerate}

The next example applies SAC to Inverted Pendulum.

\begin{example}[SAC for Inverted Pendulum]
\protect\hypertarget{exm:pendulum-sac}{}\label{exm:pendulum-sac}

Fig. \ref{fig:pendulum-learning-curve-sac} plots the learning curve.

Fig. \ref{fig:pendulum-policy-rollout-sac} visualizes two sample rollouts of the policy.

Code can be found \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_sac.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/pendulum_learning_curve_sac} 

}

\caption{Learning curve (Soft Actor--Critic).}\label{fig:pendulum-learning-curve-sac}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{images/Policy-Gradients/pendulum_rollout_sac-1} \includegraphics[width=0.4\linewidth]{images/Policy-Gradients/pendulum_rollout_sac-2} 

}

\caption{Policy rollout (Soft Actor--Critic).}\label{fig:pendulum-policy-rollout-sac}
\end{figure}

\end{example}

\subsection{Deterministic Policy Gradient}\label{deterministic-policy-gradient}

In continuous-control tasks, sampling or integrating over actions inside policy gradients is costly and noisy. The Deterministic Policy Gradient (DPG) framework \citep{silver2014deterministic} replaces the stochastic policy \(\pi_\theta(a\mid s)\) with a \emph{deterministic} actor
\[
a = \mu_\theta(s)\in\mathbb{R}^m.
\]
Its state--action value and discounted state visitation measure are
\[
Q^{\mu_\theta}(s,a) \;=\; \mathbb{E}\!\left[\sum_{t=0}^\infty \gamma^t R(s_t,a_t)\,\middle|\,s_0=s,\ a_0=a,\ a_{t>0}=\mu_\theta(s_t)\right],
\]
\[
\rho^{\pi}(s) \;=\; \sum_{t=0}^{\infty}\gamma^t \Pr(s_t=s \mid a_t\sim \pi(\cdot\mid s_t)),\qquad
\rho^{\mu_\theta}\equiv \rho^{\pi=\mu_\theta}.
\]

We consider two objectives:

\begin{itemize}
\item
  \textbf{On-policy objective}:
  \begin{equation}
  J(\theta)\;=\;\mathbb{E}_{s\sim \rho^{\mu_\theta}}\big[\,Q^{\mu_\theta}(s,\mu_\theta(s))\,\big].
  \label{eq:DPG-obj-on}
  \end{equation}
\item
  \textbf{Off-policy surrogate} (with behavior policy \(\beta\)):
  \begin{equation}
  J_\beta(\theta)\;=\;\mathbb{E}_{s\sim \rho^{\beta}} \big[\,Q^{\mu_\theta}(s,\mu_\theta(s))\,\big].
  \label{eq:DPG-obj-off}
  \end{equation}
\end{itemize}

The on-policy objective \eqref{eq:DPG-obj-on} is the usual RL objective in policy gradient methods, as \(Q^{\mu_\theta}(s, \mu_\theta(s)) = V^{\mu_\theta}(s)\) by definition.

A key result in Deterministic Policy Gradient is that under mild conditions, optimizating the surrogate off-policy objective \eqref{eq:DPG-obj-off} is the same as optimizating the original on-policy objective.

To see this, assume

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(R\) and \(P(\cdot\mid s,a)\) (the transition dynamics) are bounded/measurable; \(Q^{\mu_\theta}\) exists and is continuously differentiable in \(a\);
\item
  \(\mu_\theta(s)\) is continuously differentiable in \(\theta\);
\item
  Interchange of integration and differentiation is valid (e.g., dominated convergence).
\end{enumerate}

Then, the on-policy and off-policy deterministic policy gradients (DPG) are:

\begin{itemize}
\item
  \textbf{On-policy DPG.}
  \begin{equation}
  \nabla_\theta J(\theta)
  \;=\;
  \mathbb{E}_{s\sim \rho^{\mu_\theta}} \left[
  \nabla_\theta \mu_\theta(s)\;
  \nabla_a Q^{\mu_\theta}(s,a)\big|_{a=\mu_\theta(s)}
  \right].
  \label{eq:DPG-grad-on}
  \end{equation}
\item
  \textbf{Off-policy DPG.} For any behavior policy \(\beta\) with visitation \(\rho^\beta\),
  \begin{equation}
  \nabla_\theta J_\beta(\theta)
  \;=\;
  \mathbb{E}_{s\sim \rho^{\beta}} \left[
  \nabla_\theta \mu_\theta(s)\;
  \nabla_a Q^{\mu_\theta}(s,a)\big|_{a=\mu_\theta(s)}
  \right].
  \label{eq:DPG-grad-off}
  \end{equation}
\end{itemize}

In particular, the off-policy DPG \eqref{eq:DPG-grad-off} can be estimated from replay sampled under \(\beta\) without action-importance ratios; only the state weighting changes.

The following result states that the on-policy and off-policy objectives share the same stationary points.

\begin{theorem}[Common First-Order Optima]
\protect\hypertarget{thm:dpg-common-foc}{}\label{thm:dpg-common-foc}Let
\begin{equation}
g(s;\theta)\;:=\;\nabla_\theta \mu_\theta(s)\,\nabla_a Q^{\mu_\theta}(s,a)\big|_{a=\mu_\theta(s)}\in\mathbb{R}^{d},
\label{eq:per-state-gradient}
\end{equation}
where \(d\) is the dimension of \(\theta\).

Suppose \(\rho^\beta\) has coverage of the on-policy support, i.e.,
\[
\text{supp}(\rho^{\mu_\theta})\ \subseteq\ \text{supp}(\rho^\beta),\quad
\text{and}\quad \rho^\beta(s)>0\ \text{a.e. on }\text{supp}(\rho^{\mu_\theta}).
\]
If \(g(s;\theta^\star)=0\) for \(\rho^{\mu_{\theta^\star}}\)-almost every \(s\) (in particular, if \(\mu_{\theta^\star}\) is greedy w.r.t. \(Q^{\mu_{\theta^\star}}\), so \(\nabla_a Q^{\mu_{\theta^\star}}(s,a)|_{a=\mu_{\theta^\star}(s)}=0\) for all \(s\)), then
\[
\nabla_\theta J(\theta^\star)=0
\quad\text{and}\quad
\nabla_\theta J_\beta(\theta^\star)=0.
\]
Thus any deterministic policy satisfying the first-order optimality condition (greedy w.r.t. its own \(Q\)) is a \emph{stationary point of both} \(J\) and \(J_\beta\), regardless of the (covered) state weighting.

If additionally \(\text{supp}(\rho^{\mu_\theta})=\text{supp}(\rho^\beta)\) and both are strictly positive on that support, then
\[
\nabla_\theta J(\theta)=0 \ \Longleftrightarrow\ \nabla_\theta J_\beta(\theta)=0.
\]
\end{theorem}

\textbf{Remarks.}

\begin{itemize}
\item
  The off-policy objective \(J_\beta\) changes only the \textbf{weights} over states; the \textbf{per-state improvement direction} \(g(s;\theta)\) is identical. With sufficient coverage, ascent on \(J_\beta\) improves \(J\) and shares its stationary points.
\item
  In practice, DDPG uses exploration noise to expand support of \(\rho^\beta\) and target networks to stabilize \(Q^{\mu_\theta}\), making the off-policy gradient estimate reliable.
\end{itemize}

\textbf{From DPG to DDPG (Deep DPG).} DDPG \citep{lillicrap2015continuous} implements DPG with deep networks + standard stabilizers:

\begin{itemize}
\tightlist
\item
  \textbf{Replay buffer} \(\mathcal D\) for off-policy sample efficiency.
\item
  \textbf{Target networks} \(\mu_{\bar\theta}, Q_{\bar\psi}\) with Polyak averaging to stabilize TD targets.
\item
  \textbf{Exploration noise} added to the deterministic action: \(a_t = \mu_\theta(s_t) + \varepsilon_t\) (original paper used Ornstein--Uhlenbeck noise; Gaussian works well too).
\end{itemize}

\textbf{High-Level Algorithm (DDPG).}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Interact off-policy.} Act with exploration: \(a_t=\mu_\theta(s_t)+\varepsilon_t\). Store \((s_t,a_t,r_t,s_{t+1},\text{done}_t)\) in \(\mathcal D\).
\item
  \textbf{Critic TD(0).} For a minibatch from \(\mathcal D\),
  \[
  y = r + \gamma(1-\text{done})\,Q_{\bar\psi} \big(s',\,\mu_{\bar\theta}(s')\big),\qquad
  \min_\psi\ \frac{1}{B}\sum (Q_\psi(s,a)-y)^2.
  \]
\item
  \textbf{Actor DPG step.}
  \[
  \max_\theta\ \frac{1}{B}\sum Q_\psi\big(s,\,\mu_\theta(s)\big)
  \quad\Longleftrightarrow\quad
  \nabla_\theta J \approx \frac{1}{B}\sum \nabla_\theta \mu_\theta(s)\,\nabla_a Q_\psi(s,a)\big|_{a=\mu_\theta(s)}.
  \]
\item
  \textbf{Targets Polyak update.}
  \[
  \bar\theta \leftarrow \tau\,\theta + (1-\tau)\bar\theta,\ \ \bar\psi \leftarrow \tau\,\psi + (1-\tau)\bar\psi.
  \]
\end{enumerate}

\textbf{Remarks.}

\begin{itemize}
\tightlist
\item
  No entropy bonus or log-probabilities (in contrast to SAC). Exploration comes from additive noise.\\
\item
  Overestimation and sensitivity to hyperparameters can appear; target networks, small actor steps, and proper normalization help.
\end{itemize}

The following pseudocode implements DDPG.

\textbf{Deep Deterministic Policy Gradient (DDPG)}

\textbf{Inputs:} replay buffer \(\mathcal{D}\); deterministic actor \(\mu_\theta(s)\); critic \(Q_\psi(s,a)\); target networks \(\mu_{\bar\theta}, Q_{\bar\psi}\); discount \(\gamma\in[0,1)\); Polyak \(\tau\in(0,1]\); batch size \(B\); stepsizes \(\alpha_\theta,\alpha_\psi\); exploration noise process \(\varepsilon_t\sim \mathcal N(0,\sigma^2 I)\) (or Ornstein--Uhlenbeck).

\textbf{Initialize:} \(\bar\theta\leftarrow\theta,\ \bar\psi\leftarrow\psi\). Fill \(\mathcal D\) with a short random warm-up.

For iterations \(k=0,1,2,\dots\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Interaction (off-policy).}\\
  Observe \(s_t\). Compute action with noise\\
  \[
  a_t \leftarrow \text{clip}\big(\mu_\theta(s_t) + \varepsilon_t,\ a_{\min}, a_{\max}\big).
  \]
  Step env to get \((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\). Push into \(\mathcal D\).
\item
  \textbf{Sample minibatch.}\\
  Draw \(B\) transitions \(\{(s,a,r,s',d)\}\) from \(\mathcal D\).
\item
  \textbf{Critic target.}\\
  \[
  a' \leftarrow \mu_{\bar\theta}(s'),\qquad
  y \leftarrow r + \gamma(1-d)\,Q_{\bar\psi}(s',a').
  \]
  \emph{(Stop gradient through \(y\).)}
\item
  \textbf{Critic update.}\\
  \[
  \psi \leftarrow \psi - \alpha_\psi\,\nabla_\psi\ \frac{1}{B}\sum (Q_\psi(s,a)-y)^2.
  \]
\item
  \textbf{Actor update (DPG).}\\
  \[
  \theta \leftarrow \theta + \alpha_\theta\ \frac{1}{B}\sum
  \Big[\ \nabla_\theta \mu_\theta(s)\ \nabla_a Q_\psi(s,a)\big|_{a=\mu_\theta(s)}\ \Big].
  \]
  \emph{(Equivalently, ascend \(\frac{1}{B}\sum Q_\psi(s,\mu_\theta(s))\) by backprop.)}
\item
  \textbf{Target networks (Polyak).}\\
  \[
  \bar\theta \leftarrow \tau\,\theta + (1-\tau)\,\bar\theta,\qquad
  \bar\psi \leftarrow \tau\,\psi + (1-\tau)\,\bar\psi.
  \]
\end{enumerate}

The next example applies DDPG to Inverted Pendulum.

\begin{example}[DDPG for Inverted Pendulum]
\protect\hypertarget{exm:pendulum-ddpg}{}\label{exm:pendulum-ddpg}

Fig. \ref{fig:pendulum-learning-curve-ddpg} plots the learning curve of DDPG.

Fig. \ref{fig:pendulum-policy-rollout-ddpg} visualizes sample rollouts of the learned policy.

Code can be found \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_ddpg.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/pendulum_learning_curve_ddpg} 

}

\caption{Learning curve (DDPG).}\label{fig:pendulum-learning-curve-ddpg}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{images/Policy-Gradients/pendulum_rollout_ddpg-1} \includegraphics[width=0.3\linewidth]{images/Policy-Gradients/pendulum_rollout_ddpg-2} \includegraphics[width=0.3\linewidth]{images/Policy-Gradients/pendulum_rollout_ddpg-3} 

}

\caption{Policy rollout (DDPG).}\label{fig:pendulum-policy-rollout-ddpg}
\end{figure}

\end{example}

\section{Model-based Policy Optimization}\label{model-based-policy-optimization}

Model-based policy optimization (MBPO) \citep{janner2019trust} sits between pure model-free methods (high variance, data hungry) and ``plan-only'' model-based control (sensitive to model bias, to be introduced in Chapter \ref{model-based-plan-optimize}). The key idea is to learn a dynamics model and then use only \emph{short} rollouts from that model to create extra training data for a strong off-policy learner (usually SAC).

Consider an MDP with unknown dynamics \(s_{t+1} \sim P( \cdot \mid s_t,a_t)\). MBPO learns an ensemble \(\{\hat f_{\psi_k}\}_{k=1}^K\) that predicts the next state (often the \emph{delta-state} \(\Delta s\)). Rather than planning far ahead inside the learned model, MBPO:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Collects real transitions \(\mathcal{D}_{\text{env}}\) by interacting with the environment.
\item
  Fits the dynamics ensemble on \(\mathcal{D}_{\text{env}}\).
\item
  Periodically generates short rollouts (e.g., horizon \(H=1\ldots5\)) starting from \emph{real} states by simulating with a \emph{random} member of the ensemble, producing model transitions \(\mathcal{D}_{\text{model}}\).
\item
  Trains an off-policy actor-critic (e.g., SAC) on a \emph{mixture} of \(\mathcal{D}_{\text{env}}\) and \(\mathcal{D}_{\text{model}}\), typically with a high fraction of model data but \emph{short} \(H\) to limit bias.
\end{enumerate}

This yields the sample-efficiency benefits of model-based learning while maintaining the robustness of model-free policy optimization.

The following pseudocode implements MBPO with SAC as the off-policy learner.

\textbf{Model-based Policy Optimization (SAC as Off-Policy Learner)}

\textbf{Inputs:} environment \(\mathcal{E}\), policy \(\pi_\theta(a\mid s)\), twin critics \(Q_{\phi_1},Q_{\phi_2}\) with targets, temperature \(\alpha\) (auto-tuned), dynamics ensemble \(\{\hat f_{\psi_k}\}_{k=1}^K\), real buffer \(\mathcal{D}_{\text{env}}\), model buffer \(\mathcal{D}_{\text{model}}\). Rollout horizon \(H\), model ratio \(p_{\text{model}}\in[0,1]\), update counts \(G_{\text{dyn}}, G_{\text{rl}}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Warm-up \& data collection.}

  \begin{itemize}
  \tightlist
  \item
    Interact with \(\mathcal{E}\) using \(\pi_\theta\) (or random for a short warm-up).
  \item
    Store \((s,a,r,s')\) in \(\mathcal{D}_{\text{env}}\).
  \end{itemize}
\item
  \textbf{Fit dynamics.} For \(G_{\text{dyn}}\) steps:

  \begin{itemize}
  \tightlist
  \item
    Sample minibatch \(B\subset\mathcal{D}_{\text{env}}\).
  \item
    Update each \(\psi_k\) to predict \(\Delta s = s'-s\) (and optionally \(r\)) by minimizing, e.g, mean squared error.
  \end{itemize}
\item
  \textbf{Short model rollouts (data generation).}

  \begin{itemize}
  \tightlist
  \item
    Sample seed states \(S_0\) from recent \(\mathcal{D}_{\text{env}}\).
  \item
    For each \(s\in S_0\)

    \begin{itemize}
    \tightlist
    \item
      for \(h=1\ldots H\):

      \begin{itemize}
      \tightlist
      \item
        Sample \(a\sim \pi_\theta(\cdot\mid s)\).
      \item
        Pick random ensemble member \(k\); predict \(\hat\Delta s \leftarrow \hat f_{\psi_k}(s,a)\); set \(\hat s' = s + \hat\Delta s\).
      \item
        Compute \(r\) via a learned reward model or a known formula (when available).
      \item
        Push \((s,a,r,\hat s',\texttt{done}=0)\) into \(\mathcal{D}_{\text{model}}\).
      \item
        Set \(s\leftarrow\hat s'\); break if time-limit reached.
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  \textbf{Off-policy RL updates (SAC).} For \(G_{\text{rl}}\) steps:

  \begin{itemize}
  \tightlist
  \item
    Form a minibatch by drawing a fraction \(p_{\text{model}}\) from \(\mathcal{D}_{\text{model}}\) and \(1-p_{\text{model}}\) from \(\mathcal{D}_{\text{env}}\).
  \item
    \textbf{Critic targets:}
    \[
    y=r+\gamma(1-d)\big[\min_j Q_{\phi_j^-}(s',a')-\alpha\log\pi_\theta(a'\mid s')\big],
    \]
    where \(a'\sim\pi_\theta(\cdot\mid s')\).
  \item
    \textbf{Critic update:} regress \(Q_{\phi_j}\) to \(y\) (both heads).
  \item
    \textbf{Actor update:} minimize \(J_\pi = \mathbb{E}_s\!\big[\alpha\log\pi_\theta(a\mid s) - \min_j Q_{\phi_j}(s,a)\big]\), with \(a\sim\pi_\theta\).
  \item
    \textbf{Temperature update (optional):} adjust \(\alpha\) towards target entropy.
  \item
    Soft-update target critics.
  \end{itemize}
\item
  \textbf{Repeat} steps 1--4 until convergence or iteration limits.
\end{enumerate}

\textbf{Notes.}

\begin{itemize}
\item
  Ensembles capture epistemic uncertainty; random-member rollouts implicitly regularize toward pessimism.
\item
  Keeping \(H\) \emph{short} (e.g., \(1\!\!-\!\!5\)) is crucial to prevent model error explosion.
\item
  Use recent real states as rollout seeds to stay on-distribution.
\end{itemize}

The next example applies MBPO to Inverted Pendulum.

\begin{example}[MBPO for Inverted Pendulum]
\protect\hypertarget{exm:pendulum-mbpo}{}\label{exm:pendulum-mbpo}

Fig. \ref{fig:pendulum-learning-curve-mbpo} plots the learning curve.

Fig. \ref{fig:pendulum-policy-rollout-mbpo} visualizes sample rollouts of the policy.

Code can be found \href{https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_mbpo.py}{here}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/Policy-Gradients/pendulum_learning_curve_mbpo} 

}

\caption{Learning curve (MBPO).}\label{fig:pendulum-learning-curve-mbpo}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{images/Policy-Gradients/pendulum_rollout_mbpo-1} \includegraphics[width=0.3\linewidth]{images/Policy-Gradients/pendulum_rollout_mbpo-2} \includegraphics[width=0.3\linewidth]{images/Policy-Gradients/pendulum_rollout_mbpo-3} 

}

\caption{Policy rollout (MBPO).}\label{fig:pendulum-policy-rollout-mbpo}
\end{figure}

\end{example}

\chapter{Model-based Planning and Optimization}\label{model-based-plan-optimize}

\appendix


\chapter{Convex Analysis and Optimization}\label{appconvex}

\section{Theory}\label{appconvex-theory}

\subsection{Sets}\label{sets}

Convex set is one of the most important concepts in convex optimization. Checking convexity of sets is crucial to determining whether a problem is a convex problem. Here we will present some definitions of some set notations in convex optimization.

\begin{definition}[Affine set]
\protect\hypertarget{def:affineset}{}\label{def:affineset}A set \(C\subset \mathbb{R}^n\) is affine if the line through any two distinct points in \(C\) lies in \(C\), i.e., if for any \(x_1,x_2 \in C\) and any \(\theta \in \mathbb{R}\), we have \(\theta x_1 + (1-\theta)x_2 \in C\).
\end{definition}

\begin{definition}[Convex set]
\protect\hypertarget{def:convexset}{}\label{def:convexset}A set \(C\subset \mathbb{R}^n\) is convex if the line segment between any two distinct points in \(C\) lies in \(C\), i.e., if for any \(x_1,x_2 \in C\) and any \(\theta \in [0,1]\), we have \(\theta x_1 + (1-\theta)x_2 \in C\).
\end{definition}

\begin{definition}[Cone]
\protect\hypertarget{def:cone}{}\label{def:cone}A set \(C\subset \mathbb{R}^n\) is a cone if for any \(x\in C\) and any \(\theta\geq 0\), we have \(\theta x \in C\).
\end{definition}

\begin{definition}[Convex Cone]
\protect\hypertarget{def:convexcone}{}\label{def:convexcone}A set \(C\subset \mathbb{R}^n\) is a convex cone if \(C\) is convex and a cone.
\end{definition}

Below are some important examples of convex sets:

\begin{definition}[Hyperplane]
\protect\hypertarget{def:hyperplane}{}\label{def:hyperplane}A hyperplane is a set of the form \[\{x|a^Tx = b\}\]
\end{definition}

\begin{definition}[Halfspaces]
\protect\hypertarget{def:halfspaces}{}\label{def:halfspaces}A (closed) halfspace is a set of the form \[\{x|a^Tx \leq b\}\]
\end{definition}

\begin{definition}[Balls]
\protect\hypertarget{def:balls}{}\label{def:balls}A ball is a set of the form \[B(x,r) = \{y|\|y-x\|_2 \leq r\} = \{x+ru|\|u\|_2\leq 1\}\]
where \(r >0\).
\end{definition}

\begin{definition}[Ellipsoids]
\protect\hypertarget{def:ellipsoids}{}\label{def:ellipsoids}A ellipsoid is a set of the form \[\mathcal{E} = \{y|(y-x)^TP^{-1}(y-x)\leq 1\}\]
where \(P\) is symmetric and positive definite.
\end{definition}

\begin{definition}[Polyhedra]
\protect\hypertarget{def:polyhedra}{}\label{def:polyhedra}A polyhedra is defined as the solution set of a finite number of linear equalities
and inequalities: \[\mathcal{P} = \{x|a_j^Tx\leq b_j, j=1,...,m, c_k^Tx=d_k,k=1,...,p\}\]
\end{definition}

\begin{definition}[Norm ball]
\protect\hypertarget{def:normball}{}\label{def:normball}A norm ball \(B\) of radius \(r\) and a center \(x_c\) associated with the norm \(\|\cdot\|\) is defined as: \[B = \{x|\|x-x_c\|\leq r\}\]
\end{definition}

\begin{definition}[Norm cone]
\protect\hypertarget{def:normcone}{}\label{def:normcone}A norm cone \(C\) associated with the norm \(\|\cdot\|\) is defined as: \[C = \{(x,t)|\|x\|\leq t\}\subset \mathbb{R}^{n+1}\]
\end{definition}

Simplexes are important family of polyhedra. Suppose the \(k+1\) points \(v_0,...,v_k\in \mathbb{R}^n\) are affinely independent, which means \(v_1-v_0,...,v_k-v_0\) are linearly independent.

\begin{definition}[Simplex]
\protect\hypertarget{def:simplex}{}\label{def:simplex}A simplex \(C\) defined by points \(v_0,...,v_k\) is: \[C = \textbf{conv}\{v_0,...,v_k\} = \{\theta_0v_0 + ... \theta_kv_k|\theta \succeq 0, \textbf{1}^T\theta = 1\}\]
\end{definition}

Extremely important examples of convex sets are positive semidefinite cones:

\begin{definition}[Symmetric,positive semidefinite,positive definite matrices]
\protect\hypertarget{def:symmetricmatrices}{}\label{def:symmetricmatrices}\leavevmode

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Symmetric matrices: \(\textbf{S}^n = \{X\in\mathbb{R}^{n\times n}| X=X^T\}\)
\item
  Symmetric Positive Semidefinite matrices: \(\textbf{S}_+^n = \{X\in\textbf{S}^n| X\succeq0\}\)
\item
  Symmetric Positive definite matrices: \(\textbf{S}_{++}^n = \{X\in\textbf{S}^n| X\succ0\}\)
\end{enumerate}

\end{definition}

In most scenarios, the set we encounter is more complicated. In general it is extermely hard to determine whether a set in convex or not. But if the set is `generated' by some convex sets, we can easily determine its convexity. So let's focus on operations that preserve convexity:

\begin{proposition}
\protect\hypertarget{prp:operationpreserveconvexity}{}\label{prp:operationpreserveconvexity}

Assume \(S\) is convex, \(S_\alpha,\alpha\in\mathcal{A}\) is a family of convex sets. Following operations on convex sets will preserve convexity:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Intersection: \(\bigcap_{\alpha\in\mathcal{A}}S_\alpha\) is convex.
\item
  Image under affine function: A function \(f:\mathbb{R}^n\to\mathbb{R}^m\) is affine if it has the form \(f(x) = Ax+b\). The image of \(S\) under affine function \(f\) is convex. I.e. \(f(S) = \{f(x)|x\in S\}\) is convex
\item
  Image under perspective function: We define the perspective function \(P:\mathbb{R}^{n+1}\), with domain \(\textbf{dom}P = \mathbb{R}^n\times \mathbb{R}_{++}\)(where \(\mathbb{R}_{++}=\{x\in \mathbb{R}|x>0\}\)) as \(P(z,t) = z/t\). The image of \(S\) under perspective function is convex.
\item
  Image under linear-fractional function: We define linear fractional function \(f:\mathbb{R}^n\to\mathbb{R}^m\) as:\(f(x) = (Ax+b)/(c^Tx+d)\) with \(\textbf{dom}f = \{x|c^Tx+d>0\|\). The image of \(S\) under linear fractional functions is convex.
\end{enumerate}

\end{proposition}

In some cases, the restrictions of \textbf{interior} is too strict. For example, imagine a plane in \(\mathbb{R}^3\). The interior of the plane is \(\emptyset\). But intuitively many property should be extended to this kind of situation. Because the points in the plane also lies `inside' the convex set. Thus, we will define \textbf{relative interior}. First we will define \textbf{affine hull}.

\begin{definition}[Affine hull]
\protect\hypertarget{def:affinehull}{}\label{def:affinehull}The affine hull of a set \(S\) is the smallest affine set that contains \(S\), which can be written as:
\[\text{aff}(S) = \{\sum_{i=1}^k\alpha_ix_i|k>0,x_i\in S,\alpha_i\in\mathbb{R},\sum_{i=1}^k\alpha_i=1\}\]
\end{definition}

\begin{definition}[Relative Interior]
\protect\hypertarget{def:relint}{}\label{def:relint}The relative interior of a set \(S\) (denoted \(\text{relint}(S)\)) is defined as its interior within the affine hull of \(S\). I.e.
\[\text{relint}(S):=\{x\in S: \text{there exists } \epsilon>0 \text{ such that }N_\epsilon \cap \text{aff}(S)\subset S\}\]
where \(N_\epsilon(x)\) is a ball of radius \(\epsilon\) centered on \(x\).
\end{definition}

\subsection{Convex function}\label{appconvex-theory-convexfunction}

In this section, let's define convex functions:

\begin{definition}[Convex function]
\protect\hypertarget{def:defcvxfunc}{}\label{def:defcvxfunc}A function \(f:\mathbb{R}^n\to\mathbb{R}\) is \textbf{convex} if \(\textbf{dom}\ f\) is convex and \(\forall x,y\in \textbf{dom}\ f\) and with \(\theta \in [0,1]\), we have:\[f(\theta x +(1-\theta)y)\leq \theta f(x) + (1-\theta)f(y)\]
The function is \textbf{strictly convex} if the inequality holds whenever \(x\neq y\) and \(\theta\in (0,1)\).
\end{definition}

If a function is differentiable, it will be easier for us to check its convexity:

\begin{proposition}[Conditions for Convex function]
\protect\hypertarget{prp:decidecvx}{}\label{prp:decidecvx}1.(First order condition) Suppose \(f\) is differentiable, then \(f\) is convex if and only if \(\textbf{dom} f\) is convex and \(\forall x,y\in \textbf{dom} f\), \[f(y)\geq f(x) +\nabla f(x)^T(y-x)\]
2.(Second order conditions) Suppose \(f\) is twice differentiable, then \(f\) is convex if and only if \(\textbf{dom} f\) is convex and \(\forall x\in \textbf{dom} f\), \[\nabla^2 f(x) \succeq \textbf{0}\]
\end{proposition}

For the same purpose, some operations that preserve the convexity of the convex functions are presented here:

\begin{proposition}[Operations that preserve convexity]
\protect\hypertarget{prp:preservecvx}{}\label{prp:preservecvx}Let \(f:\mathbb{R}^n\to\mathbb{R}\) be a convex function and \(g_1,...,g_n\) be convex functions. The following operations will preserve convexity of the function:

1.(Nonnegative weighted sum): A nonnegative weighted sum of convex functions: \[f = \omega_1f_1 + ... +\omega_mf_m\]

2.(Composition with an affine mapping) Suppose \(A\in \mathbb{R}^{n\times m}\) and \(b\in \mathbb{R}^n\), then \(g(x) = f(Ax+b)\) is convex.

3.(Pointwise maximum and supremum) \(g(x) = \max\{g_1(x),...,g_n(x)\}\) is convex. If \(h(x,y)\) is convex in \(x\) for each \(y\in\mathcal{A}\), then \(\sup_{y\in\mathcal{A}} h(x,y)\) is also convex in \(x\).

4.(Minimization) If \(h(x,y)\) is convex in \((x,y)\), and \(C\) is a convex nonempty set, then \(\inf_{x\in C} h(x,y)\) is convex in \(x\).

5.(Perspective of a function) The perspective of \(f\) is the function \(h:\mathbb{R}^{n+1}\to\mathbb{R}\) defined by: \(h(x,t) = tf(x/t)\) with domain \(\textbf{dom}\ h=\{(x,t)|x/t\in\textbf{dom} f,t>0\}\). And \(h\) is convex.
\end{proposition}

\subsection{Lagrange dual}\label{lagrange-dual}

We consider an optimization problem in the standard form (without assuming convexity of anything):
\begin{equation}
\begin{aligned}
p^* = \quad \min_{x} \quad & f_0(x)\\
\textrm{s.t.} \quad & f_i(x)\leq 0\quad i=1...,m\\
  & h_i(x) = 0\quad i=1,...,p   \\
\end{aligned}
\end{equation}

\begin{definition}[Lagrange dual function]
\protect\hypertarget{def:defdualfunc}{}\label{def:defdualfunc}The Lagrangian related to the problem above is defined as: \[L(x,\lambda,\nu)=f_0(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^p\nu_ih_i(x)\]
The Lagrange dual function is defined as: \[g(\lambda,\nu) = \inf_{x\in\mathcal{D}}L(x,\lambda,\nu)\]
\end{definition}

When the Lagrangian is unbounded below in \(x\), the dual function takes on the value \(-\infty\). Note that since the Lagrange dual function is a pointwise infimum of a family of affine functions of \((\lambda,\nu)\), so it's concave. The Lagrange dual function will give us lower bounds of the optimal value of the original problem: \[g(\lambda,\nu)\leq p^*\]. We can see that, the dual function can give a nontrivial lower bound only when \(\lambda\succeq 0\). Thus we can solve the following dual problem to get the best lower bound.

\begin{definition}[Lagrange dual problem]
\protect\hypertarget{def:defdualprob}{}\label{def:defdualprob}The lagrangian dual problem is defined as follows:
\begin{equation}
  \begin{aligned}
  d^* = \quad \max_{\lambda,\nu} \quad & g(\lambda,\nu)\\
  \textrm{s.t.} \quad & \lambda\succeq 0
  \end{aligned}
  \end{equation}
This is a convex optimization problem.
\end{definition}

We can easily see that \[d^*\leq p^*\] always hold. This property is called \textbf{weak duality}. If \[d^*=p^*\], it's called \textbf{strong duality}. Strong duality does not hold in general, but it usually holfs for convex problems. We can find conditions that guarantee strong duality in convex problems, which are called constrained qualifications. Slater's constraint qualification is a useful one.

\begin{theorem}[Slater's constraint qualification]
\protect\hypertarget{thm:slater}{}\label{thm:slater}Strong duality holds for a convex problem
\begin{equation}
  \begin{aligned}
  p^* = \quad \min_{x} \quad & f_0(x)\\
  \textrm{s.t.} \quad & f_i(x)\leq 0\quad i=1...,m\\
    & Ax=b   \\
  \end{aligned}
 \end{equation}
if it is strictly feasible, i.e.
\[\exists x\in\textbf{relint}\mathcal{D}:\quad f_i(x)<0,\quad i=1...m,\quad Ax=b\]
And the linear inequalities do not need to hold with strict inequality.
\end{theorem}

\subsection{KKT condition}\label{appconvex-theory-kkt}

Note that if strong duality holds, denote \(x^*\) to be primal optimal, and \((\lambda^*,\nu^*)\) to be dual optimal. Then:

\begin{equation}
  \begin{aligned}
  f_0(x^*) = g(\lambda^*,\nu^*) = & \inf_x(f_0(x)+\sum_{i=1}^m\lambda_i^*f_i(x)+\sum_{i=1}^p\nu_i^*h_i(x))\\
 \leq & f_0(x^*)+\sum_{i=1}^m\lambda_i^*f_i(x)+\sum_{i=1}^p\nu_i^*h_i(x)\\
 \leq & f_0(x^*)\\
  \end{aligned}
  \end{equation}

from this, combining \(\lambda^*\geq 0\) and \(f_i(x^*)\leq 0\), we can know that: \(\lambda_i^*f_i(x^*)=0\quad i=1\cdots m\). This means for \(\lambda_i^*\) and \(f_i(x^*)\), one of them must be zero, which is known as complementary slackness).

Thus we arrived at the following four conditions, which are called KKT conditions.

\begin{theorem}[Karush-Kuhn-Tucker(KKT) Conditions]
\protect\hypertarget{thm:KKT}{}\label{thm:KKT}

The following four conditions are called KKT conditions (for a problem with differentiable \(f_i,h_i\))

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Primal feasible: \(f_i(x) \leq 0,i,\cdots ,m,\ h_i(x) = 0,i=1,\cdots ,p\)
\item
  Dual feasible: \(\lambda\succeq0\)
\item
  Complementary slackness: \(\lambda_if_i(x)=0,i=1,\cdots,m\)
\item
  Gradient of Lagrangian with respect to \(x\) vanishes:\(\nabla f_0(x)+\sum_{i=1}^m\lambda_i\nabla f_i(x)+\sum_{i=1}^p\nu_i\nabla h_i(x) = 0\)
\end{enumerate}

\end{theorem}

From the discussion above, we know that if strong duality holds and \(x,\lambda,\nu\) are optimal, then they must satisfy the KKT conditions.

Also if \(x,\lambda,\nu\) satisfy KKT for a convex problem, then they are optimal. However, the converse is not generally true, since KKT condition implies strong duality. If Slater's condition is satisfied, then \(x\) is optimal if and only if there exist \(\lambda,\nu\) that satisfy KKT conditions. Sometimes, by solving the KKT system, we can derive the closed-form solution of a optimization directly. Also, sometimes we will use the residual of the KKT system as the termination condition.

In general, \(f_i,h_i\) may not be differentiable. There are also KKT conditions for them, which will include knowledge of subdifferential and will not be included here.

\section{Practice}\label{appconvex-practice}

\subsection{CVX Introduction}\label{cvx-introduction}

In the last section, we have learned basic concepts and theorems in convex optimization. In this section, on the other hand, we will introduce you how to model basic convex optimization problems with CVX, an easy-to-use MATLAB package. To install CVX, please refer to this \href{http://cvxr.com/cvx/doc/install.html}{page}. Note that every time you what to use the CVX package, you should add it to your MATLAB path. For example, if I install CVX package in the parent directory of my current directory with default directory name \texttt{cvx}, the following line should be added before your CVX codes:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{addpath}\NormalTok{(}\VariableTok{genpath}\NormalTok{(}\StringTok{"../cvx/"}\NormalTok{))}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

With CVX, it is incredibly easy for us to define and solve a convex optimization problem. You just need to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  define the variables.
\item
  define the objective function you want to minimize or maximize.
\item
  define the constraints.
\end{enumerate}

After running your codes, the optimal objective value is stored in the variable \texttt{cvx\_optval}, and the problem status is stored in the variable \texttt{cvx\_status} (when your problem is well-defined, this variable's value will be \texttt{Solved}). The optimal solutions will be stored in the variables you define.

Throughout this section, we will study five types of convex optimization problems: linear programming (LP), quadratic programming (QP), (convex) quadratically constrained quadratic programming (QCQP), second-order cone programming (SOCP), and semidefinite programming (SDP). Given two types of optimization problems \(A\) and \(B\), we say \(A < B\) if \(A\) can always be converted to \(B\) while the inverse is not true. Under this notation, we have
\begin{equation*}
   \text{LP} < \text{QP} < \text{QCQP} < \text{SOCP} < \text{SDP}
\end{equation*}

\subsection{Linear Programming (LP)}\label{linear-programming-lp}

\textbf{Definition.} An LP has the following form:
\begin{equation}
   \label{eq:app-lpdef}
   \begin{aligned}
      \min_{x \in \mathbb{R}^n} & \ c^T x \\
      \text{subject to } & A x \le b  
   \end{aligned}
\end{equation}
where \(x\) is the variable, \(A \in \mathbb{R}^{m\times n}, b \in \mathbb{R}^m\), and \(c \in \mathbb{R}^n\) are the parameters. Note that the constraint \(A x \le b\) already incorporates linear equality constraints. To see this, consider the constraint \(A' x = b'\), we can reformulate it as \(A x \le b\) by
\begin{equation*}
   \begin{bmatrix}
      A' \\ -A'
   \end{bmatrix} x \le \begin{bmatrix}
      b' \\ -b'
   \end{bmatrix}
\end{equation*}

\textbf{Example.} Consider the problem of minimizing a linear function \(c_1 x_1 + c_2 x_2\) over a rectangle \([-l_1, l_1] \times [-l_2, l_2]\). We can convert it to the standard LP form in \eqref{eq:app-lpdef} by simply setting \(c\) as \([c_1, \ c_2]^T\) and the linear inequality constraint as
\begin{equation*}
   \begin{bmatrix}
      1 & 0 \\ -1 & 0 \\ 0 & 1 \\ 0 & -1
   \end{bmatrix} \begin{bmatrix}
      x_1 \\ x_2
   \end{bmatrix} \le \begin{bmatrix}
      l_1 \\ l_1 \\ l_2 \\ l_2
   \end{bmatrix}
\end{equation*}

Corresponding CVX codes are shown below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\%\% Define the LP example setting}
\VariableTok{c1} \OperatorTok{=} \FloatTok{2}\OperatorTok{;}
\VariableTok{c2} \OperatorTok{=} \OperatorTok{{-}}\FloatTok{5}\OperatorTok{;}
\VariableTok{l1} \OperatorTok{=} \FloatTok{3}\OperatorTok{;}
\VariableTok{l2} \OperatorTok{=} \FloatTok{7}\OperatorTok{;}
\CommentTok{\% parameters: c, A, b}
\VariableTok{c} \OperatorTok{=}\NormalTok{ [}\VariableTok{c1}\OperatorTok{;} \VariableTok{c2}\NormalTok{]}\OperatorTok{;}
\VariableTok{A} \OperatorTok{=}\NormalTok{ [}\FloatTok{1}\OperatorTok{,} \FloatTok{0}\OperatorTok{;} \OperatorTok{{-}}\FloatTok{1}\OperatorTok{,} \FloatTok{0}\OperatorTok{;} \FloatTok{0}\OperatorTok{,} \FloatTok{1}\OperatorTok{;} \FloatTok{0}\OperatorTok{,} \OperatorTok{{-}}\FloatTok{1}\NormalTok{]}\OperatorTok{;}
\VariableTok{b} \OperatorTok{=}\NormalTok{ [}\VariableTok{l1}\OperatorTok{;} \VariableTok{l1}\OperatorTok{;} \VariableTok{l2}\OperatorTok{;} \VariableTok{l2}\NormalTok{]}\OperatorTok{;}

\CommentTok{\%\% solve LP}
\VariableTok{cvx\_begin}
    \VariableTok{variable} \VariableTok{x}\NormalTok{(}\FloatTok{2}\NormalTok{)}\OperatorTok{;} \CommentTok{\% define variables [x1, x2]}
    \VariableTok{minimize}\NormalTok{(}\VariableTok{c}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x}\NormalTok{)}\OperatorTok{;} \CommentTok{\% define the objective}
    \VariableTok{subject} \VariableTok{to}
        \VariableTok{A} \OperatorTok{*} \VariableTok{x} \OperatorTok{\textless{}=} \VariableTok{b}\OperatorTok{;} \CommentTok{\% define the linear constraint}
\VariableTok{cvx\_end}
\end{Highlighting}
\end{Shaded}

\subsection{Quadratic Programming (QP)}\label{quadratic-programming-qp}

\textbf{Definition.} A QP has the following form:
\begin{align}
   \label{eq:app-qpdef}
   \min_{x \in \mathbb{R}^n} \ & \frac{1}{2} x^T P x + q^T x \\
   \text{subject to } & Gx \le h \\
   & Ax = b
\end{align}
where \(P \in \mathcal{S}_+^n, q\in \mathbb{R}^n, G \in \mathbb{R}^{m \times n}, h\in \mathbb{R}^m, A \in \mathbb{R}^{p \times n}, b \in \mathbb{R}^p\). Here \(\mathcal{S}_+^n\) denotes the set of positive semidefinite matrices of size \(n\times n\). Obviously, if we set \(P\) as zero, QP will degenerate to LP.

\textbf{Example.} Consider the problem of minimizing a quadratic function
\begin{equation*}
   f(x_1, x_2) = p_1 x_1^2 + 2p_2 x_1 x_2 + p_3 x_2^2 + q_1 x_1 + q_2 x_2
\end{equation*}
over a rectangle \([-l_1, l_1] \times [-l_2, l_2]\). Since \(P = 2 \begin{bmatrix}
   p_1 & p_2 \\ p_2 & p_3
\end{bmatrix} \succeq 0\), the following two conditions must hold:
\begin{equation*}
   \begin{cases}
      p_1 \ge 0 \\
      p_1 p_3 - 4 p_2^2 \ge 0
   \end{cases}
\end{equation*}
Same as in the LP example, \(G\) and \(h\) can be expressed as:
\begin{equation*}
   \begin{bmatrix}
      1 & 0 \\ -1 & 0 \\ 0 & 1 \\ 0 & -1
   \end{bmatrix} \begin{bmatrix}
      x_1 \\ x_2
   \end{bmatrix} \le \begin{bmatrix}
      l_1 \\ l_1 \\ l_2 \\ l_2
   \end{bmatrix}
\end{equation*}

Corresponding CVX codes are shown below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\%\% Define the QP example setting}
\VariableTok{p1} \OperatorTok{=} \FloatTok{2}\OperatorTok{;}
\VariableTok{p2} \OperatorTok{=} \FloatTok{0.5}\OperatorTok{;}
\VariableTok{p3} \OperatorTok{=} \FloatTok{4}\OperatorTok{;}
\VariableTok{q1} \OperatorTok{=} \OperatorTok{{-}}\FloatTok{3}\OperatorTok{;}
\VariableTok{q2} \OperatorTok{=} \OperatorTok{{-}}\FloatTok{6.5}\OperatorTok{;}
\VariableTok{l1} \OperatorTok{=} \FloatTok{2}\OperatorTok{;}
\VariableTok{l2} \OperatorTok{=} \FloatTok{2.5}\OperatorTok{;}
\CommentTok{\% check if the generated P is positive semidefinite}
\VariableTok{tmp1} \OperatorTok{=}\NormalTok{ (}\VariableTok{p1} \OperatorTok{\textgreater{}=} \FloatTok{0}\NormalTok{)}\OperatorTok{;}
\VariableTok{tmp2} \OperatorTok{=}\NormalTok{ (}\VariableTok{p1}\OperatorTok{*}\VariableTok{p3} \OperatorTok{{-}} \FloatTok{4}\OperatorTok{*}\VariableTok{p2}\OperatorTok{\^{}}\FloatTok{2} \OperatorTok{\textgreater{}=} \FloatTok{0}\NormalTok{)}\OperatorTok{;}
\KeywordTok{if} \OperatorTok{\textasciitilde{}}\NormalTok{(}\VariableTok{tmp1} \OperatorTok{\&\&} \VariableTok{tmp2}\NormalTok{)}
    \VariableTok{error}\NormalTok{(}\StringTok{"P is not positve semidefinite!"}\NormalTok{)}\OperatorTok{;}
\KeywordTok{end}
\CommentTok{\% parameters: P, q, G, h}
\VariableTok{P} \OperatorTok{=} \FloatTok{2} \OperatorTok{*}\NormalTok{ [}\VariableTok{p1}\OperatorTok{,} \VariableTok{p2}\OperatorTok{;} \VariableTok{p2}\OperatorTok{,} \VariableTok{p3}\NormalTok{]}\OperatorTok{;}
\VariableTok{q} \OperatorTok{=}\NormalTok{ [}\VariableTok{q1}\OperatorTok{;} \VariableTok{q2}\NormalTok{]}\OperatorTok{;}
\VariableTok{G} \OperatorTok{=}\NormalTok{ [}\FloatTok{1}\OperatorTok{,} \FloatTok{0}\OperatorTok{;} \OperatorTok{{-}}\FloatTok{1}\OperatorTok{,} \FloatTok{0}\OperatorTok{;} \FloatTok{0}\OperatorTok{,} \FloatTok{1}\OperatorTok{;} \FloatTok{0}\OperatorTok{,} \OperatorTok{{-}}\FloatTok{1}\NormalTok{]}\OperatorTok{;}
\VariableTok{h} \OperatorTok{=}\NormalTok{ [}\VariableTok{l1}\OperatorTok{;} \VariableTok{l1}\OperatorTok{;} \VariableTok{l2}\OperatorTok{;} \VariableTok{l2}\NormalTok{]}\OperatorTok{;}

\CommentTok{\%\% Solve the QP problem}
\VariableTok{cvx\_begin}
    \VariableTok{variable} \VariableTok{x}\NormalTok{(}\FloatTok{2}\NormalTok{)}\OperatorTok{;} \CommentTok{\% define variables [x1; x2]}
    \CommentTok{\% define the objective, where quad\_form(x, P) = x\textquotesingle{}*P*x}
    \VariableTok{obj} \OperatorTok{=} \FloatTok{0.5} \OperatorTok{*} \VariableTok{quad\_form}\NormalTok{(}\VariableTok{x}\OperatorTok{,} \VariableTok{P}\NormalTok{) }\OperatorTok{+} \VariableTok{q}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x}\OperatorTok{;} 
    \VariableTok{minimize}\NormalTok{(}\VariableTok{obj}\NormalTok{)}\OperatorTok{;} 
    \VariableTok{subject} \VariableTok{to}
        \VariableTok{G} \OperatorTok{*} \VariableTok{x} \OperatorTok{\textless{}=} \VariableTok{h}\OperatorTok{;} \CommentTok{\% define the linear constraint}
\VariableTok{cvx\_end}
\end{Highlighting}
\end{Shaded}

\subsection{Quadratically Constrained Quadratic Programming (QCQP)}\label{quadratically-constrained-quadratic-programming-qcqp}

\textbf{Definition.} An (convex) QCQP has the following form:
\begin{align}
   \label{eq:app-qcqpdef}
   \min_{x \in \mathbb{R}^n} \ & \frac{1}{2} x^T P_0 x + q_0^T x \\
   \text{subject to } & \frac{1}{2} x^T P_i x + q_i^T x + r_i \le 0, \ i = 1 \dots m \\
   & Ax = b
\end{align}
where \(P_i \in \mathcal{S}_+^n, i = 0 \dots m\), \(q_i \in \mathbb{R}^n, i = 0 \dots m\), \(A \in \mathbb{R}^{p \times n}\), and \(b \in \mathbb{R}^p\). Note that in other literature, you may find a more general form of QCQP: they don't require \(P_i\)'s to be positive semidefinite. Yet in this case, the problem is non-convex and beyond our scope.

\textbf{Example.} We study the problem of getting the minimum distance between two ellipses. By convention, when the ellipses overlap, we set the minimum distance as \(0\). This problem can be exactly solved by (convex) QCQP.
Consider two ellipses of the following form:
\begin{equation*}
   \begin{cases}
      \frac{1}{2} \begin{bmatrix} y_1 \\ z_1 \end{bmatrix}^T K_1 \begin{bmatrix} y_1 \\ z_1 \end{bmatrix}
      + k_1^T \begin{bmatrix} y_1 \\ z_1 \end{bmatrix} + c_1 \le 0 \\ 
      \frac{1}{2} \begin{bmatrix} y_2 \\ z_2 \end{bmatrix}^T K_2 \begin{bmatrix} y_2 \\ z_2 \end{bmatrix}
      + k_2^T \begin{bmatrix} y_2 \\ z_2 \end{bmatrix} + c_2 \le 0 \\ 
   \end{cases}
\end{equation*}
where \([y_1, z_1]^T\) and \([y_2, z_2]^T\) are arbitrary points inside the two ellipses respectively. Also, two ensure the ellipses are well defined, we should enforce the following properties in \((K_i, k_i, c_i), i = 1, 2\): (1) \(K_i \succ 0\); (2) Let \(K_i =  L_i L_i^T\) be the Cholesky decomposition of \(K_i\). Then, ellipse \(i\) can be rewritten as:
\begin{equation*}
   \frac{1}{2} \parallel L_i^T \begin{bmatrix} y_i \\ z_i \end{bmatrix} - L_i^{-1} k_i \parallel^2 \le 
   \frac{1}{2} \parallel L_i^{-1} k_i \parallel^2 - c_i
\end{equation*}
Thus,
\begin{equation*}
   \frac{1}{2} \parallel L_i^{-1} k_i \parallel^2 - c_i > 0
\end{equation*}
With these two assumptions, we want to minimize:
\begin{equation*}
   \frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2
\end{equation*}

Now, we construct \(P, q, r\)'s in QCQP with the above parameters. Define the variable \(x\) as \([y_1, z_1, y_2, z_2]\).

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  \(P_0\) can be obtained from:
  \begin{equation*}
     \frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 = \frac{1}{2} \begin{bmatrix}
    y_1 \\ z_1 \\ y_2 \\ z_2
     \end{bmatrix}^T \begin{bmatrix}
    1 & 0 & -1 & 0 \\ 0 & 1 & 0 & -1 \\ -1 & 0 & 1 & 0 \\ 0 & -1 & 0 & 1
     \end{bmatrix} \begin{bmatrix}
    y_1 \\ z_1 \\ y_2 \\ z_2
     \end{bmatrix}
  \end{equation*}
\item
  \(P_1, q_1, r_1\) can be obtained from:
  \begin{equation*}
     \frac{1}{2} \begin{bmatrix} y_1 \\ z_1 \end{bmatrix}^T K_1 \begin{bmatrix} y_1 \\ z_1 \end{bmatrix}
    + k_1^T \begin{bmatrix} y_1 \\ z_1 \end{bmatrix} + c_1 
     = \frac{1}{2} x^T \begin{bmatrix}
    K_1 & O \\ O & O
     \end{bmatrix} + \begin{bmatrix}
    k_1 \\ O 
     \end{bmatrix}^T x + c_1 \le 0
  \end{equation*}
\item
  \(P_2, q_2, r_2\) can be obtained from:
  \begin{equation*}
     \frac{1}{2} \begin{bmatrix} y_2 \\ z_2 \end{bmatrix}^T K_2 \begin{bmatrix} y_2 \\ z_2 \end{bmatrix}
    + k_2^T \begin{bmatrix} y_2 \\ z_2 \end{bmatrix} + c_2
     = \frac{1}{2} x^T \begin{bmatrix}
    O & O \\ O & K_2
     \end{bmatrix} + \begin{bmatrix}
    O \\ k_2
     \end{bmatrix}^T x + c_2 \le 0
  \end{equation*}
\end{enumerate}

The corresponding codes are shown below. In this example, we test the minimum distance between a circle \(y_1^2 + z_1^2 \le 1\) and another circle \((y_2 - 2)^2 + (z_2 - 2)^2 \le 1\). You can check whether the result from QCQP aligns with your manual calculation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\%\% Define the QCQP example setting}
\VariableTok{K1} \OperatorTok{=} \VariableTok{eye}\NormalTok{(}\FloatTok{2}\NormalTok{)}\OperatorTok{;}
\VariableTok{k1} \OperatorTok{=} \VariableTok{zeros}\NormalTok{(}\FloatTok{2}\OperatorTok{,} \FloatTok{1}\NormalTok{)}\OperatorTok{;}
\VariableTok{c1} \OperatorTok{=} \OperatorTok{{-}}\FloatTok{0.5}\OperatorTok{;}
\VariableTok{K2} \OperatorTok{=} \VariableTok{eye}\NormalTok{(}\FloatTok{2}\NormalTok{)}\OperatorTok{;}
\VariableTok{k2} \OperatorTok{=}\NormalTok{ [}\FloatTok{2}\OperatorTok{;} \FloatTok{2}\NormalTok{]}\OperatorTok{;}
\VariableTok{c2} \OperatorTok{=} \FloatTok{3.5}\OperatorTok{;}
\KeywordTok{if} \OperatorTok{\textasciitilde{}}\NormalTok{(}\VariableTok{if\_ellipse}\NormalTok{(}\VariableTok{K1}\OperatorTok{,} \VariableTok{k1}\OperatorTok{,} \VariableTok{c1}\NormalTok{) }\OperatorTok{\&\&} \VariableTok{if\_ellipse}\NormalTok{(}\VariableTok{K2}\OperatorTok{,} \VariableTok{k2}\OperatorTok{,} \VariableTok{c2}\NormalTok{))}
    \VariableTok{error}\NormalTok{(}\StringTok{"The example setting is not correct"}\NormalTok{)}\OperatorTok{;}
\KeywordTok{end}
\CommentTok{\% define parameters P0, P1, P2, q1, q2, r1, r2}
\VariableTok{P0} \OperatorTok{=}\NormalTok{ [}\FloatTok{1}\OperatorTok{,}\FloatTok{0}\OperatorTok{,{-}}\FloatTok{1}\OperatorTok{,}\FloatTok{0}\OperatorTok{;} \FloatTok{0}\OperatorTok{,}\FloatTok{1}\OperatorTok{,}\FloatTok{0}\OperatorTok{,{-}}\FloatTok{1}\OperatorTok{;} \OperatorTok{{-}}\FloatTok{1}\OperatorTok{,}\FloatTok{0}\OperatorTok{,}\FloatTok{1}\OperatorTok{,}\FloatTok{0}\OperatorTok{;} \FloatTok{0}\OperatorTok{,{-}}\FloatTok{1}\OperatorTok{,}\FloatTok{0}\OperatorTok{,}\FloatTok{1}\NormalTok{]}\OperatorTok{;}
\VariableTok{P1} \OperatorTok{=} \VariableTok{zeros}\NormalTok{(}\FloatTok{4}\OperatorTok{,} \FloatTok{4}\NormalTok{)}\OperatorTok{;}
\VariableTok{P1}\NormalTok{(}\FloatTok{1}\OperatorTok{:}\FloatTok{2}\OperatorTok{,} \FloatTok{1}\OperatorTok{:}\FloatTok{2}\NormalTok{) }\OperatorTok{=} \VariableTok{K1}\OperatorTok{;}
\VariableTok{P2} \OperatorTok{=} \VariableTok{zeros}\NormalTok{(}\FloatTok{4}\OperatorTok{,} \FloatTok{4}\NormalTok{)}\OperatorTok{;}
\VariableTok{P2}\NormalTok{(}\FloatTok{3}\OperatorTok{:}\FloatTok{4}\OperatorTok{,} \FloatTok{3}\OperatorTok{:}\FloatTok{4}\NormalTok{) }\OperatorTok{=} \VariableTok{K2}\OperatorTok{;}
\VariableTok{q1} \OperatorTok{=}\NormalTok{ [}\VariableTok{k1}\OperatorTok{;} \VariableTok{zeros}\NormalTok{(}\FloatTok{2}\OperatorTok{,} \FloatTok{1}\NormalTok{)]}\OperatorTok{;}
\VariableTok{q2} \OperatorTok{=}\NormalTok{ [}\VariableTok{zeros}\NormalTok{(}\FloatTok{2}\OperatorTok{,} \FloatTok{1}\NormalTok{)}\OperatorTok{;} \VariableTok{k2}\NormalTok{]}\OperatorTok{;}
\VariableTok{r1} \OperatorTok{=} \VariableTok{c1}\OperatorTok{;}
\VariableTok{r2} \OperatorTok{=} \VariableTok{c2}\OperatorTok{;}

\CommentTok{\%\% Solve the QCQP problem}
\VariableTok{cvx\_begin}
    \VariableTok{variable} \VariableTok{x}\NormalTok{(}\FloatTok{4}\NormalTok{)}\OperatorTok{;} \CommentTok{\% define variables [y1; z1; y2; z2]}
    \CommentTok{\% define the objective, where quad\_form(x, P) = x\textquotesingle{}*P*x}
    \VariableTok{obj} \OperatorTok{=} \FloatTok{0.5} \OperatorTok{*} \VariableTok{quad\_form}\NormalTok{(}\VariableTok{x}\OperatorTok{,} \VariableTok{P0}\NormalTok{)}\OperatorTok{;} 
    \VariableTok{minimize}\NormalTok{(}\VariableTok{obj}\NormalTok{)}\OperatorTok{;} 
    \VariableTok{subject} \VariableTok{to}
        \FloatTok{0.5} \OperatorTok{*} \VariableTok{quad\_form}\NormalTok{(}\VariableTok{x}\OperatorTok{,} \VariableTok{P1}\NormalTok{) }\OperatorTok{+} \VariableTok{q1}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x} \OperatorTok{+} \VariableTok{r1} \OperatorTok{\textless{}=} \FloatTok{0}\OperatorTok{;}
        \FloatTok{0.5} \OperatorTok{*} \VariableTok{quad\_form}\NormalTok{(}\VariableTok{x}\OperatorTok{,} \VariableTok{P2}\NormalTok{) }\OperatorTok{+} \VariableTok{q2}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x} \OperatorTok{+} \VariableTok{r2} \OperatorTok{\textless{}=} \FloatTok{0}\OperatorTok{;}
\VariableTok{cvx\_end}

\CommentTok{\%\% detect whether (K, k, c) generates a ellipse}
\KeywordTok{function} \VariableTok{flag} \OperatorTok{=} \VariableTok{if\_ellipse}\NormalTok{(}\VariableTok{K}\OperatorTok{,} \VariableTok{k}\OperatorTok{,} \VariableTok{c}\NormalTok{)}
    \VariableTok{L} \OperatorTok{=} \VariableTok{chol}\NormalTok{(}\VariableTok{K}\NormalTok{)}\OperatorTok{;}
    \VariableTok{radius\_square} \OperatorTok{=} \FloatTok{0.5} \OperatorTok{*} \VariableTok{norm}\NormalTok{(}\VariableTok{L} \OperatorTok{\textbackslash{}} \VariableTok{k}\NormalTok{)}\OperatorTok{\^{}}\FloatTok{2} \OperatorTok{{-}} \VariableTok{c}\OperatorTok{;} \CommentTok{\% L \textbackslash{} k = inv(L) * k}
    \VariableTok{flag} \OperatorTok{=}\NormalTok{ (}\VariableTok{radius\_square} \OperatorTok{\textgreater{}} \FloatTok{0}\NormalTok{)}\OperatorTok{;}
\KeywordTok{end}
\end{Highlighting}
\end{Shaded}

\subsection{Second-Order Cone Programming (SOCP)}\label{second-order-cone-programming-socp}

\textbf{Definition.} An SOCP has the following form:
\begin{align}
   \label{eq:app-socpdef}
   \min_{x \in \mathbb{R}^n} \ & f^T x \\
   \text{subject to } & || A_i x + b_i ||_2 \le c_i^T x + d_i, \ i = 1 \dots m \\
   & Fx = g
\end{align}
where \(f \in \mathbb{R}^n, A_i \in \mathbb{R}^{n_i \times n}, b_i \in \mathbb{R}^{n_i}, c_i \in \mathbb{R}^n, d_i \in \mathbb{R}, F \in \mathbb{R}^{p \times n}\), and \(g \in \mathbb{R}^p\).

\textbf{Example.} We consider the problem of stochastic linear programming:
\begin{align}
   \min_x \ & c^T x \\
   \text{subject to } & \mathbb{P}(a_i^T x \le b_i) \ge p, \ i = 1 \dots m \\
   & a_i \sim \mathcal{N}(\bar{a}_i, \Sigma_i), \ i = 1 \dots m
\end{align}
Here \(p\) should be more than \(0.5\). We show that this problem can be converted to a SOCP:

Since \(a_i \sim \mathcal{N}(\bar{a}_i, \Sigma_i)\), then \((a_i^T x - b_i) \sim \mathcal{N}(\bar{a}_i^T x - b_i, x^T \Sigma_i x)\). Standardize it:
\begin{equation*}
   t := ||\Sigma_i^{\frac{1}{2}} x||_2^{-1} \left\{ (a_i^T x - b_i) - (\bar{a}_i^T x - b_i) \right\} \sim \mathcal{N}(0, 1)
\end{equation*}
Then,
\begin{align}
   \mathbb{P}(a_i^T x \le b_i) & = \mathbb{P}(a_i^T x - b_i \le 0) \\
   & = \mathbb{P}(t \le -||\Sigma_i^{\frac{1}{2}} x||_2^{-1}(\bar{a}_i^T x - b_i)) \\
   & = \Phi(-||\Sigma_i^{\frac{1}{2}} x||_2^{-1}(\bar{a}_i^T x - b_i))
\end{align}
Here \(\Phi(\cdot)\) is the cumulative distribution function of the standard normal distribution:
\begin{equation*}
   \Phi(\xi) = \int_{-\infty}^{\xi} e^{-\frac{1}{2} t^2} \ dt
\end{equation*}
Thus,
\begin{align}
   & \mathbb{P}(a_i^T x \le b_i) \ge p \\
   \Longleftrightarrow & \Phi(-||\Sigma_i^{\frac{1}{2}} x||_2^{-1}(\bar{a}_i^T x - b_i)) \ge p \\
   \Longleftrightarrow & -||\Sigma_i^{\frac{1}{2}} x||_2^{-1}(\bar{a}_i^T x - b_i) \ge \Phi^{-1}(p) \\
   \Longleftrightarrow & \Phi^{-1}(p) ||\Sigma_i^{\frac{1}{2}} x||_2 \le b_i - \bar{a}_i^T x
\end{align}
which is exactly the same as inequality constraints in SOCP formulation. (You can see why we enforce \(p > 0.5\) here: otherwise \(\Phi^{-1}(p)\) will be negative and the constraint with not be an second-order cone.)

In the following code example, we set up four inequality constraints and let \(\bar{a}_i^T x \le b_i, \ i = 1 \dots 4\) form an square located at the origin of size \(2\). Then, for convenience, we set \(\Sigma_i \equiv \sigma^2 I\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\%\% Define the SOCP example setting}
\VariableTok{bar\_a1} \OperatorTok{=}\NormalTok{ [}\FloatTok{1}\OperatorTok{;} \FloatTok{0}\NormalTok{]}\OperatorTok{;}
\VariableTok{b1} \OperatorTok{=} \FloatTok{1}\OperatorTok{;}
\VariableTok{bar\_a2} \OperatorTok{=}\NormalTok{ [}\FloatTok{0}\OperatorTok{;} \FloatTok{1}\NormalTok{]}\OperatorTok{;}
\VariableTok{b2} \OperatorTok{=} \FloatTok{1}\OperatorTok{;}
\VariableTok{bar\_a3} \OperatorTok{=}\NormalTok{ [}\OperatorTok{{-}}\FloatTok{1}\OperatorTok{;} \FloatTok{0}\NormalTok{]}\OperatorTok{;}
\VariableTok{b3} \OperatorTok{=} \FloatTok{1}\OperatorTok{;}
\VariableTok{bar\_a4} \OperatorTok{=}\NormalTok{ [}\FloatTok{0}\OperatorTok{;} \OperatorTok{{-}}\FloatTok{1}\NormalTok{]}\OperatorTok{;}
\VariableTok{b4} \OperatorTok{=} \FloatTok{1}\OperatorTok{;}
\VariableTok{sigma} \OperatorTok{=} \FloatTok{0.1}\OperatorTok{;} 
\VariableTok{c} \OperatorTok{=}\NormalTok{ [}\FloatTok{2}\OperatorTok{;} \FloatTok{3}\NormalTok{]}\OperatorTok{;}
\VariableTok{p} \OperatorTok{=} \FloatTok{0.9}\OperatorTok{;} \CommentTok{\% p should be more than 0.5}
\VariableTok{Phi\_inv} \OperatorTok{=} \VariableTok{norminv}\NormalTok{(}\VariableTok{p}\NormalTok{)}\OperatorTok{;} \CommentTok{\% get Phi\^{}\{{-}1\}(p)}

\CommentTok{\%\% Solve the SOCP problem}
\VariableTok{cvx\_begin}
    \VariableTok{variable} \VariableTok{x}\NormalTok{(}\FloatTok{2}\NormalTok{)}\OperatorTok{;} \CommentTok{\% define variables [x1; x2]}
    \VariableTok{minimize}\NormalTok{(}\VariableTok{c}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x}\NormalTok{)}\OperatorTok{;} 
    \VariableTok{subject} \VariableTok{to}
        \VariableTok{sigma}\OperatorTok{*}\VariableTok{Phi\_inv} \OperatorTok{*} \VariableTok{norm}\NormalTok{(}\VariableTok{x}\NormalTok{) }\OperatorTok{\textless{}=} \VariableTok{b1} \OperatorTok{{-}} \VariableTok{bar\_a1}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x}\OperatorTok{;}
        \VariableTok{sigma}\OperatorTok{*}\VariableTok{Phi\_inv} \OperatorTok{*} \VariableTok{norm}\NormalTok{(}\VariableTok{x}\NormalTok{) }\OperatorTok{\textless{}=} \VariableTok{b2} \OperatorTok{{-}} \VariableTok{bar\_a2}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x}\OperatorTok{;}
        \VariableTok{sigma}\OperatorTok{*}\VariableTok{Phi\_inv} \OperatorTok{*} \VariableTok{norm}\NormalTok{(}\VariableTok{x}\NormalTok{) }\OperatorTok{\textless{}=} \VariableTok{b3} \OperatorTok{{-}} \VariableTok{bar\_a3}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x}\OperatorTok{;}
        \VariableTok{sigma}\OperatorTok{*}\VariableTok{Phi\_inv} \OperatorTok{*} \VariableTok{norm}\NormalTok{(}\VariableTok{x}\NormalTok{) }\OperatorTok{\textless{}=} \VariableTok{b4} \OperatorTok{{-}} \VariableTok{bar\_a4}\OperatorTok{\textquotesingle{}} \OperatorTok{*} \VariableTok{x}\OperatorTok{;}
\VariableTok{cvx\_end}
\end{Highlighting}
\end{Shaded}

\subsection{Semidefinite Programming (SDP)}\label{semidefinite-programming-sdp}

\textbf{Definition.} An SDP has the following form:
\begin{align}
   \label{eq:app-sdpdef}
   \min_{X_i, x_i} \ & \sum_{i=1}^{n_s} C_i \cdot X_i + \sum_{i=1}^{n_u} c_i \cdot x_i \\
   \text{subject to } & \sum_{i=1}^{n_s} A_{i,j} \cdot X_i + \sum_{i=1}^{n_u} a_{i,j} \cdot x_i = b_j, \quad j = 1 \dots m \\
   & X_i \in \mathcal{S}_+^{D_i}, \quad i = 1 \dots n_s \\
   & x_i \in \mathbb{R}^{d_i}, \quad i = 1 \dots n_u  
\end{align}
where \(C_i, A_{i, j} \in \mathbb{R}^{D_i \times D_i}\), \(c_i, a_{i, j} \in \mathbb{R}^{d_i}\), and \(\cdot\) means element-wise product. For two square matrices \(A, B\), the dot product \(A \cdot B\) is equal to \(\text{tr}(A B)\); for two vectors \(a, b\), the dot product \(a \cdot b\) is the same as inner product \(a^T b\).

Note that actually there are many ``standard'' forms of SDP. For example, in the convex optimization theory part, you may find an SDP that looks like:
\begin{align}
   \min_X \ & C \cdot X \\
   \text{subject to } & A \cdot X = b \\
   & X \succeq 0
\end{align}
It is convenient for us to analyze the theoretical properties of SDP with this form. Also, in SDP solvers' User Guide, you may see more complex SDP forms which involve more general convex cones. For example, see \href{https://docs.mosek.com/10.0/toolbox/tutorial-sdo-shared.html\#doc-tutorial-sdo}{MOSEK's MATLAB API docs}. Here we turn to use the form of \eqref{eq:app-sdpdef} for two reasons: (1) it is general enough: our SDP example below can be converted to this form (also, SDPs from sum-of-squares programming in this book are exactly of the form \eqref{eq:app-sdpdef}); (2) it is more readable than more complex forms.

\textbf{Example.} We consider the problem of finding the minimum eigenvalue for a positive semidefinite matrix \(S\). We will show that this problem can be converted to \eqref{eq:app-sdpdef}. Since \(S\) is positive semidefinite, the finding procedure can be cast as
\begin{align}
    \max_\lambda & \ \lambda \\
    \text{subject to } & S - \lambda I \succeq 0
\end{align}
Now define an auxiliary matrix \(X := S - \lambda I\). We have
\begin{align}
    \min_{\lambda, X} & \ -\lambda \\
   \text{subject to } &  X + \lambda I = S \\
   & X \succeq 0
\end{align}
It is obvious that the linear matrix equality constraint \(X + \lambda I = S\) can be divided into several linear scalar equality constraints in \eqref{eq:app-sdpdef}. For example, we consider \(S \in \mathbb{S}_+^3\). Thereby \(X + \lambda I = S\) will lead to \(6\) linear equality constraints (We don't consider \(X\) is a symmetric matrix here, since most solvers will implicitly consider this. Thus, only the upper-triangular part of \(X\) and \(S\) are actually used in the equality construction.):
\begin{align}
   &  \begin{bmatrix}
      1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0
   \end{bmatrix} \cdot X  + \lambda = S[0, 0],  
    \begin{bmatrix}
      0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0
   \end{bmatrix} \cdot X  = S[0, 1],
    \begin{bmatrix}
      0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0
   \end{bmatrix} \cdot X  = S[0, 2] \\
   &  \begin{bmatrix}
      0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0
   \end{bmatrix} \cdot X  + \lambda = S[1, 1],
    \begin{bmatrix}
      0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0
   \end{bmatrix} \cdot X  = S[1, 2], 
    \begin{bmatrix}
      0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1
   \end{bmatrix} \cdot X  + \lambda = S[2, 2] 
\end{align}
Seems tedious? Fortunately, CVX provides a high-level API to handle these linear equality constraints: you just need to write down

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{X} \OperatorTok{+} \VariableTok{lam} \OperatorTok{*} \VariableTok{eye}\NormalTok{(}\FloatTok{3}\NormalTok{) }\OperatorTok{==} \VariableTok{S}\OperatorTok{;} \CommentTok{\% linear equality constraints: X + lam *I = S}
\end{Highlighting}
\end{Shaded}

CVX will autometically convert this high-level constraint to \eqref{eq:app-sdpdef} and pass them to the underlying solver.

To generate a ramdom \(S \in \mathcal{S}_+^3\), you just need to assign three nonnegative eigenvalues to the program. After that, an random \(S\) will be generated by \(S = Q \ \text{diag}(\lambda_1, \lambda_2, \lambda_3) \ Q^T\), where \(Q\) is random orthonormal matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\%\% Define the SDP example setting}
\VariableTok{lam\_list} \OperatorTok{=}\NormalTok{ [}\FloatTok{0.7}\OperatorTok{;} \FloatTok{2.4}\OperatorTok{;} \FloatTok{3.7}\NormalTok{]}\OperatorTok{;}
\VariableTok{S} \OperatorTok{=} \VariableTok{generate\_random\_PD\_matrix}\NormalTok{(}\VariableTok{lam\_list}\NormalTok{)}\OperatorTok{;} \CommentTok{\% get a PD matrix S}

\CommentTok{\%\% Solve the SDP problem}
\VariableTok{cvx\_begin}
    \VariableTok{variable} \VariableTok{X}\NormalTok{(}\FloatTok{3}\OperatorTok{,} \FloatTok{3}\NormalTok{) }\VariableTok{symmetric}\OperatorTok{;}
    \VariableTok{variable} \VariableTok{lam}\OperatorTok{;} 
    \VariableTok{maximize}\NormalTok{(}\VariableTok{lam}\NormalTok{)}\OperatorTok{;} 
    \VariableTok{subject} \VariableTok{to}
        \CommentTok{\% here "==" should be read as "is in"}
        \VariableTok{X} \OperatorTok{==} \VariableTok{semidefinite}\NormalTok{(}\FloatTok{3}\NormalTok{)}\OperatorTok{;} 
        \VariableTok{X} \OperatorTok{+} \VariableTok{lam} \OperatorTok{*} \VariableTok{eye}\NormalTok{(}\FloatTok{3}\NormalTok{) }\OperatorTok{==} \VariableTok{S}\OperatorTok{;}
\VariableTok{cvx\_end}

\CommentTok{\% this function help to generate PD matrix of size 3*3 }
\CommentTok{\% if you provide the eigenvalues [lam\_1, lam\_2, lam\_3]}
\KeywordTok{function} \VariableTok{S} \OperatorTok{=} \VariableTok{generate\_random\_PD\_matrix}\NormalTok{(}\VariableTok{lam\_list}\NormalTok{)}
    \KeywordTok{if} \OperatorTok{\textasciitilde{}}\VariableTok{all}\NormalTok{(}\VariableTok{lam\_list} \OperatorTok{\textgreater{}=} \FloatTok{0}\NormalTok{) }\CommentTok{\% all eigenvalues \textgreater{}= 0}
        \VariableTok{error}\NormalTok{(}\StringTok{"All eigenvalues must be nonnegative."}\NormalTok{)}\OperatorTok{;}
    \KeywordTok{end}
    \VariableTok{D} \OperatorTok{=} \VariableTok{diag}\NormalTok{(}\VariableTok{lam\_list}\NormalTok{)}\OperatorTok{;}
    \CommentTok{\% use QR factorization to generate a random orthonormal matrix Q}
\NormalTok{    [}\VariableTok{Q}\OperatorTok{,} \OperatorTok{\textasciitilde{}}\NormalTok{] }\OperatorTok{=} \VariableTok{qr}\NormalTok{(}\VariableTok{rand}\NormalTok{(}\FloatTok{3}\OperatorTok{,} \FloatTok{3}\NormalTok{))}\OperatorTok{;}
    \VariableTok{S} \OperatorTok{=}  \VariableTok{Q} \OperatorTok{*} \VariableTok{D} \OperatorTok{*} \VariableTok{Q}\OperatorTok{\textquotesingle{};}
\KeywordTok{end}
\end{Highlighting}
\end{Shaded}

\subsection{CVXPY Introduction and Examples}\label{cvxpy-introduction-and-examples}

Apart from CVX MATLAB, we also have a Python package called CVXPY, which functions almost the same as CVX MATLAB. To define and solve a convex optimization problem CVXPY, basically, there are three steps (apart from importing necessary packages):

\begin{itemize}
\item
  Step 1: Define parameters and variables in a certain type of convex problem. Here variables are what you are trying to optimize or ``learn''. Parameters are the ``coefficients'' of variables in the objective and constraints.
\item
  Step 2: Define the objective function and constraints.
\item
  Step 3: Solve the problem and get the results.
\end{itemize}

Here we provide the CVXPY codes for the above five convex optimization examples.

\subsubsection{LP}\label{lp}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ cvxpy }\ImportTok{as}\NormalTok{ cp}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\#\# Define the LP example setting}
\NormalTok{c1 }\OperatorTok{=} \DecValTok{2}
\NormalTok{c2 }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{5}
\NormalTok{l1 }\OperatorTok{=} \DecValTok{3}
\NormalTok{l2 }\OperatorTok{=} \DecValTok{7}

\CommentTok{\#\# Step 1: define variables and parameters}
\NormalTok{x }\OperatorTok{=}\NormalTok{ cp.Variable(}\DecValTok{2}\NormalTok{) }\CommentTok{\# variable: x = [x1, x2]\^{}T}
\CommentTok{\# parameters: c, A, b}
\NormalTok{c }\OperatorTok{=}\NormalTok{ np.array([c1, c2]) }
\NormalTok{A }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]])}
\NormalTok{b }\OperatorTok{=}\NormalTok{ np.array([l1, l1, l2, l2])}

\CommentTok{\#\# Step 2: define objective and constraints}
\NormalTok{obj }\OperatorTok{=}\NormalTok{ cp.Minimize(c.T }\OperatorTok{@}\NormalTok{ x)}
\NormalTok{constraints }\OperatorTok{=}\NormalTok{ [A }\OperatorTok{@}\NormalTok{ x }\OperatorTok{\textless{}=}\NormalTok{ b]}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ cp.Problem(obj, constraints) }\CommentTok{\# form the problem}

\CommentTok{\#\# Step 3: solve problem and get results}
\NormalTok{prob.solve()  }
\BuiltInTok{print}\NormalTok{(}\StringTok{"status: "}\NormalTok{, prob.status) }\CommentTok{\# check whether the status is "optimal"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal value: "}\NormalTok{, prob.value) }\CommentTok{\# optimal objective}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal solution: "}\NormalTok{, x.value) }\CommentTok{\# optimal x}
\end{Highlighting}
\end{Shaded}

\subsubsection{QP}\label{qp}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ cvxpy }\ImportTok{as}\NormalTok{ cp}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\CommentTok{\#\# Define the LP example setting}
\NormalTok{p1 }\OperatorTok{=} \DecValTok{2}
\NormalTok{p2 }\OperatorTok{=} \FloatTok{0.5}
\NormalTok{p3 }\OperatorTok{=} \DecValTok{4}
\NormalTok{q1 }\OperatorTok{=} \OperatorTok{{-}}\DecValTok{3}
\NormalTok{q2 }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{6.5}
\NormalTok{l1 }\OperatorTok{=} \DecValTok{2}
\NormalTok{l2 }\OperatorTok{=} \FloatTok{2.5}
\CommentTok{\# check if the generated P is positive semidefinite}
\NormalTok{tmp1 }\OperatorTok{=}\NormalTok{ (p1 }\OperatorTok{\textgreater{}=} \DecValTok{0}\NormalTok{)}
\NormalTok{tmp2 }\OperatorTok{=}\NormalTok{ (p1}\OperatorTok{*}\NormalTok{p3 }\OperatorTok{{-}} \DecValTok{4}\OperatorTok{*}\NormalTok{p2}\OperatorTok{**}\DecValTok{2} \OperatorTok{\textgreater{}=} \DecValTok{0}\NormalTok{)}
\ControlFlowTok{assert}\NormalTok{(tmp1 }\KeywordTok{and}\NormalTok{ tmp2, }\StringTok{"P is not positve semidefinite!"}\NormalTok{)}

\CommentTok{\#\# Step 1: define variables and parameters}
\NormalTok{x }\OperatorTok{=}\NormalTok{ cp.Variable(}\DecValTok{2}\NormalTok{) }\CommentTok{\# variable: x = [x1, x2]\^{}T}
\CommentTok{\# parameters: P, q, G, h}
\NormalTok{P }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\NormalTok{np.array([[p1, p2], [p2, p3]])}
\NormalTok{q }\OperatorTok{=}\NormalTok{ np.array([q1, q2]) }
\NormalTok{G }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{]])}
\NormalTok{h }\OperatorTok{=}\NormalTok{ np.array([l1, l1, l2, l2])}

\CommentTok{\#\# Step 2: define the objective and constraints}
\NormalTok{fx }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ cp.quad\_form(x, P) }\OperatorTok{+}\NormalTok{ q.T }\OperatorTok{@}\NormalTok{ x}
\NormalTok{obj }\OperatorTok{=}\NormalTok{ cp.Minimize(fx)}
\NormalTok{constraints }\OperatorTok{=}\NormalTok{ [G }\OperatorTok{@}\NormalTok{ x }\OperatorTok{\textless{}=}\NormalTok{ h]}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ cp.Problem(obj, constraints) }\CommentTok{\# form the problem}

\CommentTok{\#\# Step 3: solve the problem and get results}
\NormalTok{prob.solve()  }
\BuiltInTok{print}\NormalTok{(}\StringTok{"status: "}\NormalTok{, prob.status) }\CommentTok{\# check whether the status is "optimal"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal value: "}\NormalTok{, prob.value) }\CommentTok{\# optimal objective}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal solution: "}\NormalTok{, x.value) }\CommentTok{\# optimal x}
\end{Highlighting}
\end{Shaded}

\subsubsection{QCQP}\label{qcqp}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ cvxpy }\ImportTok{as}\NormalTok{ cp}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy.linalg }\ImportTok{import}\NormalTok{ cholesky, inv, norm}

\CommentTok{\#\# Define the QCQP example setting}
\KeywordTok{def}\NormalTok{ if\_ellipse(K, k, c):}
    \CommentTok{\# examine whether 0.5*x\^{}T K x + k\^{}T x + c \textless{}= 0 is a ellipse}
    \CommentTok{\# if K is not positive semidefinite, Cholesky will raise an error}
\NormalTok{    L }\OperatorTok{=}\NormalTok{ cholesky(K) }
\NormalTok{    radius\_square }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ norm(inv(L) }\OperatorTok{@}\NormalTok{ k)}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}}\NormalTok{ c}
    \ControlFlowTok{return}\NormalTok{ radius\_square }\OperatorTok{\textgreater{}} \DecValTok{0}
\NormalTok{K1 }\OperatorTok{=}\NormalTok{ np.eye(}\DecValTok{2}\NormalTok{)}
\NormalTok{k1 }\OperatorTok{=}\NormalTok{ np.zeros(}\DecValTok{2}\NormalTok{)}
\NormalTok{c1 }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{0.5}
\NormalTok{K2 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]])}
\NormalTok{k2 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{])}
\NormalTok{c2 }\OperatorTok{=} \FloatTok{3.5}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ (if\_ellipse(K1, k1, c1) }\KeywordTok{and}\NormalTok{ if\_ellipse(K2, k2, c2)):}
    \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\StringTok{"The example setting is not correct"}\NormalTok{)}

\CommentTok{\#\# Step 1: define variables and parameters}
\NormalTok{P0 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], [}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]])}
\NormalTok{P1 }\OperatorTok{=}\NormalTok{ np.zeros((}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\NormalTok{P1[:}\DecValTok{2}\NormalTok{, :}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ K1}
\NormalTok{P2 }\OperatorTok{=}\NormalTok{ np.zeros((}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\NormalTok{P2[}\DecValTok{2}\NormalTok{:, }\DecValTok{2}\NormalTok{:] }\OperatorTok{=}\NormalTok{ K2}
\NormalTok{q1 }\OperatorTok{=}\NormalTok{ np.concatenate([k1, np.zeros(}\DecValTok{2}\NormalTok{)])}
\NormalTok{q2 }\OperatorTok{=}\NormalTok{ np.concatenate([np.zeros(}\DecValTok{2}\NormalTok{), k2])}
\NormalTok{r1 }\OperatorTok{=}\NormalTok{ c1}
\NormalTok{r2 }\OperatorTok{=}\NormalTok{ c2}

\CommentTok{\#\# Step 2: define objective and constraints}
\NormalTok{x }\OperatorTok{=}\NormalTok{ cp.Variable(}\DecValTok{4}\NormalTok{) }\CommentTok{\# variable: x = [y1, z1, y2, z2]\^{}T}
\NormalTok{fx }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ cp.quad\_form(x, P0)}
\NormalTok{obj }\OperatorTok{=}\NormalTok{ cp.Minimize(fx)}
\NormalTok{con1 }\OperatorTok{=}\NormalTok{ (}\FloatTok{0.5} \OperatorTok{*}\NormalTok{ cp.quad\_form(x, P1) }\OperatorTok{+}\NormalTok{ q1.T }\OperatorTok{@}\NormalTok{ x }\OperatorTok{+}\NormalTok{ r1 }\OperatorTok{\textless{}=} \DecValTok{0}\NormalTok{) }\CommentTok{\# ellipse 1}
\NormalTok{con2 }\OperatorTok{=}\NormalTok{ (}\FloatTok{0.5} \OperatorTok{*}\NormalTok{ cp.quad\_form(x, P2) }\OperatorTok{+}\NormalTok{ q2.T }\OperatorTok{@}\NormalTok{ x }\OperatorTok{+}\NormalTok{ r2 }\OperatorTok{\textless{}=} \DecValTok{0}\NormalTok{) }\CommentTok{\# ellipse 2}
\NormalTok{constraints }\OperatorTok{=}\NormalTok{ [con1, con2]}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ cp.Problem(obj, constraints) }\CommentTok{\# form the problem}

\CommentTok{\#\# Step 3: solve problem and get results}
\NormalTok{prob.solve()  }
\BuiltInTok{print}\NormalTok{(}\StringTok{"status: "}\NormalTok{, prob.status) }\CommentTok{\# check whether the status is "optimal"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal value: "}\NormalTok{, prob.value) }\CommentTok{\# optimal objective}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal solution: "}\NormalTok{, x.value) }\CommentTok{\# optimal x}
\end{Highlighting}
\end{Shaded}

\subsubsection{SOCP}\label{socp}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ cvxpy }\ImportTok{as}\NormalTok{ cp}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}

\CommentTok{\#\# Define the SOCP example setting}
\CommentTok{\# define bar\_ai, bi (i = 1, 2, 3, 4)}
\NormalTok{bar\_a1 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{b1 }\OperatorTok{=} \DecValTok{1}
\NormalTok{bar\_a2 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\NormalTok{b2 }\OperatorTok{=} \DecValTok{1}
\NormalTok{bar\_a3 }\OperatorTok{=}\NormalTok{ np.array([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{])}
\NormalTok{b3 }\OperatorTok{=} \DecValTok{1}
\NormalTok{bar\_a4 }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{0}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{b4 }\OperatorTok{=} \DecValTok{1}
\NormalTok{sigma }\OperatorTok{=} \FloatTok{0.1} 
\NormalTok{c }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{])}
\NormalTok{p }\OperatorTok{=} \FloatTok{0.9} \CommentTok{\# p should be more than 0.5}

\CommentTok{\#\# Step 1: define variables and parameters}
\NormalTok{Phi\_inv }\OperatorTok{=}\NormalTok{ norm.ppf(p) }\CommentTok{\# get Phi\^{}\{{-}1\}(p)}

\CommentTok{\#\# Step 2: define objective and constraints}
\NormalTok{x }\OperatorTok{=}\NormalTok{ cp.Variable(}\DecValTok{2}\NormalTok{) }\CommentTok{\# variable: x = [x1, x2]\^{}T}
\NormalTok{obj }\OperatorTok{=}\NormalTok{ cp.Minimize(c.T }\OperatorTok{@}\NormalTok{ x)}
\CommentTok{\# use cp.SOC(t, x) to create the SOC constraint ||x||\_2 \textless{}= t}
\NormalTok{constraints }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    cp.SOC(b1 }\OperatorTok{{-}}\NormalTok{ bar\_a1.T }\OperatorTok{@}\NormalTok{ x, sigma}\OperatorTok{*}\NormalTok{Phi\_inv}\OperatorTok{*}\NormalTok{x),}
\NormalTok{    cp.SOC(b2 }\OperatorTok{{-}}\NormalTok{ bar\_a2.T }\OperatorTok{@}\NormalTok{ x, sigma}\OperatorTok{*}\NormalTok{Phi\_inv}\OperatorTok{*}\NormalTok{x),}
\NormalTok{    cp.SOC(b3 }\OperatorTok{{-}}\NormalTok{ bar\_a3.T }\OperatorTok{@}\NormalTok{ x, sigma}\OperatorTok{*}\NormalTok{Phi\_inv}\OperatorTok{*}\NormalTok{x),}
\NormalTok{    cp.SOC(b4 }\OperatorTok{{-}}\NormalTok{ bar\_a4.T }\OperatorTok{@}\NormalTok{ x, sigma}\OperatorTok{*}\NormalTok{Phi\_inv}\OperatorTok{*}\NormalTok{x),}
\NormalTok{]}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ cp.Problem(obj, constraints) }\CommentTok{\# form the problem}

\CommentTok{\#\# Step 3: solve problem and get results}
\NormalTok{prob.solve()  }
\BuiltInTok{print}\NormalTok{(}\StringTok{"status: "}\NormalTok{, prob.status) }\CommentTok{\# check whether the status is "optimal"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal value: "}\NormalTok{, prob.value) }\CommentTok{\# optimal objective}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal solution: "}\NormalTok{, x.value) }\CommentTok{\# optimal x}
\end{Highlighting}
\end{Shaded}

\subsubsection{SDP}\label{sdp}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ cvxpy }\ImportTok{as}\NormalTok{ cp}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ ortho\_group}

\CommentTok{\#\# Define the SDP example setting}
\CommentTok{\# this function help to generate PD matrix of size 3*3 }
\CommentTok{\# if you provide the eigenvalues [lam\_1, lam\_2, lam\_3]}
\KeywordTok{def}\NormalTok{ generate\_random\_PD\_matrix(lam\_list):}
    \ControlFlowTok{assert}\NormalTok{ np.}\BuiltInTok{all}\NormalTok{(lam\_list }\OperatorTok{\textgreater{}=} \DecValTok{0}\NormalTok{) }\CommentTok{\# all eigenvalues \textgreater{}= 0}
    \CommentTok{\# S = Q @ D @ Q.T}
\NormalTok{    D }\OperatorTok{=}\NormalTok{ np.diag(lam\_list)}
\NormalTok{    Q }\OperatorTok{=}\NormalTok{ ortho\_group.rvs(}\DecValTok{3}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ Q }\OperatorTok{@}\NormalTok{ D }\OperatorTok{@}\NormalTok{ Q.T}
\NormalTok{lam\_list }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{0.5}\NormalTok{, }\FloatTok{2.4}\NormalTok{, }\FloatTok{3.7}\NormalTok{])}
\NormalTok{S }\OperatorTok{=}\NormalTok{ generate\_random\_PD\_matrix(lam\_list) }\CommentTok{\# get a PD matrix S}

\CommentTok{\#\# Step 1: define variables and parameters}
\CommentTok{\# get coefficients for equality constraints}
\NormalTok{A\_00 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]]) }\CommentTok{\# tr(A\_00 @ X) + lam = S\_00}
\NormalTok{A\_01 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]]) }\CommentTok{\# tr(A\_01 @ X) = S\_01}
\NormalTok{A\_02 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]]) }\CommentTok{\# tr(A\_02 @ X) = S\_02}
\NormalTok{A\_11 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]]) }\CommentTok{\# tr(A\_11 @ X) + lam = S\_11}
\NormalTok{A\_12 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]]) }\CommentTok{\# tr(A\_12 @ X) = S\_12}
\NormalTok{A\_22 }\OperatorTok{=}\NormalTok{ np.array([[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]]) }\CommentTok{\# tr(A\_22 @ X) + lam = S\_22}

\CommentTok{\#\# Step 2: define objective and constraints}
\CommentTok{\# define a PD matrix variable X of size 3*3}
\NormalTok{X }\OperatorTok{=}\NormalTok{ cp.Variable((}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), symmetric}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{constraints }\OperatorTok{=}\NormalTok{ [X }\OperatorTok{\textgreater{}\textgreater{}} \DecValTok{0}\NormalTok{] }\CommentTok{\# the operator \textgreater{}\textgreater{} denotes matrix inequality}
\NormalTok{lam }\OperatorTok{=}\NormalTok{ cp.Variable(}\DecValTok{1}\NormalTok{)}
\NormalTok{constraints }\OperatorTok{+=}\NormalTok{ [}
\NormalTok{    cp.trace(A\_00 }\OperatorTok{@}\NormalTok{ X) }\OperatorTok{+}\NormalTok{ lam }\OperatorTok{==}\NormalTok{ S[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{    cp.trace(A\_01 }\OperatorTok{@}\NormalTok{ X) }\OperatorTok{==}\NormalTok{ S[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{    cp.trace(A\_02 }\OperatorTok{@}\NormalTok{ X) }\OperatorTok{==}\NormalTok{ S[}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{],}
\NormalTok{    cp.trace(A\_11 }\OperatorTok{@}\NormalTok{ X) }\OperatorTok{+}\NormalTok{ lam }\OperatorTok{==}\NormalTok{ S[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{    cp.trace(A\_12 }\OperatorTok{@}\NormalTok{ X) }\OperatorTok{==}\NormalTok{ S[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{],}
\NormalTok{    cp.trace(A\_22 }\OperatorTok{@}\NormalTok{ X) }\OperatorTok{+}\NormalTok{ lam }\OperatorTok{==}\NormalTok{ S[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{],}
\NormalTok{]}
\NormalTok{obj }\OperatorTok{=}\NormalTok{ cp.Minimize(}\OperatorTok{{-}}\NormalTok{lam)}
\NormalTok{prob }\OperatorTok{=}\NormalTok{ cp.Problem(obj, constraints) }\CommentTok{\# form the problem}

\CommentTok{\#\# Step 3: solve problem and get results}
\NormalTok{prob.solve()  }
\BuiltInTok{print}\NormalTok{(}\StringTok{"status: "}\NormalTok{, prob.status) }\CommentTok{\# check whether the status is "optimal"}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal value: "}\NormalTok{, prob.value) }\CommentTok{\# optimal objective}
\BuiltInTok{print}\NormalTok{(}\StringTok{"optimal solution: "}\NormalTok{, lam.value) }\CommentTok{\# optimal lam}
\end{Highlighting}
\end{Shaded}

\chapter{Linear System Theory}\label{app-lti-system-theory}

\emph{Thanks to Shucheng Kang for writing this Appendix.}

\section{Stability}\label{app-lti-stability}

\subsection{Continuous-Time Stability}\label{app-lti-stability-ct}

Consider the continuous-time linear time-invariant (LTI) system
\begin{equation}
\dot{x} = A x.
\label{eq:app-stability-ct-linear-system}
\end{equation}
the system is said to be ``diagonalizable'' if \(A\) is diagonalizable.

\begin{definition}[Asymptotic and Marginal Stability]
\protect\hypertarget{def:ltistable}{}\label{def:ltistable}

The diagonalizable, LTI system \eqref{eq:app-stability-ct-linear-system} is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  ``asymptotically stable'' if \(x(t) \rightarrow 0\) as \(t \rightarrow \infty\) for every initial condition \(x_0\)
\item
  ``marginally stable'' if \(x(t) \nrightarrow 0\) but remains bounded as \(t \rightarrow \infty\) for every initial condition \(x_0\)
\item
  ``stable'' if it is either asymptotically or marginally stable
\item
  ``unstable'' if it is not stable
\end{enumerate}

\end{definition}

One can show that \(A\)'s eigenvalues determine the LTI system's stability, as the following Theorem states:

\begin{theorem}[Stability of Continuous-Time LTI System]
\protect\hypertarget{thm:ltistable}{}\label{thm:ltistable}

The diagonalizable\footnote{when \(A\) is not diagonalizable, similar results can be derived via Jordan decomposition.}, LTI system \eqref{eq:app-stability-ct-linear-system} is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  asymptotically stable if \(\text{Re} (\lambda_i) < 0\) for all \(i\)
\item
  marginally stable if \(\text{Re} (\lambda_i) \le 0\) for all \(i\) and there exists at least one \(i\) for which \(\text{Re} (\lambda_i) = 0\)
\item
  stable if \(\text{Re} (\lambda_i) \le 0\) for all \(i\)
\item
  unstable if \(\text{Re} (\lambda_i) > 0\) for at least one \(i\)
\end{enumerate}

\end{theorem}

\begin{proof}

Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4).

Since \(A\) is diagonalizable, there exists an similarity transformation matrix \(T\), s.t. \(A = T \Lambda T^{-1}\), where \(\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)\). Then, under the coordinate transformation \(z = T^{-1} x\), \(\dot{x} = Ax\) can be restated as \(\dot{z} = \Lambda z\). Consider the \(i\)'s component of \(z\):
\begin{equation*}
   \dot{z}_i = \lambda_i z_i \Longrightarrow z_i(t) = e^{\lambda_i t} z_i(0)
\end{equation*}
Since \(\text{Re}(\lambda_i) < 0\), \(z_i(t)\) will go to \(0\) as \(t \rightarrow 0\) regardless how we choose \(z_i(0)\).

\end{proof}

\subsection{Discrete-Time Stability}\label{app-lti-stability-dt}

Now consider the diagonalizable, discrete-time linear time-invariant (LTI) system
\begin{equation}
x_{t+1} = A x_t.
\label{eq:app-stability-dt-linear-system}
\end{equation}

\begin{theorem}[Stability of Discrete-Time LTI System]
\protect\hypertarget{thm:dtltisystemstability}{}\label{thm:dtltisystemstability}The diagonalizable, discrete-time LTI system \eqref{eq:app-stability-dt-linear-system} is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  asymptotically stable if \(|\lambda_i| < 1\) for all \(i\)
\item
  marginally stable if \(|\lambda_i| \leq 1\) for all \(i\) and there exists at least one \(i\) for which \(|\lambda_i| = 1\)
\item
  stable if \(|\lambda_i| \leq 1\) for all \(i\)
\item
  unstable if \(|\lambda_i| > 1\) for at least one \(i\).
\end{enumerate}

Note that \(|\lambda_i| < 1\) means the eigenvalue lies strictly inside the unit circle in the complex plane.
\end{theorem}

\begin{proof}
Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4).

Since \(A\) is diagonalizable, there exists an similarity transformation matrix \(T\), s.t. \(A = T \Lambda T^{-1}\), where \(\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)\). Then, under the coordinate transformation \(z = T^{-1} x\), \(x_{t+1} = Ax\) can be restated as \(z_{t+1} = \Lambda z_t\). Expanding the recursion, we have
\begin{equation*}
   z_{t} = \Lambda^{t-1} z_0 \Longrightarrow
   z_{t,i} = \lambda_i^{t-1} z_{0,i}
\end{equation*}
Since \(|\lambda_i| < 1\), \(z_{t,i}\) will go to \(0\) as \(t \rightarrow 0\) regardless how we choose \(z_{0,i}\).
\end{proof}

\subsection{Lyapunov Analysis}\label{lyapunov-analysis}

\begin{theorem}[Lyapunov Equation]
\protect\hypertarget{thm:lyapunovequation}{}\label{thm:lyapunovequation}

The following is equivalent for a linear time-invariant system \(\dot{x} = A x\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The system is globally asymptotically stable, i.e., \(A\) is Hurwitz and \(\lim_{t \rightarrow \infty} x(t) = 0\) regardless of the initial condition;
\item
  For any positive definite matrix \(Q\), the unique solution \(P\) to the Lyapunov equation
  \begin{equation}
  A^T P + P A = -Q
  \label{eq:lyapunov-equation}
  \end{equation}
  is positive definite.
\end{enumerate}

\end{theorem}

\begin{proof}
(a): \(2 \Rightarrow 1\). Suppose we are given two positive definite matrices \(P, Q \succ 0\) that satisfies the Lyapunov equation \eqref{eq:lyapunov-equation}. Define a scalar function
\[
V(x) = x^T P x.
\]
It is clear that \(V > 0\) for any \(x \neq 0\) and \(V(x) = 0\) (i.e., \(V(x)\) is positive definite). We also see \(V(x)\) is radially unbounded because:
\[
V(x) \geq \lambda_{\min}(P) \Vert x \Vert^2 \Rightarrow \lim_{x \rightarrow \infty} V(x) \rightarrow \infty.
\]
The time derivative of \(V\) reads
\[
\dot{V} = 2 x^T P \dot{x} = x^T (A^T P + P A) x = - x^T Q x. 
\]
Clearly, \(\dot{V} < 0\) for any \(x \neq 0\) and \(\dot{V}(0) = 0\). According to Lyapunov's global stability theorem \ref{thm:lyapunovglobalstability}, we conclude the linear system \(\dot{x} = Ax\) is globally asymptotically stable at \(x = 0\).

(b): \(1 \Rightarrow 2\). Suppose \(A\) is Hurwitz, we want to show that, for any \(Q \succ 0\), there exists a unique \(P \succ 0\) satisfying the Lyapunov equation \eqref{eq:lyapunov-equation}. In fact, consider the matrix
\[
P = \int_{t=0}^{\infty} e^{A^T t} Q e^{At} dt.
\]
Because \(A\) is Hurwitz, the integral exists, and clearly \(P \succ 0\) due to \(Q \succ 0\). To show this choice of \(P\) satisfies the Lyapunov equation, we write
\begin{align}
A^T P + P A &= \int_{t=0}^{\infty} \left( A^T e^{A^T t} Q e^{At} + e^{A^T t} Q e^{At} A  \right) dt \\
&=\int_{t=0}^{\infty} d \left( e^{A^T t} Q e^{At} \right) \\ 
& = e^{A^T t} Q e^{At}\vert_{t = \infty} - e^{A^T t} Q e^{At}\vert_{t = 0} = - Q,
\end{align}
where the last equality holds because \(e^{A \infty} = 0\) (recall \(A\) is Hurwitz).

To show the uniqueness of \(P\), we assume that there exists another matrix \(P'\) that also satisfies the Lyapunov equation. Therefore,
\begin{align}
P' &= e^{A^T t} P' e^{At} \vert_{t=0} - e^{A^T t} P' e^{At} \vert_{t=\infty} \\
 &= - \int_{t=0}^{\infty} d \left( e^{A^T t} P' e^{At} \right) \\
 &= - \int_{t=0}^{\infty} e^{A^T t} \left( A^T P' + P' A \right) e^{At} dt \\
 & = \int_{t=0}^{\infty} e^{A^T t} Q e^{At} dt = P,
\end{align}
leading to \(P' = P\). Hence, the solution is unique.
\end{proof}

\textbf{Convergence rate estimation}. We now show that Theorem \ref{thm:lyapunovequation} can allow us to quantify the convergence rate of a (stable) linear system towards zero.

For a Hurwitz linear system \(\dot{x} = Ax\), let us pick a positive definite matrix \(Q\). Theorem \ref{thm:lyapunovequation} tells us we can find a unique \(P \succ 0\) satisfying the Lyapunov equation \eqref{eq:lyapunov-equation}. In this case, we can upper bound the scalar function \(V = x^T P x\) as
\[
V \leq \lambda_{\max}(P) \Vert x \Vert^2.
\]
The time derivative of \(V\) is \(\dot{V} = - x^T Q x\), which can be upper bounded by
\begin{align}
\dot{V} & \leq - \lambda_{\min} (Q) \Vert x \Vert^2 \\
& = - \frac{\lambda_{\min} (Q)}{\lambda_{\max} (P)} \underbrace{ \left( \lambda_{\max} (P) \Vert x \Vert^2 \right)}_{\geq V} \\
& \leq - \frac{\lambda_{\min} (Q)}{\lambda_{\max} (P)} V.
\end{align}
Denoting \(\gamma(Q) = \frac{\lambda_{\min} (Q)}{\lambda_{\max}(P)}\), the above inequality implies
\[
V(0) e^{-\gamma(Q) t} \geq V(t) = x^T P x \geq \lambda_{\min}(P) \Vert x \Vert^2.
\]
As a result, \(\Vert x \Vert^2\) converges to zero exponentially with a rate at least \(\gamma(Q)\), and \(\Vert x \Vert\) converges to zero exponentially with a rate at least \(\gamma(Q) / 2\).

\textbf{Best convergence rate estimation}. I have used \(\gamma (Q)\) to make it explict that the rate \(\gamma\) depends on the choice of \(Q\), because \(P\) is computed from the Lyapunov equation as an implicit function of \(Q\). Naturally, choosing different \(Q\) will lead to different \(\gamma (Q)\). So what is the choice of \(Q\) that maximizes the convergence rate estimation?

\begin{corollary}[Maximum Convergence Rate Estimation]
\protect\hypertarget{cor:bestconvergencerate}{}\label{cor:bestconvergencerate}\(Q = I\) maximizes the convergence rate estimation.
\end{corollary}

\begin{proof}
let us denote \(P_0\) as the solution to the Lyapunov equation with \(Q = I\)
\[
A^T P_0 + P_0 A = - I.
\]
Let \(P\) be the solution corresponding to a different choice of \(Q\)
\[
A^T P + P A = - Q.
\]
Without loss of generality, we can assume \(\lambda_{\min}(Q) = 1\), because rescaling \(Q\) will recale \(P\) by the same factor, which does not affect \(\gamma(Q)\). Subtracting the two Lyapunov equations above we get
\[
A^T (P - P_0) + (P - P_0) A = - (Q - I).
\]
Since \(Q - I \succeq 0\) (due to \(\lambda_{\min}(Q) = 1\)), we know \(P - P_0 \succeq 0\) and \(\lambda_{\max} (P) \geq \lambda_{\max} (P_0)\). As a result,
\[
\gamma(Q) = \frac{\lambda_{\min}(Q)}{\lambda_{\max}(P)} =  \frac{\lambda_{\min}(I)}{\lambda_{\max}(P)} \leq \frac{\lambda_{\min}(I)}{\lambda_{\max}(P_0)} = \gamma(I),
\]
and \(Q = I\) maximizes the convergence rate estimation.
\end{proof}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Controllability and Observability}\label{app-lti-controllable-observable}

Consider the following linear time-invariant (LTI) system
\begin{equation}
    \label{eq:app-linear-system}
   \begin{split}
      \dot{x} = A x + B u \\ 
      y = C x + D u
   \end{split}
\end{equation}
where \(x \in \mathbb{R}^n\) the state, \(u \in \mathbb{R}^m\) the control input, \(y \in \mathbb{R}^p\) the output, and \(A,B,C,D\) are constant matrices with proper sizes. If we know the initial state \(x(0)\) and the control inputs \(u(t)\) over a period of time \(t \in [0, t_1]\), the system trajectory \((x(t), y(t))\) can be determined as
\begin{equation}
    \label{eq:app-lti-xy}
   \begin{split}
      x(t) & = e^{At} x(0) + \int_{0}^{t} e^{A(t-\tau)} B u(\tau) d\tau \\ 
      y(t) & = C x(t) + D u(t)
   \end{split}
\end{equation}

To study the internal structure of linear systems, two important properties should be considered: controllability and observability. In the following analysis, we will see that they are actually dual concepts. Their definitions \citep{chen1984book-linear} are given below.

\begin{definition}[Controllability]
\protect\hypertarget{def:lticontrollable}{}\label{def:lticontrollable}The LTI system \eqref{eq:app-linear-system}, or the pair \((A, B)\), is controllable, if for any initial state \(x(0) = x_0\) and final state \(x_f\), there exists a sequence of control inputs that transfer the system from \(x_0\) to \(x_f\) in finite time.
\end{definition}

\begin{definition}[Observability]
\protect\hypertarget{def:ltiobservable}{}\label{def:ltiobservable}The LTI system \eqref{eq:app-linear-system}, or the pair \((C, A)\), is observable, if for any unknown initial state \(x(0)\), there exists a finite time \(t_1 > 0\), such that knowing \(y\) and \(u\) over \([0, t_1]\) suffices to determine \(x(0)\).
\end{definition}

Sometimes it will become more convenient for us to analyze the system \eqref{eq:app-linear-system} under another coordinate basis, i.e., \(z = T x\), where the coordinate transformation \(T\) is nonsingular (i.e., full-rank). Define \(A' = TAT^{-1}, B' = PB, C' = CT^{-1}, D' = D\), we get
\begin{equation*}
   \begin{split}
      \dot{z} = A' z + B' u \\ 
      y = C' z + D' u
   \end{split}
\end{equation*}
Since the coordinate transformation only changes the system's coordinate basis, physical properties like controllability and observability will not change.

\subsection{Cayley-Hamilton Theorem}\label{cayley-hamilton-theorem}

In the analysis of controllability and observability, Cayley Hamilton Theorem lays the foundation. The statement of the theory and its (elegant) proof are given blow. Some useful corollaries are also presented.

\begin{theorem}[Cayley-Hamilton]
\protect\hypertarget{thm:cayham}{}\label{thm:cayham}Let \(A \in \mathbb{C}^{n \times n}\) and denote the characteristic polynomial of \(A\) as
\[
   \text{det}(\lambda I - A) = \lambda^n + a_1 \lambda^{n-1} + \dots + a_n \in \mathbb{C}[\lambda],
\]
which is a polynomial in a single variable \(\lambda\) with coefficients \(a_1,\dots,a_n\).
Then
\[
   A^n + a_1 A^{n-1} + \dots + a_n I = 0
\]
\end{theorem}

\begin{proof}
Define the \href{https://en.wikipedia.org/wiki/Adjugate_matrix}{adjugate} of \(\lambda I - A\) as
\[
   B = \text{adj}(\lambda I - A)
\]
From \(B\)'s definition, we have
\begin{equation}
(\lambda I - A) B  = \text{det}(\lambda I - A) I = (\lambda^n + a_1 \lambda^{n-1} + \dots + a_n) I
\label{eq:adjugate-1}
\end{equation}
Also, \(B\) is a polynomial matrix over \(\lambda\), whose maximum degree is no more than \(n - 1\). Therefore, we write \(B\) as follows:
\[
   B = \sum_{i=0}^{n-1} \lambda^i B_i
\]
where \(B_i\)'s are constant matrices. In this way, we unfold \((\lambda I - A)B\):
\begin{equation}
\label{eq:adjugate-2}
   \begin{split}
      (\lambda I - A) B & = (\lambda I - A) \sum_{i=0}^{n-1} \lambda^i B_i \\
      & = \lambda^n B_{n-1} + \sum_{i=1}^{n-1} \lambda^i (-A B_i + B_{i-1}) - A B_0
   \end{split}
\end{equation}

Since \(\lambda\) can be arbitrarily set, matching the coefficients of \eqref{eq:adjugate-1} and \eqref{eq:adjugate-2}, we have
\begin{equation*}
   \begin{split}
      B_{n-1} & = I \\
      -A B_i + B_{i-1} & = a_{n-i} I, \quad i = 1 \dots n - 1 \\
      -A B_0 & = a_n I
   \end{split}
\end{equation*}
Thus, we have
\begin{equation*}
   \begin{split}
      & B_{n-1} \cdot A^n + \sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \cdot A^i + (-A B_0) \cdot I \\
      = & I \cdot A^n + \sum_{i=1}^{n-1} (a_{n-i} I) \cdot A^i + (a_n I) \cdot I \\
      = & A^n + a_1 A^{n-1} + a_2 A^{n-2} + \dots + a_n I 
   \end{split}
\end{equation*}
On the other hand, one can easily check that
\[
   B_{n-1} \cdot A^n + \sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \cdot A^i + (-A B_0) \cdot I = 0
\]
since each term offsets completely. Therefore,
\[
   A^n + a_1 A^{n-1} + a_2 A^{n-2} + \dots + a_n I = 0,
\]
concluding the proof.
\end{proof}

Here are some corollaries of the Cayley-Hamilton Theorem.

\begin{corollary}
\protect\hypertarget{cor:cayham-1}{}\label{cor:cayham-1}For any \(A \in \mathbb{C}^{n \times n}, B \in \mathbb{C}^{n \times m}, k \ge n\), \(A^k B\) is a linear combination of \(B, AB, A^2B, \dots, A^{n-1}B\).
\end{corollary}

\begin{proof}
Directly from Cayley Hamilton Theorem, \(A^n\) can be expressed as a linear combination of \(I, A, A^2, \dots, A^{n-1}\). By recursion, it is easy to show that for all \(m > n\), \(A^m\) is also a linear combination of \(I, A, A^2, \dots, A^{n-1}\). Post-multiply both sides with \(B\), we get what we want.
\end{proof}

\begin{corollary}
\protect\hypertarget{cor:cayham-2}{}\label{cor:cayham-2}For any \(A \in \mathbb{C}^{n \times n}, B \in \mathbb{C}^{n \times m}, k > n\), the following equality always holds:
\[
   \text{rank}(\begin{bmatrix} B & AB & \dots & A^{n-1} B \end{bmatrix}) = 
   \text{rank}(\begin{bmatrix} B & AB & \dots & A^{k-1} B \end{bmatrix})
\]
\end{corollary}

\begin{proof}
First prove LHS \(\le\) RHS. \(\forall v \in \mathbb{C}^n\) such that
\[
   v^* \begin{bmatrix} B & AB & \dots & A^{k-1} B \end{bmatrix} = v^* \begin{bmatrix} B & AB & \dots & A^{n-1}B & \dots A^{k-1}B \end{bmatrix} = 0
\]
\(v^* \begin{bmatrix} B & AB & \dots & A^{n-1} B \end{bmatrix} = 0\) must hold.

Second prove LHS \(\ge\) RHS. For any \(v \in \mathbb{C}^n\) such that \(v^* \begin{bmatrix} B & AB & \dots & A^{n-1} B \end{bmatrix} = 0\) and any \(k > n\), by Corollary \ref{cor:cayham-1}, there exists a sequence \(c_i, i = 0 \dots n-1\) satisfy the following:
\[
   v^* A^k B = v^* \sum_{i=0}^{n-1} c_i A^i B = 0
\]
Therefore, \(v^* \begin{bmatrix} B & AB & \dots & A^{k-1} B \end{bmatrix} = 0\).
\end{proof}

\begin{corollary}
\protect\hypertarget{cor:cayham-3}{}\label{cor:cayham-3}For any \(A \in \mathbb{C}^{n \times n}, B \in \mathbb{C}^{n \times m}\), define
\[
   \mathcal{C} = \begin{bmatrix} B & AB & \dots & A^{n-1} B \end{bmatrix}
\]
If \(\text{rank}(\mathcal{C}) = k_1 < n\), there exist a similarity transformation \(T\) such that
\[
   T A T^{-1} = \begin{bmatrix}
      \bar{A}_c & \bar{A}_{12} \\
      0 & \bar{A}_{\bar{c}} 
   \end{bmatrix}, T B = \begin{bmatrix}
      \bar{B}_c \\ 0
   \end{bmatrix}
\]
where \(\bar{A}_c \in \mathbb{C}^{k_1 \times k_1}, \bar{B}_c \in \mathbb{C}^{k_1 \times m}\). Moreover, the matrix
\begin{equation*}
   \bar{\mathcal{C}} := \begin{bmatrix}
      \bar{B}_c & \bar{A}_c \bar{B}_c & \bar{A}_c^2 \bar{B}_c & \dots & \bar{A}_c^{k_1 - 1} \bar{B}_c
   \end{bmatrix}
\end{equation*}
has full row rank.
\end{corollary}

\begin{proof}
Since \(\mathcal{C}\) is not full row rank, we pick \(k_1\) linearly independent columns from \(\mathcal{C}\). Denote them as \(q_1\dots q_{k_1}\), \(q_i \in \mathbb{C}^n\). Then, we arbitrarily set other \(n-k_1\) vectors \(q_{k_1+1} \dots q_{n}\) as long as
\[
Q = \begin{bmatrix}
   q_1 & \dots & q_{k_1} & q_{k_1+1} & \dots & q_{n} 
\end{bmatrix}
\]
is invertible. Define the similarity transformation matrix by \(T = Q^{-1}\). Note that \(A q_i\) can be seen as a column picked from \(A^{k} B, k \in \left\{1 \dots n\right\}\), which is guaranteed to be a linear combination of \(B, AB, \dots, A^{n-1}B\) from Cayley Hamilton Theorem. Thus, \(A q_i\) is bound to be a linear transformation of columns from \(\begin{bmatrix} B & AB & \dots & A^{n-1} B \end{bmatrix} = \mathcal{C}\). Since \(q_1\dots q_{k_1}\) is the largest linearly independent column vector set from \(\mathcal{C}\), this implies \(A q_i\) can be expressed as a linear combination of \(q_1\dots q_{k_1}\):
\begin{equation*}
   \begin{split}
      A Q & = A T^{-1} = A \begin{bmatrix}
         q_1 & \dots & q_{k_1} & q_{k_1+1} & \dots & q_{n} 
      \end{bmatrix} \\
      & = \begin{bmatrix}
         q_1 & \dots & q_{k_1} & q_{k_1+1} & \dots & q_{n} 
      \end{bmatrix} \begin{bmatrix}
         \bar{A}_c & \bar{A}_{12} \\
         0 & \bar{A}_{\bar{c}} 
      \end{bmatrix} = T^{-1} \begin{bmatrix}
         \bar{A}_c & \bar{A}_{12} \\
         0 & \bar{A}_{\bar{c}} 
      \end{bmatrix}
   \end{split} 
\end{equation*}
Similarly, \(B\) itself is part of \(\mathcal{C}\). Therefore, each column of \(B\) is naturally a linear combination of \(q_1 \dots q_{k_1}\):
\begin{equation*}
   \begin{split}
      B = \begin{bmatrix}
         q_1 & \dots & q_{k_1} & q_{k_1+1} & \dots & q_{n} 
      \end{bmatrix} \begin{bmatrix}
         \bar{B}_c \\ 0
      \end{bmatrix}
   \end{split} = T^{-1} \begin{bmatrix}
      \bar{B}_c \\ 0
   \end{bmatrix}
\end{equation*}
To see \(\bar{\mathcal{C}}\) has full row rank, note that \(\text{rank} \mathcal{C} = k_1\) and
\begin{equation*}
   \mathcal{C} = T^{-1} \begin{bmatrix}
      \bar{B}_c & \bar{A}_c \bar{B}_c & \bar{A}_c^2 \bar{B}_c & \dots & \bar{A}_c^{k_1 - 1} \bar{B}_c & \dots & \bar{A}_c^{n - 1} \bar{B}_c \\
      0 & 0 & 0 & \dots & 0 & \dots & 0
   \end{bmatrix}
\end{equation*}
Thus,
\[\text{rank}\begin{bmatrix}
   \bar{B}_c & \bar{A}_c \bar{B}_c & \bar{A}_c^2 \bar{B}_c & \dots & \bar{A}_c^{k_1 - 1} \bar{B}_c & \dots & \bar{A}_c^{n - 1} \bar{B}_c
\end{bmatrix} = k_1.
\]
By Corollary \ref{cor:cayham-2}, \(\text{rank}\bar{\mathcal{C}} = k_1\).
\end{proof}

The following Corollary is especially useful in the study of pole assignment in the single-input-multiple-output (SIMO) LTI system.

\begin{corollary}
\protect\hypertarget{cor:cayham-4}{}\label{cor:cayham-4}For any \(A \in \mathbb{C}^{n \times n}, b \in \mathbb{C}^{n}\), if
\begin{equation*}
   \mathcal{C} = \begin{bmatrix}
      b & Ab & \dots & A^{n-1}b
   \end{bmatrix} \in \mathbb{C}^{n \times n}
\end{equation*}
has full rank, then there exists a similarity transformation \(T\) such that
\begin{equation*}
      T A T^{-1} = A_1 := \begin{bmatrix}
         -a_1 & -a_2 & \dots & -a_{n-1} & -a_n \\
         1 & 0 & \dots & 0 & 0 \\
         0 & 1 & \dots & 0 & 0 \\
         \vdots & \vdots &  & \vdots & \vdots \\
         0 & 0 & \dots & 1 & 0
      \end{bmatrix}, \quad
      T b = b_1 := \begin{bmatrix}
         1 \\ 0 \\ 0 \\ \vdots \\ 0
      \end{bmatrix}
\end{equation*}
where \(a_1, \dots, a_n\) are the coefficients of \(A\)'s characteristic polynomial:
\begin{equation*}
   \det(A - \lambda I) = \lambda^{n} + a_1 \lambda^{n-1} + \dots + a_n \lambda  
\end{equation*}
\end{corollary}

\begin{proof}
Since \(\mathcal{C}\) is invertible, define its inverse
\begin{equation*}
   \mathcal{C}^{-1} = \begin{bmatrix}
      M_1 \\ M_2 \\ \dots \\ M_n
   \end{bmatrix}
\end{equation*}
where \(M_i \in \mathbb{C}^{1 \times n}\). Then,
\begin{equation*}
   I = \mathcal{C}^{-1} \mathcal{C} = \begin{bmatrix}
      M_1 b & M_1 Ab & \dots & M_1 A^{n-1}b \\
      M_2 b & M_2 Ab & \dots & M_2 A^{n-1}b \\
      \vdots & \vdots &  & \vdots \\
      M_n b & M_n Ab & \dots & M_n A^{n-1}b
   \end{bmatrix} \Longrightarrow \begin{cases}
      M_n A^{n-1}b = 1 \\
      M_n A^ib = 0, \ i = 0,\dots, n-2
   \end{cases}
\end{equation*}
Now we claim that the transformation matrix \(T\) can be constructed as follows:
\begin{equation*}
   T = \begin{bmatrix}
      M_n A^{n-1} \\ M_n A^{n-2} \\ \dots \\ M_n
   \end{bmatrix}
\end{equation*}
We first show \(T\) is invertible by calculating \(T \mathcal{C}\):
\begin{equation*}
   T \mathcal{C} = \begin{bmatrix}
      M_n A^{n-1}b & \star & \dots & \star \\
      M_n A^{n-2}b & M_n A^{n-1}b & \dots & \star \\
      \vdots & \vdots &  & \vdots \\
      M_n b & M_n Ab & \dots & M_n A^{n-1}b
   \end{bmatrix} = \begin{bmatrix}
      1 & \star & \dots & \star \\
      0 & 1 & \dots & \star \\
      \vdots & \vdots &  & \vdots \\
      0 & 0 & \dots & 1
   \end{bmatrix}
\end{equation*}
Then we calculate \(Tb\) and \(TA\):
\begin{equation*}
   \begin{split}
      Tb & = \begin{bmatrix}
         M_n A^{n-1}b \\ M_n A^{n-2}b \\ \vdots \\ M_n b
      \end{bmatrix} = \begin{bmatrix}
         1 \\ 0 \\ \vdots \\ 0
      \end{bmatrix} \\
      T A & = \begin{bmatrix}
         M_n A^n \\ M_n A^{n-1} \\ \vdots \\ M_n A
      \end{bmatrix} = \begin{bmatrix}
         -M_n \cdot \sum_{i=0}^{n-1} a_{n-i} A^i \\ M_n A^{n-1} \\ \vdots \\ M_n A
      \end{bmatrix} \\
      & = \begin{bmatrix}
            -a_1 & -a_2 & \dots & -a_{n-1} & -a_n \\
            1 & 0 & \dots & 0 & 0 \\
            0 & 1 & \dots & 0 & 0 \\
            \vdots & \vdots &  & \vdots & \vdots \\
            0 & 0 & \dots & 1 & 0
      \end{bmatrix} \begin{bmatrix}
         M_n A^{n-1} \\ M_n A^{n-2} \\ \vdots \\ M_n A \\ M_n
      \end{bmatrix} = A_1 T
   \end{split}
\end{equation*}
where the penultimate equality uses Cayley Hamilton Theorem.
\end{proof}

\subsection{Equivalent Statements for Controllability}\label{equivalent-statements-for-controllability}

There are a few equivalent statements to express an LTI system's controllability that one should be familiar with:

\begin{theorem}[Equivalent Statements for Controllability]
\protect\hypertarget{thm:lticontrollable}{}\label{thm:lticontrollable}

The following statements are equivalent \citep{chen1984book-linear}, \citep{zhou1996book-robust}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \((A, B)\) is controllable.
\item
  The matrix
  \[
  W_c(t) := \int_{0}^{t} e^{A\tau} B B^* e^{A^* \tau} d\tau
  \]
  is positive definite for any \(t > 0\).
\item
  The controllability matrix
  \[
  \mathcal{C} = \begin{bmatrix}
     B & AB & A^2 B & \dots & A^{n-1} B 
  \end{bmatrix}
  \]
  has full row rank.
\item
  The matrix \([A - \lambda I, B]\) has full row rank for all \(\lambda \in \mathbb{C}\).
\item
  Let \(\lambda\) and \(x\) be any eigenvalue and any corresponding left eigenvector \(A\), i.e., \(x^* A = x^* \lambda\), then \(x^* B \ne 0\).
\item
  The eigenvalues of \(A+BF\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \(F\).
\item
  If, in addition, all eigenvalues of \(A\) have negative real parts, then the unique solution of
  \[ 
  A W_c + W_c A^* = -B B^* 
  \]
  is positive definite. The solution is called the \emph{controllability Gramian} and can be expressed as
  \[
  W_c = \int_{0}^{\infty} e^{A \tau} B B^* e^{A^* \tau} d\tau
  \]
\end{enumerate}

\end{theorem}

\begin{proof}
(\(1. \Rightarrow 2.\)) Prove by contradiction. Assume that \((A, B)\) is controllable but \(W_c(t_1)\) is singular for some \(t_1 > 0\). This implies there exists a real vector \(v \ne 0 \in \mathbb{R}^n\), s.t.
\[
   v^* W_c(t_1) v = v^* (\int_{0}^{t_1} e^{At} B B^* e^{A^*t} dt) v = \int_{0}^{t_1} v^* (e^{At} B B^* e^{A^*t}) v \ dt = 0
\]
Since \(e^{At} BB^* e^{A^*t} \succeq 0\) for all \(t\), we must have
\begin{equation*}
   \begin{split}
      & v^* (e^{At} B B^* e^{A^*t}) v = \parallel v^* B e^{At} \parallel^2 = 0, \quad \forall t \in [0, t_1] \\
      \Longrightarrow & v^* B e^{At} = 0, \quad \forall t \in [0, t_1]
   \end{split}
\end{equation*}
Setting \(x(t_1) = 0\), from \eqref{eq:app-lti-xy}, we have
\[
   0 = e^{A t_1} x(0) + \int_{0}^{t_1} e^{A (t_1 - \tau)} B u(\tau) d\tau = 0
\]
Pre-multiply the above equation by \(v^*\), then
\[
   0 = v^* e^{A t_1} x(0)
\]
Since \(x(0)\) can be chosen arbitrarily, we set \(x(0) = v e^{-A t_1}\), which results in \(v = 0\). Contradiction!

(\(2. \Rightarrow 1.\)) For any \(x(0) = x_0, t_1 > 0, x(t_1) = x_1\), since \(W_c(t_1) \succ 0\), we set the control inputs as
\[
   u(t) = -B^* e^{A^*(t_1 - t)} W_c^{-1}(t_1) [e^{At_1} x_0 - x_1]
\]
We claim that the picked \(u(t)\) satisfies \eqref{eq:app-lti-xy} by
\begin{equation*}
   \begin{split}
      & e^{At} x_0 + \int_{0}^{t_1} e^{A(t_1-t)} B u(t) dt \\
      & = e^{At} x_0 - \int_{0}^{t_1} e^{A(t_1-t)} B B^* e^{A^*(t_1-t)} dt \cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\
      & \overset{\tau = t_1-t}{=} e^{At} x_0 - \underbrace{\int_{0}^{t_1} e^{A\tau} BB^* e^{A^*\tau} d\tau}_{W_c(t_1)} \cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\
      & = e^{At} x_0 - [e^{At_1} x_0 - x_1] = x_1
   \end{split}
\end{equation*}

(\(2. \Rightarrow 3.\)) Prove by contradiction. Suppose \(W_c(t) \succ 0, \forall t > 0\) but \(\mathcal{C}\) is not of full row rank. Then there exists \(v \ne 0 \in \mathbb{C}^n\), s.t.
\[
   v^* A^k B = 0, \quad k = 0 \dots n - 1
\]
By Corollary \ref{cor:cayham-1}, we have
\[
   v^* A^k B = 0, \ \forall k \in \mathbb{N} \Longrightarrow v^* e^{At} B = 0, \ \forall t > 0
\]
which implies
\[
   v^* W_c(t) v = v^* (\int_{0}^{t} e^{A\tau} B B^* e^{A^*\tau} d\tau) v = 0, \quad \forall t > 0
\]
Contradiction!

(\(3. \Rightarrow 2.\)) Prove by contradiction. Suppose \(\mathcal{C}\) has full row rank but \(W_c(t_1)\) is singular at some \(t_1 > 0\). Then, similar to the proof in (\(1. \Rightarrow 2.\)), there exists \(v \ne 0 \in \mathbb{C}^n\), s.t. \(F(t) := v^* e^{At} B \equiv 0, \forall t \in [0, t_1]\). Since \(F(t)\) is infinitely differentiable, we get its \(i\)'s derivative at \(t=0\), where \(i = 0, 1, \dots n-1\). This results in
\begin{equation*}
   \left. \frac{d^i F}{dt^i} \right|_{t=0} = \left. v^* A^{i} e^{At} B \right|_{t=0} = v^* A^i B = 0, \quad i = 0 \dots n-1
\end{equation*}
Thus, \(v^* \begin{bmatrix}
   B & AB & \dots & A^{n-1} B
\end{bmatrix} = 0\). Contradiction!

(\(3. \Rightarrow 4.\)) Proof by contradiction. Suppose \([A - \lambda I, B]\) does not have full row rank for some \(\lambda \in \mathbb{C}\). Then, there exists \(v \ne 0 \in \mathbb{C}^n\), s.t. \(v^* [A - \lambda I, B] = 0\). This implies \(v^* A = v^* \lambda\) and \(v^* B = 0\). On the other hand,
\begin{equation*}
   v^* \begin{bmatrix}
      B & AB & \dots & A^{n-1}B
   \end{bmatrix} = v^* \begin{bmatrix}
      B & \lambda B & \dots & \lambda^{n-1} B
   \end{bmatrix} = 0
\end{equation*}
Contradiction!

(\(4. \Rightarrow 5.\)) Proof by contradiction. If there exists a left eigenvector and eigenvalue pair \((x, \lambda)\), s.t. \(x^* A = \lambda x^*\) while \(x^*B = 0\), then \(x^* [A - \lambda I, B] = 0\). Contradiction!

(\(5. \Rightarrow 3.\)) Proof by contradiction. If the controllability matrix \(\mathcal{C}\) does not have full row rank, i.e., \(\text{rank}(\mathcal{C}) = k < n\). Then, from Corollary \ref{cor:cayham-3}, there exists a similarity transformation \(T\), s.t.
\begin{equation*}
   TAT^{-1} = \begin{bmatrix}
      \bar{A}_{c} & \bar{A}_{12} \\
      0 & \bar{A}_{\bar{c}}
   \end{bmatrix}, \quad TB = \begin{bmatrix}
      \bar{B}_c \\ 0
   \end{bmatrix}
\end{equation*}
where \(\bar{A}_c \in \mathbb{R}^{k \times k}, \bar{A}_{\bar{c}} \in \mathbb{R}^{(n-k) \times (n-k)}\). Now arbitrarily pick one of \(\bar{A}_{\bar{c}}\)'s left eigenvector \(x_{\bar{c}}\) and its corresponding eigenvalue \(\lambda_1\). Define the vector \(x = \begin{bmatrix}
   0 \\ x_{\bar{c}}
\end{bmatrix}\). Then,
\begin{equation*}
   \begin{split}
      x^* (TAT^{-1}) = \begin{bmatrix}
         0 & x_{\bar{c}}^*
      \end{bmatrix} \begin{bmatrix}
         \bar{A}_{c} & \bar{A}_{12} \\
         0 & \bar{A}_{\bar{c}}
      \end{bmatrix} & = \begin{bmatrix}
         0 & x_{\bar{c}}^* \bar{A}_{\bar{c}}
      \end{bmatrix} = \begin{bmatrix}
         0 & \lambda_1 x_{\bar{c}}^*
      \end{bmatrix} = \lambda_1 x^* \\
      x^* (TB) & = \begin{bmatrix}
         0 & x_{\bar{x}}
      \end{bmatrix} \begin{bmatrix}
         B_{\bar{c}} \\ 0
      \end{bmatrix} = 0
   \end{split}
\end{equation*}
which implies \((TAT^{-1}, TB)\) is not controllable. However, similarity transformation does not change controllability. Contradiction!

(\(6. \Rightarrow 1.\)) Prove by contradiction. If \((A, B)\) is not controllable, i.e., \(\text{rank}(\mathcal{C}) = k < n\). Then from Corollary \ref{cor:cayham-3}, there exists a similarity transformation \(T\) s.t.
\begin{equation*}
   TAT^{-1} = \begin{bmatrix}
      \bar{A}_c & \bar{A}_{12} \\
      0 & \bar{A}_{\bar{c}}
   \end{bmatrix}, \quad TB = \begin{bmatrix}
      \bar{B}_c \\
      0
   \end{bmatrix}
\end{equation*}
Now arbitrarily pick \(F \in \mathbb{R}^{m\times n}\) and define \(FT^{-1} = [F_1, F_2]\), where \(F_1 \in \mathbb{R}^{m\times k}, F_2 \in \mathbb{R}^{m\times (n-k)}\). Thus,
\begin{equation*}
   \begin{split}
      \text{det}(A+BF-\lambda I) & = \text{det}\left(
         T^{-1} \begin{bmatrix}
            \bar{A}_c & \bar{A}_{12} \\
            0 & \bar{A}_{\bar{c}}
         \end{bmatrix} T + T^{-1} \begin{bmatrix}
            \bar{B}_c \\ 0
         \end{bmatrix} F - \lambda \begin{bmatrix}
            I_1 & 0 \\
            0 & I_2
         \end{bmatrix}
      \right) \\
      & = \text{det}\left(
         T^{-1} \left\{
            \begin{bmatrix}
               \bar{A}_c & \bar{A}_{12} \\
               0 & \bar{A}_{\bar{c}}
            \end{bmatrix} + \begin{bmatrix}
               \bar{B}_c \\ 0
            \end{bmatrix} FT^{-1} - \lambda \begin{bmatrix}
               I_1 & 0 \\
               0 & I_2
            \end{bmatrix}
         \right\} T
      \right) \\
      & = \text{det}\left(
         \begin{bmatrix}
            \bar{A}_c & \bar{A}_{12} \\ 0 & \bar{A}_{\bar{c}}
         \end{bmatrix} + \begin{bmatrix}
            \bar{B}_c \\ 0
         \end{bmatrix} \begin{bmatrix}
            F_1 & F_2
         \end{bmatrix} - \lambda \begin{bmatrix}
            I_1 & 0 \\
            0 & I_2
         \end{bmatrix}
      \right) \\
      & = \text{det} \begin{bmatrix}
            \bar{A}_c + \bar{B}_c F_1 - \lambda I_1 & \bar{A}_{12} + \bar{B}_c F_2 \\ 0 & \bar{A}_{\bar{c}} - \lambda I_2
         \end{bmatrix} \\
      & = \text{det}(\bar{A}_c + \bar{B}_c F_1 - \lambda I_1) \cdot \text{det}(\bar{A}_{\bar{c}} - \lambda I_2)
   \end{split}
\end{equation*}
where \(I_1\) is the identity matrix of size \(k\). Similarly, \(I_2\) of size \(n-k\). Thus, at least \(n-k\) eigenvalues of \(A+BF\) cannot be freely assigned by choosing \(F\). Contradiction!

(\(1. \Rightarrow 6.\)) Here we only represent the SIMO case. For the MIMO case, the proof is far more complex. Interesting readers can refer to \citep{davison1968tac-poleassign} (the shortest proof I can find). Since there is only one input, the matrix \(B\) degenerate to vector \(b\). From Corollary \ref{cor:cayham-4}, there exist a similarity transformation matrix \(T\), s.t.
\begin{equation*}
   T A T^{-1} = A_1 := \begin{bmatrix}
      -a_1 & -a_2 & \dots & -a_{n-1} & -a_n \\
      1 & 0 & \dots & 0 & 0 \\
      0 & 1 & \dots & 0 & 0 \\
      \vdots & \vdots &  & \vdots & \vdots \\
      0 & 0 & \dots & 1 & 0
   \end{bmatrix}, \quad
   T b = b_1 := \begin{bmatrix}
      1 \\ 0 \\ 0 \\ \vdots \\ 0
   \end{bmatrix}
\end{equation*}
For any \(F \in \mathbb{C}^{1 \times n}\), denote \(FT^{-1}\) as \([f_1, f_2, \dots, f_n]\). Calculating the characteristic polynomial of \(A + bF\):
\begin{equation*}
   \begin{split}
      \text{det}(\lambda I - A - bF) & = \text{det}(\lambda I - T^{-1}A_1 T - T^{-1} b_1 F) \\
      & = \text{det}(\lambda I - A_1 - b_1 F T^{-1}) \\
      & = \text{det} \begin{bmatrix}
         \lambda + a_1 - f_1 & \lambda + a_2 - f_2 & \dots & \lambda + a_{n-1} - f_{n-1} & \lambda + a_n - f_n \\
         -1 & \lambda & \dots & 0 & 0 \\
         0 & -1 & \dots & 0 & 0 \\
         \vdots & \vdots &  & \vdots & \vdots \\
         0 & 0 & \dots & -1 & \lambda
      \end{bmatrix} \\
      & = \lambda^n + (a_1 - f_1) \lambda^{n-1} + \dots + (a_n - f_n)
   \end{split}
\end{equation*}
By choosing \([f_1, f_2, \dots, f_n]\), \(A+bF\)'s eigenvalues can be arbitrarily set.

(\(7. \Rightarrow 1.\)) Prove by contradiction. Assume that \((A, B)\) is not controllable. Then from 2., there exists \(v \ne 0 \in \mathbb{C}^n\) and \(t_1 > 0\),
\begin{equation*}
   F(t) = v^* e^{At} B = 0, \quad \forall t \in [0, t_1]
\end{equation*}
Now consider \(F(z) = v^* e^{Az} B, z\in \mathcal{C}\), which is a vector of analytic function in complex analysis. For a arbitrary \(t_2 \in (0, t_1)\), we have \(F^{(i)}(t_2) = 0, \forall i \in \mathbb{N}\). Then, by invoking the fact from complex analysis: ``Let \(G\) a connected open set and \(f: G \rightarrow \mathbb{C}\) be analytic, then \(f \equiv 0\) on \(G\), if and only if there is a point \(a \in G\) such that \(f^{(i)}(a) = 0, \forall n \in \mathbb{N}\)'', we have \(f(z) \equiv 0, \forall z \in \mathbb{C}\).

On the other hand, however, \(W_c \succ 0\) implies there exists \(t_3 > 0\), such that for the above \(v\), we have \(v^* e^{At_3} B \ne 0\). Contradiction!

(\(1. \Rightarrow 7.\)) Since \((A, B)\) is controllable, from 2., \(W_c(t) \succ 0, \forall t\). Therefore, \(W_c \succ 0\). The existence and uniqueness of the solution for \(AW_c + W_cA^* = -BB^*\) can be obtained directly from the proof of Theorem \ref{thm:lyapunovequation}, by setting \(Q\) there to be positive semidefinite.
\end{proof}

\subsection{Duality}\label{duality}

Although controllability and observability seemingly have no direct connections from their definitions \ref{def:lticontrollable} and \ref{def:ltiobservable}, the following theorem \citep{chen1984book-linear} states their tight relations.

\begin{theorem}[Theorem of Duality]
\protect\hypertarget{thm:lti-dual-conobs}{}\label{thm:lti-dual-conobs}The pair \((C,A)\) is observable if and only if \((A^*,C^*)\) is controllable.
\end{theorem}

\begin{proof}
\leavevmode

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  We first show that \((C,A)\) is observable if and only if the \(n \times n\) matrix \(W_o(t) = \int_{0}^{t} e^{A^*\tau} C^*C e^{A\tau}\) is positive definite (nonsingular) for any \(t>0\):
\end{enumerate}

``\(\Longleftarrow\)'': From \eqref{eq:app-lti-xy}, given initial state \(x(0)\) and the inputs \(u(t)\), \(y(t)\) can be expressed as
\begin{equation*}
   y(t) = Ce^{At} x(0) + C \int_{0}^{t} e^{A(t-\tau)} Bu(\tau) d\tau + Du(t) 
\end{equation*}
Define a known function \(\bar{y}(t)\) as \(y(t) - C \int_{0}^{t} e^{A(t-\tau)} Bu(\tau) d\tau - Du(t)\) and we will get
\begin{equation*}
   Ce^{At} x(0) = \bar{y}(t)
\end{equation*}
Pre-multiply the above equation by \(e^{A^*t}C^*\) and integrate it over \([0,t_1]\) to yield
\begin{equation*}
   (\int_{0}^{t_1} e^{A^*t} C^*C e^{At} dt) x(0) = W_o(t_1) x(0) = \int_{0}^{t_1} e^{A^*t} C^* \bar{y}(t) dt 
\end{equation*}
Since \(W_o(t_1) \succ 0\),
\begin{equation*}
   x(0) = W_o(t_1)^{-1} \int_{0}^{t_1} e^{A^*t} C^* \bar{y}(t) dt
\end{equation*}
can be observed.

``\(\Longrightarrow\)'': Prove by contradiction. Suppose \((C,A)\) is observable but there exists \(t_1 >0\), s.t. \(W_o(t_1)\) is singular. This implies there exists \(v \ne 0 \in \mathbb{C}^n\), s.t.
\begin{equation*}
   v^* W_o(t_1) v = 0 \Longrightarrow Ce^{At} v \equiv 0, \ \forall t \in [0,t_1]
\end{equation*}
Similar to the proof of Theorem \ref{thm:lticontrollable} (\(7. \Rightarrow 1.\)), we can use conclusions from complex analysis to claim that \(Ce^{At} v \equiv 0, \forall t >0\).
On the other hand, we set \(u(t) \equiv 0\), which results in \(y(t) = Ce^{At}x(0)\). In this case \(x(0) = 0\) and \(x(0) = v \ne 0\) will lead to the same output responses \(y(t)\) over \(t>0\), which implies \((C,A)\) is not observable. Contradiction!

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Next we show the duality of controllability and observability:
\end{enumerate}

From (1) we know \((C,A)\) is controllable if and only of
\begin{equation*}
   \int_{0}^{t} e^{A^*\tau} C^*C e^{A\tau} d\tau = \int_{0}^{t} e^{(A^*)\tau} (C^*)^* (C^*) e^{(A^*)^*\tau} d\tau
\end{equation*}
is nonsingular for all \(t >0\). The latter is exactly the definition of \((A^*, C^*)\)'s controllability Gramian \(W_c(t)\).

\end{proof}

\subsection{Equivalent Statements for Observability}\label{equivalent-statements-for-observability}

With the Theorem of Duality \ref{thm:lti-dual-conobs}, we can directly write down the equivalent statements of observability without any additional proofs:

\begin{theorem}[Equivalent Statements for Observability]
\protect\hypertarget{thm:ltiobservable}{}\label{thm:ltiobservable}

The following statements are equivalent \citep{chen1984book-linear}, \citep{zhou1996book-robust}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \((C, A)\) is observable.
\item
  The matrix
  \begin{equation*}
  W_o(t) := \int_{0}^{t} e^{A^*\tau} C^* C e^{A\tau} d\tau
  \end{equation*}
  is positive definite for any \(t>0\).
\item
  The observability matrix
  \begin{equation*}
  \mathcal{O} = \begin{bmatrix}
     C \\ CA \\ CA^2 \\ \dots \\ CA^{n-1}
  \end{bmatrix}
  \end{equation*}
  has full column rank.
\item
  The matrix \(\begin{bmatrix}
  A - \lambda I \\ C
  \end{bmatrix}\) has full column rank for all \(\lambda \in \mathbb{C}\).
\item
  Let \(\lambda\) and \(y\) be any eigenvalue and any corresponding right eigenvector of \(A\), i.e., \(Ay = \lambda y\), then \(Cy \ne 0\).
\item
  The eigenvalues of \(A+LC\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \(L\).
\item
  \((A^*, C^*)\) is controllable.
\item
  If, in addition, all eigenvalues of \(A\) have negative parts, then the unique solution of
  \begin{equation*} 
  A^* W_o + W_o A = -C^* C 
  \end{equation*}
  is positive definite. The solution is called the \emph{observability Gramian} and can be expressed as
  \begin{equation*}
  W_o = \int_{0}^{\infty} e^{A^*\tau} C^* C e^{A\tau} d\tau
  \end{equation*}
\end{enumerate}

\end{theorem}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Stabilizability And Detectability}\label{stabilizability-and-detectability}

To define stabilizability and detectability of an LTI system, we first introduce the concept of \emph{system mode}, which can be naturally derived from the fifth definition of controllability \ref{thm:lticontrollable} (observability \ref{thm:ltiobservable}).

\begin{definition}[System Mode]
\protect\hypertarget{def:ltisystemmode}{}\label{def:ltisystemmode}\(\lambda\) is a mode of an LTI system, if it is an eigenvalue of \(A\). The mode \(\lambda\) is said to be:

\begin{itemize}
\tightlist
\item
  stable, if \(\text{Re}\lambda < 0\),
\item
  controllable, if \(x^* B \ne 0\) for all left eigenvectors of \(A\) associated with \(\lambda\),
\item
  observable, if \(C x \ne 0\) for all right eigenvectors of \(A\) associated with \(\lambda\).
\end{itemize}

Otherwise, the mode is said to be uncontrollable (unobservable).
\end{definition}

With the concept of system mode, the fifth definition of controllability \ref{thm:lticontrollable} (observability \ref{thm:ltiobservable}) can be restated as

\begin{quote}
An LTI system is controllable (observable) if and only if all modes are controllable (observable).
\end{quote}

Stabilizability (detectability) is defined similarly via loosening part of controllability (observability) conditions.

\begin{definition}[Stabilizability]
\protect\hypertarget{def:ltistabilizable}{}\label{def:ltistabilizable}An LTI system is said to be stabilizable if all of its unstable modes are controllable.
\end{definition}

\begin{definition}[Detectability]
\protect\hypertarget{def:ltidetectable}{}\label{def:ltidetectable}An LTI system is said to be detectable if all of its unstable modes are observable.
\end{definition}

Like in the case of controllability and observability, duality also holds in stabilizability and detectability. Moreover, similarity transformation will not influence an LTI system's stabilizability and detectability.

\subsection{Equivalent Statements for Stabilizability}\label{equivalent-statements-for-stabilizability}

\begin{theorem}[Equivalent Statements for Stabilizability]
\protect\hypertarget{thm:ltistabilizable}{}\label{thm:ltistabilizable}

The following statements are equivalent \citep{zhou1996book-robust}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \((A,B)\) is stabilizable.
\item
  For all \(\lambda\) and \(x\) such that \(x^* A = \lambda x^*\) and \(\text{Re} \lambda \ge 0\), \(x^* B \ne 0\).
\item
  The matrix \([A-\lambda I, B]\) has full rank for all \(\text{Re} \lambda \ge 0\).
\item
  There exists a matrix \(F\) such that \(A+BF\) are Hurwitz.
\end{enumerate}

\end{theorem}

\begin{proof}
(\(1. \Leftrightarrow 2.\)) Directly from stabilizability's definition.

(\(2. \Leftrightarrow 3.\)) If 2. holds but 3. not hold, then there exists \(v \ne 0 \in \mathbb{C}^n\), s.t.
\begin{equation*}
   v^* [A-\lambda I, B] = 0 \Leftrightarrow v^* A = \lambda v^*, v^* B = 0, \text{Re} \lambda \ge 0
\end{equation*}
Contradiction! Vice versa.

(\(4. \Rightarrow 2.\)) Prove by contradiction. Suppose there \(x \ne 0 \in \mathbb{C}^n\), s.t.
\begin{equation*}
   x^* [A-\lambda I, B] = 0 \Leftrightarrow x^* A = \lambda x^*, x^* B = 0, \text{Re} \lambda \ge 0
\end{equation*}
Thus, for any \(F\),
\begin{equation*}
   x^* (A+BF) = \lambda x^*, \text{Re} \lambda \ge 0
\end{equation*}
On the other hand, suppose \(A+BF\) has \(I\) Jordon blocks, with each equipped with an eigenvalue \(\eta_i, i = 1\dots I\) (note that \(\eta_\alpha\) may be equal to \(\eta_\beta\), i.e., they are equivalent eigenvalues with different Jordon blocks). Since \(A+BF\)'s eigenvalues all have negative real parts, \(\text{Re} (\eta_i) < 0, i = 1\dots I\). For each \(\eta_i,i \in \left\{1\dots i\right\}\), denote its \(K_i\) generalized left eigenvectors as \(v_{i,1}, v_{i,2}, \dots v_{i,K_i}\). By definition, \(\sum_{i=1}^{I} K_i = n\) and\\
\begin{equation*}
   \begin{split}
      v_{i,1}^* (A+BF) & = v_{i,1}^* \cdot \eta_i \\
      v_{i,2}^* (A+BF) & = v_{i,1}^* + v_{i,2}^* \cdot \eta_i \\
      & \vdots \\
      v_{i,K_i}^* (A+BF) & = v_{i,K_i-1}^* + v_{i,K_i}^* \cdot \eta_i
   \end{split}
\end{equation*}
for all \(i \in \left\{1\dots i\right\}\). Also, \(v_{i,k},i=1\dots I, k=1\dots K_i\) are linearly independent and spans \(\mathbb{C}^n\). Therefore,
\begin{equation*}
   x^* = \sum_{i=1}^{I} \sum_{k=1}^{K_i} \xi_{i,k} \cdot v_{i,k}^*
\end{equation*}
which leads to
\begin{equation*}
   \sum_{i=1}^{I} \sum_{k=1}^{K_i} \xi_{i,k} \cdot v_{i,k}^* (A+BF) 
   = \sum_{i=1}^{I} \sum_{k=1}^{K_i} \xi_{i,k} \cdot \lambda \cdot v_{i,k}^* 
\end{equation*}
Since \(v_{i,k}\)'s are \(A+BF\)'s generalized eigenvectors, we have
\begin{equation*}
   \begin{split}
      & \sum_{i=1}^{I} \sum_{k=1}^{K_i} \xi_{i,k} \cdot v_{i,k}^* \cdot (A+BF) \\
      = & \sum_{i=1}^{I} \left\{
         \xi_{i,1} \cdot \eta_i \cdot v_{i,1}^*  + \sum_{k=2}^{K_i} \xi_{i,k} (v_{i,k-1}^* + \eta_i \cdot v_{i,k}^* )
      \right\} \\
      = & \sum_{i=1}^{I} \left\{
         \sum_{k=1}^{K_i - 1} (\xi_{i,k}\cdot \eta_i + \xi_{i,k+1}) v_{i,k}^* + \xi_{i,K_i} \cdot \eta_i \cdot v_{i,K_i}^*
      \right\}
   \end{split}
\end{equation*}
Combining the above two equations:
\begin{equation*}
   \sum_{i=1}^{I} \left\{
      \sum_{k=1}^{K_i - 1} \left[ \xi_{i,k}\cdot (\eta_i - \lambda) + \xi_{i,k+1} \right] v_{i,k}^* 
      + \xi_{i,K_i} \cdot (\eta_i - \lambda) \cdot v_{i,K_i}^* = 0
   \right\}
\end{equation*}
Since \(v_{i,k}\)'s are linearly independent, for any \(i \in \left\{i\dots I\right\}\):
\begin{equation*}
   \begin{split}
      \xi_{i,1} \cdot (\eta_i - \lambda) + \xi_{i,2} & = 0 \Rightarrow \xi_{i,2} = (-1) \cdot \xi_{i,1} \cdot (\eta_i - \lambda) \\
      \xi_{i,2} \cdot (\eta_i - \lambda) + \xi_{i,3} & = 0 \Rightarrow \xi_{i,3} = (-1)^2 \cdot \xi_{i,1} \cdot (\eta_i - \lambda)^2 \\
      & \vdots \\
      \xi_{i,K_i-1} \cdot (\eta_i - \lambda) + \xi_{i,K_i} & = 0 \Rightarrow \xi_{i,K_i} = (-1)^{K_i-1} \cdot \xi_{i,1} \cdot (\eta_i - \lambda)^{K_i-1} \\
      \xi_{i,K_i} \cdot (\eta_i - \lambda) & = 0
   \end{split}
\end{equation*}
Thus,
\begin{equation*}
   (-1)^{K_i-1} \cdot \xi_{i,1} \cdot (\eta_i - \lambda)^{K_i} = 0
\end{equation*}
Denote \(\xi_{i,1}\) as \(r_1 e^{\theta_1}\), \((\eta_i - \lambda)\) as \(r_2 e^{\theta_2}\). Since \(\text{Re} \lambda \ge 0, \text{Re}(\eta_i) < 0\), \(r_2 > 0\). On the other hand, the following equation suggests
\begin{equation*}
   r_1 r_2^{K_i-1} e^{j[\theta_1 + \theta_2 (K_i-1)]} = 0 
\end{equation*}
Thus, \(r_1\) has to be \(0\), which implies \(\xi_{i,1} = 0\). By recursion, \(\xi_{i,k} = 0, \forall k = 1\dots K_i\). Contradiction!

(\(1. \Rightarrow 4.\)) If \((A,B)\) is controllable, then from Theorem \ref@(thm:lticontrollable)'s sixth definition, we can freely assign the poles of \(A+BF\) via choosing \(F\) properly.

Otherwise, if \((A,B)\) is uncontrollable, then from Corollary \ref{cor:cayham-3} and proof of Theorem \ref{thm:lticontrollable} (\(6. \Rightarrow 1.\)), there exists a similarity transformation \(T\), s.t.
\begin{equation*}
   TAT^{-1} = \begin{bmatrix}
      \bar{A}_c & \bar{A}_{12} \\
      0 & \bar{A}_{\bar{c}}
   \end{bmatrix}, \quad TB = \begin{bmatrix}
      \bar{B}_c \\
      0
   \end{bmatrix}
\end{equation*}
and
\begin{equation*}
   \text{det}(A+BF-\lambda I) = \underbrace{\text{det}(\bar{A}_c + \bar{B}_c F_1 - \lambda I_1)}_{\chi_c(\lambda)} \cdot 
   \underbrace{\text{det}(\bar{A}_{\bar{c}} - \lambda I_2)}_{\chi_{\bar{c}}(\lambda)}
\end{equation*}
where \(\bar{A}_c \in \mathbb{C}^{k_1 \times k_1}\), \(I_1\) identity matrix of size \(k_1\), \([F_1,F_2] = FT^{-1}\), and \(k_1 = \text{rank} \mathcal{C}\). Additionally, \((\bar{A}_c, \bar{B}_c)\) is controllable. Thus, \(\chi_c(\lambda)\)'s zeros can be freely assigned by choosing proper \(F\), i.e., system modes with \(\chi_c(\lambda)\) is controllable, regardless of its stability. On the other hand, system modes with \(\chi_{\bar{c}}(\lambda)\) must be stable. Otherwise, we cannot affect it by assigning \(F\), which is a contradiction to statement (1). Therefore, \((TAT^{-1}, TB)\) is stabilizable. Since similarity transformation does not change stabilizability, \((A,B)\) is stabilizable.
\end{proof}

\subsection{Equivalent Statements for Detectability}\label{equivalent-statements-for-detectability}

Thanks to duality, we can directly write down the equivalent statements of observability without any additional proofs:

\begin{theorem}[Equivalent Statements for Detectability]
\protect\hypertarget{thm:ltidetectable}{}\label{thm:ltidetectable}

The following statements are equivalent \citep{zhou1996book-robust}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \((C,A)\) is detectable.
\item
  For all \(\lambda\) and \(x\) such that \(A x = \lambda x\) and \(\text{Re} \lambda \ge 0\), \(C x \ne 0\).
\item
  The matrix \(\begin{bmatrix} A - \lambda I \\ C \end{bmatrix}\) has full rank for all \(\text{Re} \lambda \ge 0\).
\item
  There exists a matrix \(L\) such that \(A+LC\) are Hurwitz.
\item
  \((A^*, C^*)\) is stabilizable.
\end{enumerate}

\end{theorem}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

  \bibliography{book.bib,packages.bib}

\end{document}
