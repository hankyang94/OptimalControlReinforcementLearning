\contentsline {chapter}{Preface}{5}{chapter*.2}%
\contentsline {section}{Feedback}{5}{section*.3}%
\contentsline {section}{Offerings}{5}{section*.4}%
\contentsline {subsubsection}{2025 Fall}{5}{section*.5}%
\contentsline {subsubsection}{2023 Fall}{5}{section*.6}%
\contentsline {chapter}{\numberline {1}Markov Decision Process}{7}{chapter.1}%
\contentsline {section}{\numberline {1.1}Finite-Horizon MDP}{7}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Value Functions}{9}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Policy Evaluation}{10}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Principle of Optimality}{16}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming}{18}{subsection.1.1.4}%
\contentsline {section}{\numberline {1.2}Infinite-Horizon MDP}{22}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Value Functions}{23}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Policy Evaluation}{24}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Principle of Optimality}{31}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Policy Improvement}{33}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Policy Iteration}{35}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Value Iteration}{41}{subsection.1.2.6}%
\contentsline {chapter}{\numberline {2}Value-based Reinforcement Learning}{47}{chapter.2}%
\contentsline {section}{\numberline {2.1}Tabular Methods}{47}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Policy Evaluation}{48}{subsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.1}Monte Carlo Estimation}{48}{subsubsection.2.1.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.2}Temporal-Difference Learning}{50}{subsubsection.2.1.1.2}%
\contentsline {subsubsection}{\numberline {2.1.1.3}Multi-Step TD Learning}{51}{subsubsection.2.1.1.3}%
\contentsline {subsubsection}{\numberline {2.1.1.4}Eligibility Traces and TD(\(\mitlambda \))}{52}{subsubsection.2.1.1.4}%
\contentsline {subsection}{\numberline {2.1.2}Convergence Proof of TD Learning}{57}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}On-Policy Control}{62}{subsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.3.1}Monte Carlo Control}{62}{subsubsection.2.1.3.1}%
\contentsline {subsubsection}{\numberline {2.1.3.2}SARSA (On-Policy TD Control)}{64}{subsubsection.2.1.3.2}%
\contentsline {subsection}{\numberline {2.1.4}Off-Policy Control}{65}{subsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.4.1}Importance Sampling for Policy Evaluation}{65}{subsubsection.2.1.4.1}%
\contentsline {subsubsection}{\numberline {2.1.4.2}Off-Policy Monte Carlo Control}{67}{subsubsection.2.1.4.2}%
\contentsline {subsubsection}{\numberline {2.1.4.3}Q-Learning}{68}{subsubsection.2.1.4.3}%
\contentsline {subsubsection}{\numberline {2.1.4.4}Double Q-Learning}{69}{subsubsection.2.1.4.4}%
\contentsline {section}{\numberline {2.2}Function Approximation}{72}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Basics of Continuous MDP}{72}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Policy Evaluation}{73}{subsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.2.1}Monte Carlo Estimation}{74}{subsubsection.2.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2.2}Semi-Gradient TD(0)}{75}{subsubsection.2.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}On-Policy Control}{78}{subsection.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.3.1}Semi-Gradient SARSA(0)}{78}{subsubsection.2.2.3.1}%
\contentsline {subsection}{\numberline {2.2.4}Off-Policy Control}{81}{subsection.2.2.4}%
\contentsline {subsubsection}{\numberline {2.2.4.1}Off-Policy Semi-Gradient TD(0)}{81}{subsubsection.2.2.4.1}%
\contentsline {subsubsection}{\numberline {2.2.4.2}Deep Q Network}{84}{subsubsection.2.2.4.2}%
\contentsline {chapter}{\numberline {3}Policy Gradient Methods}{89}{chapter.3}%
\contentsline {section}{\numberline {3.1}Gradient-based Optimization}{90}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Basic Setup}{90}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Gradient Ascent and Descent}{90}{subsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.2.1}Convergence Guarantees}{91}{subsubsection.3.1.2.1}%
\contentsline {subsection}{\numberline {3.1.3}Stochastic Gradients}{93}{subsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.3.1}Convergence Guarantees}{93}{subsubsection.3.1.3.1}%
\contentsline {subsection}{\numberline {3.1.4}Beyond Vanilla Gradient Methods}{95}{subsection.3.1.4}%
\contentsline {chapter}{\numberline {A}Convex Analysis and Optimization}{97}{appendix.A}%
\contentsline {section}{\numberline {A.1}Theory}{97}{section.A.1}%
\contentsline {subsection}{\numberline {A.1.1}Sets}{97}{subsection.A.1.1}%
\contentsline {subsection}{\numberline {A.1.2}Convex function}{99}{subsection.A.1.2}%
\contentsline {subsection}{\numberline {A.1.3}Lagrange dual}{100}{subsection.A.1.3}%
\contentsline {subsection}{\numberline {A.1.4}KKT condition}{102}{subsection.A.1.4}%
\contentsline {section}{\numberline {A.2}Practice}{103}{section.A.2}%
\contentsline {subsection}{\numberline {A.2.1}CVX Introduction}{103}{subsection.A.2.1}%
\contentsline {subsection}{\numberline {A.2.2}Linear Programming (LP)}{103}{subsection.A.2.2}%
\contentsline {subsection}{\numberline {A.2.3}Quadratic Programming (QP)}{104}{subsection.A.2.3}%
\contentsline {subsection}{\numberline {A.2.4}Quadratically Constrained Quadratic Programming (QCQP)}{106}{subsection.A.2.4}%
\contentsline {subsection}{\numberline {A.2.5}Second-Order Cone Programming (SOCP)}{108}{subsection.A.2.5}%
\contentsline {subsection}{\numberline {A.2.6}Semidefinite Programming (SDP)}{110}{subsection.A.2.6}%
\contentsline {subsection}{\numberline {A.2.7}CVXPY Introduction and Examples}{112}{subsection.A.2.7}%
\contentsline {subsubsection}{\numberline {A.2.7.1}LP}{112}{subsubsection.A.2.7.1}%
\contentsline {subsubsection}{\numberline {A.2.7.2}QP}{113}{subsubsection.A.2.7.2}%
\contentsline {subsubsection}{\numberline {A.2.7.3}QCQP}{114}{subsubsection.A.2.7.3}%
\contentsline {subsubsection}{\numberline {A.2.7.4}SOCP}{115}{subsubsection.A.2.7.4}%
\contentsline {subsubsection}{\numberline {A.2.7.5}SDP}{116}{subsubsection.A.2.7.5}%
\contentsline {chapter}{\numberline {B}Linear System Theory}{119}{appendix.B}%
\contentsline {section}{\numberline {B.1}Stability}{119}{section.B.1}%
\contentsline {subsection}{\numberline {B.1.1}Continuous-Time Stability}{119}{subsection.B.1.1}%
\contentsline {subsection}{\numberline {B.1.2}Discrete-Time Stability}{120}{subsection.B.1.2}%
\contentsline {subsection}{\numberline {B.1.3}Lyapunov Analysis}{121}{subsection.B.1.3}%
\contentsline {section}{\numberline {B.2}Controllability and Observability}{123}{section.B.2}%
\contentsline {subsection}{\numberline {B.2.1}Cayley-Hamilton Theorem}{124}{subsection.B.2.1}%
\contentsline {subsection}{\numberline {B.2.2}Equivalent Statements for Controllability}{128}{subsection.B.2.2}%
\contentsline {subsection}{\numberline {B.2.3}Duality}{132}{subsection.B.2.3}%
\contentsline {subsection}{\numberline {B.2.4}Equivalent Statements for Observability}{134}{subsection.B.2.4}%
\contentsline {section}{\numberline {B.3}Stabilizability And Detectability}{134}{section.B.3}%
\contentsline {subsection}{\numberline {B.3.1}Equivalent Statements for Stabilizability}{135}{subsection.B.3.1}%
\contentsline {subsection}{\numberline {B.3.2}Equivalent Statements for Detectability}{137}{subsection.B.3.2}%
