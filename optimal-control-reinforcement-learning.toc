\contentsline {chapter}{Preface}{5}{chapter*.2}%
\contentsline {section}{Feedback}{5}{section*.3}%
\contentsline {section}{Offerings}{5}{section*.4}%
\contentsline {subsubsection}{2025 Fall}{5}{section*.5}%
\contentsline {subsubsection}{2023 Fall}{5}{section*.6}%
\contentsline {chapter}{\numberline {1}Markov Decision Process}{7}{chapter.1}%
\contentsline {section}{\numberline {1.1}Finite-Horizon MDP}{7}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Value Functions}{9}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Policy Evaluation}{10}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Principle of Optimality}{16}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming}{18}{subsection.1.1.4}%
\contentsline {section}{\numberline {1.2}Infinite-Horizon MDP}{22}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Value Functions}{23}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Policy Evaluation}{24}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Principle of Optimality}{31}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Policy Improvement}{33}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Policy Iteration}{35}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Value Iteration}{41}{subsection.1.2.6}%
\contentsline {chapter}{\numberline {2}Value-based Reinforcement Learning}{47}{chapter.2}%
\contentsline {section}{\numberline {2.1}Tabular Methods}{47}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Policy Evaluation}{48}{subsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.1}Monte Carlo Estimation}{48}{subsubsection.2.1.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.2}Temporal-Difference Learning}{50}{subsubsection.2.1.1.2}%
\contentsline {subsubsection}{\numberline {2.1.1.3}Multi-Step TD Learning}{51}{subsubsection.2.1.1.3}%
\contentsline {subsubsection}{\numberline {2.1.1.4}Eligibility Traces and TD(\(\mitlambda \))}{52}{subsubsection.2.1.1.4}%
\contentsline {subsection}{\numberline {2.1.2}Convergence Proof of TD Learning}{57}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}On-Policy Control}{62}{subsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.3.1}Monte Carlo Control}{62}{subsubsection.2.1.3.1}%
\contentsline {subsubsection}{\numberline {2.1.3.2}SARSA (On-Policy TD Control)}{64}{subsubsection.2.1.3.2}%
\contentsline {subsection}{\numberline {2.1.4}Off-Policy Control}{65}{subsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.4.1}Importance Sampling for Policy Evaluation}{65}{subsubsection.2.1.4.1}%
\contentsline {subsubsection}{\numberline {2.1.4.2}Off-Policy Monte Carlo Control}{67}{subsubsection.2.1.4.2}%
\contentsline {subsubsection}{\numberline {2.1.4.3}Q-Learning}{68}{subsubsection.2.1.4.3}%
\contentsline {subsubsection}{\numberline {2.1.4.4}Double Q-Learning}{69}{subsubsection.2.1.4.4}%
\contentsline {section}{\numberline {2.2}Function Approximation}{72}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Basics of Continuous MDP}{72}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Policy Evaluation}{73}{subsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.2.1}Monte Carlo Estimation}{74}{subsubsection.2.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2.2}Semi-Gradient TD(0)}{75}{subsubsection.2.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}On-Policy Control}{78}{subsection.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.3.1}Semi-Gradient SARSA(0)}{78}{subsubsection.2.2.3.1}%
\contentsline {subsection}{\numberline {2.2.4}Off-Policy Control}{81}{subsection.2.2.4}%
\contentsline {subsubsection}{\numberline {2.2.4.1}Off-Policy Semi-Gradient TD(0)}{81}{subsubsection.2.2.4.1}%
\contentsline {subsubsection}{\numberline {2.2.4.2}Deep Q Network}{84}{subsubsection.2.2.4.2}%
\contentsline {chapter}{\numberline {3}Policy Gradient Methods}{89}{chapter.3}%
\contentsline {section}{\numberline {3.1}Gradient-based Optimization}{90}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Basic Setup}{90}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Gradient Ascent and Descent}{90}{subsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.2.1}Convergence Guarantees}{91}{subsubsection.3.1.2.1}%
\contentsline {subsection}{\numberline {3.1.3}Stochastic Gradients}{93}{subsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.3.1}Convergence Guarantees}{93}{subsubsection.3.1.3.1}%
\contentsline {subsection}{\numberline {3.1.4}Beyond Vanilla Gradient Methods}{95}{subsection.3.1.4}%
\contentsline {section}{\numberline {3.2}Policy Gradients}{95}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Setup}{95}{subsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.1.1}Policy models}{96}{subsubsection.3.2.1.1}%
\contentsline {subsection}{\numberline {3.2.2}The Policy Gradient Lemma}{97}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}REINFORCE}{99}{subsection.3.2.3}%
\contentsline {chapter}{\numberline {A}Convex Analysis and Optimization}{101}{appendix.A}%
\contentsline {section}{\numberline {A.1}Theory}{101}{section.A.1}%
\contentsline {subsection}{\numberline {A.1.1}Sets}{101}{subsection.A.1.1}%
\contentsline {subsection}{\numberline {A.1.2}Convex function}{103}{subsection.A.1.2}%
\contentsline {subsection}{\numberline {A.1.3}Lagrange dual}{104}{subsection.A.1.3}%
\contentsline {subsection}{\numberline {A.1.4}KKT condition}{106}{subsection.A.1.4}%
\contentsline {section}{\numberline {A.2}Practice}{107}{section.A.2}%
\contentsline {subsection}{\numberline {A.2.1}CVX Introduction}{107}{subsection.A.2.1}%
\contentsline {subsection}{\numberline {A.2.2}Linear Programming (LP)}{107}{subsection.A.2.2}%
\contentsline {subsection}{\numberline {A.2.3}Quadratic Programming (QP)}{108}{subsection.A.2.3}%
\contentsline {subsection}{\numberline {A.2.4}Quadratically Constrained Quadratic Programming (QCQP)}{110}{subsection.A.2.4}%
\contentsline {subsection}{\numberline {A.2.5}Second-Order Cone Programming (SOCP)}{112}{subsection.A.2.5}%
\contentsline {subsection}{\numberline {A.2.6}Semidefinite Programming (SDP)}{114}{subsection.A.2.6}%
\contentsline {subsection}{\numberline {A.2.7}CVXPY Introduction and Examples}{116}{subsection.A.2.7}%
\contentsline {subsubsection}{\numberline {A.2.7.1}LP}{116}{subsubsection.A.2.7.1}%
\contentsline {subsubsection}{\numberline {A.2.7.2}QP}{117}{subsubsection.A.2.7.2}%
\contentsline {subsubsection}{\numberline {A.2.7.3}QCQP}{118}{subsubsection.A.2.7.3}%
\contentsline {subsubsection}{\numberline {A.2.7.4}SOCP}{119}{subsubsection.A.2.7.4}%
\contentsline {subsubsection}{\numberline {A.2.7.5}SDP}{120}{subsubsection.A.2.7.5}%
\contentsline {chapter}{\numberline {B}Linear System Theory}{123}{appendix.B}%
\contentsline {section}{\numberline {B.1}Stability}{123}{section.B.1}%
\contentsline {subsection}{\numberline {B.1.1}Continuous-Time Stability}{123}{subsection.B.1.1}%
\contentsline {subsection}{\numberline {B.1.2}Discrete-Time Stability}{124}{subsection.B.1.2}%
\contentsline {subsection}{\numberline {B.1.3}Lyapunov Analysis}{125}{subsection.B.1.3}%
\contentsline {section}{\numberline {B.2}Controllability and Observability}{127}{section.B.2}%
\contentsline {subsection}{\numberline {B.2.1}Cayley-Hamilton Theorem}{128}{subsection.B.2.1}%
\contentsline {subsection}{\numberline {B.2.2}Equivalent Statements for Controllability}{132}{subsection.B.2.2}%
\contentsline {subsection}{\numberline {B.2.3}Duality}{136}{subsection.B.2.3}%
\contentsline {subsection}{\numberline {B.2.4}Equivalent Statements for Observability}{138}{subsection.B.2.4}%
\contentsline {section}{\numberline {B.3}Stabilizability And Detectability}{138}{section.B.3}%
\contentsline {subsection}{\numberline {B.3.1}Equivalent Statements for Stabilizability}{139}{subsection.B.3.1}%
\contentsline {subsection}{\numberline {B.3.2}Equivalent Statements for Detectability}{141}{subsection.B.3.2}%
