\contentsline {chapter}{Preface}{5}{chapter*.2}%
\contentsline {section}{Feedback}{5}{section*.3}%
\contentsline {section}{Offerings}{5}{section*.4}%
\contentsline {subsubsection}{2025 Fall}{5}{section*.5}%
\contentsline {subsubsection}{2023 Fall}{5}{section*.6}%
\contentsline {chapter}{\numberline {1}Markov Decision Process}{7}{chapter.1}%
\contentsline {section}{\numberline {1.1}Finite-Horizon MDP}{7}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Value Functions}{9}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Policy Evaluation}{10}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Principle of Optimality}{16}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming}{18}{subsection.1.1.4}%
\contentsline {section}{\numberline {1.2}Infinite-Horizon MDP}{22}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Value Functions}{23}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Policy Evaluation}{24}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Principle of Optimality}{31}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Policy Improvement}{33}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Policy Iteration}{35}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Value Iteration}{41}{subsection.1.2.6}%
\contentsline {chapter}{\numberline {2}Value-based Reinforcement Learning}{47}{chapter.2}%
\contentsline {section}{\numberline {2.1}Tabular Methods}{47}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Policy Evaluation}{48}{subsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.1}Monte Carlo Estimation}{48}{subsubsection.2.1.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.2}Temporal-Difference Learning}{50}{subsubsection.2.1.1.2}%
\contentsline {subsubsection}{\numberline {2.1.1.3}Multi-Step TD Learning}{51}{subsubsection.2.1.1.3}%
\contentsline {subsubsection}{\numberline {2.1.1.4}Eligibility Traces and TD(\(\mitlambda \))}{52}{subsubsection.2.1.1.4}%
\contentsline {subsection}{\numberline {2.1.2}Convergence Proof of TD Learning}{57}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}On-Policy Control}{62}{subsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.3.1}Monte Carlo Control}{62}{subsubsection.2.1.3.1}%
\contentsline {subsubsection}{\numberline {2.1.3.2}SARSA (On-Policy TD Control)}{64}{subsubsection.2.1.3.2}%
\contentsline {subsection}{\numberline {2.1.4}Off-Policy Control}{65}{subsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.4.1}Importance Sampling for Policy Evaluation}{65}{subsubsection.2.1.4.1}%
\contentsline {subsubsection}{\numberline {2.1.4.2}Off-Policy Monte Carlo Control}{67}{subsubsection.2.1.4.2}%
\contentsline {subsubsection}{\numberline {2.1.4.3}Q-Learning}{68}{subsubsection.2.1.4.3}%
\contentsline {subsubsection}{\numberline {2.1.4.4}Double Q-Learning}{69}{subsubsection.2.1.4.4}%
\contentsline {section}{\numberline {2.2}Function Approximation}{72}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Basics of Continuous MDP}{72}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Policy Evaluation}{73}{subsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.2.1}Monte Carlo Estimation}{74}{subsubsection.2.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2.2}Semi-Gradient TD(0)}{75}{subsubsection.2.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}On-Policy Control}{78}{subsection.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.3.1}Semi-Gradient SARSA(0)}{78}{subsubsection.2.2.3.1}%
\contentsline {subsection}{\numberline {2.2.4}Off-Policy Control}{82}{subsection.2.2.4}%
\contentsline {subsubsection}{\numberline {2.2.4.1}Off-Policy Semi-Gradient TD(0)}{82}{subsubsection.2.2.4.1}%
\contentsline {subsubsection}{\numberline {2.2.4.2}Deep Q Network}{85}{subsubsection.2.2.4.2}%
\contentsline {chapter}{\numberline {3}Policy Gradient Methods}{89}{chapter.3}%
\contentsline {section}{\numberline {3.1}Gradient-based Optimization}{90}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Basic Setup}{90}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Gradient Ascent and Descent}{90}{subsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.2.1}Convergence Guarantees}{91}{subsubsection.3.1.2.1}%
\contentsline {subsection}{\numberline {3.1.3}Stochastic Gradients}{92}{subsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.3.1}Convergence Guarantees}{93}{subsubsection.3.1.3.1}%
\contentsline {subsection}{\numberline {3.1.4}Beyond Vanilla Gradient Methods}{94}{subsection.3.1.4}%
\contentsline {section}{\numberline {3.2}Policy Gradients}{95}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Setup}{95}{subsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.1.1}Policy models}{95}{subsubsection.3.2.1.1}%
\contentsline {subsection}{\numberline {3.2.2}The Policy Gradient Lemma}{96}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}REINFORCE}{98}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}Baselines and Variance Reduction}{101}{subsection.3.2.4}%
\contentsline {subsubsection}{\numberline {3.2.4.1}Baseline}{101}{subsubsection.3.2.4.1}%
\contentsline {subsubsection}{\numberline {3.2.4.2}Optimal Baseline and Advantage}{103}{subsubsection.3.2.4.2}%
\contentsline {subsubsection}{\numberline {3.2.4.3}Intuition for the Advantage}{105}{subsubsection.3.2.4.3}%
\contentsline {subsubsection}{\numberline {3.2.4.4}REINFORCE with a Learned Value Baseline}{106}{subsubsection.3.2.4.4}%
\contentsline {section}{\numberline {3.3}Actor--Critic Methods}{109}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Anatomy of an Actor--Critic}{109}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}On-Policy Actor--Critic with TD(0)}{109}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Generalized Advantage Estimation (GAE)}{111}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Off-Policy Actor--Critic}{113}{subsection.3.3.4}%
\contentsline {chapter}{\numberline {A}Convex Analysis and Optimization}{119}{appendix.A}%
\contentsline {section}{\numberline {A.1}Theory}{119}{section.A.1}%
\contentsline {subsection}{\numberline {A.1.1}Sets}{119}{subsection.A.1.1}%
\contentsline {subsection}{\numberline {A.1.2}Convex function}{121}{subsection.A.1.2}%
\contentsline {subsection}{\numberline {A.1.3}Lagrange dual}{122}{subsection.A.1.3}%
\contentsline {subsection}{\numberline {A.1.4}KKT condition}{124}{subsection.A.1.4}%
\contentsline {section}{\numberline {A.2}Practice}{125}{section.A.2}%
\contentsline {subsection}{\numberline {A.2.1}CVX Introduction}{125}{subsection.A.2.1}%
\contentsline {subsection}{\numberline {A.2.2}Linear Programming (LP)}{125}{subsection.A.2.2}%
\contentsline {subsection}{\numberline {A.2.3}Quadratic Programming (QP)}{126}{subsection.A.2.3}%
\contentsline {subsection}{\numberline {A.2.4}Quadratically Constrained Quadratic Programming (QCQP)}{128}{subsection.A.2.4}%
\contentsline {subsection}{\numberline {A.2.5}Second-Order Cone Programming (SOCP)}{130}{subsection.A.2.5}%
\contentsline {subsection}{\numberline {A.2.6}Semidefinite Programming (SDP)}{132}{subsection.A.2.6}%
\contentsline {subsection}{\numberline {A.2.7}CVXPY Introduction and Examples}{134}{subsection.A.2.7}%
\contentsline {subsubsection}{\numberline {A.2.7.1}LP}{134}{subsubsection.A.2.7.1}%
\contentsline {subsubsection}{\numberline {A.2.7.2}QP}{135}{subsubsection.A.2.7.2}%
\contentsline {subsubsection}{\numberline {A.2.7.3}QCQP}{136}{subsubsection.A.2.7.3}%
\contentsline {subsubsection}{\numberline {A.2.7.4}SOCP}{137}{subsubsection.A.2.7.4}%
\contentsline {subsubsection}{\numberline {A.2.7.5}SDP}{138}{subsubsection.A.2.7.5}%
\contentsline {chapter}{\numberline {B}Linear System Theory}{141}{appendix.B}%
\contentsline {section}{\numberline {B.1}Stability}{141}{section.B.1}%
\contentsline {subsection}{\numberline {B.1.1}Continuous-Time Stability}{141}{subsection.B.1.1}%
\contentsline {subsection}{\numberline {B.1.2}Discrete-Time Stability}{142}{subsection.B.1.2}%
\contentsline {subsection}{\numberline {B.1.3}Lyapunov Analysis}{143}{subsection.B.1.3}%
\contentsline {section}{\numberline {B.2}Controllability and Observability}{145}{section.B.2}%
\contentsline {subsection}{\numberline {B.2.1}Cayley-Hamilton Theorem}{146}{subsection.B.2.1}%
\contentsline {subsection}{\numberline {B.2.2}Equivalent Statements for Controllability}{150}{subsection.B.2.2}%
\contentsline {subsection}{\numberline {B.2.3}Duality}{154}{subsection.B.2.3}%
\contentsline {subsection}{\numberline {B.2.4}Equivalent Statements for Observability}{156}{subsection.B.2.4}%
\contentsline {section}{\numberline {B.3}Stabilizability And Detectability}{156}{section.B.3}%
\contentsline {subsection}{\numberline {B.3.1}Equivalent Statements for Stabilizability}{157}{subsection.B.3.1}%
\contentsline {subsection}{\numberline {B.3.2}Equivalent Statements for Detectability}{159}{subsection.B.3.2}%
