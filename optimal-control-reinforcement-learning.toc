\contentsline {chapter}{Preface}{5}{chapter*.2}%
\contentsline {section}{Feedback}{5}{section*.3}%
\contentsline {section}{Offerings}{5}{section*.4}%
\contentsline {subsubsection}{2025 Fall}{5}{section*.5}%
\contentsline {subsubsection}{2023 Fall}{5}{section*.6}%
\contentsline {chapter}{\numberline {1}Markov Decision Process}{7}{chapter.1}%
\contentsline {section}{\numberline {1.1}Finite-Horizon MDP}{7}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Value Functions}{9}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Policy Evaluation}{10}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Principle of Optimality}{16}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming}{18}{subsection.1.1.4}%
\contentsline {section}{\numberline {1.2}Infinite-Horizon MDP}{22}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Value Functions}{23}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Policy Evaluation}{24}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Principle of Optimality}{31}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Policy Improvement}{33}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Policy Iteration}{35}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Value Iteration}{41}{subsection.1.2.6}%
\contentsline {chapter}{\numberline {2}Value-based Reinforcement Learning}{47}{chapter.2}%
\contentsline {section}{\numberline {2.1}Tabular Methods}{47}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Policy Evaluation}{48}{subsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.1}Monte Carlo Estimation}{48}{subsubsection.2.1.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.2}Temporal-Difference Learning}{50}{subsubsection.2.1.1.2}%
\contentsline {subsubsection}{\numberline {2.1.1.3}Multi-Step TD Learning}{51}{subsubsection.2.1.1.3}%
\contentsline {subsubsection}{\numberline {2.1.1.4}Eligibility Traces and TD(\(\mitlambda \))}{52}{subsubsection.2.1.1.4}%
\contentsline {subsection}{\numberline {2.1.2}Convergence Proof of TD Learning}{57}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}On-Policy Control}{58}{subsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.3.1}Monte Carlo Control}{58}{subsubsection.2.1.3.1}%
\contentsline {subsubsection}{\numberline {2.1.3.2}SARSA (On-Policy TD Control)}{60}{subsubsection.2.1.3.2}%
\contentsline {subsection}{\numberline {2.1.4}Off-Policy Control}{61}{subsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.4.1}Importance Sampling for Policy Evaluation}{61}{subsubsection.2.1.4.1}%
\contentsline {subsubsection}{\numberline {2.1.4.2}Off-Policy Monte Carlo Control}{63}{subsubsection.2.1.4.2}%
\contentsline {subsubsection}{\numberline {2.1.4.3}Q-Learning}{64}{subsubsection.2.1.4.3}%
\contentsline {subsubsection}{\numberline {2.1.4.4}Double Q-Learning}{64}{subsubsection.2.1.4.4}%
\contentsline {section}{\numberline {2.2}Function Approximation}{67}{section.2.2}%
\contentsline {chapter}{\numberline {3}Policy Gradients}{69}{chapter.3}%
\contentsline {chapter}{\numberline {A}Convex Analysis and Optimization}{71}{appendix.A}%
\contentsline {section}{\numberline {A.1}Theory}{71}{section.A.1}%
\contentsline {subsection}{\numberline {A.1.1}Sets}{71}{subsection.A.1.1}%
\contentsline {subsection}{\numberline {A.1.2}Convex function}{73}{subsection.A.1.2}%
\contentsline {subsection}{\numberline {A.1.3}Lagrange dual}{74}{subsection.A.1.3}%
\contentsline {subsection}{\numberline {A.1.4}KKT condition}{76}{subsection.A.1.4}%
\contentsline {section}{\numberline {A.2}Practice}{77}{section.A.2}%
\contentsline {subsection}{\numberline {A.2.1}CVX Introduction}{77}{subsection.A.2.1}%
\contentsline {subsection}{\numberline {A.2.2}Linear Programming (LP)}{77}{subsection.A.2.2}%
\contentsline {subsection}{\numberline {A.2.3}Quadratic Programming (QP)}{78}{subsection.A.2.3}%
\contentsline {subsection}{\numberline {A.2.4}Quadratically Constrained Quadratic Programming (QCQP)}{80}{subsection.A.2.4}%
\contentsline {subsection}{\numberline {A.2.5}Second-Order Cone Programming (SOCP)}{82}{subsection.A.2.5}%
\contentsline {subsection}{\numberline {A.2.6}Semidefinite Programming (SDP)}{84}{subsection.A.2.6}%
\contentsline {subsection}{\numberline {A.2.7}CVXPY Introduction and Examples}{86}{subsection.A.2.7}%
\contentsline {subsubsection}{\numberline {A.2.7.1}LP}{86}{subsubsection.A.2.7.1}%
\contentsline {subsubsection}{\numberline {A.2.7.2}QP}{87}{subsubsection.A.2.7.2}%
\contentsline {subsubsection}{\numberline {A.2.7.3}QCQP}{88}{subsubsection.A.2.7.3}%
\contentsline {subsubsection}{\numberline {A.2.7.4}SOCP}{89}{subsubsection.A.2.7.4}%
\contentsline {subsubsection}{\numberline {A.2.7.5}SDP}{90}{subsubsection.A.2.7.5}%
\contentsline {chapter}{\numberline {B}Linear System Theory}{93}{appendix.B}%
\contentsline {section}{\numberline {B.1}Stability}{93}{section.B.1}%
\contentsline {subsection}{\numberline {B.1.1}Continuous-Time Stability}{93}{subsection.B.1.1}%
\contentsline {subsection}{\numberline {B.1.2}Discrete-Time Stability}{94}{subsection.B.1.2}%
\contentsline {subsection}{\numberline {B.1.3}Lyapunov Analysis}{95}{subsection.B.1.3}%
\contentsline {section}{\numberline {B.2}Controllability and Observability}{97}{section.B.2}%
\contentsline {subsection}{\numberline {B.2.1}Cayley-Hamilton Theorem}{98}{subsection.B.2.1}%
\contentsline {subsection}{\numberline {B.2.2}Equivalent Statements for Controllability}{102}{subsection.B.2.2}%
\contentsline {subsection}{\numberline {B.2.3}Duality}{106}{subsection.B.2.3}%
\contentsline {subsection}{\numberline {B.2.4}Equivalent Statements for Observability}{108}{subsection.B.2.4}%
\contentsline {section}{\numberline {B.3}Stabilizability And Detectability}{108}{section.B.3}%
\contentsline {subsection}{\numberline {B.3.1}Equivalent Statements for Stabilizability}{109}{subsection.B.3.1}%
\contentsline {subsection}{\numberline {B.3.2}Equivalent Statements for Detectability}{111}{subsection.B.3.2}%
