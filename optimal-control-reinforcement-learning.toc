\contentsline {chapter}{Preface}{5}{chapter*.2}%
\contentsline {section}{Feedback}{5}{section*.3}%
\contentsline {section}{Offerings}{5}{section*.4}%
\contentsline {subsubsection}{2025 Fall}{5}{section*.5}%
\contentsline {subsubsection}{2023 Fall}{5}{section*.6}%
\contentsline {chapter}{\numberline {1}Markov Decision Process}{7}{chapter.1}%
\contentsline {section}{\numberline {1.1}Finite-Horizon MDP}{7}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Value Functions}{9}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Policy Evaluation}{10}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Principle of Optimality}{16}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Dynamic Programming}{18}{subsection.1.1.4}%
\contentsline {section}{\numberline {1.2}Infinite-Horizon MDP}{22}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Value Functions}{23}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Policy Evaluation}{24}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Principle of Optimality}{31}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}Policy Improvement}{33}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}Policy Iteration}{35}{subsection.1.2.5}%
\contentsline {subsection}{\numberline {1.2.6}Value Iteration}{41}{subsection.1.2.6}%
\contentsline {chapter}{\numberline {2}Value-based Reinforcement Learning}{47}{chapter.2}%
\contentsline {section}{\numberline {2.1}Tabular Methods}{47}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Policy Evaluation}{48}{subsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.1}Monte Carlo Estimation}{48}{subsubsection.2.1.1.1}%
\contentsline {subsubsection}{\numberline {2.1.1.2}Temporal-Difference Learning}{50}{subsubsection.2.1.1.2}%
\contentsline {subsubsection}{\numberline {2.1.1.3}Multi-Step TD Learning}{51}{subsubsection.2.1.1.3}%
\contentsline {subsubsection}{\numberline {2.1.1.4}Eligibility Traces and TD(\(\mitlambda \))}{52}{subsubsection.2.1.1.4}%
\contentsline {subsection}{\numberline {2.1.2}Convergence Proof of TD Learning}{57}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}On-Policy Control}{62}{subsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.3.1}Monte Carlo Control}{62}{subsubsection.2.1.3.1}%
\contentsline {subsubsection}{\numberline {2.1.3.2}SARSA (On-Policy TD Control)}{64}{subsubsection.2.1.3.2}%
\contentsline {subsection}{\numberline {2.1.4}Off-Policy Control}{65}{subsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.4.1}Importance Sampling for Policy Evaluation}{65}{subsubsection.2.1.4.1}%
\contentsline {subsubsection}{\numberline {2.1.4.2}Off-Policy Monte Carlo Control}{67}{subsubsection.2.1.4.2}%
\contentsline {subsubsection}{\numberline {2.1.4.3}Q-Learning}{68}{subsubsection.2.1.4.3}%
\contentsline {subsubsection}{\numberline {2.1.4.4}Double Q-Learning}{69}{subsubsection.2.1.4.4}%
\contentsline {section}{\numberline {2.2}Function Approximation}{72}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Basics of Continuous MDP}{72}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Policy Evaluation}{73}{subsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.2.1}Monte Carlo Estimation}{74}{subsubsection.2.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2.2}Semi-Gradient TD(0)}{75}{subsubsection.2.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}On-Policy Control}{78}{subsection.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.3.1}Semi-Gradient SARSA(0)}{78}{subsubsection.2.2.3.1}%
\contentsline {subsection}{\numberline {2.2.4}Off-Policy Control}{82}{subsection.2.2.4}%
\contentsline {subsubsection}{\numberline {2.2.4.1}Off-Policy Semi-Gradient TD(0)}{82}{subsubsection.2.2.4.1}%
\contentsline {subsubsection}{\numberline {2.2.4.2}Deep Q Network}{85}{subsubsection.2.2.4.2}%
\contentsline {chapter}{\numberline {3}Policy Gradient Methods}{89}{chapter.3}%
\contentsline {section}{\numberline {3.1}Gradient-based Optimization}{90}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Basic Setup}{90}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Gradient Ascent and Descent}{90}{subsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.2.1}Convergence Guarantees}{91}{subsubsection.3.1.2.1}%
\contentsline {subsection}{\numberline {3.1.3}Stochastic Gradients}{92}{subsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.3.1}Convergence Guarantees}{93}{subsubsection.3.1.3.1}%
\contentsline {subsection}{\numberline {3.1.4}Beyond Vanilla Gradient Methods}{94}{subsection.3.1.4}%
\contentsline {section}{\numberline {3.2}Policy Gradients}{95}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Setup}{95}{subsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.1.1}Policy models}{95}{subsubsection.3.2.1.1}%
\contentsline {subsection}{\numberline {3.2.2}The Policy Gradient Lemma}{96}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}REINFORCE}{98}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}Baselines and Variance Reduction}{101}{subsection.3.2.4}%
\contentsline {subsubsection}{\numberline {3.2.4.1}Baseline}{101}{subsubsection.3.2.4.1}%
\contentsline {subsubsection}{\numberline {3.2.4.2}Optimal Baseline and Advantage}{103}{subsubsection.3.2.4.2}%
\contentsline {subsubsection}{\numberline {3.2.4.3}Intuition for the Advantage}{105}{subsubsection.3.2.4.3}%
\contentsline {subsubsection}{\numberline {3.2.4.4}REINFORCE with a Learned Value Baseline}{106}{subsubsection.3.2.4.4}%
\contentsline {section}{\numberline {3.3}Actor--Critic Methods}{109}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Anatomy of an Actor--Critic}{109}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}On-Policy Actor--Critic with TD(0)}{109}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Generalized Advantage Estimation (GAE)}{111}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Off-Policy Actor--Critic}{113}{subsection.3.3.4}%
\contentsline {section}{\numberline {3.4}Advanced Policy Gradients}{117}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Revisiting Generalized Policy Iteration}{117}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Performance Difference Lemma}{119}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Trust Region Constraint}{120}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Natural Policy Gradient}{121}{subsection.3.4.4}%
\contentsline {subsection}{\numberline {3.4.5}Proof of Fisher Information}{122}{subsection.3.4.5}%
\contentsline {subsection}{\numberline {3.4.6}Trust Region Policy Optimization}{124}{subsection.3.4.6}%
\contentsline {subsubsection}{\numberline {3.4.6.1}Backtracking Line Search}{125}{subsubsection.3.4.6.1}%
\contentsline {subsection}{\numberline {3.4.7}Proximal Policy Optimization}{126}{subsection.3.4.7}%
\contentsline {subsubsection}{\numberline {3.4.7.1}Gradient of the KL--Regularized Surrogate}{126}{subsubsection.3.4.7.1}%
\contentsline {subsubsection}{\numberline {3.4.7.2}From the Lagrangian Relaxation to PPO Updates}{126}{subsubsection.3.4.7.2}%
\contentsline {subsection}{\numberline {3.4.8}Soft Actor--Critic}{129}{subsection.3.4.8}%
\contentsline {subsubsection}{\numberline {3.4.8.1}SAC for Discrete Actions}{129}{subsubsection.3.4.8.1}%
\contentsline {subsubsection}{\numberline {3.4.8.2}SAC for Continuous Actions}{132}{subsubsection.3.4.8.2}%
\contentsline {subsection}{\numberline {3.4.9}Deterministic Policy Gradient}{136}{subsection.3.4.9}%
\contentsline {section}{\numberline {3.5}Model-based Policy Optimization}{140}{section.3.5}%
\contentsline {chapter}{\numberline {4}Model-based Planning and Optimization}{143}{chapter.4}%
\contentsline {section}{\numberline {4.1}Linear Quadratic Regulator}{144}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Finite-Horizon LQR}{144}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Infinite-Horizon LQR}{146}{subsection.4.1.2}%
\contentsline {chapter}{\numberline {5}Advanced Materials}{151}{chapter.5}%
\contentsline {chapter}{\numberline {A}Convex Analysis and Optimization}{153}{appendix.A}%
\contentsline {section}{\numberline {A.1}Theory}{153}{section.A.1}%
\contentsline {subsection}{\numberline {A.1.1}Sets}{153}{subsection.A.1.1}%
\contentsline {subsection}{\numberline {A.1.2}Convex function}{155}{subsection.A.1.2}%
\contentsline {subsection}{\numberline {A.1.3}Lagrange dual}{156}{subsection.A.1.3}%
\contentsline {subsection}{\numberline {A.1.4}KKT condition}{158}{subsection.A.1.4}%
\contentsline {section}{\numberline {A.2}Practice}{159}{section.A.2}%
\contentsline {subsection}{\numberline {A.2.1}CVX Introduction}{159}{subsection.A.2.1}%
\contentsline {subsection}{\numberline {A.2.2}Linear Programming (LP)}{159}{subsection.A.2.2}%
\contentsline {subsection}{\numberline {A.2.3}Quadratic Programming (QP)}{160}{subsection.A.2.3}%
\contentsline {subsection}{\numberline {A.2.4}Quadratically Constrained Quadratic Programming (QCQP)}{162}{subsection.A.2.4}%
\contentsline {subsection}{\numberline {A.2.5}Second-Order Cone Programming (SOCP)}{164}{subsection.A.2.5}%
\contentsline {subsection}{\numberline {A.2.6}Semidefinite Programming (SDP)}{166}{subsection.A.2.6}%
\contentsline {subsection}{\numberline {A.2.7}CVXPY Introduction and Examples}{168}{subsection.A.2.7}%
\contentsline {subsubsection}{\numberline {A.2.7.1}LP}{168}{subsubsection.A.2.7.1}%
\contentsline {subsubsection}{\numberline {A.2.7.2}QP}{169}{subsubsection.A.2.7.2}%
\contentsline {subsubsection}{\numberline {A.2.7.3}QCQP}{170}{subsubsection.A.2.7.3}%
\contentsline {subsubsection}{\numberline {A.2.7.4}SOCP}{171}{subsubsection.A.2.7.4}%
\contentsline {subsubsection}{\numberline {A.2.7.5}SDP}{172}{subsubsection.A.2.7.5}%
\contentsline {chapter}{\numberline {B}Linear System Theory}{175}{appendix.B}%
\contentsline {section}{\numberline {B.1}Stability}{175}{section.B.1}%
\contentsline {subsection}{\numberline {B.1.1}Continuous-Time Stability}{175}{subsection.B.1.1}%
\contentsline {subsection}{\numberline {B.1.2}Discrete-Time Stability}{176}{subsection.B.1.2}%
\contentsline {subsection}{\numberline {B.1.3}Lyapunov Analysis}{177}{subsection.B.1.3}%
\contentsline {section}{\numberline {B.2}Controllability and Observability}{179}{section.B.2}%
\contentsline {subsection}{\numberline {B.2.1}Cayley-Hamilton Theorem}{180}{subsection.B.2.1}%
\contentsline {subsection}{\numberline {B.2.2}Equivalent Statements for Controllability}{184}{subsection.B.2.2}%
\contentsline {subsection}{\numberline {B.2.3}Duality}{188}{subsection.B.2.3}%
\contentsline {subsection}{\numberline {B.2.4}Equivalent Statements for Observability}{190}{subsection.B.2.4}%
\contentsline {section}{\numberline {B.3}Stabilizability And Detectability}{190}{section.B.3}%
\contentsline {subsection}{\numberline {B.3.1}Equivalent Statements for Stabilizability}{191}{subsection.B.3.1}%
\contentsline {subsection}{\numberline {B.3.2}Equivalent Statements for Detectability}{193}{subsection.B.3.2}%
