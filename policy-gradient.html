<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-10-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="value-rl.html"/>
<link rel="next" href="appconvex.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.1.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.1.4" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.1.4</b> Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.2</b> Infinite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mdp.html"><a href="mdp.html#value-functions"><i class="fa fa-check"></i><b>1.2.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation-1"><i class="fa fa-check"></i><b>1.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.2.3" data-path="mdp.html"><a href="mdp.html#principle-of-optimality"><i class="fa fa-check"></i><b>1.2.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.2.4" data-path="mdp.html"><a href="mdp.html#policy-improvement"><i class="fa fa-check"></i><b>1.2.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="1.2.5" data-path="mdp.html"><a href="mdp.html#policy-iteration"><i class="fa fa-check"></i><b>1.2.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="1.2.6" data-path="mdp.html"><a href="mdp.html#value-iteration"><i class="fa fa-check"></i><b>1.2.6</b> Value Iteration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="value-rl.html"><a href="value-rl.html"><i class="fa fa-check"></i><b>2</b> Value-based Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="value-rl.html"><a href="value-rl.html#tabular-methods"><i class="fa fa-check"></i><b>2.1</b> Tabular Methods</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-2"><i class="fa fa-check"></i><b>2.1.1</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.1.2" data-path="value-rl.html"><a href="value-rl.html#value-rl-convergence-td"><i class="fa fa-check"></i><b>2.1.2</b> Convergence Proof of TD Learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control"><i class="fa fa-check"></i><b>2.1.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.1.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control"><i class="fa fa-check"></i><b>2.1.4</b> Off-Policy Control</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="value-rl.html"><a href="value-rl.html#function-approximation"><i class="fa fa-check"></i><b>2.2</b> Function Approximation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="value-rl.html"><a href="value-rl.html#basics-of-continuous-mdp"><i class="fa fa-check"></i><b>2.2.1</b> Basics of Continuous MDP</a></li>
<li class="chapter" data-level="2.2.2" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-3"><i class="fa fa-check"></i><b>2.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.2.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control-1"><i class="fa fa-check"></i><b>2.2.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.2.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control-1"><i class="fa fa-check"></i><b>2.2.4</b> Off-Policy Control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="policy-gradient.html"><a href="policy-gradient.html"><i class="fa fa-check"></i><b>3</b> Policy Gradient Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-optimization"><i class="fa fa-check"></i><b>3.1</b> Gradient-based Optimization</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="policy-gradient.html"><a href="policy-gradient.html#basic-setup"><i class="fa fa-check"></i><b>3.1.1</b> Basic Setup</a></li>
<li class="chapter" data-level="3.1.2" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-ascent-and-descent"><i class="fa fa-check"></i><b>3.1.2</b> Gradient Ascent and Descent</a></li>
<li class="chapter" data-level="3.1.3" data-path="policy-gradient.html"><a href="policy-gradient.html#stochastic-gradients"><i class="fa fa-check"></i><b>3.1.3</b> Stochastic Gradients</a></li>
<li class="chapter" data-level="3.1.4" data-path="policy-gradient.html"><a href="policy-gradient.html#beyond-vanilla-gradient-methods"><i class="fa fa-check"></i><b>3.1.4</b> Beyond Vanilla Gradient Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#policy-gradients"><i class="fa fa-check"></i><b>3.2</b> Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="policy-gradient.html"><a href="policy-gradient.html#setup"><i class="fa fa-check"></i><b>3.2.1</b> Setup</a></li>
<li class="chapter" data-level="3.2.2" data-path="policy-gradient.html"><a href="policy-gradient.html#the-policy-gradient-lemma"><i class="fa fa-check"></i><b>3.2.2</b> The Policy Gradient Lemma</a></li>
<li class="chapter" data-level="3.2.3" data-path="policy-gradient.html"><a href="policy-gradient.html#reinforce"><i class="fa fa-check"></i><b>3.2.3</b> REINFORCE</a></li>
<li class="chapter" data-level="3.2.4" data-path="policy-gradient.html"><a href="policy-gradient.html#baselines-and-variance-reduction"><i class="fa fa-check"></i><b>3.2.4</b> Baselines and Variance Reduction</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="policy-gradient" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Policy Gradient Methods<a href="policy-gradient.html#policy-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="value-rl.html#value-rl">2</a>, we relaxed two key assumptions of the MDP introduced in Chapter <a href="mdp.html#mdp">1</a>:</p>
<ul>
<li><strong>Unknown dynamics</strong>: the transition function <span class="math inline">\(P\)</span> was no longer assumed to be known.<br />
</li>
<li><strong>Continuous states</strong>: the state space <span class="math inline">\(\mathcal{S}\)</span> was extended from finite to continuous.</li>
</ul>
<p>When only the dynamics are unknown but the MDP remains tabular, we introduced generalized versions of policy iteration (e.g., SARSA) and value iteration (e.g., Q-learning). These algorithms can recover near-optimal value functions with strong convergence guarantees.</p>
<p>When both the dynamics are unknown and the state space is continuous, tabular methods become infeasible. In this setting, we employed function approximation to represent value functions, and generalized SARSA and Q-learning accordingly. We also introduced stabilization techniques such as experience replay and target networks to ensure more reliable learning.</p>
<hr />
<p>In this chapter, we relax a third assumption: the action space <span class="math inline">\(\mathcal{A}\)</span> is also continuous. This setting captures many important real-world systems, such as autonomous vehicles and robots. Handling continuous actions requires a departure from the value-based methods of Chapter <a href="value-rl.html#value-rl">2</a>. The key difficulty is that even if we had access to a near-optimal action-value function <span class="math inline">\(Q(s,a)\)</span>, selecting the control action requires solving
<span class="math display">\[
\max_a Q(s,a),
\]</span>
which is often computationally expensive and can lead to suboptimal solutions.</p>
<p>To address this challenge, we introduce a new paradigm: policy gradient methods. Rather than learning value functions to derive policies indirectly, we directly optimize parameterized policies using gradient-based methods.</p>
<p>We begin this chapter by reviewing the fundamentals of gradient-based optimization, and then build upon them to develop algorithms for searching optimal policies via policy gradients.</p>
<div id="gradient-optimization" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Gradient-based Optimization<a href="policy-gradient.html#gradient-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gradient-based optimization is the workhorse behind most modern machine learning algorithms, including policy gradient methods. The central idea is to iteratively update the parameters of a model in the direction that most improves an objective function.</p>
<div id="basic-setup" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Basic Setup<a href="policy-gradient.html#basic-setup" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a differentiable objective function <span class="math inline">\(J(\theta)\)</span>, where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> represents the parameter vector. The goal is to find
<span class="math display">\[
\theta^\star \in \arg\max_\theta J(\theta).
\]</span></p>
<p>The gradient of the objective with respect to the parameters,
<span class="math display">\[
\nabla_\theta J(\theta) =
\begin{bmatrix}
\frac{\partial J}{\partial \theta_1} &amp;
\frac{\partial J}{\partial \theta_2} &amp;
\cdots &amp;
\frac{\partial J}{\partial \theta_d}
\end{bmatrix}^\top,
\]</span>
provides the local direction of steepest ascent. Gradient-based optimization uses this direction to iteratively update the parameters. Note that modern machine learning software tools such as PyTorch allow the user to conveniently query the gradient of any function <span class="math inline">\(J\)</span> defined by neural networks.</p>
</div>
<div id="gradient-ascent-and-descent" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Gradient Ascent and Descent<a href="policy-gradient.html#gradient-ascent-and-descent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The simplest method is <strong>gradient ascent</strong> (for maximization):
<span class="math display">\[
\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k),
\]</span>
where <span class="math inline">\(\alpha &gt; 0\)</span> is the learning rate.<br />
For minimization, the update rule uses <strong>gradient descent</strong>:
<span class="math display">\[
\theta_{k+1} = \theta_k - \alpha \nabla_\theta J(\theta_k).
\]</span></p>
<p>The choice of learning rate <span class="math inline">\(\alpha\)</span> is critical:</p>
<ul>
<li>Too large <span class="math inline">\(\alpha\)</span> can cause divergence.<br />
</li>
<li>Too small <span class="math inline">\(\alpha\)</span> leads to slow convergence.</li>
</ul>
<div id="convergence-guarantees" class="section level4 hasAnchor" number="3.1.2.1">
<h4><span class="header-section-number">3.1.2.1</span> Convergence Guarantees<a href="policy-gradient.html#convergence-guarantees" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For convex functions <span class="math inline">\(J(\theta)\)</span>, gradient descent (or ascent) can be shown to converge to the <strong>global optimum</strong> under appropriate conditions on the learning rate.</p>
<p>For non-convex functions—which are common in reinforcement learning—gradient methods may only find so-called <strong>first-order stationary points</strong>, i.e., points <span class="math inline">\(\theta\)</span> at which the gradient <span class="math inline">\(\nabla_\theta J(\theta) = 0\)</span>. Nevertheless, they remain effective in practice.</p>
<p><span class="red">TODO: graph different stationary points</span></p>
<p>We now formalize the convergence speed of Gradient Descent (GD) for minimizing a smooth convex function. We switch to the minimization convention and write the objective as <span class="math inline">\(f:\mathbb{R}^d \to \mathbb{R}\)</span> (to avoid sign confusions with <span class="math inline">\(J\)</span> used for maximization). We assume exact gradients <span class="math inline">\(\nabla f(\theta)\)</span> are available.</p>
<p><strong>Setup and Assumptions.</strong></p>
<ul>
<li><p>(<strong>Convexity</strong>) For all <span class="math inline">\(\theta,\vartheta\in\mathbb{R}^d\)</span>,
<span class="math display" id="eq:PG-GO-convexity">\[\begin{equation}
f(\vartheta) \;\ge\; f(\theta) + \nabla f(\theta)^\top(\vartheta-\theta).
\tag{3.1}
\end{equation}\]</span></p></li>
<li><p>(<strong><span class="math inline">\(L\)</span>-smoothness</strong>) The gradient is <span class="math inline">\(L\)</span>-Lipschitz: for all <span class="math inline">\(\theta,\vartheta\)</span>,
<span class="math display" id="eq:PG-GO-Lsmooth">\[\begin{equation}
\|\nabla f(\vartheta)-\nabla f(\theta)\| \;\le\; L\|\vartheta-\theta\|.
\tag{3.2}
\end{equation}\]</span>
Equivalently (the <strong>descent lemma</strong>), for all <span class="math inline">\(\theta,\Delta\)</span>,
<span class="math display" id="eq:PG-GO-descent-lemma">\[\begin{equation}
f(\theta+\Delta) \;\le\; f(\theta) + \nabla f(\theta)^\top \Delta + \frac{L}{2}\|\Delta\|^2.
\tag{3.3}
\end{equation}\]</span></p></li>
</ul>
<p>Consider Gradient Descent with a constant stepsize <span class="math inline">\(\alpha&gt;0\)</span>:
<span class="math display">\[
\theta_{k+1} \;=\; \theta_k \;-\; \alpha\, \nabla f(\theta_k).
\]</span></p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:gd-convex-smooth" class="theorem"><strong>Theorem 3.1  (GD on smooth convex function) </strong></span>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(L\)</span>-smooth with a minimizer
<span class="math display">\[
\theta^\star\in\arg\min_\theta f(\theta).
\]</span>
and the global minimum <span class="math inline">\(f^\star = f(\theta^\star)\)</span>.
If <span class="math inline">\(0&lt;\alpha\le \frac{2}{L}\)</span>, then the GD iterates satisfy for all <span class="math inline">\(k\ge 0\)</span>:
<span class="math display" id="eq:GD-SmoothConvex-Value">\[\begin{equation}
f(\theta_k) - f^\star \leq \frac{2 (f(\theta_0) - f^\star) \Vert \theta_0 - \theta^\star \Vert^2 }{2 \Vert \theta_0 - \theta^\star \Vert^2 + k\alpha ( 2 - L \alpha) (f(\theta_0) - f^\star)}
\tag{3.4}
\end{equation}\]</span>
In particular, choosing <span class="math inline">\(\alpha=\frac{1}{L}\)</span> yields the canonical <span class="math inline">\(O(1/k)\)</span> convergence rate in suboptimality:
<span class="math display" id="eq:GD-SmoothConvex-optimal-rate">\[\begin{equation}
f(\theta_k) - f^\star \leq \frac{2L \Vert \theta_0 - \theta^\star \Vert^2}{k+4}
\tag{3.5}
\end{equation}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>See Theorem 2.1.14 and Corollary 2.1.2 in <span class="citation">(<a href="#ref-nesterov2018lectures">Nesterov 2018</a>)</span>.</p>
</div>
</div>
<p><strong>Strongly Convex Case (Linear Rate).</strong> If, in addition, <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex (<span class="math inline">\(\mu&gt;0\)</span>), i.e., for all <span class="math inline">\(\theta,\vartheta\in\mathbb{R}^d\)</span>,
<span class="math display" id="eq:PG-GO-strongly-convex">\[\begin{equation}
f(\vartheta)\;\ge\; f(\theta) + \nabla f(\theta)^\top(\vartheta-\theta) \;+\; \frac{\mu}{2}\,\|\vartheta-\theta\|^2.
\tag{3.6}
\end{equation}\]</span>
Then, GD with <span class="math inline">\(0&lt;\alpha\le \frac{2}{\mu + L}\)</span> enjoys a <strong>linear</strong> (geometric) rate:</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:gd-strongly-convex" class="theorem"><strong>Theorem 3.2  (GD on smooth strongly convex function) </strong></span>If <span class="math inline">\(f\)</span> is <span class="math inline">\(L\)</span>-smooth and <span class="math inline">\(\mu\)</span>-strongly convex, then for <span class="math inline">\(0&lt;\alpha\le \frac{2}{\mu + L}\)</span>,
<span class="math display" id="eq:GD-Strongly-Convex-1">\[\begin{equation}
\Vert \theta_k - \theta^\star \Vert^2 \leq \left( 1 - \frac{2\alpha \mu L}{\mu + L} \right)^k \Vert \theta_0 - \theta^\star \Vert^2.
\tag{3.7}
\end{equation}\]</span>
If <span class="math inline">\(\alpha = \frac{2}{\mu + L}\)</span>, then
<span class="math display" id="eq:GD-Strongly-Convex-2">\[\begin{equation}
\begin{split}
\Vert \theta_k - \theta^\star \Vert &amp; \leq \left( \frac{Q_f - 1}{Q_f + 1} \right)^k \Vert \theta_0 - \theta^\star \Vert \\
f(\theta_k) - f^\star &amp; \leq \frac{L}{2} \left( \frac{Q_f - 1}{Q_f + 1} \right)^{2k} \Vert \theta_0 - \theta^\star \Vert^2,
\end{split}
\tag{3.8}
\end{equation}\]</span>
where <span class="math inline">\(Q_f = L/\mu\)</span>.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span>See Theorem 2.1.15 in <span class="citation">(<a href="#ref-nesterov2018lectures">Nesterov 2018</a>)</span>.</p>
</div>
</div>
<p><strong>Practical Notes.</strong></p>
<ul>
<li><p>The step size <span class="math inline">\(\alpha=\frac{1}{L}\)</span> is <strong>optimal among fixed stepsizes</strong> for the above worst-case bounds on smooth convex <span class="math inline">\(f\)</span>.</p></li>
<li><p>In practice, backtracking line search or adaptive schedules can approach similar behavior without knowing <span class="math inline">\(L\)</span>.</p></li>
<li><p>For policy gradients (which maximize <span class="math inline">\(J\)</span>), apply the results to <span class="math inline">\(f=-J\)</span> and flip the update sign (gradient ascent). The smooth/convex assumptions rarely hold globally in RL, but these results calibrate expectations about step sizes and motivate variance reduction and curvature-aware methods used later.</p></li>
</ul>
</div>
</div>
<div id="stochastic-gradients" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Stochastic Gradients<a href="policy-gradient.html#stochastic-gradients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In reinforcement learning and other large-scale machine learning problems, computing the exact gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span> is often infeasible. Instead, we use an unbiased estimator <span class="math inline">\(\hat{\nabla}_\theta J(\theta)\)</span> computed from a subset of data (or trajectories in RL). The update becomes
<span class="math display">\[
\theta_{k+1} = \theta_k + \alpha \hat{\nabla}_\theta J(\theta_k).
\]</span></p>
<p>This approach, known as <strong>stochastic gradient ascent/descent (SGD)</strong>, trades off exactness for computational efficiency. Variance in the gradient estimates plays an important role in convergence speed and stability.</p>
<div id="convergence-guarantees-1" class="section level4 hasAnchor" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> Convergence Guarantees<a href="policy-gradient.html#convergence-guarantees-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We now turn to the convergence guarantees of stochastic gradient methods, which replace exact gradients with unbiased noisy estimates. Throughout this section we consider the minimization problem <span class="math inline">\(\min_\theta f(\theta)\)</span> and assume <span class="math inline">\(\nabla f\)</span> is available only through a stochastic oracle.</p>
<p><strong>Setup and Assumptions.</strong></p>
<p>Let <span class="math inline">\(f:\mathbb{R}^d\!\to\!\mathbb{R}\)</span> be differentiable. At iterate <span class="math inline">\(\theta_k\)</span>, we observe a random vector <span class="math inline">\(g_k\)</span> such that
<span class="math display">\[
\mathbb{E}[\,g_k \mid \theta_k\,] = \nabla f(\theta_k)
\quad\text{and}\quad
\mathbb{E}\!\left[\|g_k-\nabla f(\theta_k)\|^2 \mid \theta_k\right] \le \sigma^2.
\]</span>
We will also use one of the following standard regularity conditions:</p>
<ul>
<li>(<strong>Convex + <span class="math inline">\(L\)</span>-smooth</strong>) <span class="math inline">\(f\)</span> is convex and the gradient is <span class="math inline">\(L\)</span>-Lipschitz.<br />
</li>
<li>(<strong>Strongly convex + <span class="math inline">\(L\)</span>-smooth</strong>) <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex and <span class="math inline">\(L\)</span>-smooth.</li>
</ul>
<p>We consider the SGD update
<span class="math display">\[
\theta_{k+1} \;=\; \theta_k - \alpha_k\, g_k,
\]</span>
and define the <strong>averaged iterate</strong>
<span class="math display">\[
\bar\theta_K := \frac{1}{K+1}\sum_{k=0}^{K}\theta_k.
\]</span></p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:sgd-convex-rate" class="theorem"><strong>Theorem 3.3  (SGD on smooth convex function) </strong></span>Assume <span class="math inline">\(f\)</span> is convex and <span class="math inline">\(L\)</span>-smooth. Suppose there exists <span class="math inline">\(G\!&gt;\!0\)</span> with <span class="math inline">\(\mathbb{E}\|g_k\|^2 \le G^2\)</span> for all <span class="math inline">\(k\)</span>.</p>
<ul>
<li><p>Choose a constant stepsize <span class="math inline">\(\alpha_k = \alpha &gt; 0\)</span>. Then for all <span class="math inline">\(K \ge 1\)</span>,
<span class="math display" id="eq:SGD-convex-fixed-step-size">\[\begin{equation}
\mathbb{E}\big[f(\bar\theta_K)\big] - f^\star \leq \frac{\Vert \theta_0 - \theta^\star \Vert^2}{2 \alpha (K+1)} + \frac{\alpha G^2}{2}.
\tag{3.9}
\end{equation}\]</span></p></li>
<li><p>Choose a diminishing step size <span class="math inline">\(\alpha_k = \frac{\Vert \theta_0 - \theta^\star \Vert}{G \sqrt{k+1}}\)</span>, then
<span class="math display" id="eq:SGD-convex-diminishing-step-size">\[\begin{equation}
\mathbb{E}\big[f(\bar\theta_K)\big] - f^\star \leq \frac{\Vert \theta_0 - \theta^\star \Vert G}{\sqrt{K+1}} = \mathcal{O}\left(  \frac{1}{\sqrt{K}} \right).
\tag{3.10}
\end{equation}\]</span></p></li>
</ul>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span>See this <a href="https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_24.pdf">lecture note</a> and <span class="citation">(<a href="#ref-garrigos2023handbook">Garrigos and Gower 2023</a>)</span>.</p>
</div>
</div>
<p><strong>Remarks.</strong></p>
<ul>
<li><p>The bound is on the <em>averaged</em> iterate <span class="math inline">\(\bar\theta_K\)</span> (the last iterate may be worse by constants without further assumptions).</p></li>
<li><p>Replacing the second-moment bound by a variance bound <span class="math inline">\(\sigma^2\)</span> yields the same rate with <span class="math inline">\(G^2\)</span> replaced by <span class="math inline">\(\sigma^2 + \sup_k\|\nabla f(\theta_k)\|^2\)</span>.</p></li>
<li><p>With a constant stepsize, SGD converges <span class="math inline">\(\mathcal{O}(1/k)\)</span> up to a neighborhood set by the gradient noise.</p></li>
</ul>
<p>The next theorem states the convergence rate of SGD for minimizing strongly convex functions.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:sgd-strong-rate" class="theorem"><strong>Theorem 3.4  (SGD on smooth strongly convex function) </strong></span>Assume <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex and <span class="math inline">\(L\)</span>-smooth, and <span class="math inline">\(\mathbb{E}\!\left[\|g_k\|^2 \right]\le G^2\)</span>.<br />
With stepsize <span class="math inline">\(\alpha_k = \frac{1}{\mu(k+1)}\)</span>, the SGD iterates satisfy for all <span class="math inline">\(K\!\ge\!1\)</span>,
<span class="math display" id="eq:SGD-Strongly-Convex">\[\begin{equation}
\begin{split}
\mathbb{E}[f(\bar\theta_K)] - f^\star &amp; \leq \frac{G^2}{2 \mu (K+1)} (1 + \log(K+1)), \\
\mathbb{E} \Vert \bar\theta_K - \theta^\star \Vert^2 &amp; \leq \frac{Q}{K+1}, \ \ Q = \max \left( \frac{G^2}{\mu^2}, \Vert \theta_0 - \theta^\star \Vert^2 \right).
\end{split}
\tag{3.11}
\end{equation}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>Proof</em>. </span>See this <a href="https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_24.pdf">lecture note</a> and <span class="citation">(<a href="#ref-garrigos2023handbook">Garrigos and Gower 2023</a>)</span>.</p>
</div>
</div>
<p><strong>Practical Takeaways for Policy Gradients.</strong></p>
<ul>
<li><p>Use <strong>diminishing stepsizes</strong> for theoretical convergence (<span class="math inline">\(\alpha_k \propto 1/\sqrt{k}\)</span> for general convex, <span class="math inline">\(\alpha_k \propto 1/k\)</span> for strongly convex surrogates).</p></li>
<li><p>With <strong>constant stepsizes</strong>, expect fast initial progress down to a variance-limited plateau; lowering variance (e.g., via baselines/advantage estimation) is as important as tuning <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
<p><span class="red">TODO: graph the different trajectories between minimizing a convex function using GD and SGD.</span></p>
</div>
</div>
<div id="beyond-vanilla-gradient-methods" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Beyond Vanilla Gradient Methods<a href="policy-gradient.html#beyond-vanilla-gradient-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Several refinements to basic gradient updates are widely used:</p>
<ul>
<li><strong>Momentum methods</strong>: incorporate past gradients to smooth updates and accelerate convergence.</li>
<li><strong>Adaptive learning rates (Adam, RMSProp, AdaGrad)</strong>: adjust the learning rate per parameter based on historical gradient magnitudes.</li>
<li><strong>Second-order methods</strong>: approximate or use curvature information (the Hessian) for more informed updates, though often impractical in high dimensions.</li>
</ul>
</div>
</div>
<div id="policy-gradients" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Policy Gradients<a href="policy-gradient.html#policy-gradients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Policy gradients optimize a <em>parameterized stochastic policy</em> directly, without requiring an explicit action-value maximization step. They are applicable to both finite and continuous action spaces and are especially useful when actions are continuous or when “<span class="math inline">\(\arg\max\)</span>” over <span class="math inline">\(Q(s,a)\)</span> is costly or ill-posed.</p>
<div id="setup" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Setup<a href="policy-gradient.html#setup" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We consider a Markov decision process (MDP) with (possibly continuous) state space <span class="math inline">\(\mathcal{S}\)</span>, action space <span class="math inline">\(\mathcal{A}\)</span>, unknown dynamics <span class="math inline">\(P\)</span>, reward function <span class="math inline">\(R(s,a)\)</span>, and discount factor <span class="math inline">\(\gamma\in[0,1)\)</span>. Let <span class="math inline">\(\pi_\theta(a\mid s)\)</span> be a differentiable stochastic policy with parameters <span class="math inline">\(\theta\in\mathbb{R}^d\)</span>.</p>
<ul>
<li><p><strong>Trajectory.</strong> A state-action trajectory is <span class="math inline">\(\tau=(s_0,a_0,s_1,a_1,\dots,s_{T})\)</span> with probability density/mass
<span class="math display" id="eq:trajectory-density">\[\begin{equation}
p_\theta(\tau) = \rho(s_0)\prod_{t=0}^{T-1} \pi_\theta(a_t\mid s_t)\,P(s_{t+1}\mid s_t,a_t),
\tag{3.12}
\end{equation}\]</span>
where <span class="math inline">\(\rho\)</span> is the initial state distribution and <span class="math inline">\(T\)</span> is the (random or fixed) episode length.</p></li>
<li><p><strong>Return.</strong> Define the (discounted) return
<span class="math display" id="eq:PG-Trajectory-Return">\[\begin{equation}
R(\tau) \;=\; \sum_{t=0}^{T-1}\gamma^t R(s_t,a_t),
\tag{3.13}
\end{equation}\]</span>
and the return-to-go
<span class="math display" id="eq:PG-return-to-go">\[\begin{equation}
g_t \;=\; \sum_{t&#39;=t}^{T-1}\gamma^{t&#39;-t} R(s_{t&#39;},a_{t&#39;}).
\tag{3.14}
\end{equation}\]</span></p></li>
<li><p><strong>Optimization objective.</strong> The goal is to maximize the expected return
<span class="math display" id="eq:PG-objective">\[\begin{equation}
J(\theta) \;\equiv\; \mathbb{E}_{\tau\sim p_\theta}\!\left[R(\tau)\right]
\;=\; \mathbb{E}\!\left[\sum_{t=0}^{T-1}\gamma^t R(s_t,a_t)\right],
\tag{3.15}
\end{equation}\]</span>
where the expectation is taken over the randomness in (i) the initial state <span class="math inline">\(s_0 \sim \rho\)</span>, (ii) the policy <span class="math inline">\(\pi_\theta\)</span>, and (iii) the transition dynamics <span class="math inline">\(P\)</span>.</p></li>
</ul>
<div id="policy-models" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Policy models<a href="policy-gradient.html#policy-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Finite action spaces (<span class="math inline">\(\mathcal{A}\)</span> discrete).</strong> A common choice is a <strong>softmax (categorical) policy</strong> over a score (logit) function <span class="math inline">\(f_\theta(s,a)\)</span>:
<span class="math display" id="eq:finite-action-policy">\[\begin{equation}
\pi_\theta(a\mid s)
\;=\;
\frac{\exp\{f_\theta(s,a)\}}{\sum_{a&#39;\in\mathcal{A}}\exp\{f_\theta(s,a&#39;)\}}.
\tag{3.16}
\end{equation}\]</span>
Here we use <span class="math inline">\(\exp\{f_\theta(s,a)\} = e^{f_\theta(s,a)}\)</span> for pretty formatting. Typically <span class="math inline">\(f_\theta\)</span> is a neural network or a linear function over features.</p></li>
<li><p><strong>Continuous action spaces (<span class="math inline">\(\mathcal{A}\subseteq\mathbb{R}^m\)</span>).</strong> A standard choice is a <strong>Gaussian policy</strong>:
<span class="math display" id="eq:continuous-action-policy">\[\begin{equation}
\pi_\theta(a\mid s) \;=\; \mathcal{N}\!\big(a;\;\mu_\theta(s),\,\Sigma_\theta(s)\big),
\tag{3.17}
\end{equation}\]</span>
where <span class="math inline">\(\mu_\theta(s)\)</span> and (often diagonal) covariance <span class="math inline">\(\Sigma_\theta(s)\)</span> are differentiable functions (e.g., neural networks) parameterized by <span class="math inline">\(\theta\)</span>. The policy <span class="math inline">\(\pi_\theta(a \mid s)\)</span> samples actions from the Gaussian parameterized by <span class="math inline">\(\mu_\theta(s)\)</span> and <span class="math inline">\(\Sigma_\theta(s)\)</span>. Other choices include squashed Gaussians (e.g., <span class="math inline">\(\tanh\)</span>) or Beta distributions for bounded actions.</p></li>
</ul>
</div>
</div>
<div id="the-policy-gradient-lemma" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> The Policy Gradient Lemma<a href="policy-gradient.html#the-policy-gradient-lemma" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With the gradient-based optimization machinery from Section <a href="policy-gradient.html#gradient-optimization">3.1</a>, a natural strategy for the policy optimization problem in <a href="policy-gradient.html#eq:PG-objective">(3.15)</a> is gradient ascent on the objective <span class="math inline">\(J(\theta)\)</span>.
Consequently, the central task is to characterize the ascent direction, i.e., to compute <span class="math inline">\(\nabla_\theta J(\theta)\)</span>.</p>
<p>The policy gradient lemma, stated below, provides exactly this characterization. Crucially, it expresses <span class="math inline">\(\nabla_\theta J(\theta)\)</span> in terms of the policy’s score function <span class="math inline">\(\nabla_\theta \log \pi_\theta(a\mid s)\)</span> and returns, without differentiating through the environment dynamics. This likelihood-ratio form makes policy optimization feasible even when the transition model is unknown or non-differentiable.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:policy-gradient-lemma" class="theorem"><strong>Theorem 3.5  (Policy Gradient Lemma) </strong></span>Let <span class="math inline">\(J(\theta)=\mathbb{E}_{\tau \sim p_\theta}[R(\tau)]\)</span> as defined in <a href="policy-gradient.html#eq:PG-objective">(3.15)</a> Then:
<span class="math display" id="eq:PG-PGLemma-1">\[\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Big[R(\tau)\,\nabla_\theta \log p_\theta(\tau)\Big]
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Bigg[\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t\mid s_t)\;R(\tau)\Bigg].
\tag{3.18}
\end{equation}\]</span>
By causality (future action does not affect past reward), the full return can be replaced by return-to-go:
<span class="math display" id="eq:PG-PGLemma-2">\[\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Bigg[\sum_{t=0}^{T-1} \gamma^t
\nabla_\theta \log \pi_\theta(a_t\mid s_t)\;g_t\Bigg].
\tag{3.19}
\end{equation}\]</span>
Equivalently, using value functions,
<span class="math display" id="eq:PG-PGLemma-3">\[\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\frac{1}{1-\gamma} \mathbb{E}_{s\sim d_\theta,\;a\sim\pi_\theta} \Big[\nabla_\theta \log \pi_\theta(a\mid s)\,Q^{\pi_\theta}(s,a)\Big],
\tag{3.20}
\end{equation}\]</span>
where <span class="math inline">\(d_\theta\)</span> is the (discounted) on-policy state visitation distribution for infinite-horizon MDPs:
<span class="math display" id="eq:state-visitation-distribution">\[\begin{equation}
d_\theta(s) = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t \Pr_\theta(s_t=s).
\tag{3.21}
\end{equation}\]</span></p>
</div>
</div>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>Proof</em>. </span>We prove the three equivalent forms step by step. Throughout, we assume <span class="math inline">\(\theta\)</span> parameterizes only the policy <span class="math inline">\(\pi_\theta\)</span> (not the dynamics <span class="math inline">\(P\)</span> nor the initial distribution <span class="math inline">\(\rho\)</span>), and that interchanging <span class="math inline">\(\nabla_\theta\)</span> with the trajectory integral/sum is justified (e.g., bounded rewards and finite horizon or standard dominated-convergence conditions).
Let the return-to-go <span class="math inline">\(g_t\)</span> be defined as in <a href="policy-gradient.html#eq:PG-return-to-go">(3.14)</a>.</p>
<p><strong>Step 1 (Log-derivative trick).</strong> Write the objective as an expectation over trajectories:
<span class="math display">\[
J(\theta) \;=\; \int R(\tau)\, p_\theta(\tau)\, d\tau.
\]</span>
Differentiate under the integral and use
<span class="math display" id="eq:log-derivative-trick">\[\begin{equation}
\nabla_\theta p_\theta(\tau)=p_\theta(\tau)\nabla_\theta\log p_\theta(\tau)
\tag{3.22}
\end{equation}\]</span>
we can write:
<span class="math display">\[
\nabla_\theta J(\theta)
= \int R(\tau)\,\nabla_\theta p_\theta(\tau)\, d\tau
= \int R(\tau)\, p_\theta(\tau)\,\nabla_\theta \log p_\theta(\tau)\, d\tau
= \mathbb{E}_{\tau\sim p_\theta}\!\big[R(\tau)\,\nabla_\theta \log p_\theta(\tau)\big],
\]</span>
which is <a href="policy-gradient.html#eq:PG-PGLemma-1">(3.18)</a> up to expanding <span class="math inline">\(\log p_\theta(\tau)\)</span>. To see why <a href="policy-gradient.html#eq:log-derivative-trick">(3.22)</a> is true, write
<span class="math display">\[
\nabla_\theta \log p_\theta(\tau) = \frac{1}{p_\theta(\tau)} \nabla_\theta p_\theta(\tau),
\]</span>
using the chain rule.</p>
<p><strong>Step 2 (Policy-only dependence).</strong> Factor the trajectory likelihood/mass:
<span class="math display">\[
p_\theta(\tau)
= \rho(s_0)\,\prod_{t=0}^{T-1}\pi_\theta(a_t\mid s_t)\,P(s_{t+1}\mid s_t,a_t).
\]</span>
Since <span class="math inline">\(\rho\)</span> and <span class="math inline">\(P\)</span> do not depend on <span class="math inline">\(\theta\)</span>,
<span class="math display">\[
\log p_\theta(\tau)
= \text{const} \;+\; \sum_{t=0}^{T-1}\log \pi_\theta(a_t\mid s_t)
\quad\Rightarrow\quad
\nabla_\theta \log p_\theta(\tau) \;=\; \sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t).
\]</span>
Substitute into Step 1 to obtain the second equality in <a href="policy-gradient.html#eq:PG-PGLemma-1">(3.18)</a>:
<span class="math display">\[
\nabla_\theta J(\theta)
= \mathbb{E}_{\tau\sim p_\theta}\!\Bigg[\sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,R(\tau)\Bigg].
\]</span></p>
<p><strong>Step 3 (Causality <span class="math inline">\(\Rightarrow\)</span> return-to-go).</strong> Expand <span class="math inline">\(R(\tau)=\sum_{t=0}^{T-1}\gamma^{t} r_{t}\)</span> (with <span class="math inline">\(r_{t}:=R(s_{t},a_{t})\)</span>) and swap sums:
<span class="math display">\[
\mathbb{E} \Bigg[\sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,R(\tau)\Bigg]
=
\sum_{t=0}^{T-1}\sum_{t&#39;=0}^{T-1}\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\gamma^{t&#39;} r_{t&#39;}\big].
\]</span>
For <span class="math inline">\(t&#39;&lt;t\)</span>, the factor <span class="math inline">\(\gamma^{t&#39;} r_{t&#39;}\)</span> is measurable w.r.t. the history <span class="math inline">\(\mathcal{F}_t=\sigma(s_0,a_0,\dots,s_t)\)</span>, while
<span class="math display">\[
\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big|\,\mathcal{F}_t\big]
= \sum_{a} \pi_\theta(a\mid s_t)\,\nabla_\theta \log \pi_\theta(a\mid s_t) = \nabla_\theta \sum_{a}\pi_\theta(a\mid s_t) = \nabla_\theta 1 = 0,
\]</span>
(and analogously with integrals for continuous <span class="math inline">\(\mathcal{A}\)</span>). Hence by the tower property,
<span class="math display">\[
\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\gamma^{t&#39;} r_{t&#39;}\big]=0\quad\text{for all }t&#39;&lt;t.
\]</span>
Therefore only the terms with <span class="math inline">\(t&#39;\ge t\)</span> survive, and
<span class="math display">\[
\nabla_\theta J(\theta)
= \sum_{t=0}^{T-1}\mathbb{E} \Big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\sum_{t&#39;=t}^{T-1}\gamma^{t&#39;} r_{t&#39;}\Big]
= \mathbb{E} \Bigg[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\Bigg],
\]</span>
which is <a href="policy-gradient.html#eq:PG-PGLemma-2">(3.19)</a>.</p>
<p><strong>Step 4 (Value-function form).</strong> Condition on <span class="math inline">\((s_t,a_t)\)</span> and use the definition of the action-value function:
<span class="math display">\[
Q^{\pi_\theta}(s_t,a_t) \;\equiv\; \mathbb{E}\!\left[g_t \,\middle|\, s_t,a_t\right].
\]</span>
Taking expectations then yields
<span class="math display">\[
\mathbb{E} \big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\big]
= \mathbb{E} \big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,Q^{\pi_\theta}(s_t,a_t)\big].
\]</span>
Summing over <span class="math inline">\(t\)</span> and collecting terms with the (discounted) on-policy state visitation distribution <span class="math inline">\(d_\theta\)</span> (for the infinite-horizon case, e.g., <span class="math inline">\(d_\theta(s)=(1-\gamma)\sum_{t=0}^\infty \gamma^t\,\Pr_\theta(s_t=s)\)</span>; for finite <span class="math inline">\(T\)</span>, use the corresponding finite-horizon weighting), we obtain
<span class="math display">\[
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{s\sim d_\theta,\;a\sim \pi_\theta} \Big[\nabla_\theta \log \pi_\theta(a\mid s)\,Q^{\pi_\theta}(s,a)\Big],
\]</span>
which is <a href="policy-gradient.html#eq:PG-PGLemma-3">(3.20)</a>.</p>
<p><strong>Conclusion.</strong> Combining Steps 1–4 proves all three stated forms of the policy gradient.</p>
</div>
</div>
<div id="reinforce" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> REINFORCE<a href="policy-gradient.html#reinforce" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The policy gradient lemma immediately gives us an algorithm. Specifically, the gradient receipe in <a href="policy-gradient.html#eq:PG-PGLemma-1">(3.18)</a> tells us that if we generate one trajectory <span class="math inline">\(\tau\)</span> by following the policy <span class="math inline">\(\pi\)</span>, then
<span class="math display" id="eq:PG-Estimator-1">\[\begin{equation}
\widehat{\nabla_\theta J} = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \mid s_t) R(\tau)
\tag{3.23}
\end{equation}\]</span>
is an unbiased estimator of the true gradient.</p>
<p>With this sample gradient estimator, we obtain the classical REINFORCE algorithm.</p>
<div class="highlightbox">
<div style="text-align: center;">
<p><strong>Single-Trajectory (Naive) REINFORCE</strong></p>
</div>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\theta_0\)</span> for the initial policy <span class="math inline">\(\pi_{\theta_0}(a \mid s)\)</span></li>
<li>For <span class="math inline">\(k=0,1,\dots,\)</span> do:</li>
</ol>
<ul>
<li>Obtain a trajectory <span class="math inline">\(\tau \sim p_{\theta_k}\)</span></li>
<li>Compute the stochastic gradient <span class="math inline">\(g_k\)</span> as in <a href="policy-gradient.html#eq:PG-Estimator-1">(3.23)</a></li>
<li>Update <span class="math inline">\(\theta_{k+1} = \theta_k + \alpha_k g_k\)</span></li>
</ul>
</div>
<p>To reduce variance of the gradient estimator, we can use a minibatch of trajectories. For example, given a batch of <span class="math inline">\(N\)</span> trajectories <span class="math inline">\(\{\tau^{(i)}\}_{i=1}^N\)</span> collected by <span class="math inline">\(\pi_\theta\)</span>, define for each timestep the return-to-go
<span class="math display">\[
g_t^{(i)} = \sum_{t&#39;=t}^{T^{(i)}-1} \gamma^{t&#39;-t} R\!\left(s_{t&#39;}^{(i)},a_{t&#39;}^{(i)}\right).
\]</span>
An unbiased gradient estimator, from <a href="policy-gradient.html#eq:PG-PGLemma-2">(3.19)</a> is
<span class="math display" id="eq:PG-Estimator-2">\[\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1} \gamma^t
\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big) g_t^{(i)}.
\tag{3.24}
\end{equation}\]</span></p>
<p>This leads to the following minibatch REINFORCE algorithm.</p>
<div class="highlightbox">
<div style="text-align: center;">
<p><strong>Minibatch REINFORCE</strong></p>
</div>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\theta_0\)</span> for the initial policy <span class="math inline">\(\pi_{\theta_0}(a \mid s)\)</span></li>
<li>For <span class="math inline">\(k=0,1,\dots,\)</span> do:</li>
</ol>
<ul>
<li>Obtain N trajectories <span class="math inline">\(\{ \tau^{(i)} \}_{i=1}^N \sim p_{\theta_k}\)</span></li>
<li>Compute the stochastic gradient <span class="math inline">\(g_k\)</span> as in <a href="policy-gradient.html#eq:PG-Estimator-2">(3.24)</a></li>
<li>Update <span class="math inline">\(\theta_{k+1} = \theta_k + \alpha_k g_k\)</span></li>
</ul>
</div>
<p>We apply both the single-trajectory (naive) REINFORCE and a minibatch variant to the CartPole-v1 balancing task. The results show that variance reduction via minibatching is crucial for stable learning and for obtaining strong policies with policy-gradient methods.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:cartpole-reinforce" class="example"><strong>Example 3.1  (REINFORCE for Cart-Pole Balancing) </strong></span>Consider the cart-pole balancing task illustrated in Fig. <a href="policy-gradient.html#fig:cart-pole-illustration">3.1</a>. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-illustration"></span>
<img src="images/Policy-Gradients/cart_pole.gif" alt="Cart Pole balance." width="60%" />
<p class="caption">
Figure 3.1: Cart Pole balance.
</p>
</div>
<p><strong>State Space.</strong> The state of the cart-pole system is denoted by <span class="math inline">\(s \in \mathcal{S} \subset \mathbb{R}^4\)</span>, containing the position and velocity of the cart, as well as the angle and angular velocity of the pole.</p>
<p><strong>Action Space.</strong> The action space <span class="math inline">\(\mathcal{A}\)</span> is discrete and contains two elements: pushing to the left and pushing to the right.</p>
<p>The dynamics of the MDP is provided by the <a href="https://gymnasium.farama.org/environments/classic_control/cart_pole/">Gym simulator</a> and is described in the original paper <span class="citation">(<a href="#ref-barto2012neuronlike">Barto, Sutton, and Anderson 2012</a>)</span>. At the beginning of the episode, all state variables are randomly initialized in <span class="math inline">\([-0.05,0.05]\)</span> and the goal for the agent is to apply the actions to balance the cart-pole for as long as possible—the agent gets a reward of <span class="math inline">\(+1\)</span> every step if (1) the pole angle remains between <span class="math inline">\(-12^\circ\)</span> and <span class="math inline">\(+12^\circ\)</span> and (2) the cart position remains between <span class="math inline">\(-2.4\)</span> and <span class="math inline">\(2.4\)</span>. The maximum episode length is <span class="math inline">\(500\)</span>.</p>
<p>We design a policy network in the form of <a href="policy-gradient.html#eq:finite-action-policy">(3.16)</a> since the action space is finite.</p>
<p><strong>REINFORCE.</strong> We first apply the naive REINFORCE algorithm where the gradient estimator is computed from a single trajectory as in <a href="policy-gradient.html#eq:PG-Estimator-1">(3.23)</a>. Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-reinforce">3.2</a> shows the learning curve, which indicates that the REINFORCE algorithm was not able to learn a good policy after 2000 episodes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-reinforce"></span>
<img src="images/Policy-Gradients/cartpole_returns_reinforce.png" alt="Learning curve (Naive REINFORCE)." width="60%" />
<p class="caption">
Figure 3.2: Learning curve (Naive REINFORCE).
</p>
</div>
<p><strong>Minibatch REINFORCE.</strong> We then apply the minibatch REINFORCE algorithm where the gradient estimator is computed from multiple (<span class="math inline">\(20\)</span> in our case) trajectories as in <a href="policy-gradient.html#eq:PG-Estimator-2">(3.24)</a>. Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-minibatch-reinforce">3.3</a> shows the learning curve, which shows steady increase in the per-episode return that eventually gets close to the maximum per-episode return <span class="math inline">\(500\)</span>.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-policy-rollout-minibatch-reinforce">3.4</a> shows a rollout video of applying the policy training from minibatch REINFORCE. We can see the policy nicely balances the cart-pole system.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_reinforce.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-minibatch-reinforce"></span>
<img src="images/Policy-Gradients/cartpole_returns_minibatch_reinforce.png" alt="Learning curve (Minibatch REINFORCE)." width="60%" />
<p class="caption">
Figure 3.3: Learning curve (Minibatch REINFORCE).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-policy-rollout-minibatch-reinforce"></span>
<img src="images/Policy-Gradients/cartpole_policy_rollout_minibatch_reinforce.gif" alt="Policy rollout (Minibatch REINFORCE)." width="60%" />
<p class="caption">
Figure 3.4: Policy rollout (Minibatch REINFORCE).
</p>
</div>
</div>
</div>
</div>
<div id="baselines-and-variance-reduction" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Baselines and Variance Reduction<a href="policy-gradient.html#baselines-and-variance-reduction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From the REINFORCE experiments above, we have seen firsthand that <strong>variance reduction</strong> is critical for stable policy-gradient learning.</p>
<p>A natural question is: <em>what framework can we use to systematically reduce the variance of the gradient estimator while preserving unbiasedness?</em></p>
<div id="baseline" class="section level4 hasAnchor" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Baseline<a href="policy-gradient.html#baseline" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A key device is a <strong>baseline</strong> <span class="math inline">\(b:\mathcal{S}\to\mathbb{R}\)</span> added at each timestep:
<span class="math display" id="eq:baseline-estimator">\[\begin{equation}
\widehat{g}
\;=\;
\sum_{t=0}^{T-1} \gamma^t\,\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t - b(s_t)\big).
\tag{3.25}
\end{equation}\]</span></p>
<p>The only difference between <a href="policy-gradient.html#eq:baseline-estimator">(3.25)</a> and the original gradient estimator <a href="policy-gradient.html#eq:PG-PGLemma-2">(3.19)</a> is that the baseline <span class="math inline">\(b(s_t)\)</span> is subtracted from the return-to-go <span class="math inline">\(g_t\)</span>. The next theorem states that any state-only baseline does not change the expectation of the gradient estimator.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:baseline-invariance" class="theorem"><strong>Theorem 3.6  (Baseline Invariance) </strong></span>Let <span class="math inline">\(b:\mathcal{S}\to\mathbb{R}\)</span> be any function independent of the action <span class="math inline">\(a_t\)</span>. Then
<span class="math display">\[
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]=0,
\]</span>
and thus
<span class="math display" id="eq:PG-baseline-form">\[\begin{equation}
\nabla_\theta J(\theta) \;=\;
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t - b(s_t)\big)\right].
\tag{3.26}
\end{equation}\]</span></p>
<p>Equivalently, using action-values,
<span class="math display" id="eq:baseline-estimator-Qvalue">\[\begin{equation}
\nabla_\theta J(\theta)
=
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\theta,\;a\sim\pi_\theta}
\!\Big[\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^{\pi_\theta}(s,a)-b(s)\big)\Big].
\tag{3.27}
\end{equation}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-21" class="proof"><em>Proof</em>. </span>We prove (i) the baseline term has zero expectation, (ii) the baseline-subtracted estimator in <a href="policy-gradient.html#eq:PG-baseline-form">(3.26)</a> is unbiased, and (iii) the equivalent <span class="math inline">\(Q\)</span>-value form <a href="policy-gradient.html#eq:baseline-estimator-Qvalue">(3.27)</a>.</p>
<p>Throughout we assume standard conditions ensuring interchange of expectation and differentiation (e.g., bounded rewards with finite horizon or discounted infinite horizon, and a differentiable policy).</p>
<p><strong>Step 1 (Score-function expectation is zero).</strong> Fix a state <span class="math inline">\(s\in\mathcal{S}\)</span>. The <strong>score function</strong> integrates/sums to zero under the policy:
<span class="math display">\[\begin{equation}
\begin{split}
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s)}\!\big[\nabla_\theta \log \pi_\theta(a\mid s)\big]
&amp; =
\sum_{a\in\mathcal{A}} \pi_\theta(a\mid s)\,\nabla_\theta \log \pi_\theta(a\mid s)
=
\sum_{a\in\mathcal{A}} \nabla_\theta \pi_\theta(a\mid s) \\
&amp; =
\nabla_\theta \sum_{a\in\mathcal{A}} \pi_\theta(a\mid s)
=
\nabla_\theta 1
=
0,
\end{split}
\end{equation}\]</span>
with the obvious replacement of sums by integrals for continuous <span class="math inline">\(\mathcal{A}\)</span>. This identity is the standard “score has zero mean” property.</p>
<p><strong>Step 2 (Baseline term has zero expectation).</strong> Let <span class="math inline">\(\mathcal{F}_t := \sigma(s_0,a_0,\ldots,s_t)\)</span> be the history up to time <span class="math inline">\(t\)</span> and recall that <span class="math inline">\(b(s_t)\)</span> is <strong>independent of</strong> <span class="math inline">\(a_t\)</span>. Using iterated expectations:
<span class="math display">\[
\mathbb{E}\!\left[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]
=
\mathbb{E}\!\left[
\gamma^t\, b(s_t)\,
\underbrace{\mathbb{E}\!\left[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\middle|\, s_t\right]}_{=\,0~\text{by Step 1}}
\right]
= 0.
\]</span>
Summing over <span class="math inline">\(t\)</span> yields
<span class="math display">\[
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]=0.
\]</span></p>
<p><strong>Step 3 (Unbiasedness of the baseline-subtracted estimator).</strong> By the policy gradient lemma (likelihood-ratio form with return-to-go; see <a href="policy-gradient.html#eq:PG-PGLemma-2">(3.19)</a>),
<span class="math display">\[
\nabla_\theta J(\theta)
=
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\right].
\]</span>
Subtract and add the baseline term inside the expectation:
<span class="math display">\[
  \begin{split}
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\right]
&amp; =
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\right]
\;+\; \\
&amp; \quad \quad \underbrace{\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]}_{=\,0~\text{by Step 2}}.
\end{split}
\]</span>
Therefore <a href="policy-gradient.html#eq:PG-baseline-form">(3.26)</a> holds, proving that <strong>any</strong> state-only baseline preserves unbiasedness.</p>
<p><strong>Step 4 (Equivalent <span class="math inline">\(Q\)</span>-value form).</strong> Condition on <span class="math inline">\((s_t,a_t)\)</span> and use the definition <span class="math inline">\(Q^{\pi_\theta}(s_t,a_t):=\mathbb{E}[g_t\mid s_t,a_t]\)</span>:
<span class="math display">\[
\mathbb{E}\!\big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\big]
=
\mathbb{E}\!\Big[
\gamma^t\,
\mathbb{E}\!\big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\mid s_t\big]
\Big].
\]</span>
Inside the inner expectation (over <span class="math inline">\(a_t\sim \pi_\theta(\cdot\mid s_t)\)</span>) and using <span class="math inline">\(b(s_t)\)</span>’s independence from <span class="math inline">\(a_t\)</span>,
<span class="math display">\[
\mathbb{E}\!\big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\mid s_t\big]
=
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s_t)}\!\Big[\nabla_\theta \log \pi_\theta(a\mid s_t)\,\big(Q^{\pi_\theta}(s_t,a)-b(s_t)\big)\Big].
\]</span>
Summing over <span class="math inline">\(t\)</span> with discount <span class="math inline">\(\gamma^t\)</span> and collecting terms with the (discounted) on-policy state-visitation distribution <span class="math inline">\(d_\theta\)</span> (cf. <a href="policy-gradient.html#eq:state-visitation-distribution">(3.21)</a>) yields the infinite-horizon identity
<span class="math display">\[
\nabla_\theta J(\theta)
=
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\theta,\;a\sim \pi_\theta}\!
\Big[\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^{\pi_\theta}(s,a)-b(s)\big)\Big],
\]</span>
which is <a href="policy-gradient.html#eq:baseline-estimator-Qvalue">(3.27)</a>.</p>
</div>
</div>
</div>
<div id="optimal-baseline-and-advantage" class="section level4 hasAnchor" number="3.2.4.2">
<h4><span class="header-section-number">3.2.4.2</span> Optimal Baseline and Advantage<a href="policy-gradient.html#optimal-baseline-and-advantage" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Among all state-only baselines <span class="math inline">\(b(s)\)</span>, which one minimizes the variance of the gradient estimator?</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:variance-minimizing-baseline" class="theorem"><strong>Theorem 3.7  (Variance-Minimizing Baseline (per-state)) </strong></span>For the estimator
<span class="math display">\[
g(s,a)=\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big),
\]</span>
the <span class="math inline">\(b(s)\)</span> minimizing <span class="math inline">\(\operatorname{Var}[g\mid s]\)</span> is
<span class="math display" id="eq:best-baseline">\[
b^\star(s)=
\frac{\mathbb{E}_{a\sim \pi_\theta}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\, Q^\pi(s,a)\right]}
{\mathbb{E}_{a\sim \pi_\theta}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\right]}.
\tag{3.28}
\]</span>
Assuming that the norm factor <span class="math inline">\(\Vert \nabla_\theta \log \pi_\theta(a\mid s) \Vert^2\)</span> varies slowly with <span class="math inline">\(a\)</span>, then
<span class="math display">\[
b^\star(s) \approx V^\pi(s).
\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-22" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(s\in\mathcal{S}\)</span> be fixed and write
<span class="math display">\[
u(a\mid s) \;\equiv\; \nabla_\theta \log \pi_\theta(a\mid s)\in\mathbb{R}^d,
\qquad
w(a\mid s) \;\equiv\; \|u(a\mid s)\|^2 \;\ge 0.
\]</span>
Consider the vector-valued random variable
<span class="math display">\[
g(s,a) \;=\; u(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big),
\]</span>
where the randomness is over <span class="math inline">\(a\sim \pi_\theta(\cdot\mid s)\)</span>.</p>
<p>We aim to choose <span class="math inline">\(b(s)\in\mathbb{R}\)</span> to minimize the <strong>conditional variance</strong>
<span class="math display">\[
\operatorname{Var}[g\mid s] \;=\; \mathbb{E}\!\left[\|g(s,a)-\mathbb{E}[g\mid s]\|^2 \,\middle|\, s\right].
\]</span>
Using the identity <span class="math inline">\(\operatorname{Var}[X]=\mathbb{E}\|X\|^2-\|\mathbb{E}X\|^2\)</span> (for vector <span class="math inline">\(X\)</span> with Euclidean norm), we have
<span class="math display">\[
\operatorname{Var}[g\mid s]
\;=\;
\underbrace{\mathbb{E}\!\left[\|g(s,a)\|^2 \mid s\right]}_{\text{depends on } b(s)}
\;-\;
\underbrace{\big\|\mathbb{E}[g\mid s]\big\|^2}_{\text{independent of } b(s)}.
\]</span>
We first show that the mean term is independent of <span class="math inline">\(b(s)\)</span>. Indeed,
<span class="math display">\[
\mathbb{E}[g\mid s]
=
\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\big[u(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)\big]
=
\mathbb{E}\!\big[u(a\mid s)\,Q^\pi(s,a)\big]
\;-\;
b(s)\,\underbrace{\mathbb{E}\!\big[u(a\mid s)\big]}_{=\,0},
\]</span>
where <span class="math inline">\(\mathbb{E}[u(a\mid s)]=\sum_a \pi_\theta(a\mid s)\nabla_\theta\log\pi_\theta(a\mid s)=\nabla_\theta \sum_a \pi_\theta(a\mid s)=\nabla_\theta 1=0\)</span> (replace sums by integrals in the continuous case). Therefore <span class="math inline">\(\mathbb{E}[g\mid s]\)</span> does <strong>not</strong> depend on <span class="math inline">\(b(s)\)</span>.</p>
<p>Consequently, minimizing <span class="math inline">\(\operatorname{Var}[g\mid s]\)</span> is equivalent to minimizing the conditional <strong>second moment</strong>
<span class="math display">\[
\mathbb{E}\!\left[\|g(s,a)\|^2 \mid s\right]
=
\mathbb{E}\!\left[\|u(a\mid s)\|^2 \,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right]
=
\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right].
\]</span>
The right-hand side is a convex quadratic in the scalar <span class="math inline">\(b(s)\)</span>. Differentiate w.r.t. <span class="math inline">\(b(s)\)</span> and set to zero:
<span class="math display">\[
\frac{\partial}{\partial b(s)}
\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right]
=
-2\,\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)\,\middle|\, s\right]
= 0.
\]</span>
Hence,
<span class="math display">\[
\mathbb{E}\!\left[w(a\mid s)\,Q^\pi(s,a)\,\middle|\, s\right]
=
b(s)\,\mathbb{E}\!\left[w(a\mid s)\,\middle|\, s\right],
\]</span>
and provided <span class="math inline">\(\mathbb{E}[w(a\mid s)\mid s]&gt;0\)</span> (i.e., the Fisher information at <span class="math inline">\(s\)</span> is non-degenerate), the unique minimizer is
<span class="math display">\[
b^\star(s)
=
\frac{\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\, Q^\pi(s,a)\right]}
{\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\right]},
\]</span>
which is <a href="policy-gradient.html#eq:best-baseline">(3.28)</a>. If <span class="math inline">\(\mathbb{E}[w(a\mid s)\mid s]=0\)</span> (e.g., a locally deterministic policy), then <span class="math inline">\(g\equiv 0\)</span> almost surely and any <span class="math inline">\(b(s)\)</span> attains the minimum.</p>
<p>Finally, when the weight <span class="math inline">\(w(a\mid s)=\|\nabla_\theta \log \pi_\theta(a\mid s)\|^2\)</span> varies slowly with <span class="math inline">\(a\)</span> (or is approximately constant) for a fixed <span class="math inline">\(s\)</span>, the ratio simplifies to
<span class="math display">\[
b^\star(s)\;\approx\;\frac{\mathbb{E}[c(s)\,Q^\pi(s,a)\mid s]}{\mathbb{E}[c(s)\mid s]}
\;=\;
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s)}\!\big[Q^\pi(s,a)\big]
\;=\;
V^\pi(s),
\]</span>
so that the baseline-subtracted target becomes the <strong>advantage</strong>
<span class="math inline">\(A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)\)</span>.</p>
</div>
</div>
<p>When using <span class="math inline">\(V^\pi(s)\)</span> as the baseline, the baseline-subtracted target is called the <strong>advantage function</strong>
<span class="math display" id="eq:advantage-def">\[\begin{equation}
A^{\pi_\theta}(s,a)\;=\;Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s).
\tag{3.29}
\end{equation}\]</span>
The corresponding minibatch gradient estimator becomes
<span class="math display" id="eq:reinforce-adv-estimator">\[\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1} \gamma^t\,
\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big)\,
\widehat{A}_t^{(i)},
\quad
\widehat{A}_t^{(i)} \approx g_t^{(i)} - V_\phi \big(s_t^{(i)}\big),
\tag{3.30}
\end{equation}\]</span>
where <span class="math inline">\(V_\phi\)</span> is a learned approximation to <span class="math inline">\(V^{\pi_\theta}\)</span>.</p>
</div>
<div id="intuition-for-the-advantage" class="section level4 hasAnchor" number="3.2.4.3">
<h4><span class="header-section-number">3.2.4.3</span> Intuition for the Advantage<a href="policy-gradient.html#intuition-for-the-advantage" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The advantage
<span class="math display">\[
A^\pi(s,a) \;=\; Q^\pi(s,a) - V^\pi(s)
\]</span>
measures how much <em>better or worse</em> action <span class="math inline">\(a\)</span> is at state <span class="math inline">\(s\)</span> <em>relative to the policy’s average action quality</em> <span class="math inline">\(V^\pi(s)=\mathbb{E}_{a\sim\pi}[Q^\pi(s,a)\mid s]\)</span>.<br />
Hence <span class="math inline">\(\mathbb{E}_{a\sim\pi}[A^\pi(s,a)\mid s]=0\)</span>: it is a <em>relative</em> score.</p>
<p>With a value baseline, the policy-gradient update is
<span class="math display">\[
\nabla_\theta J(\theta)
\;=\;
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\pi,\;a\sim\pi}\!\big[
\nabla_\theta \log \pi_\theta(a\mid s)\,A^\pi(s,a)
\big].
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(A^\pi(s,a) &gt; 0\)</span>: the term <span class="math inline">\(\nabla_\theta \log \pi_\theta(a\mid s)\,A^\pi(s,a)\)</span> <strong>increases</strong> <span class="math inline">\(\log \pi_\theta(a\mid s)\)</span> (and thus <span class="math inline">\(\pi_\theta(a\mid s)\)</span>)—the policy puts <strong>more</strong> probability mass on actions that outperformed its average at <span class="math inline">\(s\)</span>.</p></li>
<li><p>If <span class="math inline">\(A^\pi(s,a) &lt; 0\)</span>: it <strong>decreases</strong> <span class="math inline">\(\log \pi_\theta(a\mid s)\)</span>—the policy puts <strong>less</strong> probability mass on actions that underperformed at <span class="math inline">\(s\)</span>.</p></li>
<li><p>If <span class="math inline">\(A^\pi(s,a) \approx 0\)</span>: the action performed about as expected; the update at that <span class="math inline">\((s,a)\)</span> is <strong>negligible</strong>.</p></li>
</ul>
<p>Subtracting <span class="math inline">\(V^\pi(s)\)</span> centers returns <em>per state</em>, so the update depends only on <em>relative</em> goodness. This:</p>
<ul>
<li><p>preserves unbiasedness (baseline invariance),</p></li>
<li><p>reduces variance (no large, shared offset),</p></li>
<li><p>focuses learning on which actions at <span class="math inline">\(s\)</span> should get more/less probability.</p></li>
</ul>
</div>
<div id="reinforce-with-a-learned-value-baseline" class="section level4 hasAnchor" number="3.2.4.4">
<h4><span class="header-section-number">3.2.4.4</span> REINFORCE with a Learned Value Baseline<a href="policy-gradient.html#reinforce-with-a-learned-value-baseline" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall that in Section <a href="value-rl.html#function-approximation">2.2</a>, we have introduced multiple algorithms that can learn an approximate value function for policy evaluation. For example, we can use Monte Carlo estimation.</p>
<p>We now combine REINFORCE with a learned baseline <span class="math inline">\(V_\phi(s)\approx V^{\pi_\theta}(s)\)</span>, yielding a lower-variance update while keeping the estimator unbiased.</p>
<div class="highlightbox">
<div style="text-align: center;">
<p><strong>Minibatch REINFORCE with a Learned Value Baseline</strong></p>
</div>
<p><strong>Inputs:</strong> policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span>, value <span class="math inline">\(V_\phi(s)\)</span>, discount <span class="math inline">\(\gamma\in[0,1)\)</span>, stepsizes <span class="math inline">\(\alpha_\theta,\alpha_\phi&gt;0\)</span>, batch size <span class="math inline">\(N\)</span>.<br />
<strong>Convergence controls:</strong> tolerance <span class="math inline">\(\varepsilon&gt;0\)</span>, maximum inner steps <span class="math inline">\(K_{\max}\)</span> (value-fit loop), optional patience <span class="math inline">\(P\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Collect trajectories.</strong> Roll out <span class="math inline">\(N\)</span> on-policy trajectories <span class="math inline">\(\{\tau^{(i)}\}_{i=1}^N\)</span> using <span class="math inline">\(\pi_\theta\)</span>.<br />
For each trajectory <span class="math inline">\(i\)</span> and timestep <span class="math inline">\(t\)</span>, record <span class="math inline">\((s_t^{(i)},a_t^{(i)},r_t^{(i)})\)</span>.</p></li>
<li><p><strong>Compute returns-to-go.</strong> For each <span class="math inline">\(i,t\)</span>,
<span class="math display">\[
g_t^{(i)} \;=\; \sum_{t&#39;=t}^{T^{(i)}-1} \gamma^{\,t&#39;-t}\, r_{t&#39;}^{(i)}.
\]</span></p></li>
<li><p><strong>Fit the value to convergence (critic inner loop).</strong> Define the batch regression loss
<span class="math display">\[
\mathcal{L}_V(\phi)
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1}
\big(g_t^{(i)} - V_\phi(s_t^{(i)})\big)^2.
\]</span>
Perform gradient steps on <span class="math inline">\(\phi\)</span> <strong>until convergence</strong> on this fixed batch:
<span class="math display">\[
\phi \leftarrow \phi - \alpha_\phi \,\nabla_\phi \mathcal{L}_V(\phi).
\]</span>
Repeat for <span class="math inline">\(k=1,\dots,K_{\max}\)</span> or until
<span class="math display">\[
\frac{\mathcal{L}_V^{(k-1)}-\mathcal{L}_V^{(k)}}{\max\{1,|\mathcal{L}_V^{(k-1)}|\}} &lt; \varepsilon
\]</span>
for <span class="math inline">\(M\)</span> consecutive checks.
Denote the (approximately) converged parameters by <span class="math inline">\(\phi^\star\)</span>.</p></li>
<li><p><strong>Form (optionally standardized) advantages using the converged value.</strong>
<span class="math display">\[
\widehat{A}_t^{(i)} \;=\; g_t^{(i)} - V_{\phi^\star}\!\big(s_t^{(i)}\big),
\qquad
\tilde{A}_t^{(i)} \;=\; \frac{\widehat{A}_t^{(i)} - \mu_A}{\sigma_A+\delta}\ \ (\text{optional, batch-wise}),
\]</span>
where <span class="math inline">\(\mu_A,\sigma_A\)</span> are the mean and std of <span class="math inline">\(\{\widehat{A}_t^{(i)}\}\)</span> over the <strong>whole</strong> batch, and <span class="math inline">\(\delta&gt;0\)</span> is a small constant.</p></li>
<li><p><strong>Single policy (actor) update.</strong> Using the converged baseline, take <strong>one</strong> ascent step:
<span class="math display">\[
\theta \;\leftarrow\; \theta
\;+\; \alpha_\theta \cdot
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1}
\gamma^t\,\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big)\,\tilde{A}_t^{(i)}.
\]</span>
<em>(If not standardizing, use <span class="math inline">\(\widehat{A}_t^{(i)}\)</span> in place of <span class="math inline">\(\tilde{A}_t^{(i)}\)</span>.)</em></p></li>
<li><p><strong>Repeat</strong> from Step 1 with the updated policy.</p></li>
</ol>
</div>
<p><strong>Notes.</strong></p>
<ul>
<li><p>By baseline invariance, subtracting <span class="math inline">\(V_{\phi^\star}(s)\)</span> keeps the policy-gradient unbiased while reducing variance.</p></li>
<li><p>Converging the critic on each fixed batch (Steps 3–4) approximates the variance-minimizing baseline for that batch before a single actor step, often stabilizing learning in high-variance settings.</p></li>
</ul>
<div class="examplebox">
<div class="example">
<p><span id="exm:cartpole-reinforce-learned-value" class="example"><strong>Example 3.2  (REINFORCE with a Learned Value Baseline for Cart-Pole) </strong></span>Consider the same cart-pole balancing task in Example <a href="policy-gradient.html#exm:cartpole-reinforce">3.1</a>. We use minibatch REINFORCE with a learned value baseline (batch size <span class="math inline">\(50\)</span>), the algorithm described above.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-minibatch-reinforce-learned-value">3.5</a> shows the learning curve. The algorithm is able to steadily increase the per-episode returns.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-policy-rollout-minibatch-reinforce-learned-value">3.6</a> shows a rollout of the system trajectory under the learned policy.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_reinforce_learned_value.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-minibatch-reinforce-learned-value"></span>
<img src="images/Policy-Gradients/cartpole_returns_minibatch_reinforce_value_baseline.png" alt="Learning curve (Minibatch REINFORCE with a Learned Value Baseline)." width="60%" />
<p class="caption">
Figure 3.5: Learning curve (Minibatch REINFORCE with a Learned Value Baseline).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-policy-rollout-minibatch-reinforce-learned-value"></span>
<img src="images/Policy-Gradients/cartpole_policy_rollout_minibatch_reinforce_value_baseline.gif" alt="Policy rollout (Minibatch REINFORCE with a Learned Value Baseline)." width="60%" />
<p class="caption">
Figure 3.6: Policy rollout (Minibatch REINFORCE with a Learned Value Baseline).
</p>
</div>
</div>
</div>
<!-- ## Actor–Critic Methods

Actor–critic (AC) algorithms marry **policy gradients** (the *actor*) with **value function learning** (the *critic*). The critic reduces variance by supplying low-noise estimates of action quality (values or advantages), while the actor updates the policy using these estimates. In contrast to pure Monte Carlo baselines, actor–critic **bootstraps** from its own predictions, enabling **online**, **incremental**, and often far more **sample-efficient** learning.

### Anatomy of an Actor–Critic

- **Actor (policy):** a differentiable policy $\pi_\theta(a\mid s)$.
- **Critic (value):** an approximator for $V_\phi(s)$, $Q_\psi(s,a)$, or directly $A_\eta(s,a)$.
- **Update coupling:** the actor ascends a baseline-subtracted log-likelihood objective using *advantage-like* targets supplied by the critic.

Concretely, the actor uses the generic gradient form
\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N\sum_{t=0}^{T^{(i)}-1}
\gamma^t \,\nabla_\theta\log\pi_\theta\!\big(a_t^{(i)}\mid s_t^{(i)}\big)\,\widehat{A}_t^{(i)},
(\#eq:ac-actor-update)
\end{equation}
where $\widehat{A}_t$ is supplied by the critic (examples below).

### TD Residuals and GAE as Advantage Targets

The simplest on-policy AC uses a **value critic** $V_\phi$ and the **TD residual**
\begin{equation}
\delta_t
\;=\;
r_t + \gamma\,V_\phi(s_{t+1}) - V_\phi(s_t).
(\#eq:td-error)
\end{equation}
If $V_\phi\equiv V^\pi$, then $\mathbb{E}[\delta_t\mid s_t,a_t]=A^\pi(s_t,a_t)$, so $\delta_t$ is an unbiased *advantage* target. 

Variance can be further reduced with **generalized advantage estimation (GAE)**:
\begin{equation}
\widehat{A}_t^{(\lambda)}
\;=\;
\sum_{\ell=0}^{\infty} (\gamma\lambda)^\ell\,\delta_{t+\ell},
\qquad \lambda\in[0,1],
(\#eq:gae)
\end{equation}
which trades bias for variance via $\lambda$.

### Learning the Critic

**Value critic ($V$).** Minimize TD error (semi-gradient TD) or fitted value regression:
\begin{equation}
\phi \leftarrow \phi - \alpha_\phi \nabla_\phi
\Big(r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)\Big)^2.
(\#eq:critic-v-update)
\end{equation}

**Action-value critic ($Q$).** Regress to one-step bootstrapped targets:
\begin{equation}
\psi \leftarrow \psi - \alpha_\psi \nabla_\psi
\Big(r_t + \gamma \,\mathbb{E}_{a'\sim\pi_\theta(\cdot\mid s_{t+1})}[Q_\psi(s_{t+1},a')] - Q_\psi(s_t,a_t)\Big)^2.
(\#eq:critic-q-update)
\end{equation}

**Advantage critic ($A$).** Fit $A_\eta(s,a)$ directly and use $\widehat{A}_t=A_\eta(s_t,a_t)$ in \@ref(eq:ac-actor-update). In practice, $A$ is often represented implicitly via $Q-V$.

::: {.theorembox}
::: {.theorem #compatible-fa name="Compatible Function Approximation and Natural Gradient"}
Let the critic be $Q_w(s,a)=w^\top \nabla_\theta \log \pi_\theta(a\mid s)$ (compatible features), and let $w$ minimize
$\mathbb{E}_{s\sim d_\pi,a\sim\pi}\!\big[(Q_w(s,a)-A^\pi(s,a))^2\big]$.
Then the policy gradient equals a **natural gradient** direction:
\[
\nabla_\theta J(\theta) \;=\; F(\theta)\,w,
\]
where $F(\theta)=\mathbb{E}_{s,a}\!\big[\nabla_\theta \log\pi\,\nabla_\theta \log\pi^\top\big]$ is the Fisher information matrix. 
:::
:::

This result motivates critics that approximate advantages in the **policy’s score-feature space**, yielding well-conditioned actor updates.

### On-Policy Actor–Critic (TD/GAE)

::: {.highlightbox}
<div style="text-align:center;">**On-Policy Advantage Actor–Critic (TD/GAE)**</div>

**Inputs:** $\pi_\theta$, $V_\phi$, $\gamma\in[0,1)$, $\lambda\in[0,1]$, stepsizes $\alpha_\theta,\alpha_\phi$, batch size $N$.

1. **Collect rollouts.** Run $\pi_\theta$ to gather $\{(s_t,a_t,r_t,s_{t+1})\}$ for $N$ trajectories.
2. **Compute TD errors.** $\delta_t \leftarrow r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)$.
3. **Advantages.** $\widehat{A}_t \leftarrow$ either $\delta_t$ or GAE from \@ref(eq:gae).
4. **Actor update.** $\theta \leftarrow \theta + \alpha_\theta \frac{1}{N}\sum \gamma^t \nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t$.
5. **Critic update.** Minimize $\sum (r_t+\gamma V_\phi(s_{t+1})-V_\phi(s_t))^2$ as in \@ref(eq:critic-v-update).
6. **Repeat.**
:::

**Synchronous (A2C) and Asynchronous (A3C).**  
A2C aggregates gradients from multiple parallel workers *synchronously*, while A3C applies them *asynchronously* (Hogwild-style), improving exploration and wall-clock efficiency. Both are instances of the template above.

### Off-Policy Actor–Critic and Importance Sampling

When data come from a **behavior policy** $\mu$ (e.g., replay buffer), correct the distribution mismatch with **per-decision importance ratios**
\begin{equation}
\rho_t \;=\; \frac{\pi_\theta(a_t\mid s_t)}{\mu(a_t\mid s_t)}.
(\#eq:is-ratio)
\end{equation}
A basic off-policy actor update is
\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\mathbb{E}\big[\rho_t\,\nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t\big],
(\#eq:offpolicy-ac)
\end{equation}
often with **clipping** (e.g., $\min\{\bar\rho,\rho_t\}$) to control variance. Modern variants use **trace corrections** (e.g., Retrace, V-trace) to stabilize value targets while keeping bias small.

::: {.highlightbox}
<div style="text-align:center;">**Experience-Replay Off-Policy AC (with clipped IS)**</div>

1. Interact with the environment using $\mu$ (e.g., slightly exploratory $\pi$) and store transitions in a buffer.  
2. Sample minibatches $(s_t,a_t,r_t,s_{t+1})$.  
3. Critic: update $V_\phi$ or $Q_\psi$ with off-policy targets (optionally with trace corrections).  
4. Actor: update with \@ref(eq:offpolicy-ac) using clipped $\rho_t$.  
5. Periodically refresh $\mu$ toward $\pi_\theta$.
:::

### Deterministic Actor–Critic (DPG, DDPG, TD3)

For continuous actions, a **deterministic policy** $\mu_\theta:\mathcal{S}\to\mathcal{A}$ yields the **deterministic policy gradient**:
\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{s\sim d^\mu}\!\Big[\nabla_\theta \mu_\theta(s)\,\nabla_a Q^\mu(s,a)\big|_{a=\mu_\theta(s)}\Big].
(\#eq:dpg)
\end{equation}
In practice (DDPG/TD3), learn a critic $Q_\psi$ off-policy with replay:
\begin{equation}
\psi \leftarrow \psi - \alpha_\psi \nabla_\psi
\Big(r + \gamma\, Q_{\bar\psi}\!\big(s',\,\mu_{\bar\theta}(s')\big) - Q_\psi(s,a)\Big)^2,
(\#eq:ddpg-critic)
\end{equation}
and update the actor with the sampled gradient from \@ref(eq:dpg).  
**Stability tricks:** target networks $(\bar\theta,\bar\psi)$, action noise for exploration, **TD3**’s *twin critics* and *target policy smoothing* (reduces overestimation).

### Entropy-Regularized Actor–Critic and SAC

Max-entropy RL augments rewards with policy entropy to encourage exploration:
\begin{equation}
J(\pi)
\;=\;
\mathbb{E}\!\Big[\sum_{t\ge 0}\gamma^t \big(r(s_t,a_t)\;+\;\alpha\,\mathcal{H}\!\left(\pi(\cdot\mid s_t)\right)\big)\Big],
\quad
\mathcal{H}(\pi(\cdot\mid s)) = -\mathbb{E}_{a\sim\pi}\![\log\pi(a\mid s)].
(\#eq:soft-objective)
\end{equation}
The associated **soft Bellman backup** for $Q$ is
\begin{equation}
Q(s,a)
\;\leftarrow\;
r(s,a) + \gamma\,\mathbb{E}_{s'}\!\Big[V(s')\Big],
\quad
V(s) \;=\; \mathbb{E}_{a\sim\pi}\!\big[Q(s,a) - \alpha \log \pi(a\mid s)\big].
(\#eq:soft-bellman)
\end{equation}
**Soft Actor–Critic (SAC)** uses (i) reparameterized stochastic Gaussian policies, (ii) twin $Q$ critics, and (iii) **automatic temperature tuning** by minimizing
\begin{equation}
\mathcal{L}(\alpha) \;=\; \mathbb{E}_{a\sim\pi}\!\big[-\alpha\,(\log\pi(a\mid s)+\bar{\mathcal{H}})\big],
(\#eq:sac-alpha)
\end{equation}
driving the policy’s entropy toward a target $\bar{\mathcal{H}}$. The actor is trained by minimizing the KL to a Boltzmann distribution induced by $Q$:
\begin{equation}
\theta \; \leftarrow \; \arg\min_\theta\;
\mathbb{E}_{s\sim\mathcal{D}}\!\Big[
\mathrm{KL}\!\Big(\pi_\theta(\cdot\mid s)\;\big\|\;
\frac{\exp\{\tfrac{1}{\alpha}Q(s,\cdot)\}}{Z(s)}\Big)
\Big].
(\#eq:sac-policy)
\end{equation}

### Practical Refinements

- **Entropy bonus (on-policy):** add $\beta\,\mathbb{E}[\nabla_\theta\log\pi_\theta \,(-\log\pi_\theta)]$ to encourage exploration.  
- **Advantage normalization:** center/scale $\widehat{A}_t$ within a batch.  
- **Gradient clipping & value clipping:** stabilize learning.  
- **Target networks & Polyak averaging:** reduce moving-target drift for bootstrapped critics.  
- **Parallel workers:** better state coverage and lower variance (A2C/A3C).

### Putting It Together: Minimal A2C (TD + Entropy)

::: {.highlightbox}
<div style="text-align:center;">**Minimal A2C (Single Machine, Parallel Envs)**</div>

**Inputs:** $\pi_\theta$, $V_\phi$, $\gamma$, rollout length $K$, workers $M$, stepsizes $\alpha_\theta,\alpha_\phi$, entropy weight $\beta$.

For iterations $k=0,1,\dots$:

1. **Parallel rollouts:** each worker runs $\pi_\theta$ for $K$ steps, collecting $\{(s_t,a_t,r_t,s_{t+1})\}_{t=0}^{K-1}$.
2. **Compute $\delta_t$ and $\widehat{A}_t$:** use \@ref(eq:td-error) (or GAE, \@ref(eq:gae)).  
3. **Actor update:**
\[
\theta \leftarrow \theta + \alpha_\theta \frac{1}{MK}\sum\nolimits_{m,t}
\Big(
\nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t
\;+\; \beta\,\nabla_\theta \mathcal{H}\big(\pi_\theta(\cdot\mid s_t)\big)
\Big).
\]
4. **Critic update:** minimize $\sum_{m,t}\delta_t^2$ as in \@ref(eq:critic-v-update).  
5. **Repeat.**
:::

### When to Use Which AC Variant?

- **On-policy A2C/A3C (+GAE):** stable, simple, good for simulators where fresh data are cheap.  
- **DPG/DDPG/TD3:** high-dimensional continuous control when off-policy data and replay are crucial.  
- **SAC:** robust defaults for continuous control; strong exploration via entropy and reliable critics.

### Summary

Actor–critic methods reduce variance via bootstrapped critics while preserving the core policy-gradient structure. By choosing appropriate critics (value, action-value, or soft variants), off-policy corrections, and stability tricks (GAE, target networks, twin critics), AC algorithms form the backbone of modern deep RL for both discrete and continuous control. -->

</div>
</div>
</div>
</div>



</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-barto2012neuronlike" class="csl-entry">
Barto, Andrew G, Richard S Sutton, and Charles W Anderson. 2012. <span>“Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, no. 5: 834–46.
</div>
<div id="ref-garrigos2023handbook" class="csl-entry">
Garrigos, Guillaume, and Robert M Gower. 2023. <span>“Handbook of Convergence Theorems for (Stochastic) Gradient Methods.”</span> <em>arXiv Preprint arXiv:2301.11235</em>.
</div>
<div id="ref-nesterov2018lectures" class="csl-entry">
Nesterov, Yurii. 2018. <em>Lectures on Convex Optimization</em>. Vol. 137. Springer.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="value-rl.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appconvex.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/03-policy-gradient.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
