<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-10-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="value-rl.html"/>
<link rel="next" href="model-based-plan-optimize.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.1.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.1.4" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.1.4</b> Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.2</b> Infinite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mdp.html"><a href="mdp.html#value-functions"><i class="fa fa-check"></i><b>1.2.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation-1"><i class="fa fa-check"></i><b>1.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.2.3" data-path="mdp.html"><a href="mdp.html#principle-of-optimality"><i class="fa fa-check"></i><b>1.2.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.2.4" data-path="mdp.html"><a href="mdp.html#policy-improvement"><i class="fa fa-check"></i><b>1.2.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="1.2.5" data-path="mdp.html"><a href="mdp.html#policy-iteration"><i class="fa fa-check"></i><b>1.2.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="1.2.6" data-path="mdp.html"><a href="mdp.html#value-iteration"><i class="fa fa-check"></i><b>1.2.6</b> Value Iteration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="value-rl.html"><a href="value-rl.html"><i class="fa fa-check"></i><b>2</b> Value-based Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="value-rl.html"><a href="value-rl.html#tabular-methods"><i class="fa fa-check"></i><b>2.1</b> Tabular Methods</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="value-rl.html"><a href="value-rl.html#tabular-PE"><i class="fa fa-check"></i><b>2.1.1</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.1.2" data-path="value-rl.html"><a href="value-rl.html#value-rl-convergence-td"><i class="fa fa-check"></i><b>2.1.2</b> Convergence Proof of TD Learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control"><i class="fa fa-check"></i><b>2.1.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.1.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control"><i class="fa fa-check"></i><b>2.1.4</b> Off-Policy Control</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="value-rl.html"><a href="value-rl.html#function-approximation"><i class="fa fa-check"></i><b>2.2</b> Function Approximation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="value-rl.html"><a href="value-rl.html#basics-of-continuous-mdp"><i class="fa fa-check"></i><b>2.2.1</b> Basics of Continuous MDP</a></li>
<li class="chapter" data-level="2.2.2" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-2"><i class="fa fa-check"></i><b>2.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.2.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control-1"><i class="fa fa-check"></i><b>2.2.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.2.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control-1"><i class="fa fa-check"></i><b>2.2.4</b> Off-Policy Control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="policy-gradient.html"><a href="policy-gradient.html"><i class="fa fa-check"></i><b>3</b> Policy Gradient Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-optimization"><i class="fa fa-check"></i><b>3.1</b> Gradient-based Optimization</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="policy-gradient.html"><a href="policy-gradient.html#basic-setup"><i class="fa fa-check"></i><b>3.1.1</b> Basic Setup</a></li>
<li class="chapter" data-level="3.1.2" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-ascent-and-descent"><i class="fa fa-check"></i><b>3.1.2</b> Gradient Ascent and Descent</a></li>
<li class="chapter" data-level="3.1.3" data-path="policy-gradient.html"><a href="policy-gradient.html#stochastic-gradients"><i class="fa fa-check"></i><b>3.1.3</b> Stochastic Gradients</a></li>
<li class="chapter" data-level="3.1.4" data-path="policy-gradient.html"><a href="policy-gradient.html#beyond-vanilla-gradient-methods"><i class="fa fa-check"></i><b>3.1.4</b> Beyond Vanilla Gradient Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#policy-gradients"><i class="fa fa-check"></i><b>3.2</b> Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="policy-gradient.html"><a href="policy-gradient.html#setup"><i class="fa fa-check"></i><b>3.2.1</b> Setup</a></li>
<li class="chapter" data-level="3.2.2" data-path="policy-gradient.html"><a href="policy-gradient.html#the-policy-gradient-lemma"><i class="fa fa-check"></i><b>3.2.2</b> The Policy Gradient Lemma</a></li>
<li class="chapter" data-level="3.2.3" data-path="policy-gradient.html"><a href="policy-gradient.html#reinforce"><i class="fa fa-check"></i><b>3.2.3</b> REINFORCE</a></li>
<li class="chapter" data-level="3.2.4" data-path="policy-gradient.html"><a href="policy-gradient.html#baselines-and-variance-reduction"><i class="fa fa-check"></i><b>3.2.4</b> Baselines and Variance Reduction</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#actorcritic-methods"><i class="fa fa-check"></i><b>3.3</b> Actor–Critic Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#anatomy-of-an-actorcritic"><i class="fa fa-check"></i><b>3.3.1</b> Anatomy of an Actor–Critic</a></li>
<li class="chapter" data-level="3.3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#ActorCriticTD"><i class="fa fa-check"></i><b>3.3.2</b> On-Policy Actor–Critic with TD(0)</a></li>
<li class="chapter" data-level="3.3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#PG-GAE"><i class="fa fa-check"></i><b>3.3.3</b> Generalized Advantage Estimation (GAE)</a></li>
<li class="chapter" data-level="3.3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#off-policy-actorcritic"><i class="fa fa-check"></i><b>3.3.4</b> Off-Policy Actor–Critic</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#advanced-policy-gradients"><i class="fa fa-check"></i><b>3.4</b> Advanced Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="policy-gradient.html"><a href="policy-gradient.html#revisiting-generalized-policy-iteration"><i class="fa fa-check"></i><b>3.4.1</b> Revisiting Generalized Policy Iteration</a></li>
<li class="chapter" data-level="3.4.2" data-path="policy-gradient.html"><a href="policy-gradient.html#performance-difference-lemma"><i class="fa fa-check"></i><b>3.4.2</b> Performance Difference Lemma</a></li>
<li class="chapter" data-level="3.4.3" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-constraint"><i class="fa fa-check"></i><b>3.4.3</b> Trust Region Constraint</a></li>
<li class="chapter" data-level="3.4.4" data-path="policy-gradient.html"><a href="policy-gradient.html#natural-policy-gradient"><i class="fa fa-check"></i><b>3.4.4</b> Natural Policy Gradient</a></li>
<li class="chapter" data-level="3.4.5" data-path="policy-gradient.html"><a href="policy-gradient.html#proof-fisher"><i class="fa fa-check"></i><b>3.4.5</b> Proof of Fisher Information</a></li>
<li class="chapter" data-level="3.4.6" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-policy-optimization"><i class="fa fa-check"></i><b>3.4.6</b> Trust Region Policy Optimization</a></li>
<li class="chapter" data-level="3.4.7" data-path="policy-gradient.html"><a href="policy-gradient.html#proximal-policy-optimization"><i class="fa fa-check"></i><b>3.4.7</b> Proximal Policy Optimization</a></li>
<li class="chapter" data-level="3.4.8" data-path="policy-gradient.html"><a href="policy-gradient.html#soft-actorcritic"><i class="fa fa-check"></i><b>3.4.8</b> Soft Actor–Critic</a></li>
<li class="chapter" data-level="3.4.9" data-path="policy-gradient.html"><a href="policy-gradient.html#deterministic-policy-gradient"><i class="fa fa-check"></i><b>3.4.9</b> Deterministic Policy Gradient</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="policy-gradient.html"><a href="policy-gradient.html#model-based-policy-optimization"><i class="fa fa-check"></i><b>3.5</b> Model-based Policy Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html"><i class="fa fa-check"></i><b>4</b> Model-based Planning and Optimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr"><i class="fa fa-check"></i><b>4.1</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#finite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.1</b> Finite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#infinite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.2</b> Infinite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#linear-system-basics"><i class="fa fa-check"></i><b>4.1.3</b> Linear System Basics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-materials.html"><a href="advanced-materials.html"><i class="fa fa-check"></i><b>5</b> Advanced Materials</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="policy-gradient" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Policy Gradient Methods<a href="policy-gradient.html#policy-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="value-rl.html#value-rl">2</a>, we relaxed two key assumptions of the MDP introduced in Chapter <a href="mdp.html#mdp">1</a>:</p>
<ul>
<li><strong>Unknown dynamics</strong>: the transition function <span class="math inline">\(P\)</span> was no longer assumed to be known.<br />
</li>
<li><strong>Continuous states</strong>: the state space <span class="math inline">\(\mathcal{S}\)</span> was extended from finite to continuous.</li>
</ul>
<p>When only the dynamics are unknown but the MDP remains tabular, we introduced generalized versions of policy iteration (e.g., SARSA) and value iteration (e.g., Q-learning). These algorithms can recover near-optimal value functions with strong convergence guarantees.</p>
<p>When both the dynamics are unknown and the state space is continuous, tabular methods become infeasible. In this setting, we employed function approximation to represent value functions, and generalized SARSA and Q-learning accordingly. We also introduced stabilization techniques such as experience replay and target networks to ensure more reliable learning.</p>
<hr />
<p>In this chapter, we relax a third assumption: the action space <span class="math inline">\(\mathcal{A}\)</span> is also continuous. This setting captures many important real-world systems, such as autonomous vehicles and robots. Handling continuous actions requires a departure from the value-based methods of Chapter <a href="value-rl.html#value-rl">2</a>. The key difficulty is that even if we had access to a near-optimal action-value function <span class="math inline">\(Q(s,a)\)</span>, selecting the control action requires solving
<span class="math display">\[
\max_a Q(s,a),
\]</span>
which is often computationally expensive and can lead to suboptimal solutions.</p>
<p>To address this challenge, we introduce a new paradigm: policy gradient methods. Rather than learning value functions to derive policies indirectly, we directly optimize parameterized policies using gradient-based methods.</p>
<p>We begin this chapter by reviewing the fundamentals of gradient-based optimization, and then build upon them to develop algorithms for searching optimal policies via policy gradients.</p>
<div id="gradient-optimization" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Gradient-based Optimization<a href="policy-gradient.html#gradient-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gradient-based optimization is the workhorse behind most modern machine learning algorithms, including policy gradient methods. The central idea is to iteratively update the parameters of a model in the direction that most improves an objective function.</p>
<div id="basic-setup" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Basic Setup<a href="policy-gradient.html#basic-setup" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a differentiable objective function <span class="math inline">\(J(\theta)\)</span>, where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> represents the parameter vector. The goal is to find
<span class="math display">\[
\theta^\star \in \arg\max_\theta J(\theta).
\]</span></p>
<p>The gradient of the objective with respect to the parameters,
<span class="math display">\[
\nabla_\theta J(\theta) =
\begin{bmatrix}
\frac{\partial J}{\partial \theta_1} &amp;
\frac{\partial J}{\partial \theta_2} &amp;
\cdots &amp;
\frac{\partial J}{\partial \theta_d}
\end{bmatrix}^\top,
\]</span>
provides the local direction of steepest ascent. Gradient-based optimization uses this direction to iteratively update the parameters. Note that modern machine learning software tools such as PyTorch allow the user to conveniently query the gradient of any function <span class="math inline">\(J\)</span> defined by neural networks.</p>
</div>
<div id="gradient-ascent-and-descent" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Gradient Ascent and Descent<a href="policy-gradient.html#gradient-ascent-and-descent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The simplest method is <strong>gradient ascent</strong> (for maximization):
<span class="math display">\[
\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k),
\]</span>
where <span class="math inline">\(\alpha &gt; 0\)</span> is the learning rate.<br />
For minimization, the update rule uses <strong>gradient descent</strong>:
<span class="math display">\[
\theta_{k+1} = \theta_k - \alpha \nabla_\theta J(\theta_k).
\]</span></p>
<p>The choice of learning rate <span class="math inline">\(\alpha\)</span> is critical:</p>
<ul>
<li>Too large <span class="math inline">\(\alpha\)</span> can cause divergence.<br />
</li>
<li>Too small <span class="math inline">\(\alpha\)</span> leads to slow convergence.</li>
</ul>
<div id="convergence-guarantees" class="section level4 hasAnchor" number="3.1.2.1">
<h4><span class="header-section-number">3.1.2.1</span> Convergence Guarantees<a href="policy-gradient.html#convergence-guarantees" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For convex functions <span class="math inline">\(J(\theta)\)</span>, gradient descent (or ascent) can be shown to converge to the <strong>global optimum</strong> under appropriate conditions on the learning rate.</p>
<p>For non-convex functions—which are common in reinforcement learning—gradient methods may only find so-called <strong>first-order stationary points</strong>, i.e., points <span class="math inline">\(\theta\)</span> at which the gradient <span class="math inline">\(\nabla_\theta J(\theta) = 0\)</span>. Nevertheless, they remain effective in practice.</p>
<p><span class="red">TODO: graph different stationary points</span></p>
<p>We now formalize the convergence speed of Gradient Descent (GD) for minimizing a smooth convex function. We switch to the minimization convention and write the objective as <span class="math inline">\(f:\mathbb{R}^d \to \mathbb{R}\)</span> (to avoid sign confusions with <span class="math inline">\(J\)</span> used for maximization). We assume exact gradients <span class="math inline">\(\nabla f(\theta)\)</span> are available.</p>
<p><strong>Setup and Assumptions.</strong></p>
<ul>
<li><p>(<strong>Convexity</strong>) For all <span class="math inline">\(\theta,\vartheta\in\mathbb{R}^d\)</span>,
<span class="math display" id="eq:PG-GO-convexity">\[\begin{equation}
f(\vartheta) \;\ge\; f(\theta) + \nabla f(\theta)^\top(\vartheta-\theta).
\tag{3.1}
\end{equation}\]</span></p></li>
<li><p>(<strong><span class="math inline">\(L\)</span>-smoothness</strong>) The gradient is <span class="math inline">\(L\)</span>-Lipschitz: for all <span class="math inline">\(\theta,\vartheta\)</span>,
<span class="math display" id="eq:PG-GO-Lsmooth">\[\begin{equation}
\|\nabla f(\vartheta)-\nabla f(\theta)\| \;\le\; L\|\vartheta-\theta\|.
\tag{3.2}
\end{equation}\]</span>
Equivalently (the <strong>descent lemma</strong>), for all <span class="math inline">\(\theta,\Delta\)</span>,
<span class="math display" id="eq:PG-GO-descent-lemma">\[\begin{equation}
f(\theta+\Delta) \;\le\; f(\theta) + \nabla f(\theta)^\top \Delta + \frac{L}{2}\|\Delta\|^2.
\tag{3.3}
\end{equation}\]</span></p></li>
</ul>
<p>Consider Gradient Descent with a constant stepsize <span class="math inline">\(\alpha&gt;0\)</span>:
<span class="math display">\[
\theta_{k+1} \;=\; \theta_k \;-\; \alpha\, \nabla f(\theta_k).
\]</span></p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:gd-convex-smooth" class="theorem"><strong>Theorem 3.1  (GD on smooth convex function) </strong></span>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(L\)</span>-smooth with a minimizer
<span class="math display">\[
\theta^\star\in\arg\min_\theta f(\theta).
\]</span>
and the global minimum <span class="math inline">\(f^\star = f(\theta^\star)\)</span>.
If <span class="math inline">\(0&lt;\alpha\le \frac{2}{L}\)</span>, then the GD iterates satisfy for all <span class="math inline">\(k\ge 0\)</span>:
<span class="math display" id="eq:GD-SmoothConvex-Value">\[\begin{equation}
f(\theta_k) - f^\star \leq \frac{2 (f(\theta_0) - f^\star) \Vert \theta_0 - \theta^\star \Vert^2 }{2 \Vert \theta_0 - \theta^\star \Vert^2 + k\alpha ( 2 - L \alpha) (f(\theta_0) - f^\star)}
\tag{3.4}
\end{equation}\]</span>
In particular, choosing <span class="math inline">\(\alpha=\frac{1}{L}\)</span> yields the canonical <span class="math inline">\(O(1/k)\)</span> convergence rate in suboptimality:
<span class="math display" id="eq:GD-SmoothConvex-optimal-rate">\[\begin{equation}
f(\theta_k) - f^\star \leq \frac{2L \Vert \theta_0 - \theta^\star \Vert^2}{k+4}
\tag{3.5}
\end{equation}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>See Theorem 2.1.14 and Corollary 2.1.2 in <span class="citation">(<a href="#ref-nesterov2018lectures">Nesterov 2018</a>)</span>.</p>
</div>
</div>
<p><strong>Strongly Convex Case (Linear Rate).</strong> If, in addition, <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex (<span class="math inline">\(\mu&gt;0\)</span>), i.e., for all <span class="math inline">\(\theta,\vartheta\in\mathbb{R}^d\)</span>,
<span class="math display" id="eq:PG-GO-strongly-convex">\[\begin{equation}
f(\vartheta)\;\ge\; f(\theta) + \nabla f(\theta)^\top(\vartheta-\theta) \;+\; \frac{\mu}{2}\,\|\vartheta-\theta\|^2.
\tag{3.6}
\end{equation}\]</span>
Then, GD with <span class="math inline">\(0&lt;\alpha\le \frac{2}{\mu + L}\)</span> enjoys a <strong>linear</strong> (geometric) rate:</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:gd-strongly-convex" class="theorem"><strong>Theorem 3.2  (GD on smooth strongly convex function) </strong></span>If <span class="math inline">\(f\)</span> is <span class="math inline">\(L\)</span>-smooth and <span class="math inline">\(\mu\)</span>-strongly convex, then for <span class="math inline">\(0&lt;\alpha\le \frac{2}{\mu + L}\)</span>,
<span class="math display" id="eq:GD-Strongly-Convex-1">\[\begin{equation}
\Vert \theta_k - \theta^\star \Vert^2 \leq \left( 1 - \frac{2\alpha \mu L}{\mu + L} \right)^k \Vert \theta_0 - \theta^\star \Vert^2.
\tag{3.7}
\end{equation}\]</span>
If <span class="math inline">\(\alpha = \frac{2}{\mu + L}\)</span>, then
<span class="math display" id="eq:GD-Strongly-Convex-2">\[\begin{equation}
\begin{split}
\Vert \theta_k - \theta^\star \Vert &amp; \leq \left( \frac{Q_f - 1}{Q_f + 1} \right)^k \Vert \theta_0 - \theta^\star \Vert \\
f(\theta_k) - f^\star &amp; \leq \frac{L}{2} \left( \frac{Q_f - 1}{Q_f + 1} \right)^{2k} \Vert \theta_0 - \theta^\star \Vert^2,
\end{split}
\tag{3.8}
\end{equation}\]</span>
where <span class="math inline">\(Q_f = L/\mu\)</span>.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span>See Theorem 2.1.15 in <span class="citation">(<a href="#ref-nesterov2018lectures">Nesterov 2018</a>)</span>.</p>
</div>
</div>
<p><strong>Practical Notes.</strong></p>
<ul>
<li><p>The step size <span class="math inline">\(\alpha=\frac{1}{L}\)</span> is <strong>optimal among fixed stepsizes</strong> for the above worst-case bounds on smooth convex <span class="math inline">\(f\)</span>.</p></li>
<li><p>In practice, backtracking line search or adaptive schedules can approach similar behavior without knowing <span class="math inline">\(L\)</span>.</p></li>
<li><p>For policy gradients (which maximize <span class="math inline">\(J\)</span>), apply the results to <span class="math inline">\(f=-J\)</span> and flip the update sign (gradient ascent). The smooth/convex assumptions rarely hold globally in RL, but these results calibrate expectations about step sizes and motivate variance reduction and curvature-aware methods used later.</p></li>
</ul>
</div>
</div>
<div id="stochastic-gradients" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Stochastic Gradients<a href="policy-gradient.html#stochastic-gradients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In reinforcement learning and other large-scale machine learning problems, computing the exact gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span> is often infeasible. Instead, we use an unbiased estimator <span class="math inline">\(\hat{\nabla}_\theta J(\theta)\)</span> computed from a subset of data (or trajectories in RL). The update becomes
<span class="math display">\[
\theta_{k+1} = \theta_k + \alpha \hat{\nabla}_\theta J(\theta_k).
\]</span></p>
<p>This approach, known as <strong>stochastic gradient ascent/descent (SGD)</strong>, trades off exactness for computational efficiency. Variance in the gradient estimates plays an important role in convergence speed and stability.</p>
<div id="convergence-guarantees-1" class="section level4 hasAnchor" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> Convergence Guarantees<a href="policy-gradient.html#convergence-guarantees-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We now turn to the convergence guarantees of stochastic gradient methods, which replace exact gradients with unbiased noisy estimates. Throughout this section we consider the minimization problem <span class="math inline">\(\min_\theta f(\theta)\)</span> and assume <span class="math inline">\(\nabla f\)</span> is available only through a stochastic oracle.</p>
<p><strong>Setup and Assumptions.</strong></p>
<p>Let <span class="math inline">\(f:\mathbb{R}^d\!\to\!\mathbb{R}\)</span> be differentiable. At iterate <span class="math inline">\(\theta_k\)</span>, we observe a random vector <span class="math inline">\(g_k\)</span> such that
<span class="math display">\[
\mathbb{E}[\,g_k \mid \theta_k\,] = \nabla f(\theta_k)
\quad\text{and}\quad
\mathbb{E}\!\left[\|g_k-\nabla f(\theta_k)\|^2 \mid \theta_k\right] \le \sigma^2.
\]</span>
We will also use one of the following standard regularity conditions:</p>
<ul>
<li>(<strong>Convex + <span class="math inline">\(L\)</span>-smooth</strong>) <span class="math inline">\(f\)</span> is convex and the gradient is <span class="math inline">\(L\)</span>-Lipschitz.<br />
</li>
<li>(<strong>Strongly convex + <span class="math inline">\(L\)</span>-smooth</strong>) <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex and <span class="math inline">\(L\)</span>-smooth.</li>
</ul>
<p>We consider the SGD update
<span class="math display">\[
\theta_{k+1} \;=\; \theta_k - \alpha_k\, g_k,
\]</span>
and define the <strong>averaged iterate</strong>
<span class="math display">\[
\bar\theta_K := \frac{1}{K+1}\sum_{k=0}^{K}\theta_k.
\]</span></p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:sgd-convex-rate" class="theorem"><strong>Theorem 3.3  (SGD on smooth convex function) </strong></span>Assume <span class="math inline">\(f\)</span> is convex and <span class="math inline">\(L\)</span>-smooth. Suppose there exists <span class="math inline">\(G\!&gt;\!0\)</span> with <span class="math inline">\(\mathbb{E}\|g_k\|^2 \le G^2\)</span> for all <span class="math inline">\(k\)</span>.</p>
<ul>
<li><p>Choose a constant stepsize <span class="math inline">\(\alpha_k = \alpha &gt; 0\)</span>. Then for all <span class="math inline">\(K \ge 1\)</span>,
<span class="math display" id="eq:SGD-convex-fixed-step-size">\[\begin{equation}
\mathbb{E}\big[f(\bar\theta_K)\big] - f^\star \leq \frac{\Vert \theta_0 - \theta^\star \Vert^2}{2 \alpha (K+1)} + \frac{\alpha G^2}{2}.
\tag{3.9}
\end{equation}\]</span></p></li>
<li><p>Choose a diminishing step size <span class="math inline">\(\alpha_k = \frac{\Vert \theta_0 - \theta^\star \Vert}{G \sqrt{k+1}}\)</span>, then
<span class="math display" id="eq:SGD-convex-diminishing-step-size">\[\begin{equation}
\mathbb{E}\big[f(\bar\theta_K)\big] - f^\star \leq \frac{\Vert \theta_0 - \theta^\star \Vert G}{\sqrt{K+1}} = \mathcal{O}\left(  \frac{1}{\sqrt{K}} \right).
\tag{3.10}
\end{equation}\]</span></p></li>
</ul>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span>See this <a href="https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_24.pdf">lecture note</a> and <span class="citation">(<a href="#ref-garrigos2023handbook">Garrigos and Gower 2023</a>)</span>.</p>
</div>
</div>
<p><strong>Remarks.</strong></p>
<ul>
<li><p>The bound is on the <em>averaged</em> iterate <span class="math inline">\(\bar\theta_K\)</span> (the last iterate may be worse by constants without further assumptions).</p></li>
<li><p>Replacing the second-moment bound by a variance bound <span class="math inline">\(\sigma^2\)</span> yields the same rate with <span class="math inline">\(G^2\)</span> replaced by <span class="math inline">\(\sigma^2 + \sup_k\|\nabla f(\theta_k)\|^2\)</span>.</p></li>
<li><p>With a constant stepsize, SGD converges <span class="math inline">\(\mathcal{O}(1/k)\)</span> up to a neighborhood set by the gradient noise.</p></li>
</ul>
<p>The next theorem states the convergence rate of SGD for minimizing strongly convex functions.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:sgd-strong-rate" class="theorem"><strong>Theorem 3.4  (SGD on smooth strongly convex function) </strong></span>Assume <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex and <span class="math inline">\(L\)</span>-smooth, and <span class="math inline">\(\mathbb{E}\!\left[\|g_k\|^2 \right]\le G^2\)</span>.<br />
With stepsize <span class="math inline">\(\alpha_k = \frac{1}{\mu(k+1)}\)</span>, the SGD iterates satisfy for all <span class="math inline">\(K\!\ge\!1\)</span>,
<span class="math display" id="eq:SGD-Strongly-Convex">\[\begin{equation}
\begin{split}
\mathbb{E}[f(\bar\theta_K)] - f^\star &amp; \leq \frac{G^2}{2 \mu (K+1)} (1 + \log(K+1)), \\
\mathbb{E} \Vert \bar\theta_K - \theta^\star \Vert^2 &amp; \leq \frac{Q}{K+1}, \ \ Q = \max \left( \frac{G^2}{\mu^2}, \Vert \theta_0 - \theta^\star \Vert^2 \right).
\end{split}
\tag{3.11}
\end{equation}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-19" class="proof"><em>Proof</em>. </span>See this <a href="https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_24.pdf">lecture note</a> and <span class="citation">(<a href="#ref-garrigos2023handbook">Garrigos and Gower 2023</a>)</span>.</p>
</div>
</div>
<p><strong>Practical Takeaways for Policy Gradients.</strong></p>
<ul>
<li><p>Use <strong>diminishing stepsizes</strong> for theoretical convergence (<span class="math inline">\(\alpha_k \propto 1/\sqrt{k}\)</span> for general convex, <span class="math inline">\(\alpha_k \propto 1/k\)</span> for strongly convex surrogates).</p></li>
<li><p>With <strong>constant stepsizes</strong>, expect fast initial progress down to a variance-limited plateau; lowering variance (e.g., via baselines/advantage estimation) is as important as tuning <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
<p><span class="red">TODO: graph the different trajectories between minimizing a convex function using GD and SGD.</span></p>
</div>
</div>
<div id="beyond-vanilla-gradient-methods" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Beyond Vanilla Gradient Methods<a href="policy-gradient.html#beyond-vanilla-gradient-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Several refinements to basic gradient updates are widely used:</p>
<ul>
<li><strong>Momentum methods</strong>: incorporate past gradients to smooth updates and accelerate convergence.</li>
<li><strong>Adaptive learning rates (Adam, RMSProp, AdaGrad)</strong>: adjust the learning rate per parameter based on historical gradient magnitudes.</li>
<li><strong>Second-order methods</strong>: approximate or use curvature information (the Hessian) for more informed updates, though often impractical in high dimensions.</li>
</ul>
</div>
</div>
<div id="policy-gradients" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Policy Gradients<a href="policy-gradient.html#policy-gradients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Policy gradients optimize a <em>parameterized stochastic policy</em> directly, without requiring an explicit action-value maximization step. They are applicable to both finite and continuous action spaces and are especially useful when actions are continuous or when “<span class="math inline">\(\arg\max\)</span>” over <span class="math inline">\(Q(s,a)\)</span> is costly or ill-posed.</p>
<div id="setup" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Setup<a href="policy-gradient.html#setup" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We consider a Markov decision process (MDP) with (possibly continuous) state space <span class="math inline">\(\mathcal{S}\)</span>, action space <span class="math inline">\(\mathcal{A}\)</span>, unknown dynamics <span class="math inline">\(P\)</span>, reward function <span class="math inline">\(R(s,a)\)</span>, and discount factor <span class="math inline">\(\gamma\in[0,1)\)</span>. Let <span class="math inline">\(\pi_\theta(a\mid s)\)</span> be a differentiable stochastic policy with parameters <span class="math inline">\(\theta\in\mathbb{R}^d\)</span>.</p>
<ul>
<li><p><strong>Trajectory.</strong> A state-action trajectory is <span class="math inline">\(\tau=(s_0,a_0,s_1,a_1,\dots,s_{T})\)</span> with probability density/mass
<span class="math display" id="eq:trajectory-density">\[\begin{equation}
p_\theta(\tau) = \rho(s_0)\prod_{t=0}^{T-1} \pi_\theta(a_t\mid s_t)\,P(s_{t+1}\mid s_t,a_t),
\tag{3.12}
\end{equation}\]</span>
where <span class="math inline">\(\rho\)</span> is the initial state distribution and <span class="math inline">\(T\)</span> is the (random or fixed) episode length.</p></li>
<li><p><strong>Return.</strong> Define the (discounted) return
<span class="math display" id="eq:PG-Trajectory-Return">\[\begin{equation}
R(\tau) \;=\; \sum_{t=0}^{T-1}\gamma^t R(s_t,a_t),
\tag{3.13}
\end{equation}\]</span>
and the return-to-go
<span class="math display" id="eq:PG-return-to-go">\[\begin{equation}
g_t \;=\; \sum_{t&#39;=t}^{T-1}\gamma^{t&#39;-t} R(s_{t&#39;},a_{t&#39;}).
\tag{3.14}
\end{equation}\]</span></p></li>
<li><p><strong>Optimization objective.</strong> The goal is to maximize the expected return
<span class="math display" id="eq:PG-objective">\[\begin{equation}
J(\theta) \;\equiv\; \mathbb{E}_{\tau\sim p_\theta}\!\left[R(\tau)\right]
\;=\; \mathbb{E}\!\left[\sum_{t=0}^{T-1}\gamma^t R(s_t,a_t)\right],
\tag{3.15}
\end{equation}\]</span>
where the expectation is taken over the randomness in (i) the initial state <span class="math inline">\(s_0 \sim \rho\)</span>, (ii) the policy <span class="math inline">\(\pi_\theta\)</span>, and (iii) the transition dynamics <span class="math inline">\(P\)</span>.</p></li>
</ul>
<div id="policy-models" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Policy models<a href="policy-gradient.html#policy-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Finite action spaces (<span class="math inline">\(\mathcal{A}\)</span> discrete).</strong> A common choice is a <strong>softmax (categorical) policy</strong> over a score (logit) function <span class="math inline">\(f_\theta(s,a)\)</span>:
<span class="math display" id="eq:finite-action-policy">\[\begin{equation}
\pi_\theta(a\mid s)
\;=\;
\frac{\exp\{f_\theta(s,a)\}}{\sum_{a&#39;\in\mathcal{A}}\exp\{f_\theta(s,a&#39;)\}}.
\tag{3.16}
\end{equation}\]</span>
Here we use <span class="math inline">\(\exp\{f_\theta(s,a)\} = e^{f_\theta(s,a)}\)</span> for pretty formatting. Typically <span class="math inline">\(f_\theta\)</span> is a neural network or a linear function over features.</p></li>
<li><p><strong>Continuous action spaces (<span class="math inline">\(\mathcal{A}\subseteq\mathbb{R}^m\)</span>).</strong> A standard choice is a <strong>Gaussian policy</strong>:
<span class="math display" id="eq:continuous-action-policy">\[\begin{equation}
\pi_\theta(a\mid s) \;=\; \mathcal{N}\!\big(a;\;\mu_\theta(s),\,\Sigma_\theta(s)\big),
\tag{3.17}
\end{equation}\]</span>
where <span class="math inline">\(\mu_\theta(s)\)</span> and (often diagonal) covariance <span class="math inline">\(\Sigma_\theta(s)\)</span> are differentiable functions (e.g., neural networks) parameterized by <span class="math inline">\(\theta\)</span>. The policy <span class="math inline">\(\pi_\theta(a \mid s)\)</span> samples actions from the Gaussian parameterized by <span class="math inline">\(\mu_\theta(s)\)</span> and <span class="math inline">\(\Sigma_\theta(s)\)</span>. Other choices include squashed Gaussians (e.g., <span class="math inline">\(\tanh\)</span>) or Beta distributions for bounded actions.</p></li>
</ul>
</div>
</div>
<div id="the-policy-gradient-lemma" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> The Policy Gradient Lemma<a href="policy-gradient.html#the-policy-gradient-lemma" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With the gradient-based optimization machinery from Section <a href="policy-gradient.html#gradient-optimization">3.1</a>, a natural strategy for the policy optimization problem in <a href="policy-gradient.html#eq:PG-objective">(3.15)</a> is gradient ascent on the objective <span class="math inline">\(J(\theta)\)</span>.
Consequently, the central task is to characterize the ascent direction, i.e., to compute <span class="math inline">\(\nabla_\theta J(\theta)\)</span>.</p>
<p>The policy gradient lemma, stated below, provides exactly this characterization. Crucially, it expresses <span class="math inline">\(\nabla_\theta J(\theta)\)</span> in terms of the policy’s score function <span class="math inline">\(\nabla_\theta \log \pi_\theta(a\mid s)\)</span> and returns, without differentiating through the environment dynamics. This likelihood-ratio form makes policy optimization feasible even when the transition model is unknown or non-differentiable.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:policy-gradient-lemma" class="theorem"><strong>Theorem 3.5  (Policy Gradient Lemma) </strong></span>Let <span class="math inline">\(J(\theta)=\mathbb{E}_{\tau \sim p_\theta}[R(\tau)]\)</span> as defined in <a href="policy-gradient.html#eq:PG-objective">(3.15)</a> Then:
<span class="math display" id="eq:PG-PGLemma-1">\[\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Big[R(\tau)\,\nabla_\theta \log p_\theta(\tau)\Big]
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Bigg[\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t\mid s_t)\;R(\tau)\Bigg].
\tag{3.18}
\end{equation}\]</span>
By causality (future action does not affect past reward), the full return can be replaced by return-to-go:
<span class="math display" id="eq:PG-PGLemma-2">\[\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\tau\sim p_\theta} \Bigg[\sum_{t=0}^{T-1} \gamma^t
\nabla_\theta \log \pi_\theta(a_t\mid s_t)\;g_t\Bigg].
\tag{3.19}
\end{equation}\]</span>
Equivalently, using value functions,
<span class="math display" id="eq:PG-PGLemma-3">\[\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\frac{1}{1-\gamma} \mathbb{E}_{s\sim d_\theta,\;a\sim\pi_\theta} \Big[\nabla_\theta \log \pi_\theta(a\mid s)\,Q^{\pi_\theta}(s,a)\Big],
\tag{3.20}
\end{equation}\]</span>
where <span class="math inline">\(d_\theta\)</span> is the (discounted) on-policy state visitation distribution for infinite-horizon MDPs:
<span class="math display" id="eq:state-visitation-distribution">\[\begin{equation}
d_\theta(s) = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t \Pr_\theta(s_t=s).
\tag{3.21}
\end{equation}\]</span></p>
</div>
</div>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>Proof</em>. </span>We prove the three equivalent forms step by step. Throughout, we assume <span class="math inline">\(\theta\)</span> parameterizes only the policy <span class="math inline">\(\pi_\theta\)</span> (not the dynamics <span class="math inline">\(P\)</span> nor the initial distribution <span class="math inline">\(\rho\)</span>), and that interchanging <span class="math inline">\(\nabla_\theta\)</span> with the trajectory integral/sum is justified (e.g., bounded rewards and finite horizon or standard dominated-convergence conditions).
Let the return-to-go <span class="math inline">\(g_t\)</span> be defined as in <a href="policy-gradient.html#eq:PG-return-to-go">(3.14)</a>.</p>
<p><strong>Step 1 (Log-derivative trick).</strong> Write the objective as an expectation over trajectories:
<span class="math display">\[
J(\theta) \;=\; \int R(\tau)\, p_\theta(\tau)\, d\tau.
\]</span>
Differentiate under the integral and use
<span class="math display" id="eq:log-derivative-trick">\[\begin{equation}
\nabla_\theta p_\theta(\tau)=p_\theta(\tau)\nabla_\theta\log p_\theta(\tau)
\tag{3.22}
\end{equation}\]</span>
we can write:
<span class="math display">\[
\nabla_\theta J(\theta)
= \int R(\tau)\,\nabla_\theta p_\theta(\tau)\, d\tau
= \int R(\tau)\, p_\theta(\tau)\,\nabla_\theta \log p_\theta(\tau)\, d\tau
= \mathbb{E}_{\tau\sim p_\theta}\!\big[R(\tau)\,\nabla_\theta \log p_\theta(\tau)\big],
\]</span>
which is <a href="policy-gradient.html#eq:PG-PGLemma-1">(3.18)</a> up to expanding <span class="math inline">\(\log p_\theta(\tau)\)</span>. To see why <a href="policy-gradient.html#eq:log-derivative-trick">(3.22)</a> is true, write
<span class="math display">\[
\nabla_\theta \log p_\theta(\tau) = \frac{1}{p_\theta(\tau)} \nabla_\theta p_\theta(\tau),
\]</span>
using the chain rule.</p>
<p><strong>Step 2 (Policy-only dependence).</strong> Factor the trajectory likelihood/mass:
<span class="math display">\[
p_\theta(\tau)
= \rho(s_0)\,\prod_{t=0}^{T-1}\pi_\theta(a_t\mid s_t)\,P(s_{t+1}\mid s_t,a_t).
\]</span>
Since <span class="math inline">\(\rho\)</span> and <span class="math inline">\(P\)</span> do not depend on <span class="math inline">\(\theta\)</span>,
<span class="math display">\[
\log p_\theta(\tau)
= \text{const} \;+\; \sum_{t=0}^{T-1}\log \pi_\theta(a_t\mid s_t)
\quad\Rightarrow\quad
\nabla_\theta \log p_\theta(\tau) \;=\; \sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t).
\]</span>
Substitute into Step 1 to obtain the second equality in <a href="policy-gradient.html#eq:PG-PGLemma-1">(3.18)</a>:
<span class="math display">\[
\nabla_\theta J(\theta)
= \mathbb{E}_{\tau\sim p_\theta}\!\Bigg[\sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,R(\tau)\Bigg].
\]</span></p>
<p><strong>Step 3 (Causality <span class="math inline">\(\Rightarrow\)</span> return-to-go).</strong> Expand <span class="math inline">\(R(\tau)=\sum_{t=0}^{T-1}\gamma^{t} r_{t}\)</span> (with <span class="math inline">\(r_{t}:=R(s_{t},a_{t})\)</span>) and swap sums:
<span class="math display">\[
\mathbb{E} \Bigg[\sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,R(\tau)\Bigg]
=
\sum_{t=0}^{T-1}\sum_{t&#39;=0}^{T-1}\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\gamma^{t&#39;} r_{t&#39;}\big].
\]</span>
For <span class="math inline">\(t&#39;&lt;t\)</span>, the factor <span class="math inline">\(\gamma^{t&#39;} r_{t&#39;}\)</span> is measurable w.r.t. the history <span class="math inline">\(\mathcal{F}_t=\sigma(s_0,a_0,\dots,s_t)\)</span>, while
<span class="math display">\[
\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big|\,\mathcal{F}_t\big]
= \sum_{a} \pi_\theta(a\mid s_t)\,\nabla_\theta \log \pi_\theta(a\mid s_t) = \nabla_\theta \sum_{a}\pi_\theta(a\mid s_t) = \nabla_\theta 1 = 0,
\]</span>
(and analogously with integrals for continuous <span class="math inline">\(\mathcal{A}\)</span>). Hence by the tower property,
<span class="math display">\[
\mathbb{E} \big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\gamma^{t&#39;} r_{t&#39;}\big]=0\quad\text{for all }t&#39;&lt;t.
\]</span>
Therefore only the terms with <span class="math inline">\(t&#39;\ge t\)</span> survive, and
<span class="math display">\[
\nabla_\theta J(\theta)
= \sum_{t=0}^{T-1}\mathbb{E} \Big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\sum_{t&#39;=t}^{T-1}\gamma^{t&#39;} r_{t&#39;}\Big]
= \mathbb{E} \Bigg[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\Bigg],
\]</span>
which is <a href="policy-gradient.html#eq:PG-PGLemma-2">(3.19)</a>.</p>
<p><strong>Step 4 (Value-function form).</strong> Condition on <span class="math inline">\((s_t,a_t)\)</span> and use the definition of the action-value function:
<span class="math display">\[
Q^{\pi_\theta}(s_t,a_t) \;\equiv\; \mathbb{E}\!\left[g_t \,\middle|\, s_t,a_t\right].
\]</span>
Taking expectations then yields
<span class="math display">\[
\mathbb{E} \big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\big]
= \mathbb{E} \big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,Q^{\pi_\theta}(s_t,a_t)\big].
\]</span>
Summing over <span class="math inline">\(t\)</span> and collecting terms with the (discounted) on-policy state visitation distribution <span class="math inline">\(d_\theta\)</span> (for the infinite-horizon case, e.g., <span class="math inline">\(d_\theta(s)=(1-\gamma)\sum_{t=0}^\infty \gamma^t\,\Pr_\theta(s_t=s)\)</span>; for finite <span class="math inline">\(T\)</span>, use the corresponding finite-horizon weighting), we obtain
<span class="math display">\[
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{s\sim d_\theta,\;a\sim \pi_\theta} \Big[\nabla_\theta \log \pi_\theta(a\mid s)\,Q^{\pi_\theta}(s,a)\Big],
\]</span>
which is <a href="policy-gradient.html#eq:PG-PGLemma-3">(3.20)</a>.</p>
<p><strong>Conclusion.</strong> Combining Steps 1–4 proves all three stated forms of the policy gradient.</p>
</div>
</div>
<div id="reinforce" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> REINFORCE<a href="policy-gradient.html#reinforce" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The policy gradient lemma immediately gives us an algorithm. Specifically, the gradient receipe in <a href="policy-gradient.html#eq:PG-PGLemma-1">(3.18)</a> tells us that if we generate one trajectory <span class="math inline">\(\tau\)</span> by following the policy <span class="math inline">\(\pi\)</span>, then
<span class="math display" id="eq:PG-Estimator-1">\[\begin{equation}
\widehat{\nabla_\theta J} = \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \mid s_t) R(\tau)
\tag{3.23}
\end{equation}\]</span>
is an unbiased estimator of the true gradient.</p>
<p>With this sample gradient estimator, we obtain the classical REINFORCE algorithm.</p>
<div class="highlightbox">
<div style="text-align: center;">
<p><strong>Single-Trajectory (Naive) REINFORCE</strong></p>
</div>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\theta_0\)</span> for the initial policy <span class="math inline">\(\pi_{\theta_0}(a \mid s)\)</span></li>
<li>For <span class="math inline">\(k=0,1,\dots,\)</span> do:</li>
</ol>
<ul>
<li>Obtain a trajectory <span class="math inline">\(\tau \sim p_{\theta_k}\)</span></li>
<li>Compute the stochastic gradient <span class="math inline">\(g_k\)</span> as in <a href="policy-gradient.html#eq:PG-Estimator-1">(3.23)</a></li>
<li>Update <span class="math inline">\(\theta_{k+1} = \theta_k + \alpha_k g_k\)</span></li>
</ul>
</div>
<p>To reduce variance of the gradient estimator, we can use a minibatch of trajectories. For example, given a batch of <span class="math inline">\(N\)</span> trajectories <span class="math inline">\(\{\tau^{(i)}\}_{i=1}^N\)</span> collected by <span class="math inline">\(\pi_\theta\)</span>, define for each timestep the return-to-go
<span class="math display">\[
g_t^{(i)} = \sum_{t&#39;=t}^{T^{(i)}-1} \gamma^{t&#39;-t} R\!\left(s_{t&#39;}^{(i)},a_{t&#39;}^{(i)}\right).
\]</span>
An unbiased gradient estimator, from <a href="policy-gradient.html#eq:PG-PGLemma-2">(3.19)</a> is
<span class="math display" id="eq:PG-Estimator-2">\[\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1} \gamma^t
\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big) g_t^{(i)}.
\tag{3.24}
\end{equation}\]</span></p>
<p>This leads to the following minibatch REINFORCE algorithm.</p>
<div class="highlightbox">
<div style="text-align: center;">
<p><strong>Minibatch REINFORCE</strong></p>
</div>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\theta_0\)</span> for the initial policy <span class="math inline">\(\pi_{\theta_0}(a \mid s)\)</span></li>
<li>For <span class="math inline">\(k=0,1,\dots,\)</span> do:</li>
</ol>
<ul>
<li>Obtain N trajectories <span class="math inline">\(\{ \tau^{(i)} \}_{i=1}^N \sim p_{\theta_k}\)</span></li>
<li>Compute the stochastic gradient <span class="math inline">\(g_k\)</span> as in <a href="policy-gradient.html#eq:PG-Estimator-2">(3.24)</a></li>
<li>Update <span class="math inline">\(\theta_{k+1} = \theta_k + \alpha_k g_k\)</span></li>
</ul>
</div>
<p>We apply both the single-trajectory (naive) REINFORCE and a minibatch variant to the CartPole-v1 balancing task. The results show that variance reduction via minibatching is crucial for stable learning and for obtaining strong policies with policy-gradient methods.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:cartpole-reinforce" class="example"><strong>Example 3.1  (REINFORCE for Cart-Pole Balancing) </strong></span>Consider the cart-pole balancing task illustrated in Fig. <a href="policy-gradient.html#fig:cart-pole-illustration">3.1</a>. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-illustration"></span>
<img src="images/Policy-Gradients/cart_pole.gif" alt="Cart Pole balance." width="60%" />
<p class="caption">
Figure 3.1: Cart Pole balance.
</p>
</div>
<p><strong>State Space.</strong> The state of the cart-pole system is denoted by <span class="math inline">\(s \in \mathcal{S} \subset \mathbb{R}^4\)</span>, containing the position and velocity of the cart, as well as the angle and angular velocity of the pole.</p>
<p><strong>Action Space.</strong> The action space <span class="math inline">\(\mathcal{A}\)</span> is discrete and contains two elements: pushing to the left and pushing to the right.</p>
<p>The dynamics of the MDP is provided by the <a href="https://gymnasium.farama.org/environments/classic_control/cart_pole/">Gym simulator</a> and is described in the original paper <span class="citation">(<a href="#ref-barto2012neuronlike">Barto, Sutton, and Anderson 2012</a>)</span>. At the beginning of the episode, all state variables are randomly initialized in <span class="math inline">\([-0.05,0.05]\)</span> and the goal for the agent is to apply the actions to balance the cart-pole for as long as possible—the agent gets a reward of <span class="math inline">\(+1\)</span> every step if (1) the pole angle remains between <span class="math inline">\(-12^\circ\)</span> and <span class="math inline">\(+12^\circ\)</span> and (2) the cart position remains between <span class="math inline">\(-2.4\)</span> and <span class="math inline">\(2.4\)</span>. The maximum episode length is <span class="math inline">\(500\)</span>.</p>
<p>We design a policy network in the form of <a href="policy-gradient.html#eq:finite-action-policy">(3.16)</a> since the action space is finite.</p>
<p><strong>REINFORCE.</strong> We first apply the naive REINFORCE algorithm where the gradient estimator is computed from a single trajectory as in <a href="policy-gradient.html#eq:PG-Estimator-1">(3.23)</a>. Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-reinforce">3.2</a> shows the learning curve, which indicates that the REINFORCE algorithm was not able to learn a good policy after 2000 episodes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-reinforce"></span>
<img src="images/Policy-Gradients/cartpole_returns_reinforce.png" alt="Learning curve (Naive REINFORCE)." width="60%" />
<p class="caption">
Figure 3.2: Learning curve (Naive REINFORCE).
</p>
</div>
<p><strong>Minibatch REINFORCE.</strong> We then apply the minibatch REINFORCE algorithm where the gradient estimator is computed from multiple (<span class="math inline">\(20\)</span> in our case) trajectories as in <a href="policy-gradient.html#eq:PG-Estimator-2">(3.24)</a>. Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-minibatch-reinforce">3.3</a> shows the learning curve, which shows steady increase in the per-episode return that eventually gets close to the maximum per-episode return <span class="math inline">\(500\)</span>.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-policy-rollout-minibatch-reinforce">3.4</a> shows a rollout video of applying the policy training from minibatch REINFORCE. We can see the policy nicely balances the cart-pole system.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_reinforce.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-minibatch-reinforce"></span>
<img src="images/Policy-Gradients/cartpole_returns_minibatch_reinforce.png" alt="Learning curve (Minibatch REINFORCE)." width="60%" />
<p class="caption">
Figure 3.3: Learning curve (Minibatch REINFORCE).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-policy-rollout-minibatch-reinforce"></span>
<img src="images/Policy-Gradients/cartpole_policy_rollout_minibatch_reinforce.gif" alt="Policy rollout (Minibatch REINFORCE)." width="60%" />
<p class="caption">
Figure 3.4: Policy rollout (Minibatch REINFORCE).
</p>
</div>
</div>
</div>
</div>
<div id="baselines-and-variance-reduction" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Baselines and Variance Reduction<a href="policy-gradient.html#baselines-and-variance-reduction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From the REINFORCE experiments above, we have seen firsthand that <strong>variance reduction</strong> is critical for stable policy-gradient learning.</p>
<p>A natural question is: <em>what framework can we use to systematically reduce the variance of the gradient estimator while preserving unbiasedness?</em></p>
<div id="baseline" class="section level4 hasAnchor" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Baseline<a href="policy-gradient.html#baseline" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A key device is a <strong>baseline</strong> <span class="math inline">\(b:\mathcal{S}\to\mathbb{R}\)</span> added at each timestep:
<span class="math display" id="eq:baseline-estimator">\[\begin{equation}
\widehat{g}
\;=\;
\sum_{t=0}^{T-1} \gamma^t\,\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t - b(s_t)\big).
\tag{3.25}
\end{equation}\]</span></p>
<p>The only difference between <a href="policy-gradient.html#eq:baseline-estimator">(3.25)</a> and the original gradient estimator <a href="policy-gradient.html#eq:PG-PGLemma-2">(3.19)</a> is that the baseline <span class="math inline">\(b(s_t)\)</span> is subtracted from the return-to-go <span class="math inline">\(g_t\)</span>. The next theorem states that any state-only baseline does not change the expectation of the gradient estimator.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:baseline-invariance" class="theorem"><strong>Theorem 3.6  (Baseline Invariance) </strong></span>Let <span class="math inline">\(b:\mathcal{S}\to\mathbb{R}\)</span> be any function independent of the action <span class="math inline">\(a_t\)</span>. Then
<span class="math display">\[
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]=0,
\]</span>
and thus
<span class="math display" id="eq:PG-baseline-form">\[\begin{equation}
\nabla_\theta J(\theta) \;=\;
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t - b(s_t)\big)\right].
\tag{3.26}
\end{equation}\]</span></p>
<p>Equivalently, using action-values,
<span class="math display" id="eq:baseline-estimator-Qvalue">\[\begin{equation}
\nabla_\theta J(\theta)
=
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\theta,\;a\sim\pi_\theta}
\!\Big[\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^{\pi_\theta}(s,a)-b(s)\big)\Big].
\tag{3.27}
\end{equation}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-21" class="proof"><em>Proof</em>. </span>We prove (i) the baseline term has zero expectation, (ii) the baseline-subtracted estimator in <a href="policy-gradient.html#eq:PG-baseline-form">(3.26)</a> is unbiased, and (iii) the equivalent <span class="math inline">\(Q\)</span>-value form <a href="policy-gradient.html#eq:baseline-estimator-Qvalue">(3.27)</a>.</p>
<p>Throughout we assume standard conditions ensuring interchange of expectation and differentiation (e.g., bounded rewards with finite horizon or discounted infinite horizon, and a differentiable policy).</p>
<p><strong>Step 1 (Score-function expectation is zero).</strong> Fix a state <span class="math inline">\(s\in\mathcal{S}\)</span>. The <strong>score function</strong> integrates/sums to zero under the policy:
<span class="math display">\[\begin{equation}
\begin{split}
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s)}\!\big[\nabla_\theta \log \pi_\theta(a\mid s)\big]
&amp; =
\sum_{a\in\mathcal{A}} \pi_\theta(a\mid s)\,\nabla_\theta \log \pi_\theta(a\mid s)
=
\sum_{a\in\mathcal{A}} \nabla_\theta \pi_\theta(a\mid s) \\
&amp; =
\nabla_\theta \sum_{a\in\mathcal{A}} \pi_\theta(a\mid s)
=
\nabla_\theta 1
=
0,
\end{split}
\end{equation}\]</span>
with the obvious replacement of sums by integrals for continuous <span class="math inline">\(\mathcal{A}\)</span>. This identity is the standard “score has zero mean” property.</p>
<p><strong>Step 2 (Baseline term has zero expectation).</strong> Let <span class="math inline">\(\mathcal{F}_t := \sigma(s_0,a_0,\ldots,s_t)\)</span> be the history up to time <span class="math inline">\(t\)</span> and recall that <span class="math inline">\(b(s_t)\)</span> is <strong>independent of</strong> <span class="math inline">\(a_t\)</span>. Using iterated expectations:
<span class="math display">\[
\mathbb{E}\!\left[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]
=
\mathbb{E}\!\left[
\gamma^t\, b(s_t)\,
\underbrace{\mathbb{E}\!\left[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\middle|\, s_t\right]}_{=\,0~\text{by Step 1}}
\right]
= 0.
\]</span>
Summing over <span class="math inline">\(t\)</span> yields
<span class="math display">\[
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]=0.
\]</span></p>
<p><strong>Step 3 (Unbiasedness of the baseline-subtracted estimator).</strong> By the policy gradient lemma (likelihood-ratio form with return-to-go; see <a href="policy-gradient.html#eq:PG-PGLemma-2">(3.19)</a>),
<span class="math display">\[
\nabla_\theta J(\theta)
=
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\right].
\]</span>
Subtract and add the baseline term inside the expectation:
<span class="math display">\[
  \begin{split}
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,g_t\right]
&amp; =
\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\right]
\;+\; \\
&amp; \quad \quad \underbrace{\mathbb{E}\!\left[\sum_{t=0}^{T-1} \gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right]}_{=\,0~\text{by Step 2}}.
\end{split}
\]</span>
Therefore <a href="policy-gradient.html#eq:PG-baseline-form">(3.26)</a> holds, proving that <strong>any</strong> state-only baseline preserves unbiasedness.</p>
<p><strong>Step 4 (Equivalent <span class="math inline">\(Q\)</span>-value form).</strong> Condition on <span class="math inline">\((s_t,a_t)\)</span> and use the definition <span class="math inline">\(Q^{\pi_\theta}(s_t,a_t):=\mathbb{E}[g_t\mid s_t,a_t]\)</span>:
<span class="math display">\[
\mathbb{E}\!\big[\gamma^t \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\big]
=
\mathbb{E}\!\Big[
\gamma^t\,
\mathbb{E}\!\big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\mid s_t\big]
\Big].
\]</span>
Inside the inner expectation (over <span class="math inline">\(a_t\sim \pi_\theta(\cdot\mid s_t)\)</span>) and using <span class="math inline">\(b(s_t)\)</span>’s independence from <span class="math inline">\(a_t\)</span>,
<span class="math display">\[
\mathbb{E}\!\big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(g_t-b(s_t)\big)\mid s_t\big]
=
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s_t)}\!\Big[\nabla_\theta \log \pi_\theta(a\mid s_t)\,\big(Q^{\pi_\theta}(s_t,a)-b(s_t)\big)\Big].
\]</span>
Summing over <span class="math inline">\(t\)</span> with discount <span class="math inline">\(\gamma^t\)</span> and collecting terms with the (discounted) on-policy state-visitation distribution <span class="math inline">\(d_\theta\)</span> (cf. <a href="policy-gradient.html#eq:state-visitation-distribution">(3.21)</a>) yields the infinite-horizon identity
<span class="math display">\[
\nabla_\theta J(\theta)
=
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\theta,\;a\sim \pi_\theta}\!
\Big[\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^{\pi_\theta}(s,a)-b(s)\big)\Big],
\]</span>
which is <a href="policy-gradient.html#eq:baseline-estimator-Qvalue">(3.27)</a>.</p>
</div>
</div>
</div>
<div id="optimal-baseline-and-advantage" class="section level4 hasAnchor" number="3.2.4.2">
<h4><span class="header-section-number">3.2.4.2</span> Optimal Baseline and Advantage<a href="policy-gradient.html#optimal-baseline-and-advantage" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Among all state-only baselines <span class="math inline">\(b(s)\)</span>, which one minimizes the variance of the gradient estimator?</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:variance-minimizing-baseline" class="theorem"><strong>Theorem 3.7  (Variance-Minimizing Baseline (per-state)) </strong></span>For the estimator
<span class="math display">\[
g(s,a)=\nabla_\theta \log \pi_\theta(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big),
\]</span>
the <span class="math inline">\(b(s)\)</span> minimizing <span class="math inline">\(\operatorname{Var}[g\mid s]\)</span> is
<span class="math display" id="eq:best-baseline">\[
b^\star(s)=
\frac{\mathbb{E}_{a\sim \pi_\theta}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\, Q^\pi(s,a)\right]}
{\mathbb{E}_{a\sim \pi_\theta}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\right]}.
\tag{3.28}
\]</span>
Assuming that the norm factor <span class="math inline">\(\Vert \nabla_\theta \log \pi_\theta(a\mid s) \Vert^2\)</span> varies slowly with <span class="math inline">\(a\)</span>, then
<span class="math display">\[
b^\star(s) \approx V^\pi(s).
\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-22" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(s\in\mathcal{S}\)</span> be fixed and write
<span class="math display">\[
u(a\mid s) \;\equiv\; \nabla_\theta \log \pi_\theta(a\mid s)\in\mathbb{R}^d,
\qquad
w(a\mid s) \;\equiv\; \|u(a\mid s)\|^2 \;\ge 0.
\]</span>
Consider the vector-valued random variable
<span class="math display">\[
g(s,a) \;=\; u(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big),
\]</span>
where the randomness is over <span class="math inline">\(a\sim \pi_\theta(\cdot\mid s)\)</span>.</p>
<p>We aim to choose <span class="math inline">\(b(s)\in\mathbb{R}\)</span> to minimize the <strong>conditional variance</strong>
<span class="math display">\[
\operatorname{Var}[g\mid s] \;=\; \mathbb{E}\!\left[\|g(s,a)-\mathbb{E}[g\mid s]\|^2 \,\middle|\, s\right].
\]</span>
Using the identity <span class="math inline">\(\operatorname{Var}[X]=\mathbb{E}\|X\|^2-\|\mathbb{E}X\|^2\)</span> (for vector <span class="math inline">\(X\)</span> with Euclidean norm), we have
<span class="math display">\[
\operatorname{Var}[g\mid s]
\;=\;
\underbrace{\mathbb{E}\!\left[\|g(s,a)\|^2 \mid s\right]}_{\text{depends on } b(s)}
\;-\;
\underbrace{\big\|\mathbb{E}[g\mid s]\big\|^2}_{\text{independent of } b(s)}.
\]</span>
We first show that the mean term is independent of <span class="math inline">\(b(s)\)</span>. Indeed,
<span class="math display">\[
\mathbb{E}[g\mid s]
=
\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\big[u(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)\big]
=
\mathbb{E}\!\big[u(a\mid s)\,Q^\pi(s,a)\big]
\;-\;
b(s)\,\underbrace{\mathbb{E}\!\big[u(a\mid s)\big]}_{=\,0},
\]</span>
where <span class="math inline">\(\mathbb{E}[u(a\mid s)]=\sum_a \pi_\theta(a\mid s)\nabla_\theta\log\pi_\theta(a\mid s)=\nabla_\theta \sum_a \pi_\theta(a\mid s)=\nabla_\theta 1=0\)</span> (replace sums by integrals in the continuous case). Therefore <span class="math inline">\(\mathbb{E}[g\mid s]\)</span> does <strong>not</strong> depend on <span class="math inline">\(b(s)\)</span>.</p>
<p>Consequently, minimizing <span class="math inline">\(\operatorname{Var}[g\mid s]\)</span> is equivalent to minimizing the conditional <strong>second moment</strong>
<span class="math display">\[
\mathbb{E}\!\left[\|g(s,a)\|^2 \mid s\right]
=
\mathbb{E}\!\left[\|u(a\mid s)\|^2 \,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right]
=
\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right].
\]</span>
The right-hand side is a convex quadratic in the scalar <span class="math inline">\(b(s)\)</span>. Differentiate w.r.t. <span class="math inline">\(b(s)\)</span> and set to zero:
<span class="math display">\[
\frac{\partial}{\partial b(s)}
\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)^2 \,\middle|\, s\right]
=
-2\,\mathbb{E}\!\left[w(a\mid s)\,\big(Q^\pi(s,a)-b(s)\big)\,\middle|\, s\right]
= 0.
\]</span>
Hence,
<span class="math display">\[
\mathbb{E}\!\left[w(a\mid s)\,Q^\pi(s,a)\,\middle|\, s\right]
=
b(s)\,\mathbb{E}\!\left[w(a\mid s)\,\middle|\, s\right],
\]</span>
and provided <span class="math inline">\(\mathbb{E}[w(a\mid s)\mid s]&gt;0\)</span> (i.e., the Fisher information at <span class="math inline">\(s\)</span> is non-degenerate), the unique minimizer is
<span class="math display">\[
b^\star(s)
=
\frac{\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\, Q^\pi(s,a)\right]}
{\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}\!\left[\| \nabla_\theta \log \pi_\theta(a\mid s)\|^2\right]},
\]</span>
which is <a href="policy-gradient.html#eq:best-baseline">(3.28)</a>. If <span class="math inline">\(\mathbb{E}[w(a\mid s)\mid s]=0\)</span> (e.g., a locally deterministic policy), then <span class="math inline">\(g\equiv 0\)</span> almost surely and any <span class="math inline">\(b(s)\)</span> attains the minimum.</p>
<p>Finally, when the weight <span class="math inline">\(w(a\mid s)=\|\nabla_\theta \log \pi_\theta(a\mid s)\|^2\)</span> varies slowly with <span class="math inline">\(a\)</span> (or is approximately constant) for a fixed <span class="math inline">\(s\)</span>, the ratio simplifies to
<span class="math display">\[
b^\star(s)\;\approx\;\frac{\mathbb{E}[c(s)\,Q^\pi(s,a)\mid s]}{\mathbb{E}[c(s)\mid s]}
\;=\;
\mathbb{E}_{a\sim \pi_\theta(\cdot\mid s)}\!\big[Q^\pi(s,a)\big]
\;=\;
V^\pi(s),
\]</span>
so that the baseline-subtracted target becomes the <strong>advantage</strong>
<span class="math inline">\(A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)\)</span>.</p>
</div>
</div>
<p>When using <span class="math inline">\(V^\pi(s)\)</span> as the baseline, the baseline-subtracted target is called the <strong>advantage function</strong>
<span class="math display" id="eq:advantage-def">\[\begin{equation}
A^{\pi_\theta}(s,a)\;=\;Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s).
\tag{3.29}
\end{equation}\]</span>
The corresponding minibatch gradient estimator becomes
<span class="math display" id="eq:reinforce-adv-estimator">\[\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1} \gamma^t\,
\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big)\,
\widehat{A}_t^{(i)},
\quad
\widehat{A}_t^{(i)} \approx g_t^{(i)} - V_\phi \big(s_t^{(i)}\big),
\tag{3.30}
\end{equation}\]</span>
where <span class="math inline">\(V_\phi\)</span> is a learned approximation to <span class="math inline">\(V^{\pi_\theta}\)</span>.</p>
</div>
<div id="intuition-for-the-advantage" class="section level4 hasAnchor" number="3.2.4.3">
<h4><span class="header-section-number">3.2.4.3</span> Intuition for the Advantage<a href="policy-gradient.html#intuition-for-the-advantage" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The advantage
<span class="math display">\[
A^\pi(s,a) \;=\; Q^\pi(s,a) - V^\pi(s)
\]</span>
measures how much <em>better or worse</em> action <span class="math inline">\(a\)</span> is at state <span class="math inline">\(s\)</span> <em>relative to the policy’s average action quality</em> <span class="math inline">\(V^\pi(s)=\mathbb{E}_{a\sim\pi}[Q^\pi(s,a)\mid s]\)</span>.<br />
Hence <span class="math inline">\(\mathbb{E}_{a\sim\pi}[A^\pi(s,a)\mid s]=0\)</span>: it is a <em>relative</em> score.</p>
<p>With a value baseline, the policy-gradient update is
<span class="math display">\[
\nabla_\theta J(\theta)
\;=\;
\frac{1}{1-\gamma}\,
\mathbb{E}_{s\sim d_\pi,\;a\sim\pi}\!\big[
\nabla_\theta \log \pi_\theta(a\mid s)\,A^\pi(s,a)
\big].
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(A^\pi(s,a) &gt; 0\)</span>: the term <span class="math inline">\(\nabla_\theta \log \pi_\theta(a\mid s)\,A^\pi(s,a)\)</span> <strong>increases</strong> <span class="math inline">\(\log \pi_\theta(a\mid s)\)</span> (and thus <span class="math inline">\(\pi_\theta(a\mid s)\)</span>)—the policy puts <strong>more</strong> probability mass on actions that outperformed its average at <span class="math inline">\(s\)</span>.</p></li>
<li><p>If <span class="math inline">\(A^\pi(s,a) &lt; 0\)</span>: it <strong>decreases</strong> <span class="math inline">\(\log \pi_\theta(a\mid s)\)</span>—the policy puts <strong>less</strong> probability mass on actions that underperformed at <span class="math inline">\(s\)</span>.</p></li>
<li><p>If <span class="math inline">\(A^\pi(s,a) \approx 0\)</span>: the action performed about as expected; the update at that <span class="math inline">\((s,a)\)</span> is <strong>negligible</strong>.</p></li>
</ul>
<p>Subtracting <span class="math inline">\(V^\pi(s)\)</span> centers returns <em>per state</em>, so the update depends only on <em>relative</em> goodness. This:</p>
<ul>
<li><p>preserves unbiasedness (baseline invariance),</p></li>
<li><p>reduces variance (no large, shared offset),</p></li>
<li><p>focuses learning on which actions at <span class="math inline">\(s\)</span> should get more/less probability.</p></li>
</ul>
</div>
<div id="REINFORCE-LearnedValue" class="section level4 hasAnchor" number="3.2.4.4">
<h4><span class="header-section-number">3.2.4.4</span> REINFORCE with a Learned Value Baseline<a href="policy-gradient.html#REINFORCE-LearnedValue" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall that in Section <a href="value-rl.html#function-approximation">2.2</a>, we have introduced multiple algorithms that can learn an approximate value function for policy evaluation. For example, we can use Monte Carlo estimation.</p>
<p>We now combine REINFORCE with a learned baseline <span class="math inline">\(V_\phi(s)\approx V^{\pi_\theta}(s)\)</span>, yielding a lower-variance update while keeping the estimator unbiased.</p>
<div class="highlightbox">
<div style="text-align: center;">
<p><strong>Minibatch REINFORCE with a Learned Value Baseline</strong></p>
</div>
<p><strong>Inputs:</strong> policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span>, value <span class="math inline">\(V_\phi(s)\)</span>, discount <span class="math inline">\(\gamma\in[0,1)\)</span>, stepsizes <span class="math inline">\(\alpha_\theta,\alpha_\phi&gt;0\)</span>, batch size <span class="math inline">\(N\)</span>.<br />
<strong>Convergence controls:</strong> tolerance <span class="math inline">\(\varepsilon&gt;0\)</span>, maximum inner steps <span class="math inline">\(K_{\max}\)</span> (value-fit loop), optional patience <span class="math inline">\(P\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Collect trajectories.</strong> Roll out <span class="math inline">\(N\)</span> on-policy trajectories <span class="math inline">\(\{\tau^{(i)}\}_{i=1}^N\)</span> using <span class="math inline">\(\pi_\theta\)</span>.<br />
For each trajectory <span class="math inline">\(i\)</span> and timestep <span class="math inline">\(t\)</span>, record <span class="math inline">\((s_t^{(i)},a_t^{(i)},r_t^{(i)})\)</span>.</p></li>
<li><p><strong>Compute returns-to-go.</strong> For each <span class="math inline">\(i,t\)</span>,
<span class="math display">\[
g_t^{(i)} \;=\; \sum_{t&#39;=t}^{T^{(i)}-1} \gamma^{\,t&#39;-t}\, r_{t&#39;}^{(i)}.
\]</span></p></li>
<li><p><strong>Fit the value to convergence (critic inner loop).</strong> Define the batch regression loss
<span class="math display">\[
\mathcal{L}_V(\phi)
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1}
\big(g_t^{(i)} - V_\phi(s_t^{(i)})\big)^2.
\]</span>
Perform gradient steps on <span class="math inline">\(\phi\)</span> <strong>until convergence</strong> on this fixed batch:
<span class="math display">\[
\phi \leftarrow \phi - \alpha_\phi \,\nabla_\phi \mathcal{L}_V(\phi).
\]</span>
Repeat for <span class="math inline">\(k=1,\dots,K_{\max}\)</span> or until
<span class="math display">\[
\frac{\mathcal{L}_V^{(k-1)}-\mathcal{L}_V^{(k)}}{\max\{1,|\mathcal{L}_V^{(k-1)}|\}} &lt; \varepsilon
\]</span>
for <span class="math inline">\(M\)</span> consecutive checks.
Denote the (approximately) converged parameters by <span class="math inline">\(\phi^\star\)</span>.</p></li>
<li><p><strong>Form (optionally standardized) advantages using the converged value.</strong>
<span class="math display">\[
\widehat{A}_t^{(i)} \;=\; g_t^{(i)} - V_{\phi^\star}\!\big(s_t^{(i)}\big),
\qquad
\tilde{A}_t^{(i)} \;=\; \frac{\widehat{A}_t^{(i)} - \mu_A}{\sigma_A+\delta}\ \ (\text{optional, batch-wise}),
\]</span>
where <span class="math inline">\(\mu_A,\sigma_A\)</span> are the mean and std of <span class="math inline">\(\{\widehat{A}_t^{(i)}\}\)</span> over the <strong>whole</strong> batch, and <span class="math inline">\(\delta&gt;0\)</span> is a small constant.</p></li>
<li><p><strong>Single policy (actor) update.</strong> Using the converged baseline, take <strong>one</strong> ascent step:
<span class="math display">\[
\theta \;\leftarrow\; \theta
\;+\; \alpha_\theta \cdot
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1}
\gamma^t\,\nabla_\theta \log \pi_\theta \big(a_t^{(i)}\mid s_t^{(i)}\big)\,\tilde{A}_t^{(i)}.
\]</span>
<em>(If not standardizing, use <span class="math inline">\(\widehat{A}_t^{(i)}\)</span> in place of <span class="math inline">\(\tilde{A}_t^{(i)}\)</span>.)</em></p></li>
<li><p><strong>Repeat</strong> from Step 1 with the updated policy.</p></li>
</ol>
</div>
<p><strong>Notes.</strong></p>
<ul>
<li><p>By baseline invariance, subtracting <span class="math inline">\(V_{\phi^\star}(s)\)</span> keeps the policy-gradient unbiased while reducing variance.</p></li>
<li><p>Converging the critic on each fixed batch (Steps 3–4) approximates the variance-minimizing baseline for that batch before a single actor step, often stabilizing learning in high-variance settings.</p></li>
</ul>
<div class="examplebox">
<div class="example">
<p><span id="exm:cartpole-reinforce-learned-value" class="example"><strong>Example 3.2  (REINFORCE with a Learned Value Baseline for Cart-Pole) </strong></span>Consider the same cart-pole balancing task in Example <a href="policy-gradient.html#exm:cartpole-reinforce">3.1</a>. We use minibatch REINFORCE with a learned value baseline (batch size <span class="math inline">\(50\)</span>), the algorithm described above.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-minibatch-reinforce-learned-value">3.5</a> shows the learning curve. The algorithm is able to steadily increase the per-episode returns.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-policy-rollout-minibatch-reinforce-learned-value">3.6</a> shows a rollout of the system trajectory under the learned policy.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_reinforce_learned_value.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-minibatch-reinforce-learned-value"></span>
<img src="images/Policy-Gradients/cartpole_returns_minibatch_reinforce_value_baseline.png" alt="Learning curve (Minibatch REINFORCE with a Learned Value Baseline)." width="60%" />
<p class="caption">
Figure 3.5: Learning curve (Minibatch REINFORCE with a Learned Value Baseline).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-policy-rollout-minibatch-reinforce-learned-value"></span>
<img src="images/Policy-Gradients/cartpole_policy_rollout_minibatch_reinforce_value_baseline.gif" alt="Policy rollout (Minibatch REINFORCE with a Learned Value Baseline)." width="60%" />
<p class="caption">
Figure 3.6: Policy rollout (Minibatch REINFORCE with a Learned Value Baseline).
</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="actorcritic-methods" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Actor–Critic Methods<a href="policy-gradient.html#actorcritic-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Actor–critic (AC) algorithms marry <strong>policy gradients</strong> (the <em>actor</em>) with <strong>value function learning</strong> (the <em>critic</em>). The critic reduces variance by supplying low-noise estimates of action quality (values or advantages), while the actor updates the policy using these estimates. In contrast to pure Monte Carlo baselines, actor–critic <strong>bootstraps</strong> from its own predictions, enabling online, incremental, and often more sample-efficient learning.</p>
<div id="anatomy-of-an-actorcritic" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Anatomy of an Actor–Critic<a href="policy-gradient.html#anatomy-of-an-actorcritic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Actor (policy):</strong> a differentiable policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span>.</li>
<li><strong>Critic (value):</strong> an approximator for <span class="math inline">\(V_\phi(s)\)</span>, <span class="math inline">\(Q_\psi(s,a)\)</span>, or directly the advantage <span class="math inline">\(A_\eta(s,a)\)</span>.</li>
<li><strong>Update coupling:</strong> the actor ascends a baseline-subtracted log-likelihood objective using <em>advantage-like</em> targets supplied by the critic.</li>
</ul>
<!-- Concretely, the actor uses the generic gradient form
\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N\sum_{t=0}^{T^{(i)}-1}
\gamma^t \,\nabla_\theta\log\pi_\theta\big(a_t^{(i)}\mid s_t^{(i)}\big)\,\widehat{A}_t^{(i)},
(\#eq:ac-actor-update)
\end{equation}
where $N$ is the number of trajectories in a minibatch and $\widehat{A}_t$ is supplied by the critic. -->
</div>
<div id="ActorCriticTD" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> On-Policy Actor–Critic with TD(0)<a href="policy-gradient.html#ActorCriticTD" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We first learn a state value function <span class="math inline">\(V_\phi(s)\)</span> with a <strong>one-step bootstrapped</strong> TD(0) target:
<span class="math display" id="eq:td0">\[
\delta_t \;\equiv\; r_t + \gamma\,V_\phi(s_{t+1}) - V_\phi(s_t),
\qquad
\mathcal{L}_V(\phi) \;=\; \frac12\,\delta_t^{\,2}.
\tag{3.31}
\]</span>
If <span class="math inline">\(V_\phi \approx V^\pi\)</span>, then <span class="math inline">\(\mathbb{E}[\delta_t\mid s_t,a_t]\approx A^\pi(s_t,a_t)\)</span>, so <span class="math inline">\(\delta_t\)</span> serves as a low-variance <strong>advantage</strong> target for the actor:
<span class="math display" id="eq:ac-actor-delta">\[
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{|\mathcal{B}|}
\sum_{(s_t,a_t)\in \mathcal{B}}
\nabla_\theta \log\pi_\theta(a_t\mid s_t)\,\underbrace{\delta_t}_{\text{advantage target}}.
\tag{3.32}
\]</span>
<em>(Practical: normalize <span class="math inline">\(\{\delta_t\}_{\mathcal{B}}\)</span> to mean <span class="math inline">\(0\)</span> and unit variance within a batch; clip gradients for stability.)</em></p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>On-Policy Actor–Critic with One-Step Bootstrap (TD(0))</strong>
</div>
<p><strong>Inputs:</strong> policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span>, value <span class="math inline">\(V_\phi(s)\)</span>, discount <span class="math inline">\(\gamma\in[0,1)\)</span>, stepsizes <span class="math inline">\(\alpha_\theta,\alpha_\phi&gt;0\)</span>, rollout length <span class="math inline">\(K\)</span>, minibatch size <span class="math inline">\(|\mathcal{B}|\)</span>.</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Collect on-policy rollouts.</strong> Run <span class="math inline">\(\pi_\theta\)</span> for <span class="math inline">\(K\)</span> steps (optionally across parallel envs), storing transitions <span class="math inline">\(\{(s_t,a_t,r_t,s_{t+1}\}\)</span>.</p></li>
<li><p><strong>Compute TD errors.</strong> For each transition, compute the TD error
<span class="math display">\[
\delta_t \leftarrow r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t).
\]</span></p></li>
<li><p><strong>Critic update (value).</strong> Minimize <span class="math inline">\(\sum_{t\in\mathcal{B}} \frac12\,\delta_t^{\,2}\)</span>: perform multiple steps of
<span class="math display">\[
\phi \leftarrow \phi - \alpha_\phi \,\nabla_\phi \Big(\frac{1}{|\mathcal{B}|}\sum_{t\in\mathcal{B}}\frac12\,\delta_t^{\,2}\Big).
\]</span></p></li>
<li><p><strong>Actor advantages.</strong> Set <span class="math inline">\(\widehat{A}_t \leftarrow \delta_t\)</span> (optionally normalize over <span class="math inline">\(\mathcal{B}\)</span>).</p></li>
<li><p><strong>Actor update (policy gradient).</strong>
<span class="math display">\[
\theta \leftarrow \theta + \alpha_\theta \,\frac{1}{|\mathcal{B}|}\sum_{t\in\mathcal{B}}
\nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t.
\]</span></p></li>
<li><p><strong>Repeat</strong> from step 1.</p></li>
</ol>
</div>
<p>We apply the on-policy actor-critic algorithm to the cart-pole balancing task.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:cartpole-actor-critic-TD" class="example"><strong>Example 3.3  (Actor–Critic with One-Step Bootstrap for Cart-Pole) </strong></span>Consider the same cart-pole balancing control task as before, and this time apply the on-policy actor-critic with one-step bootstrap.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-actor-critic-TD">3.7</a> shows the learning curve.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-policy-rollout-actor-critic-TD">3.8</a> shows an example rollout of the policy.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_actor-critic_on-policy_td0.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-actor-critic-TD"></span>
<img src="images/Policy-Gradients/cartpole_returns_actor-critic-td.png" alt="Learning curve (Actor--Critic with One-Step Bootstrap)." width="60%" />
<p class="caption">
Figure 3.7: Learning curve (Actor–Critic with One-Step Bootstrap).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-policy-rollout-actor-critic-TD"></span>
<img src="images/Policy-Gradients/cartpole_policy_rollout_actor-critic-td.gif" alt="Policy rollout (Actor--Critic with One-Step Bootstrap)." width="60%" />
<p class="caption">
Figure 3.8: Policy rollout (Actor–Critic with One-Step Bootstrap).
</p>
</div>
</div>
</div>
</div>
<div id="PG-GAE" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Generalized Advantage Estimation (GAE)<a href="policy-gradient.html#PG-GAE" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In REINFORCE with a learned value baseline (Section <a href="policy-gradient.html#REINFORCE-LearnedValue">3.2.4.4</a>), we used the full Monte Carlo return <span class="math inline">\(g_t\)</span> as the target for value function approximation; while in on-policy Actor-Critic with TD(0) (Section <a href="policy-gradient.html#ActorCriticTD">3.3.2</a>), we used the one-step bootstrap return <span class="math inline">\(r_t + \gamma V_\phi (s_{t+1})\)</span> as the target for value function estimation.</p>
<p>Recall in policy evaluation (Section <a href="value-rl.html#tabular-PE">2.1.1</a>), we have introduced a spectrum of methods that sit in between Monte Carlo and TD(0): they are methods that leverage the <span class="math inline">\(n\)</span>-step bootstrap return that balance bias and variance. (Section <a href="value-rl.html#PE-MultiStepTD">2.1.1.3</a> and <a href="value-rl.html#TDlambda">2.1.1.4</a>).</p>
<p>In particular, recall the definition of an <span class="math inline">\(n\)</span>-step bootstrap return
<span class="math display" id="eq:nstep-target">\[\begin{equation}
g^{(n)}_t = \sum_{k=0}^{n-1} \gamma^k r_{t+k} \;+\; \gamma^{n}\,V_\phi(s_{t+n}),
\tag{3.33}
\end{equation}\]</span>
where <span class="math inline">\(V_\phi\)</span> denotes the approximate value function.
The <span class="math inline">\(\lambda\)</span>-return (with <span class="math inline">\(\lambda \in [0,1]\)</span>) performs a convex combination of all the <span class="math inline">\(n\)</span>-step returns
<span class="math display" id="eq:lambda-return">\[\begin{equation}
g^{(\lambda)}_t = (1-\lambda)\sum_{n=1}^{\infty} \lambda^{n-1} \, g^{(n)}_t.
\tag{3.34}
\end{equation}\]</span></p>
<p>The Generalized Advantage Estimation (GAE) algorithm <span class="citation">(<a href="#ref-schulman2015high">Schulman, Moritz, et al. 2015</a>)</span> is an Actor-Critic type of policy gradient method that leverages the <span class="math inline">\(\lambda\)</span>-return as the target for fitting the critic (i.e., the approximate value function).</p>
<p><strong>GAE-<span class="math inline">\(\lambda\)</span> Advantage.</strong> Start from the TD residual
<span class="math display" id="eq:td-residual">\[
\delta_t \;=\; r_t + \gamma\,V_\phi(s_{t+1}) - V_\phi(s_t),
\qquad
\tag{3.35}
\]</span>
and define the GAE-<span class="math inline">\(\lambda\)</span> advantage as the exponentially-weighted sum of future TD residuals:
<span class="math display" id="eq:gae-def">\[
\widehat{A}^{(\lambda)}_t
\;=\;
\sum_{\ell=0}^{T-1-t}
(\gamma\lambda)^\ell \,\delta_{t+\ell}.
\qquad
\tag{3.36}
\]</span>
This admits an efficient backward recursion:
<span class="math display" id="eq:gae-recursion">\[
\widehat{A}^{(\lambda)}_t
\;=\;
\delta_t \;+\; \gamma\lambda\,\widehat{A}^{(\lambda)}_{t+1},
\quad
\widehat{A}^{(\lambda)}_{T}=0
\text{ (at terminal).}
\qquad
\tag{3.37}
\]</span></p>
<p><strong>From Advantage to Return.</strong> A key identity (obtained by expanding the sum of TD residuals and grouping terms) is
<span class="math display" id="eq:gae-equals-mix">\[
\sum_{\ell=0}^{\infty} (\gamma\lambda)^\ell \,\delta_{t+\ell}
\;=\;
\Big(1-\lambda\Big)\sum_{n=1}^{\infty}\lambda^{n-1}\Big(g^{(n)}_t - V_\phi(s_t)\Big).
\tag{3.38}
\]</span>
The left-hand side is the GAE-<span class="math inline">\(\lambda\)</span> advantage, and the right-hand side is <span class="math inline">\(g^{(\lambda)}_t - V_{\phi}(s_t)\)</span>. Therefore,
<span class="math display">\[
\widehat{A}^{(\lambda)}_t
\;=\;
g^{(\lambda)}_t \;-\; V_\phi(s_t),
\qquad\text{and hence}\qquad
g_t^{(\lambda)}
\;=\;
\widehat{A}^{(\lambda)}_t + V_\phi(s_t).
\]</span>
In GAE, we use
<span class="math display" id="eq:gae-vtarget">\[
\widehat{V}^{\,\text{targ}}_t
\;=\;
\widehat{A}^{(\lambda)}_t \;+\; V_\phi(s_t),
\tag{3.39}
\]</span>
as the target for fitting <span class="math inline">\(V_\phi\)</span>.</p>
<p><strong>GAE Policy Gradient.</strong> The true on-policy policy gradient can be written as
<span class="math display" id="eq:true-pg">\[
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}\!\left[\sum_{t=0}^{T-1}\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,A^\pi(s_t,a_t)\right].
\qquad
\tag{3.40}
\]</span>
An estimator remains unbiased if we replace <span class="math inline">\(A^\pi\)</span> by any <span class="math inline">\(\widehat{A}\)</span> satisfying
<span class="math display" id="eq:cond-unbiased">\[
\mathbb{E}\!\left[\widehat{A}_t \,\middle|\, s_t,a_t\right] \;=\; A^\pi(s_t,a_t).
\qquad
\tag{3.41}
\]</span></p>
<p>When the critic is exact, <span class="math inline">\(V_\phi\equiv V^\pi\)</span>, each <span class="math inline">\(n\)</span>-step bootstrap return has expectation
<span class="math display">\[
\mathbb{E}\!\left[g^{(n)}_t \,\middle|\, s_t,a_t\right] \;=\; Q^\pi(s_t,a_t),
\]</span>
so by linearity and <a href="policy-gradient.html#eq:lambda-return">(3.34)</a>,
<span class="math display">\[
\mathbb{E}\!\left[g^{(\lambda)}_t \,\middle|\, s_t,a_t\right] \;=\; Q^\pi(s_t,a_t).
\]</span>
Using <span class="math inline">\(\widehat{A}^{(\lambda)}_t = g^{(\lambda)}_t - V^\pi(s_t)\)</span> gives
<span class="math display">\[
\mathbb{E}\!\left[\widehat{A}^{(\lambda)}_t \,\middle|\, s_t,a_t\right]
\;=\; Q^\pi(s_t,a_t) - V^\pi(s_t)
\;=\; A^\pi(s_t,a_t),
\]</span>
which satisfies <a href="policy-gradient.html#eq:cond-unbiased">(3.41)</a>. Plugging <span class="math inline">\(\widehat{A}^{(\lambda)}_t\)</span> into <a href="policy-gradient.html#eq:true-pg">(3.40)</a> thus yields an unbiased policy-gradient estimator.</p>
<p>The pseudocode for GAE is presented below.</p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>On-Policy Actor–Critic with Generalized Advantage Estimation (GAE)</strong>
</div>
<p><strong>Inputs:</strong> policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span>, value <span class="math inline">\(V_\phi(s)\)</span>, discount <span class="math inline">\(\gamma\in[0,1)\)</span>, GAE parameter <span class="math inline">\(\lambda\in[0,1]\)</span>; stepsizes <span class="math inline">\(\alpha_\theta,\alpha_\phi&gt;0\)</span>; rollout length <span class="math inline">\(T\)</span>; minibatch size <span class="math inline">\(B\)</span>.</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Collect rollouts.</strong> Run <span class="math inline">\(\pi_\theta\)</span> to collect <span class="math inline">\(B\)</span> trajectories and each trajectory has <span class="math inline">\(T\)</span> steps , storing <span class="math inline">\((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\)</span>.</p></li>
<li><p><strong>Values &amp; residuals.</strong> Compute
<span class="math display">\[
v_t\!\leftarrow\!V_\phi(s_t), \ \ v_{t+1}\!\leftarrow\!V_\phi(s_{t+1}),\ \ m_t\!\leftarrow\!1-\mathrm{done}_t, \ \ \delta_t \leftarrow r_t + \gamma m_t v_{t+1} - v_t.
\]</span></p></li>
<li><p><strong>Backward GAE.</strong> Set <span class="math inline">\(\widehat{A}_{T}\!\leftarrow\!0\)</span>, and for <span class="math inline">\(t=T-1\)</span> to <span class="math inline">\(0\)</span> do:<br />
<span class="math display">\[
\widehat{A}_t \leftarrow \delta_t + \gamma\lambda m_t \widehat{A}_{t+1}.
\]</span>
<em>(Optionally normalize <span class="math inline">\(\{\widehat{A}_t\}\)</span> within the minibatch.)</em></p></li>
<li><p><strong>Critic target (<span class="math inline">\(\lambda\)</span>-return).</strong> Set critic target
<span class="math display">\[
\widehat{V}^{\,\text{targ}}_t \leftarrow \widehat{A}_t + v_t \;\;(=g^{(\lambda)}_t).
\]</span></p></li>
<li><p><strong>Critic update.</strong> Gradient descent:
<span class="math display">\[
\phi \leftarrow \phi - \alpha_\phi \nabla_\phi \frac{1}{ B }\sum_{t }\big(V_\phi(s_t)-\widehat{V}^{\,\text{targ}}_t\big)^2.
\]</span><br />
<em>(Often take several critic steps here.)</em></p></li>
<li><p><strong>Actor update.</strong> Gradient ascent
<span class="math display">\[
\theta \leftarrow \theta + \alpha_\theta \frac{1}{ B }\sum_{t } \nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\widehat{A}_t.
\]</span></p></li>
</ol>
</div>
<p>The next example applies GAE to the cart-pole balancing problem.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:cartpole-gae" class="example"><strong>Example 3.4  (GAE for Cart-Pole Balancing) </strong></span>Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-gae">3.9</a> shows the learning curve using Actor-Critic with GAE and Fig. <a href="policy-gradient.html#fig:cart-pole-policy-rollout-gae">3.10</a> shows a sample rollout of the trained policy.</p>
<p>The Python code can be found <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_actor-critic_gae.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-gae"></span>
<img src="images/Policy-Gradients/cartpole_returns_gae.png" alt="Learning curve (Actor--Critic with GAE)." width="60%" />
<p class="caption">
Figure 3.9: Learning curve (Actor–Critic with GAE).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-policy-rollout-gae"></span>
<img src="images/Policy-Gradients/cartpole_policy_rollout_gae.gif" alt="Policy rollout (Actor--Critic with GAE)." width="60%" />
<p class="caption">
Figure 3.10: Policy rollout (Actor–Critic with GAE).
</p>
</div>
</div>
</div>
</div>
<div id="off-policy-actorcritic" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Off-Policy Actor–Critic<a href="policy-gradient.html#off-policy-actorcritic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>On-policy actor–critic discards data after a single update. <em>Off-policy</em> methods decouple the behavior policy (that collects data) from the target policy (that we improve), enabling replay buffers and better sample efficiency.</p>
<p><strong>Off-Policy Policy Gradient.</strong> When data come from a behavior policy <span class="math inline">\(b\neq \pi_\theta\)</span>, define the per-decision likelihood ratio
<span class="math display" id="eq:is-ratio">\[
\rho_t \;=\; \frac{\pi_\theta(a_t\mid s_t)}{b (a_t\mid s_t)}.
\qquad \tag{3.42}
\]</span>
A basic off-policy policy gradient with an advantage target <span class="math inline">\(\widehat{A}_t\)</span> is
<span class="math display" id="eq:offpolicy-actor">\[\begin{equation}
\widehat{\nabla_\theta J}
\;=\;
\mathbb{E}\!\left[\rho_t\,\nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t\right].
\tag{3.43}
\end{equation}\]</span>
In practice we often clip the ratio to control variance:
<span class="math display">\[
\bar\rho_t \;=\; \min\{\rho_t,\;c\}, \quad c\!\ge\!1,
\qquad
\widehat{\nabla_\theta J}
\approx
\mathbb{E}\!\left[\bar\rho_t\,\nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\widehat{A}_t\right].
\]</span>
Clipping introduces small bias but usually reduces variance.</p>
<p><strong>Off-policy Critic.</strong> A convenient choice is an <em>action-value critic</em> <span class="math inline">\(Q_\psi(s,a)\)</span> trained with an expected SARSA style target under the current <span class="math inline">\(\pi_\theta\)</span>:
<span class="math display" id="eq:offpolicy-critic">\[
\begin{split}
y_t &amp; = r_t \;+\; \gamma \,
\mathbb{E}_{a&#39;\sim\pi_\theta(\cdot\mid s_{t+1})}\!\left[Q_{\bar\psi}(s_{t+1},a&#39;)\right],
\\
\psi &amp; \leftarrow \arg\min_\psi \;\mathbb{E}\big[(Q_\psi(s_t,a_t)-y_t)^2\big],
\end{split}
\tag{3.44}
\]</span>
where <span class="math inline">\(Q_{\bar\psi}\)</span> is a target network used to stabilize bootstrapping (i.e., mitigate the deadly triad).
For discrete actions, the expectation is an exact sum <span class="math inline">\(\sum_{a&#39;}\pi_\theta(a&#39;\mid s&#39;)Q_{\bar\psi}(s&#39;,a&#39;)\)</span>; for continuous, we approximate the expectation with a few samples <span class="math inline">\(a&#39;\!\sim\!\pi_\theta(\cdot\mid s&#39;)\)</span>.</p>
<p><strong>Advantage.</strong> Given <span class="math inline">\(Q_\psi\)</span>, we can estimate the advantage by
<span class="math display" id="eq:offpolicy-adv">\[
\widehat{A}_t
\;=\;Q_\psi(s_t,a_t) \;-\; V_\psi(s_t),
\quad
V_\psi(s)
\;\equiv\;
\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[Q_\psi(s,a)].
\qquad \tag{3.45}
\]</span>
Again, for discrete actions, we can compute <span class="math inline">\(V_\psi\)</span> exactly; for continuous actions, we approximate using a few samples.</p>
<p>Pseudocode for off-policy actor-critic is presented below.</p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>Experience-Replay Off-Policy Actor–Critic</strong>
</div>
<p><strong>Inputs:</strong> target policy <span class="math inline">\(\pi_\theta\)</span>, Q-critic <span class="math inline">\(Q_\psi\)</span> (and target <span class="math inline">\(Q_{\bar\psi}\)</span>), discount <span class="math inline">\(\gamma\)</span>, stepsizes <span class="math inline">\(\alpha_\theta,\alpha_\psi\)</span>, replay buffer <span class="math inline">\(\mathcal{D}\)</span>, IS clip <span class="math inline">\(c\ge 1\)</span>, minibatch size <span class="math inline">\(B\)</span>.</p>
<p><strong>Initialize:</strong> <span class="math inline">\(\bar\psi\leftarrow\psi\)</span>. Behavior policy <span class="math inline">\(b\)</span> can be <span class="math inline">\(\pi_\theta\)</span> with exploration (e.g., <span class="math inline">\(\varepsilon\)</span>-greedy).</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Interact &amp; store.</strong> Use <span class="math inline">\(b\)</span> to step the env and append to <span class="math inline">\(\mathcal{D}\)</span> tuples <span class="math inline">\((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t,\;p_t^b)\)</span>, where <span class="math inline">\(p_t^b=b(a_t\mid s_t)\)</span> (store this to compute <span class="math inline">\(\rho_t\)</span>).</p></li>
<li><p><strong>Sample minibatch</strong> Sample transitions <span class="math inline">\(\{(s,a,r,s&#39;,d,p^\mu)\}_{i=1}^B\)</span> from the replay buffer <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p><strong>Critic target (expected SARSA).</strong></p>
<ul>
<li>Compute <span class="math inline">\(\pi_\theta(a&#39;\mid s&#39;)\)</span> and <span class="math inline">\(Q_{\bar\psi}(s&#39;,a&#39;)\)</span>.</li>
<li>Set <span class="math inline">\(y \leftarrow r + \gamma(1-\mathrm{done}_t)\sum_{a&#39;} \pi_\theta(a&#39;\mid s&#39;)\,Q_{\bar\psi}(s&#39;,a&#39;)\)</span>. (for continuous actions: perform sample average)</li>
</ul></li>
<li><p><strong>Critic update.</strong>
<span class="math display">\[
\psi \leftarrow \psi - \alpha_\psi \nabla_\psi\frac{1}{B}\sum_{i=1}^B\big(Q_\psi(s_i,a_i)-y_i\big)^2.
\]</span>
<em>(Optionally clip gradients; perform multiple critic steps.)</em></p></li>
<li><p><strong>Actor advantage.</strong></p>
<ul>
<li>Compute <span class="math inline">\(V_\psi(s)=\sum_{a}\pi_\theta(a\mid s)\,Q_\psi(s,a)\)</span> (or sample-average for continuous actions).<br />
</li>
<li>Set <span class="math inline">\(\widehat{A}=Q_\psi(s,a)-V_\psi(s)\)</span>; optionally normalize <span class="math inline">\(\widehat{A}\)</span> within the batch.</li>
</ul></li>
<li><p><strong>Importance ratios (clipped).</strong>
<span class="math display">\[
\rho \leftarrow \frac{\pi_\theta(a\mid s)}{p^b},\qquad \bar\rho \leftarrow \min\{\rho,\;c\}.
\]</span></p></li>
<li><p><strong>Actor update.</strong>
<span class="math display">\[
\theta \leftarrow \theta + \alpha_\theta\,
\frac{1}{B}\sum_{i=1}^B
\bar\rho_i \,\nabla_\theta \log \pi_\theta(a_i\mid s_i)\,\widehat{A}_i.
\]</span></p></li>
<li><p><strong>Target network (moving average).</strong>
<span class="math display">\[
\bar\psi \leftarrow \tau\,\psi + (1-\tau)\,\bar\psi.
\]</span></p></li>
</ol>
</div>
<p><strong>Notes &amp; Variants.</strong></p>
<ul>
<li><strong>Unbiased vs. biased:</strong> Without clipping and with a correct critic/advantage, <a href="policy-gradient.html#eq:offpolicy-actor">(3.43)</a> is unbiased; clipping <span class="math inline">\(\bar\rho\)</span> adds bias but improves variance.<br />
</li>
<li><strong>Critic options:</strong> You can learn <span class="math inline">\(V_\phi\)</span> instead of <span class="math inline">\(Q_\psi\)</span> using off-policy TD with IS; using <span class="math inline">\(Q\)</span> with an expected SARSA target avoids IS in the critic while keeping evaluation under <span class="math inline">\(\pi_\theta\)</span>.<br />
</li>
<li><strong>Behavior refresh:</strong> Periodically update <span class="math inline">\(b\)</span> toward <span class="math inline">\(\pi_\theta\)</span> (reduce exploration) to keep ratios well-behaved.</li>
</ul>
<!-- This basic off-policy AC template sets the stage for **Soft Actor–Critic (SAC)**, which builds on it by adding a maximum-entropy objective, twin critics, and (optionally) automatic temperature tuning. -->
<p>The next example applies off-policy actor-critic to cart-pole balancing.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:cart-pole-off-policy-actor-critic" class="example"><strong>Example 3.5  (Off-Policy Actor-Critic for Cart-Pole Balancing) </strong></span>Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-off-policy-ac">3.11</a> shows the learning curve of applying off-policy actor-critic to cart-pole balancing.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-policy-rollout-off-policy-ac">3.12</a> shows a sample rollout of the learned policy.</p>
<p>The Python code can be found <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_off-policy_actor-critic.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-off-policy-ac"></span>
<img src="images/Policy-Gradients/cartpole_returns_off-policy_actor-critic.png" alt="Learning curve (Off-Policy Actor--Critic)." width="60%" />
<p class="caption">
Figure 3.11: Learning curve (Off-Policy Actor–Critic).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-policy-rollout-off-policy-ac"></span>
<img src="images/Policy-Gradients/cartpole_policy_rollout_off-policy_actor-critic.gif" alt="Policy rollout (Off-Policy Actor--Critic)." width="60%" />
<p class="caption">
Figure 3.12: Policy rollout (Off-Policy Actor–Critic).
</p>
</div>
</div>
</div>
<p>The next example applies off-policy actor-critic to a control problem with a continuous action space.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:pendulum-off-policy-ac" class="example"><strong>Example 3.6  (Off-Policy Actor-Critic for Inverted Pendulum) </strong></span>Consider the Inverted Pendulum problem illustrated in Fig. <a href="policy-gradient.html#fig:pendulum-diagram">3.13</a>. The state of the pendulum is <span class="math inline">\(s = (\theta, \dot{\theta})\)</span>, or equivalently, <span class="math inline">\(s = (x, y, \dot{\theta})\)</span> with <span class="math inline">\(x = \cos(\theta), y = \sin(\theta)\)</span>. The action space is continuous: <span class="math inline">\(\tau \in \mathcal{A} = [-2,2]\)</span>.</p>
<p>The dynamics of the pendulum is specified by <a href="https://gymnasium.farama.org/environments/classic_control/pendulum/">Gym</a>, and the reward is
<span class="math display">\[
R(s,\tau) = -(\theta^2 + 0.1  \dot{\theta}^2 + 0.001 \tau^2).
\]</span>
The episode truncates at <span class="math inline">\(200\)</span> time steps.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-diagram"></span>
<img src="images/Policy-Gradients/pendulum.png" alt="Illustration of Inverted Pendulum in Gym." width="20%" />
<p class="caption">
Figure 3.13: Illustration of Inverted Pendulum in Gym.
</p>
</div>
<p>Fig. <a href="policy-gradient.html#fig:pendulum-learning-curve-off-policy-ac">3.14</a> shows the learning curve of applying off-policy actor-critic to the pendulum problem.</p>
<p>Fig. <a href="policy-gradient.html#fig:pendulum-policy-rollout-off-policy-ac">3.15</a> shows a sample rollout of the learned policy.</p>
<p>You can find the Python code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_off-policy_actor-critic.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-learning-curve-off-policy-ac"></span>
<img src="images/Policy-Gradients/pendulum_returns_off-policy_actor-critic.png" alt="Learning curve (Off-Policy Actor-Critic)." width="60%" />
<p class="caption">
Figure 3.14: Learning curve (Off-Policy Actor-Critic).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-policy-rollout-off-policy-ac"></span>
<img src="images/Policy-Gradients/pendulum_policy_rollout_off-policy_actor-critic.gif" alt="Policy rollout (Off-Policy Actor-Critic)." width="40%" /><img src="images/Policy-Gradients/pendulum_policy_rollout_off-policy_actor-critic-1.gif" alt="Policy rollout (Off-Policy Actor-Critic)." width="40%" /><img src="images/Policy-Gradients/pendulum_policy_rollout_off-policy_actor-critic-2.gif" alt="Policy rollout (Off-Policy Actor-Critic)." width="40%" /><img src="images/Policy-Gradients/pendulum_policy_rollout_off-policy_actor-critic-3.gif" alt="Policy rollout (Off-Policy Actor-Critic)." width="40%" />
<p class="caption">
Figure 3.15: Policy rollout (Off-Policy Actor-Critic).
</p>
</div>
</div>
</div>
</div>
</div>
<div id="advanced-policy-gradients" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Advanced Policy Gradients<a href="policy-gradient.html#advanced-policy-gradients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="revisiting-generalized-policy-iteration" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Revisiting Generalized Policy Iteration<a href="policy-gradient.html#revisiting-generalized-policy-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall from Chapter <a href="value-rl.html#value-rl">2</a> that <strong>generalized policy iteration (GPI)</strong> extends tabular policy iteration (with known dynamics) to unknown-dynamics settings. At a high level, GPI iterates over policies; at iteration <span class="math inline">\(k\)</span> it performs:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Policy evaluation.</strong> Use the current policy <span class="math inline">\(\pi_k\)</span> to generate <span class="math inline">\(N\)</span> trajectories and estimate either the <span class="math inline">\(Q\)</span>-function <span class="math inline">\(\hat Q^{\pi_k}(s,a)\)</span> or the advantage function <span class="math inline">\(\hat A^{\pi_k}(s,a)\)</span>, using function approximation. This can be done, for example, with the GAE algorithm introduced in Section <a href="policy-gradient.html#PG-GAE">3.3.3</a>, and is the “critic” in the Actor–Critic family of methods.</p></li>
<li><p><strong>Policy improvement.</strong> Construct a new policy <span class="math inline">\(\pi_{k+1}\)</span> that (approximately) prefers actions deemed better by <span class="math inline">\(\hat Q^{\pi_k}\)</span> or <span class="math inline">\(\hat A^{\pi_k}\)</span>:
<span class="math display">\[
\pi_{k+1}(s) \approx \arg\max_a \hat Q^{\pi_k}(s,a) \;=\; \arg\max_a \hat A^{\pi_k}(s,a).
\]</span>
In policy gradients, we approximate <span class="math inline">\(\arg\max_a \hat A^{\pi_k}(s,a)\)</span> via gradient ascent in <span class="math inline">\(a\)</span>, i.e., using
<span class="math display" id="eq:policy-gradient-advantage-gpi">\[\begin{equation}
\nabla_\theta J(\theta)
= \frac{1}{1-\gamma}\,
\mathbb{E}_{s \sim d_{\pi_k},\, a \sim \pi_k}
\big[\nabla_\theta \log \pi_\theta(a \mid s)\, \hat A^{\pi_k}(s,a)\big].
\tag{3.46}
\end{equation}\]</span></p></li>
</ol>
<p>A key observation is that we use an advantage estimate obtained from data generated by <span class="math inline">\(\pi_k\)</span> (the old policy) to produce a new policy. In the tabular case, this improvement step guarantees monotonic improvement of <span class="math inline">\(\pi_{k+1}\)</span> over <span class="math inline">\(\pi_k\)</span>, because the evaluation produces a value (or advantage) estimate over the entire state space. In continuous state spaces, this no longer holds: we typically can only obtain an advantage estimate that is accurate <em>along the state–action distribution induced by <span class="math inline">\(\pi_k\)</span></em> rather than globally over <span class="math inline">\(\mathcal{S}\times\mathcal{A}\)</span>. (If, however, we use off-policy data, the expectation here can be different.)</p>
<p>The question “<em>how much better is <span class="math inline">\(\pi_{k+1}\)</span> than <span class="math inline">\(\pi_k\)</span>?</em>” motivates a relation between the performances of two policies that explicitly accounts for distribution shift.</p>
</div>
<div id="performance-difference-lemma" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Performance Difference Lemma<a href="policy-gradient.html#performance-difference-lemma" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following performance difference lemma (PDL) expresses the return gap between two policies in terms of the (old) policy’s advantage and the (new) policy’s state-action visitation:</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:pdl" class="theorem"><strong>Theorem 3.8  (Performance Difference Lemma) </strong></span>Let <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\pi&#39;\)</span> be two stationary policies in a discounted MDP with <span class="math inline">\(\gamma\in[0,1)\)</span>. Then
<span class="math display" id="eq:PDL">\[\begin{equation}
J(\pi&#39;) - J(\pi)
\;=\;
\frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d^{\pi&#39;},\,a\sim \pi&#39;}\!\left[A^{\pi}(s,a)\right],
\qquad
\tag{3.47}
\end{equation}\]</span>
where <span class="math inline">\(d^{\pi}(s)=(1-\gamma)\sum_{t=0}^\infty \gamma^t\,\Pr_{\pi}(s_t=s)\)</span> is the (discounted) state-visitation distribution generated by policy <span class="math inline">\(\pi\)</span> and <span class="math inline">\(A^\pi=Q^\pi-V^\pi\)</span> is the advantage.</p>
</div>
</div>
<p><strong>Interpretation.</strong> The performance difference lemma highlights <strong>distribution shift</strong>: the advantage is evaluated under policy <span class="math inline">\(\pi\)</span>, while the expectation is taken over the state–action distribution induced by <span class="math inline">\(\pi&#39;\)</span>. In policy gradients, when performing a step using <a href="policy-gradient.html#eq:policy-gradient-advantage-gpi">(3.46)</a>, we are approximately maximizing the surrogate
<span class="math display">\[
\mathcal{L}_{\pi}(\pi&#39;) := \frac{1}{1-\gamma}\;
\mathbb{E}_{s\sim d^{\pi},\,a\sim \pi&#39;}[A^{\pi}(s,a)],
\]</span>
where the state distribution is <span class="math inline">\(d_\pi\)</span>, not <span class="math inline">\(d_{\pi&#39;}\)</span>. To guarantee improvement, we want this surrogate to reflect the true gain <span class="math inline">\(J(\pi&#39;)-J(\pi)\)</span>. The two coincide when <span class="math inline">\(d^{\pi&#39;} \approx d^{\pi}\)</span>. Hence, <strong>keep <span class="math inline">\(\pi&#39;\)</span> close to <span class="math inline">\(\pi\)</span></strong> so state visitation does not change dramatically, making the surrogate reliable (to some extent, off-policy versions of actor–critic aim to achieve this). This “stay local” principle underpins TRPO, NPG, and PPO.</p>
</div>
<div id="trust-region-constraint" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Trust Region Constraint<a href="policy-gradient.html#trust-region-constraint" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How to enforce the new policy <span class="math inline">\(\pi_{\theta_{k+1}}\)</span> to be close to the old policy <span class="math inline">\(\pi_{\theta_{k}}\)</span>?</p>
<p><strong>KL Divergence.</strong> The Kullback–Leibler (KL) divergence is a type of statistical distance: a measure of how much an approximating probability distribution <span class="math inline">\(Q\)</span> is different from a true probability distribution <span class="math inline">\(P\)</span>. Formally, let <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> be two probability distributions supported on <span class="math inline">\(\mathcal{X}\)</span>, the KL divergence between <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is
<span class="math display" id="eq:KL-divergence-def">\[\begin{equation}
D_{\mathrm{KL}}(P \Vert Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right) = \mathbb{E}_{x \sim P(x)}\left[ \log \left( \frac{P(x)}{Q(x)} \right) \right].
\tag{3.48}
\end{equation}\]</span>
For example, when <span class="math inline">\(P = Q\)</span>, we have <span class="math inline">\(D_{\mathrm{KL}}(P \Vert Q) = 0\)</span>. Indeed, <span class="math inline">\(D_{\mathrm{KL}}(P \Vert Q) \geq 0\)</span> and the equality holds if and only if <span class="math inline">\(P = Q\)</span>.</p>
<p><strong>Trust Region Constraint.</strong> We now augment the usual policy optimization problem with a trust region constraint defined by the KL divergence. In particular, we wish to improve the current policy <span class="math inline">\(\pi_{\theta_k}\)</span> <strong>locally</strong> by maximizing a <strong>surrogate advantage objective</strong> while constraining the <strong>expected KL divergence</strong> from the old policy. This keeps the new policy <span class="math inline">\(\pi_{\theta}\)</span> close to <span class="math inline">\(\pi_{\theta_k}\)</span>, so the surrogate built under <span class="math inline">\(d^{\pi_{\theta_k}}\)</span> remains predictive of true improvement.</p>
<p>Formally, let <span class="math inline">\(\theta_k\)</span> denote the current policy parameters. Define the importance ratio
<span class="math display">\[
\rho_\theta(s,a)=\frac{\pi_\theta(a\mid s)}{\pi_{\theta_k}(a\mid s)}.
\]</span>
We aim to maximize the on-policy surrogate
<span class="math display" id="eq:trpo-surrogate">\[\begin{equation}
L_{\theta_k}(\theta)
\;=\;
\mathbb{E}_{s\sim d^{\pi_{\theta_k}},\,a\sim \pi_{\theta_k}}
\!\big[\rho_\theta(s,a)\,\widehat{A}^{\,\pi_{\theta_k}}(s,a)\big],
\qquad
\tag{3.49}
\end{equation}\]</span>
subject to an expected KL constraint measured under the old state distribution:
<span class="math display" id="eq:trpo-kl">\[\begin{equation}
\bar D_{\mathrm{KL}}(\theta_k\,\|\,\theta)
\;:=\;
\mathbb{E}_{s\sim d^{\pi_{\theta_k}}}
\!\left[
D_{\mathrm{KL}}\!\big(\pi_{\theta_k}(\cdot\mid s)\,\|\,\pi_{\theta}(\cdot\mid s)\big)
\right]
\;\le\;\delta,
\qquad
\tag{3.50}
\end{equation}\]</span>
with a small radius <span class="math inline">\(\delta&gt;0\)</span>. In summary, we are now interested in the following constrained policy optimization problem:
<span class="math display" id="eq:constrained-policy-optimization">\[\begin{equation}
\begin{split}
\max_\theta &amp; \quad L_{\theta_k}(\theta) \\
\text{subject to} &amp; \quad \bar D_{\mathrm{KL}}(\theta_k\,\|\,\theta) \leq \delta.
\end{split}
\tag{3.51}
\end{equation}\]</span></p>
</div>
<div id="natural-policy-gradient" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Natural Policy Gradient<a href="policy-gradient.html#natural-policy-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The natural policy gradient method <span class="citation">(<a href="#ref-kakade2001natural">Kakade 2001</a>)</span> can be seen as first performing a linear approximation to the objective of <a href="policy-gradient.html#eq:constrained-policy-optimization">(3.51)</a> and a quadratic approximation to the constraint of <a href="policy-gradient.html#eq:constrained-policy-optimization">(3.51)</a>, and then solve the resulting approximate problem in closed form.</p>
<p><strong>Leading-Order Approximation.</strong> To maximize the surrogate <span class="math inline">\(L_{\theta_k}(\theta)\)</span> in <a href="policy-gradient.html#eq:trpo-surrogate">(3.49)</a> subject to the KL trust-region constraint <a href="policy-gradient.html#eq:trpo-kl">(3.50)</a>, we linearize the surrogate around <span class="math inline">\(\theta_k\)</span> and quadratically approximate the KL trust region constraint. This leads to the following convex quadratic program (QP)
<span class="math display" id="eq:npg-qp">\[\begin{equation}
\max_{\Delta\theta}\ \ g^\top \Delta\theta
\quad\text{s.t.}\quad
\frac{1}{2}\,\Delta\theta^\top F(\theta_k)\,\Delta\theta \;\le\; \delta,
\tag{3.52}
\end{equation}\]</span>
where
<span class="math display" id="eq:npg-policy-gradient">\[\begin{equation}
g \;=\; \nabla_\theta L_{\theta_k}(\theta)\big|_{\theta=\theta_k}
= \mathbb{E}_{s\sim d^{\pi_{\theta_k}},\,a\sim \pi_{\theta_k}(\cdot\mid s)} \big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\,\widehat{A}(s,a)\big]
\tag{3.53}
\end{equation}\]</span>
is the policy gradient, and
<span class="math display" id="eq:npg-fisher">\[\begin{equation}
F(\theta_k) \;=\; \mathbb{E}_{s\sim d^{\pi_{\theta_k}},\,a\sim \pi_{\theta_k}(\cdot\mid s)} \big[\nabla_\theta \log \pi_{\theta_k}(a \mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\big]
\tag{3.54}
\end{equation}\]</span>
is the (empirical) <em>Fisher information</em> of the policy under the old distribution. See a proof in Section <a href="policy-gradient.html#proof-fisher">3.4.5</a>.</p>
<p>One can show that the QP <a href="policy-gradient.html#eq:npg-qp">(3.52)</a> has a closed-form solution:
<span class="math display" id="eq:npg-step">\[\begin{equation}
p_{\text{NPG}} \;=\; F(\theta_k)^{-1}\, g,
\qquad
\Delta\theta_{\text{NPG}}
\;=\;
\sqrt{\frac{2\delta}{g^\top F(\theta_k)^{-1} g}}\;\;p_{\text{NPG}},
\tag{3.55}
\end{equation}\]</span>
where <span class="math inline">\(p_{\text{NPG}}\)</span> is called the <em>natural policy gradient</em>, for the reason that the usual policy gradient <span class="math inline">\(g\)</span> is pre-multiplied by <span class="math inline">\(F(\theta_k)^{-1}\)</span>, which contains the second-order curvature of the KL constraint. In practice, <span class="math inline">\(p_{\text{NPG}}\)</span> is computed with <a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">conjugate gradient (CG)</a> using Fisher–vector products; no matrices are formed. In <a href="policy-gradient.html#eq:npg-step">(3.55)</a>,
<span class="math display">\[
\alpha = \sqrt{\frac{2\delta}{g^\top F(\theta_k)^{-1} g}} = \sqrt{ \frac{2\delta}{p^\top_{\text{NPG}} F(\theta_k) p_{\text{NPG}} } }
\]</span>
is often called the trust-region step size.</p>
<p>The following pseudocode implements NPG with GAE as the critic.</p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>Natural Policy Gradient (with GAE advantages)</strong>
</div>
<p><strong>Inputs:</strong> initial policy <span class="math inline">\(\theta_0\)</span>; value/critic <span class="math inline">\(\phi_0\)</span>; discount <span class="math inline">\(\gamma\)</span>; GAE parameter <span class="math inline">\(\lambda\)</span>; KL radius <span class="math inline">\(\delta\)</span> (or learning rate <span class="math inline">\(\eta\)</span>); CG iterations <span class="math inline">\(K_{\mathrm{cg}}\)</span>; (optional) damping <span class="math inline">\(\xi&gt;0\)</span>.</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Collect rollouts (on-policy).</strong> Run <span class="math inline">\(\pi_{\theta_k}\)</span> to obtain a batch <span class="math inline">\(\{(s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\}_{t=1}^N\)</span>; cache <span class="math inline">\(\log \pi_{\theta_k}(a_t\mid s_t)\)</span>.</p></li>
<li><p><strong>Critic / advantages (GAE).</strong><br />
Compute TD residuals <span class="math inline">\(\delta_t = r_t + \gamma(1-\mathrm{done}_t)V_\phi(s_{t+1}) - V_\phi(s_t)\)</span>;<br />
backward recursion <span class="math inline">\(\widehat{A}_t = \delta_t + \gamma\lambda(1-\mathrm{done}_t)\widehat{A}_{t+1}\)</span>, with <span class="math inline">\(\widehat{A}_{T}=0\)</span>;<br />
(optionally) standardize <span class="math inline">\(\widehat{A}\)</span>; set value targets <span class="math inline">\(\widehat{V}^{\,\text{targ}}_t=\widehat{A}_t+V_\phi(s_t)\)</span>.</p></li>
<li><p><strong>Value update.</strong> Fit <span class="math inline">\(V_\phi\)</span> by minimizing <span class="math inline">\(\sum_t (V_\phi(s_t)-\widehat{V}^{\,\text{targ}}_t)^2\)</span> (one or several epochs).</p></li>
<li><p><strong>Surrogate gradient.</strong><br />
<span class="math display">\[
g \;=\; \frac{1}{N} \sum_{t} \nabla_\theta \log \pi_{\theta_k}(a_t \mid s_t )\,\widehat{A}_t .
\]</span></p></li>
<li><p><strong>Fisher–vector product (FvP).</strong> Define the empirical KL
<span class="math inline">\(\bar D_{\mathrm{KL}}(\theta_k\,\|\,\theta)\)</span>. Implement <span class="math inline">\(v\mapsto Fv\)</span> as the <strong>Hessian–vector product</strong> of <span class="math inline">\(\bar D_{\mathrm{KL}}\)</span> at <span class="math inline">\(\theta_k\)</span> (optionally use <strong>damping</strong> <span class="math inline">\(F \leftarrow F+\xi I\)</span> to make sure <span class="math inline">\(F\)</span> is positive definite).</p></li>
<li><p><strong>Conjugate gradient (CG).</strong> Approximately solve <span class="math inline">\((F)\,p = g\)</span> to obtain <span class="math inline">\(p_{\text{NPG}}\approx F^{-1}g\)</span>.</p></li>
<li><p><strong>Step size.</strong></p>
<ul>
<li><strong>Trust-region scaling:</strong> set
<span class="math inline">\(\alpha \leftarrow \sqrt{\frac{2\delta}{p^\top_{\text{NPG}} F p_{\text{NPG}}}}\)</span> and update <span class="math inline">\(\theta_{k+1} \leftarrow \theta_k + \alpha p_{\text{NPG}}\)</span>.<br />
</li>
<li><strong>Fixed-rate natural step:</strong> choose <span class="math inline">\(\eta&gt;0\)</span> and set <span class="math inline">\(\theta_{k+1} \leftarrow \theta_k + \eta p_{\text{NPG}}\)</span> (monitor empirical KL for safety).</li>
</ul></li>
</ol>
</div>
</div>
<div id="proof-fisher" class="section level3 hasAnchor" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> Proof of Fisher Information<a href="policy-gradient.html#proof-fisher" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let the expected KL trust-region constraint (measured under the old policy’s state distribution) be
<span class="math display">\[
\bar D_{\mathrm{KL}}(\theta_k\!\parallel\!\theta)
\;:=\;
\mathbb{E}_{s\sim d^{\pi_{\theta_k}}}
\Big[ D_{\mathrm{KL}}\!\big(\pi_{\theta_k}(\cdot\mid s)\,\|\,\pi_\theta(\cdot\mid s)\big) \Big].
\]</span>
Write <span class="math inline">\(\theta=\theta_k+\Delta\theta\)</span> and define, for a fixed state <span class="math inline">\(s\)</span>,
<span class="math display">\[
f_s(\theta)
\;=\;
D_{\mathrm{KL}}\!\big(\pi_{\theta_k}(\cdot\mid s)\,\|\,\pi_\theta(\cdot\mid s)\big)
= \mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}
\big[\log \pi_{\theta_k}(a\mid s) - \log \pi_\theta(a\mid s)\big].
\]</span>
We will show that the <em>second-order Taylor expansion</em> of <span class="math inline">\(\bar D_{\mathrm{KL}}\)</span> around <span class="math inline">\(\theta_k\)</span> is
<span class="math display">\[
\bar D_{\mathrm{KL}}(\theta_k\!\parallel\!\theta_k+\Delta\theta)
\;=\;
\frac{1}{2}\,\Delta\theta^\top
\underbrace{
\mathbb{E}_{s\sim d^{\pi_{\theta_k}},\,a\sim \pi_{\theta_k}(\cdot\mid s)}
\!\big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\big]
}_{F(\theta_k)\ \text{(Fisher information)}}
\,\Delta\theta \;+\; \mathcal{O}(\|\Delta\theta\|^3).
\]</span></p>
<p><strong>Step 1: Zeroth- and first-order terms vanish at <span class="math inline">\(\theta=\theta_k\)</span>.</strong> For each <span class="math inline">\(s\)</span>,
<span class="math display">\[
f_s(\theta_k) = D_{\mathrm{KL}}\!\big(\pi_{\theta_k}\,\|\,\pi_{\theta_k}\big)=0.
\]</span>
The gradient (holding the expectation under <span class="math inline">\(\pi_{\theta_k}\)</span>) is
<span class="math display">\[
\nabla_\theta f_s(\theta)
= -\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}\big[\nabla_\theta \log \pi_\theta(a\mid s)\big].
\]</span>
Evaluating at <span class="math inline">\(\theta=\theta_k\)</span>,
<span class="math display">\[
   \begin{split}
\nabla_\theta f_s(\theta_k)
&amp; = -\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}\big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\big] \\
&amp; = -\sum_a \pi_{\theta_k}(a\mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)
= -\nabla_\theta \sum_a \pi_{\theta_k}(a\mid s)
= 0,
\end{split}
\]</span>
using the normalization <span class="math inline">\(\sum_a \pi_{\theta_k}(a\mid s)=1\)</span>. Hence both the value and the first-order term are zero.</p>
<p><strong>Step 2: The Hessian equals the (per-state) Fisher information.</strong> The Hessian of <span class="math inline">\(f_s\)</span> is
<span class="math display">\[
\nabla_\theta^2 f_s(\theta)
= -\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}\big[\nabla_\theta^2 \log \pi_\theta(a\mid s)\big].
\]</span>
At <span class="math inline">\(\theta=\theta_k\)</span>, apply the <em>information identity</em> (a.k.a. <a href="https://math.stackexchange.com/questions/2026428/what-is-second-bartlett-identity">Bartlett identity</a>):
<span class="math display">\[
-\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}\big[\nabla_\theta^2 \log \pi_{\theta_k}(a\mid s)\big]
=
\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}
\big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\, \nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\big].
\]</span></p>
<div class="proofbox">
<p><em>Proof sketch of the identity:</em> start from <span class="math inline">\(\sum_a \pi_\theta(a\mid s)=1\)</span>, differentiate once to get
<span class="math inline">\(\mathbb{E}_{a\sim \pi_\theta}[\nabla\log\pi_\theta]=0\)</span>; differentiate again and use the product rule to obtain
<span class="math inline">\(\mathbb{E}_{a\sim \pi_\theta}[\nabla^2\log\pi_\theta + (\nabla\log\pi_\theta)(\nabla\log\pi_\theta)^\top]=0\)</span>.</p>
</div>
<p>Thus,
<span class="math display">\[
\nabla_\theta^2 f_s(\theta_k)
=
\mathbb{E}_{a\sim \pi_{\theta_k}(\cdot\mid s)}
\big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\big]
\;=:\; F_s(\theta_k).
\]</span></p>
<p><strong>Step 3: Second-order Taylor expansion and averaging over states.</strong> For each <span class="math inline">\(s\)</span>,
<span class="math display">\[
f_s(\theta_k+\Delta\theta)
= \frac{1}{2}\,\Delta\theta^\top F_s(\theta_k)\,\Delta\theta \;+\; \mathcal{O}(\|\Delta\theta\|^3).
\]</span>
Taking expectation over <span class="math inline">\(s\sim d^{\pi_{\theta_k}}\)</span> gives
<span class="math display">\[
\bar D_{\mathrm{KL}}(\theta_k\!\parallel\!\theta_k+\Delta\theta)
= \mathbb{E}_{s\sim d^{\pi_{\theta_k}}}[f_s(\theta_k+\Delta\theta)]
= \frac{1}{2}\,\Delta\theta^\top
\underbrace{\mathbb{E}_{s\sim d^{\pi_{\theta_k}}}[F_s(\theta_k)]}_{F(\theta_k)}
\,\Delta\theta \;+\; \mathcal{O}(\|\Delta\theta\|^3).
\]</span></p>
<p><strong>Conclusion.</strong> The Fisher information <span class="math inline">\(F(\theta_k)\)</span> is exactly the Hessian of the expected KL at <span class="math inline">\(\theta_k\)</span>. Therefore, the KL trust-region constraint admits the quadratic local approximation
<span class="math display">\[
\bar D_{\mathrm{KL}}(\theta_k\!\parallel\!\theta_k+\Delta\theta)\;\approx\;\frac{1}{2}\,\Delta\theta^\top F(\theta_k)\,\Delta\theta,
\]</span>
which yields the TRPO/NPG quadratic constraint and identifies <span class="math inline">\(F(\theta_k)\)</span> as the local metric tensor of the policy manifold.</p>
</div>
<div id="trust-region-policy-optimization" class="section level3 hasAnchor" number="3.4.6">
<h3><span class="header-section-number">3.4.6</span> Trust Region Policy Optimization<a href="policy-gradient.html#trust-region-policy-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The NPG algorithm presented above leverages a leading-order approximation of the KL-constrained policy optimization problem <a href="policy-gradient.html#eq:constrained-policy-optimization">(3.51)</a>.</p>
<p>In Trust Region Policy Optimization <span class="citation">(<a href="#ref-schulman2015trust">Schulman, Levine, et al. 2015</a>)</span>, we still use the leading-order approximation to obtain the natural policy gradient direction, but additionally, we perform a <em>backtracking line search</em> to enforce the true (nonlinear) KL constraint and surrogate improvement.</p>
<p>The following pseudocode implements TRPO with GAE as the critic.</p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>TRPO (with GAE advantages)</strong>
</div>
<p><strong>Inputs:</strong> initial policy <span class="math inline">\(\theta_0\)</span>; value/critic parameters <span class="math inline">\(\phi_0\)</span>; discount <span class="math inline">\(\gamma\)</span>; GAE parameter <span class="math inline">\(\lambda\)</span>; KL radius <span class="math inline">\(\delta\)</span>; CG iterations <span class="math inline">\(K_{\mathrm{cg}}\)</span>; backtrack factor <span class="math inline">\(\beta\in(0,1)\)</span>; max backtracks <span class="math inline">\(M\)</span>.</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Collect rollouts (on-policy).</strong> Run <span class="math inline">\(\pi_{\theta_k}\)</span> to obtain trajectories; build a batch <span class="math inline">\(\{(s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\}_{t=1}^N\)</span>.</p></li>
<li><p><strong>Critic / advantages (GAE).</strong><br />
Compute TD residuals <span class="math inline">\(\delta_t = r_t + \gamma(1-\mathrm{done}_t)V_\phi(s_{t+1}) - V_\phi(s_t)\)</span>;<br />
backward recursion <span class="math inline">\(\widehat{A}_t = \delta_t + \gamma\lambda(1-\mathrm{done}_t)\widehat{A}_{t+1}\)</span>, with <span class="math inline">\(\widehat{A}_{T}=0\)</span>;<br />
(optionally) standardize <span class="math inline">\(\widehat{A}\)</span> within the batch; set value targets <span class="math inline">\(\widehat{V}^{\,\text{targ}}_t=\widehat{A}_t+V_\phi(s_t)\)</span>.</p></li>
<li><p><strong>Value function update.</strong> Fit <span class="math inline">\(V_\phi\)</span> by minimizing
<span class="math inline">\(\sum_t (V_\phi(s_t)-\widehat{V}^{\,\text{targ}}_t)^2\)</span> (one or several epochs).</p></li>
<li><p><strong>Policy gradient at <span class="math inline">\(\theta_k\)</span>.</strong><br />
<span class="math display">\[
g \;=\; \nabla_\theta L_{\theta_k}(\theta)\big|_{\theta=\theta_k}
  \approx
  \frac{1}{N} \sum_{t} \nabla_\theta \log \pi_{\theta_k}(a_t \mid s_t )\,\widehat{A}_t .
\]</span></p></li>
<li><p><strong>Fisher–vector product (FvP).</strong> Define the Fisher information under <span class="math inline">\(\pi_{\theta_k}\)</span>:
<span class="math display">\[
F(\theta_k)\;=\;\mathbb{E}\Big[\nabla_\theta \log \pi_{\theta_k}(a\mid s)\,\nabla_\theta \log \pi_{\theta_k}(a\mid s)^\top\Big].
\]</span>
Implement <span class="math inline">\(v\mapsto Fv\)</span> via the <strong>Hessian-vector product</strong> of the empirical KL.</p></li>
<li><p><strong>Conjugate gradient (CG) solve.</strong> Approximately solve <span class="math inline">\(F p = g\)</span> with <span class="math inline">\(K_{\mathrm{cg}}\)</span> CG iterations to get the natural direction <span class="math inline">\(p_{\text{NPG}}\approx F^{-1}g\)</span>.</p></li>
<li><p><strong>Compute step size for the quadratic trust region.</strong><br />
<span class="math display">\[
\alpha \;\leftarrow\; \sqrt{\frac{2\delta}{p^\top_{\text{NPG}} F p_{\text{NPG}} }}.
\]</span>
Candidate update: <span class="math inline">\(\theta^\star \leftarrow \theta_k + \alpha p_{\text{NPG}}\)</span>.</p></li>
<li><p><strong>Backtracking line search (feasibility + improvement).</strong> Repeatedly set <span class="math inline">\(\theta^\star \leftarrow \theta_k + \beta^j \alpha p_{\text{NPG}}\)</span> for <span class="math inline">\(j=0,1,\dots,M\)</span> until both hold on the batch:</p>
<ul>
<li><strong>KL constraint:</strong> <span class="math inline">\(\bar D_{\mathrm{KL}}(\theta_k\,\|\,\theta^\star) \le \delta\)</span>.</li>
<li><strong>Surrogate improvement:</strong> <span class="math inline">\(L_{\theta_k}(\theta^\star) \ge L_{\theta_k}(\theta_k)\)</span>.</li>
</ul>
<p>Accept the first <span class="math inline">\(\theta^\star\)</span> that satisfies both; set <span class="math inline">\(\theta_{k+1}\leftarrow \theta^\star\)</span>.</p></li>
</ol>
</div>
<div id="backtracking-line-search" class="section level4 hasAnchor" number="3.4.6.1">
<h4><span class="header-section-number">3.4.6.1</span> Backtracking Line Search<a href="policy-gradient.html#backtracking-line-search" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Batch-only evaluation.</strong> During TRPO’s line search you <strong>do not collect new trajectories</strong>. All checks are computed on the same batch gathered with the old policy <span class="math inline">\(\pi_{\theta_k}\)</span> (i.e., under <span class="math inline">\(d^{\pi_{\theta_k}}\)</span>).</p>
<p><strong>Given:</strong> a candidate update <span class="math inline">\(\theta^\star = \theta_k + \beta^j \alpha p_{\text{NPG}}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Empirical KL constraint (nonlinear, “true” KL).</strong> Compute the state-wise KL between the full action distributions of the old and candidate policies and average over the batch states:
<span class="math display">\[
\widehat{\bar D}_{\mathrm{KL}}(\theta_k \,\|\, \theta^\star)
\;=\;
\frac{1}{|\mathcal{B}|}\sum_{s\in\mathcal{B}}
D_{\mathrm{KL}}\!\big(\pi_{\theta_k}(\cdot\mid s)\,\|\,\pi_{\theta^\star}(\cdot\mid s)\big).
\]</span></p>
<ul>
<li><strong>Categorical policy:</strong><br />
<span class="math display">\[
D_{\mathrm{KL}}(\pi_{\theta_k}\,\|\,\pi_{\theta^\star})
\;=\;
\sum_{a} \pi_{\theta_k}(a\mid s)\,
\Big[\log \pi_{\theta_k}(a\mid s)-\log \pi_{\theta^\star}(a\mid s)\Big].
\]</span></li>
<li><strong>Gaussian policy</strong> (mean <span class="math inline">\(\mu(s),\)</span> covariance <span class="math inline">\(\Sigma(s)\)</span>; use pre-squash distribution if actions are squashed):<br />
<span class="math display">\[
D_{\mathrm{KL}}\big(\mathcal{N}(\mu_k,\Sigma_k)\,\|\,\mathcal{N}(\mu_\star,\Sigma_\star)\big)
= \frac{1}{2}\Big(
  \operatorname{tr}(\Sigma_\star^{-1}\Sigma_k)
  + (\mu_\star-\mu_k)^\top \Sigma_\star^{-1}(\mu_\star-\mu_k)
  - d + \log\tfrac{\det \Sigma_\star}{\det \Sigma_k}
\Big).
\]</span>
<strong>Feasibility test:</strong> accept if <span class="math inline">\(\widehat{\bar D}_{\mathrm{KL}}(\theta_k \,\|\, \theta^\star) \le \delta\)</span> (cf. <a href="policy-gradient.html#eq:trpo-kl">(3.50)</a>).</li>
</ul></li>
<li><p><strong>Surrogate improvement.</strong> Evaluate the TRPO surrogate <span class="math inline">\(L_{\theta_k}(\theta)\)</span> (cf. <a href="policy-gradient.html#eq:trpo-surrogate">(3.49)</a>) on the same batch using importance ratios from <span class="math inline">\(\theta^\star\)</span>:
<span class="math display">\[
\widehat{L}_{\theta_k}(\theta^\star)
\;=\;
\frac{1}{|\mathcal{B}|}\sum_{(s,a)\in\mathcal{B}}
\frac{\pi_{\theta^\star}(a\mid s)}{\pi_{\theta_k}(a\mid s)}\,
\widehat{A}^{\,\pi_{\theta_k}}(s,a),
\qquad
\widehat{L}_{\theta_k}(\theta_k)=\frac{1}{|\mathcal{B}|}\sum_{(s,a)}\widehat{A}^{\,\pi_{\theta_k}}(s,a).
\]</span>
<strong>Improvement test:</strong> accept if <span class="math inline">\(\widehat{L}_{\theta_k}(\theta^\star)\ge \widehat{L}_{\theta_k}(\theta_k)\)</span>.</p></li>
<li><p><strong>Backtracking loop (on-batch).</strong> Decrease the step by <span class="math inline">\(\beta\in(0,1)\)</span> until both tests pass or a maximum of <span class="math inline">\(M\)</span> backtracks is reached:
<span class="math display">\[
\theta^\star \leftarrow \theta_k + \beta^j \alpha p_{\text{NPG}},\quad
j=0,1,\dots,M.
\]</span>
If successful, set <span class="math inline">\(\theta_{k+1}\leftarrow \theta^\star\)</span>; otherwise keep <span class="math inline">\(\theta_{k+1}\leftarrow \theta_k\)</span>.</p></li>
</ol>
</div>
</div>
<div id="proximal-policy-optimization" class="section level3 hasAnchor" number="3.4.7">
<h3><span class="header-section-number">3.4.7</span> Proximal Policy Optimization<a href="policy-gradient.html#proximal-policy-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While NPG/TRPO are stable, they may be computationally heavier due to constrained solves or natural-step systems. Proximal Policy Optimization (PPO) <span class="citation">(<a href="#ref-schulman2017proximal">Schulman et al. 2017</a>)</span> replaces the hard constraint with a <em>penalized (regularized) objective</em> and optimizes it with standard first-order SGD:
<span class="math display" id="eq:PPO-regularized">\[\begin{equation}
\ell_k(\theta)
\;=\;
\mathbb{E}_{s \sim d^{\pi_{\theta_k}},a\sim \pi_{\theta_k}}
\big[\,\rho_\theta(s,a)\, \widehat{A}^{\,\pi_{\theta_k}}(s,a)\,\big]
\;-\;
\lambda\;
\mathbb{E}_{s \sim d^{\pi_{\theta_k}},a\sim \pi_{\theta_k}}
\!\left[\log\frac{\pi_{\theta_k}(a\mid s)}{\pi_\theta(a\mid s)}\right],
\tag{3.56}
\end{equation}\]</span>
where <span class="math inline">\(\lambda &gt; 0\)</span> and the second term is the <em>per-sample KL penalty</em> that discourages large departures from <span class="math inline">\(\pi_{\theta_k}\)</span>.
Conceptually, this is a <em>Lagrangian relaxation</em> of TRPO’s trust region, where the hard constraint is moved to the objective function as a soft penalty.</p>
<div id="gradient-of-the-klregularized-surrogate" class="section level4 hasAnchor" number="3.4.7.1">
<h4><span class="header-section-number">3.4.7.1</span> Gradient of the KL–Regularized Surrogate<a href="policy-gradient.html#gradient-of-the-klregularized-surrogate" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Treat <span class="math inline">\(\widehat{A}^{\,\pi_{\theta_k}}\)</span> and the sampling distribution as fixed during the policy update. Using
<span class="math inline">\(\nabla_\theta \rho_\theta = \rho_\theta\,\nabla_\theta \log \pi_\theta\)</span> and
<span class="math inline">\(\nabla_\theta \log \frac{\pi_{\theta_k}}{\pi_\theta} = - \nabla_\theta \log \pi_\theta\)</span>, the gradient of the KL-regularized objective <a href="policy-gradient.html#eq:PPO-regularized">(3.56)</a> is
<span class="math display" id="eq:PPO-penalty-gradient">\[
\nabla_\theta \ell_k(\theta)
\;=\;
\mathbb{E}_{s \sim d^{\pi_{\theta_k}}, a\sim \pi_{\theta_k}}
\!\Big[
\nabla_\theta \log \pi_\theta(a\mid s)\,
\underbrace{\big(\rho_\theta(s,a)\,\widehat{A}(s,a)-\lambda\big)}_{\text{effective advantage}}
\Big].
\qquad \tag{3.57}
\]</span>
This shows the KL penalty shifts the effective advantage by <span class="math inline">\(-\lambda\)</span>.</p>
</div>
<div id="from-the-lagrangian-relaxation-to-ppo-updates" class="section level4 hasAnchor" number="3.4.7.2">
<h4><span class="header-section-number">3.4.7.2</span> From the Lagrangian Relaxation to PPO Updates<a href="policy-gradient.html#from-the-lagrangian-relaxation-to-ppo-updates" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>There are two standard PPO realizations:</p>
<ol style="list-style-type: decimal">
<li><p><strong>PPO–KL (penalty version).</strong> Directly ascend <span class="math inline">\(\ell_k(\theta)\)</span> with minibatch SGD:
<span class="math display">\[
\theta \leftarrow \theta + \alpha\;
\frac{1}{B}\sum_{(s,a)\in\mathcal{B}}
\nabla_\theta \log \pi_\theta(a\mid s)\,
\big(\rho_\theta(s,a)\,\widehat{A}(s,a)-\lambda\big).
\]</span>
After each epoch, measure the empirical KL
<span class="math inline">\(\widehat{\bar D}_{\mathrm{KL}}(\theta_k\!\parallel\!\theta)\)</span>
on the batch; increase <span class="math inline">\(\lambda\)</span> if KL is too high (tighten the region), decrease <span class="math inline">\(\lambda\)</span> if it is too low.</p></li>
<li><p><strong>PPO–Clip (clipping version).</strong> Replace the penalty with a <em>hard</em> trust region on the ratio <span class="math inline">\(\rho_\theta\)</span>. When <span class="math inline">\(\widehat{A}&gt;0\)</span>, forbid <span class="math inline">\(\rho_\theta&gt;1+\varepsilon\)</span>; when <span class="math inline">\(\widehat{A}&lt;0\)</span>, forbid <span class="math inline">\(\rho_\theta&lt;1-\varepsilon\)</span>. This yields the clipped objective
<span class="math display" id="eq:PPO-clip-obj">\[\begin{equation}
\hspace{-16mm}
\ell^{\text{CLIP}}_k(\theta)
\;=\;
\mathbb{E}_{s \sim d^{\pi_{\theta_k}}, a \sim \pi_{\theta_k}}
\Big[\min\!\big(\rho_\theta(s,a)\,\widehat{A}(s,a),\;
\operatorname{clip}(\rho_\theta(s,a),\,1-\varepsilon,\,1+\varepsilon)\,\widehat{A}(s,a)\big)\Big],
\tag{3.58}
\end{equation}\]</span>
which is a first-order proxy to the Lagrangian/TRPO trust region: the min/clip term cancels the incentive to move <span class="math inline">\(\rho_\theta\)</span> outside <span class="math inline">\([1-\varepsilon,1+\varepsilon]\)</span> in directions that would further increase the objective.</p></li>
</ol>
<p>Both versions are typically combined with a value-function loss and an entropy bonus to encourage exploration:
<span class="math display">\[
\mathcal{L}^{\text{PPO}}(\theta,\phi)
= - \ell^{\text{PG}}_k(\theta)
+ c_v\,\mathbb{E}\big[(V_\phi(s)-\widehat{V}^{\,\text{targ}})^2\big]
- c_e\,\mathbb{E}\big[\mathcal{H}(\pi_\theta(\cdot\mid s))\big],
\]</span>
where <span class="math inline">\(\ell^{\text{PG}}_k\)</span> is either <span class="math inline">\(\ell^{\text{CLIP}}_k\)</span> or <span class="math inline">\(\ell_k\)</span>.</p>
<p><strong>Why PPO “forbids” <span class="math inline">\(\rho_\theta\)</span> from leaving <span class="math inline">\([1-\varepsilon,\,1+\varepsilon]\)</span>.</strong> Let <span class="math inline">\(r \equiv \rho_\theta(s,a)=\frac{\pi_\theta(a\mid s)}{\pi_{\theta_k}(a\mid s)}\)</span> and <span class="math inline">\(\widehat{A}=\widehat{A}^{\,\pi_{\theta_k}}(s,a)\)</span>.
The PPO–Clip objective for one sample is
<span class="math display">\[
L^{\text{CLIP}}(r,\widehat{A})
= \min\!\big(r\,\widehat{A},\ \operatorname{clip}(r,1-\varepsilon,1+\varepsilon)\,\widehat{A}\big).
\]</span></p>
<p>Let’s do a case analysis, as shown in Fig. <a href="policy-gradient.html#fig:ppo-clip-obj">3.16</a>.</p>
<ul>
<li><p>If <span class="math inline">\(\widehat{A}&gt;0\)</span>: increasing <span class="math inline">\(r\)</span> (i.e., increasing <span class="math inline">\(\pi_\theta(a\mid s)\)</span>) raises the <em>unclipped</em> term <span class="math inline">\(r\,\widehat{A}\)</span>.<br />
The clipped term equals <span class="math inline">\((1+\varepsilon)\widehat{A}\)</span> whenever <span class="math inline">\(r&gt;1+\varepsilon\)</span>.
Hence
<span class="math display">\[
L^{\text{CLIP}}(r,\widehat{A})
=
\begin{cases}
r\,\widehat{A}, &amp; r\le 1+\varepsilon,\\[4pt]
(1+\varepsilon)\widehat{A}, &amp; r&gt;1+\varepsilon,
\end{cases}
\]</span>
so <span class="math inline">\(\frac{\partial L^{\text{CLIP}}}{\partial r}=\widehat{A}\)</span> for <span class="math inline">\(r\le 1+\varepsilon\)</span> and <strong><span class="math inline">\(0\)</span></strong> for <span class="math inline">\(r&gt;1+\varepsilon\)</span>.
There is no further gain by pushing <span class="math inline">\(r\)</span> beyond <span class="math inline">\(1+\varepsilon\)</span>; the gradient vanishes.<br />
Intuitively: don’t increase an action’s probability <em>too much</em> even if it looks good—stay proximal.</p></li>
<li><p>If <span class="math inline">\(\widehat{A}&lt;0\)</span>: decreasing <span class="math inline">\(r\)</span> (i.e., reducing <span class="math inline">\(\pi_\theta(a\mid s)\)</span>) lowers the unclipped term <span class="math inline">\(r\,\widehat{A}\)</span>.
The clipped term equals <span class="math inline">\((1-\varepsilon)\widehat{A}\)</span> whenever <span class="math inline">\(r&lt;1-\varepsilon\)</span>.
Thus
<span class="math display">\[
L^{\text{CLIP}}(r,\widehat{A})
=
\begin{cases}
r\,\widehat{A}, &amp; r\ge 1-\varepsilon,\\[4pt]
(1-\varepsilon)\widehat{A}, &amp; r&lt;1-\varepsilon,
\end{cases}
\]</span>
so <span class="math inline">\(\frac{\partial L^{\text{CLIP}}}{\partial r}=\widehat{A}(&lt;0)\)</span> for <span class="math inline">\(r\ge 1-\varepsilon\)</span> and <strong><span class="math inline">\(0\)</span></strong> for <span class="math inline">\(r&lt;1-\varepsilon\)</span>.
There is no incentive to shrink <span class="math inline">\(r\)</span> below <span class="math inline">\(1-\varepsilon\)</span>; the gradient goes to zero.</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ppo-clip-obj"></span>
<img src="images/Policy-Gradients/PPO_clip.png" alt="The clipped objective function in PPO (from the original PPO paper)." width="80%" />
<p class="caption">
Figure 3.16: The clipped objective function in PPO (from the original PPO paper).
</p>
</div>
<p>Therefore, the <span class="math inline">\(\min\)</span> with a clipped ratio creates flat regions where the objective stops improving in the “profitable” outward direction. This removes the optimization incentive to move <span class="math inline">\(r\)</span> outside <span class="math inline">\([1-\varepsilon,1+\varepsilon]\)</span>, implementing a per-sample trust region on the probability ratio while retaining the standard policy-gradient inside the bracket.</p>
<p>The following pseudocode implements PPO (clipped version) with GAE.</p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>Proximal Policy Optimization (PPO–Clip)</strong>
</div>
<p><strong>Inputs:</strong> policy <span class="math inline">\(\pi_\theta\)</span>, value <span class="math inline">\(V_\phi\)</span>, discount <span class="math inline">\(\gamma\)</span>, GAE <span class="math inline">\(\lambda\)</span>, clip <span class="math inline">\(\varepsilon\)</span>, coefficients <span class="math inline">\(c_v,c_e\)</span>, learning rate <span class="math inline">\(\alpha\)</span>, epochs <span class="math inline">\(K_{\text{epoch}}\)</span>, minibatch size <span class="math inline">\(B\)</span>.</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Collect on-policy data.</strong> Roll out <span class="math inline">\(\pi_{\theta_k}\)</span> to get trajectories <span class="math inline">\(\{(s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\}\)</span>.<br />
Cache <span class="math inline">\(\log \pi_{\theta_k}(a_t\mid s_t)\)</span>.</p></li>
<li><p><strong>Compute GAE advantages and value targets.</strong><br />
<span class="math inline">\(\delta_t = r_t + \gamma(1-\mathrm{done}_t) V_\phi(s_{t+1}) - V_\phi(s_t)\)</span><br />
<span class="math inline">\(\widehat{A}_t = \delta_t + \gamma\lambda(1-\mathrm{done}_t)\widehat{A}_{t+1}\)</span>, with <span class="math inline">\(\widehat{A}_T=0\)</span>.<br />
<span class="math inline">\(\widehat{V}^{\,\text{targ}}_t = \widehat{A}_t + V_\phi(s_t)\)</span>.<br />
(Optionally standardize <span class="math inline">\(\{\widehat{A}_t\}\)</span> within the batch.)</p></li>
<li><p><strong>Policy/Value optimization (multiple epochs).</strong><br />
For <span class="math inline">\(e=1,\dots,K_{\text{epoch}}\)</span>:</p>
<ul>
<li>Split the batch into minibatches <span class="math inline">\(\mathcal{B}\)</span> of size <span class="math inline">\(B\)</span>.<br />
</li>
<li>For each <span class="math inline">\(\mathcal{B}\)</span>:
<span class="math display">\[
\rho_\theta(s,a) = \exp\!\big(\log\pi_\theta(a\mid s)-\log\pi_{\theta_k}(a\mid s)\big),
\]</span>
<span class="math display">\[
\ell^{\text{CLIP}}_{\mathcal{B}}(\theta) =
\frac{1}{B}\sum_{(s,a)\in\mathcal{B}}
\min\!\big(\rho_\theta\,\widehat{A},\;
\operatorname{clip}(\rho_\theta,1-\varepsilon,1+\varepsilon)\,\widehat{A}\big),
\]</span>
<span class="math display">\[
\ell^{\text{VAL}}_{\mathcal{B}}(\phi)=
\frac{1}{B}\sum_{s\in\mathcal{B}}(V_\phi(s)-\widehat{V}^{\,\text{targ}})^2,\quad
\mathcal{H}_{\mathcal{B}}(\theta)=\frac{1}{B}\sum_{s\in\mathcal{B}}\mathcal{H}(\pi_\theta(\cdot\mid s)).
\]</span></li>
<li>The total loss to be minimized is
<span class="math display">\[
\mathcal{J}_{\mathcal{B}}(\theta,\phi) = - \ell^{\text{CLIP}}_{\mathcal{B}}(\theta) + c_v\,\ell^{\text{VAL}}_{\mathcal{B}}(\phi) - c_e\,\mathcal{H}_{\mathcal{B}}(\theta).
\]</span></li>
<li>Take an optimizer step on <span class="math inline">\(\mathcal{J}_{\mathcal{B}}\)</span> (e.g., Adam with learning rate <span class="math inline">\(\alpha\)</span>).</li>
</ul></li>
<li><p><strong>(Optional) Early stopping by KL.</strong><br />
Estimate <span class="math inline">\(\widehat{\bar D}_{\mathrm{KL}}(\theta_k\!\parallel\!\theta)\)</span> on the whole batch; stop inner epochs early if it exceeds a threshold.</p></li>
</ol>
</div>
</div>
</div>
<div id="soft-actorcritic" class="section level3 hasAnchor" number="3.4.8">
<h3><span class="header-section-number">3.4.8</span> Soft Actor–Critic<a href="policy-gradient.html#soft-actorcritic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Standard actor–critic methods maximize expected return. Soft Actor–Critic (SAC) augments the objective with an entropy bonus that explicitly encourages exploration and robustness while remaining off-policy and sample efficient <span class="citation">(<a href="#ref-haarnoja2018soft">Haarnoja et al. 2018</a>)</span>. We first introduce a minimal implementation of SAC for discrete actions, then present full SAC with additional techniques for continuous actions.</p>
<div id="sac-for-discrete-actions" class="section level4 hasAnchor" number="3.4.8.1">
<h4><span class="header-section-number">3.4.8.1</span> SAC for Discrete Actions<a href="policy-gradient.html#sac-for-discrete-actions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Entropy of A Probability Distribution.</strong> Given a probability distribution <span class="math inline">\(P\)</span> supported on the set <span class="math inline">\(\mathcal{X}\)</span>, the entropy of the distribution is defined as
<span class="math display" id="eq:entropy">\[\begin{equation}
\mathcal{H}(P) = - \sum_{x \in \mathcal{X}} P(x) \log P(x) = - \mathbb{E}_{x \sim P} \log P(x).
\tag{3.59}
\end{equation}\]</span>
Since <span class="math inline">\(0 \leq P(x) \leq 1\)</span> for any <span class="math inline">\(x\)</span>, it is clear that <span class="math inline">\(\mathcal{H}(P) \geq 0\)</span> for any distribution <span class="math inline">\(P\)</span>.</p>
<p>Suppose the set <span class="math inline">\(\mathcal{X}\)</span> has <span class="math inline">\(N\)</span> elements <span class="math inline">\(x_1,\dots,x_N\)</span>, and suppose <span class="math inline">\(P(x_i) = p_i \geq 0,i=1,\dots,N\)</span>. We claim that the distribution <span class="math inline">\(P^\star\)</span> that maximizes <span class="math inline">\(\mathcal{H}(P)\)</span> is such that <span class="math inline">\(p^\star_i = \frac{1}{N},i=1,\dots,N\)</span>.</p>
<p>To show this, consider the function <span class="math inline">\(\log t\)</span> that is concave for <span class="math inline">\(t &gt; 0\)</span>. Using <a href="https://en.wikipedia.org/wiki/Jensen&#39;s_inequality">Jensen’s inequality</a>, we have that
<span class="math display">\[\begin{equation}
\begin{split}
\mathcal{H}(P) &amp; = - \sum_{x} P(x) \log P(x) = \sum_{x} P(x) \log \frac{1}{P(x)} \\
&amp; \leq \log \left( \sum_{x} P(x) \frac{1}{P(x)} \right) \\
&amp; = \log N,
\end{split}
\end{equation}\]</span>
with the equality holds if and only if <span class="math inline">\(P(x_1)=P(x_2)=\dots=P(x_N) = \frac{1}{N}\)</span>. Therefore, maximizing the entropy <span class="math inline">\(\mathcal{H}(P)\)</span> encourages the distribution <span class="math inline">\(P\)</span> to have a density function that spreads out evenly over the set <span class="math inline">\(\mathcal{X}\)</span>.</p>
<p><strong>Maximum-Entropy Objective.</strong> SAC maximizes the soft objective
<span class="math display" id="eq:soft-objective">\[\begin{equation}
\begin{split}
J(\pi) &amp;= \mathbb{E}\!\left[\sum_{t}\gamma^t\Big(R(s_t,a_t)\;+\;\alpha\,\mathcal{H}\!\left(\pi(\cdot\mid s_t)\right)\Big)\right],
\\
\mathcal{H}(\pi(\cdot\mid s)) &amp;= -\mathbb{E}_{a\sim\pi}[\log \pi(a\mid s)],
\end{split}
\tag{3.60}
\end{equation}\]</span>
where the entropy function <span class="math inline">\(\mathcal{H}(\cdot)\)</span> encourages the policy to explore, and the temperature <span class="math inline">\(\alpha&gt;0\)</span> balances reward maximization against exploration.</p>
<p>Given a trajectory <span class="math inline">\(\tau = (s_0, a_0, r_0, s_1, a_1,\dots)\)</span>, define the soft return:
<span class="math display">\[
g_t = \sum_{t=0} \gamma^t \left( R(s_t, a_t) - \alpha \log \pi (a_t \mid s_t) \right).
\]</span>
This leads to the “soft” state value and soft action value associated with <span class="math inline">\(\pi\)</span>:
<span class="math display" id="eq:soft-value-functions">\[\begin{equation}
\begin{split}
V^\pi(s) &amp; = \mathbb{E}_{a\sim\pi}\left[Q^\pi(s,a)\;-\;\alpha\log\pi(a\mid s)\right], \\
Q^\pi(s,a) &amp; = R(s,a) + \gamma\,\mathbb{E}_{s&#39;} \left[V^\pi(s&#39;)\right].
\end{split}
\tag{3.61}
\end{equation}\]</span>
Combining the two equations above, we obtain a soft Bellman Consistency equation on the <span class="math inline">\(Q\)</span> value:
<span class="math display" id="eq:Bellman-Consistency-Soft-Q">\[\begin{equation}
Q^{\pi}(s,a) = R(s, a) + \gamma \mathbb{E}_{s&#39;} \left[ \mathbb{E}_{a&#39; \sim \pi} \left[ Q^\pi(s&#39;, a&#39;) - \alpha \log \pi(a&#39; \mid s&#39;) \right] \right].
\tag{3.62}
\end{equation}\]</span></p>
<p><strong>Critic Update.</strong> For a replay sample <span class="math inline">\((s,a,r,s&#39;)\)</span>, assuming discrete actions, we can compute the target <span class="math inline">\(Q\)</span> value following the soft Bellman Consistency equation <a href="policy-gradient.html#eq:Bellman-Consistency-Soft-Q">(3.62)</a>
<span class="math display" id="eq:sac-target">\[\begin{equation}
y \;=\;
r + \gamma \sum_{a&#39;} \pi_{\theta} (a&#39; \mid s&#39;) \left( Q_{\bar{\psi}}(s&#39;, a&#39;) - \alpha \log  \pi_\theta (a&#39; \mid s&#39;) \right)
\tag{3.63}
\end{equation}\]</span>
where <span class="math inline">\(Q_{\bar{\psi}}\)</span> is the target Q network inspired by DQN to mitigate the deadly triad.
The critic loss is therefore
<span class="math display" id="eq:sac-critic-loss">\[\begin{equation}
\mathcal{L}_Q(\psi) \;=\; \mathbb{E}_{(s,a) \sim \mathcal{D}}\big[\big(Q_{\psi}(s,a)-y\big)^2\big],
\tag{3.64}
\end{equation}\]</span>
where the expectation is taken over a minibatch drawn from the replay buffer.</p>
<p><strong>Actor Update.</strong> Given the learned critic <span class="math inline">\(Q_\psi\)</span> and replay state distribution <span class="math inline">\(s \sim \mathcal{D}\)</span>, the SAC policy improvement step chooses <span class="math inline">\(\pi_\theta\)</span> to <strong>minimize</strong>, for each state, the soft advantage–regularized objective
<span class="math display" id="eq:sac-actor-discrete">\[\begin{equation}
J_\pi(\theta)
\;=\;
\mathbb{E}_{s\sim\mathcal{D}}
\left[
\sum_{a} \pi_\theta(a\mid s)\left( \alpha\,\log \pi_\theta(a\mid s) - Q_\psi(s,a)\right)
\right].
\tag{3.65}
\end{equation}\]</span>
For discrete actions, the expectation over <span class="math inline">\(a\)</span> is a finite sum—no action sampling is required.</p>
<p>Differentiating <a href="policy-gradient.html#eq:sac-actor-discrete">(3.65)</a> yields the policy gradient
<span class="math display" id="eq:sac-actor-grad-discrete">\[\begin{equation}
\nabla_\theta J_\pi(\theta)
=
\mathbb{E}_{s\sim\mathcal{D}}
\left[
\sum_{a}
\nabla_\theta \pi_\theta(a\mid s)\;
\Big(\alpha\,[1+\log \pi_\theta(a\mid s)] - Q_\psi(s,a)\Big)
\right].
\tag{3.66}
\end{equation}\]</span></p>
<p>The following pseudocode implements a basic SAC algorithm with discrete actions.</p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>Soft Actor–Critic (Discrete Actions, Single Q + Single Target)</strong>
</div>
<p><strong>Inputs:</strong> replay buffer <span class="math inline">\(\mathcal{D}\)</span>; policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span> over <span class="math inline">\(K\)</span> actions; single critic <span class="math inline">\(Q_{\psi}(s,\cdot)\)</span> (returns a <span class="math inline">\(K\)</span>-vector); target critic parameters <span class="math inline">\(\bar\psi\)</span>; discount <span class="math inline">\(\gamma\)</span>; temperature <span class="math inline">\(\alpha\)</span> (learned or fixed); Polyak <span class="math inline">\(\tau\in(0,1]\)</span>; batch size <span class="math inline">\(B\)</span>; stepsizes <span class="math inline">\(\alpha_\theta,\alpha_\psi\)</span>.</p>
<p><strong>Initialize:</strong> <span class="math inline">\(\bar\psi \leftarrow \psi\)</span>.</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Interaction.</strong><br />
Observe <span class="math inline">\(s_t\)</span>. Sample <span class="math inline">\(a_t \sim \pi_\theta(\cdot\mid s_t)\)</span>; step env to get <span class="math inline">\((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\)</span>; push to <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p><strong>Sample minibatch.</strong><br />
Draw <span class="math inline">\(B\)</span> transitions <span class="math inline">\(\{(s,a,r,s&#39;,d)\}\)</span> from <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p><strong>Target computation (single target network).</strong></p>
<ul>
<li>Compute <span class="math inline">\(\pi_\theta(\cdot\mid s&#39;)\)</span> and <span class="math inline">\(\log \pi_\theta(\cdot\mid s&#39;)\)</span>.</li>
<li>Evaluate target critic <span class="math inline">\(Q_{\bar\psi}(s&#39;,\cdot)\)</span>.</li>
<li>Soft value target:<br />
<span class="math display">\[
V_{\text{tgt}}(s&#39;)=\Big\langle \pi_\theta(\cdot\mid s&#39;),\; Q_{\bar\psi}(s&#39;,\cdot)-\alpha\,\log \pi_\theta(\cdot\mid s&#39;) \Big\rangle.
\]</span></li>
<li><strong>Bellman target:</strong><br />
<span class="math display">\[
y \leftarrow r + \gamma(1-d)\,V_{\text{tgt}}(s&#39;).
\]</span>
<em>(Matches <a href="policy-gradient.html#eq:sac-target">(3.63)</a> with one target network.)</em></li>
</ul></li>
<li><p><strong>Critic update.</strong>
Minimize the squared error (cf. <a href="policy-gradient.html#eq:sac-critic-loss">(3.64)</a>):<br />
<span class="math display">\[
  \psi \leftarrow \psi - \alpha_\psi \nabla_{\psi}\;\frac{1}{B}\sum\nolimits_{(s,a,r,s&#39;,d)}
  \big(Q_{\psi}(s,a)-y\big)^2.
  \]</span></p></li>
<li><p><strong>Actor update.</strong> Minimize (cf. <a href="policy-gradient.html#eq:sac-actor-discrete">(3.65)</a>):<br />
<span class="math display">\[
  \theta \leftarrow \theta - \alpha_\theta \nabla_\theta \frac{1}{B}\sum_s
  \sum_{a} \pi_\theta(a\mid s)\,\Big(\alpha\,\log \pi_\theta(a\mid s)-Q_{\psi}(s,a)\Big).
  \]</span></p></li>
<li><p><strong>Target critic (Polyak).</strong><br />
<span class="math display">\[
\bar\psi \;\leftarrow\; \tau\,\psi \;+\; (1-\tau)\,\bar\psi.
\]</span></p></li>
</ol>
</div>
<p>The next example applies the SAC algorithm above to the cart-pole problem.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:cartpole-sac" class="example"><strong>Example 3.7  (SAC for Cart-pole Balancing) </strong></span>We use a fixed temperature <span class="math inline">\(\alpha=0.2\)</span>.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-learning-curve-sac">3.17</a> shows the learning curve of SAC.</p>
<p>Fig. <a href="policy-gradient.html#fig:cart-pole-policy-rollout-sac">3.18</a> shows a sample rollout of the learned policy.</p>
<p>You can find the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/cartpole_sac.py">here</a>. Play with the temperature parameter.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-learning-curve-sac"></span>
<img src="images/Policy-Gradients/cartpole_learning_curve_sac.png" alt="Learning curve (Soft Actor--Critic)." width="60%" />
<p class="caption">
Figure 3.17: Learning curve (Soft Actor–Critic).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cart-pole-policy-rollout-sac"></span>
<img src="images/Policy-Gradients/cartpole-sac-rollout.gif" alt="Policy rollout (Soft Actor--Critic)." width="60%" />
<p class="caption">
Figure 3.18: Policy rollout (Soft Actor–Critic).
</p>
</div>
</div>
</div>
</div>
<div id="sac-for-continuous-actions" class="section level4 hasAnchor" number="3.4.8.2">
<h4><span class="header-section-number">3.4.8.2</span> SAC for Continuous Actions<a href="policy-gradient.html#sac-for-continuous-actions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In continuous action spaces we cannot sum over actions. SAC therefore:</p>
<ol style="list-style-type: decimal">
<li><p>samples actions from the current policy using a <em>reparameterization</em> trick (low-variance gradients), and</p></li>
<li><p>computes the soft Bellman target with those sampled actions and a <em>twin-target minimum</em> to reduce overestimation.</p></li>
</ol>
<p><strong>Reparameterization (pathwise) Gradient.</strong> Let the stochastic policy be a Gaussian in unconstrained space, squashed by <code>tanh</code> to the action bounds:
<span class="math display">\[
u \;=\; \mu_\theta(s)\;+\;\sigma_\theta(s)\odot\varepsilon,\quad \varepsilon\sim\mathcal N(0,I),
\qquad
a \;=\; \tanh(u)\cdot a_{\text{scale}} + a_{\text{bias}},
\]</span>
where <span class="math inline">\(\sigma_\theta(s)\)</span> outputs per-dimension standard deviation.
This gives a differentiable map <span class="math inline">\(a=f_\theta(s,\varepsilon)\)</span>. Expectations over <span class="math inline">\(a\sim \pi_\theta(\cdot\mid s)\)</span> are then written as expectations over <span class="math inline">\(\varepsilon\)</span>, so gradients can flow through <span class="math inline">\(f_\theta\)</span> (the <em>pathwise derivative</em>). The correct log-density under the squashed policy uses change-of-variables:
<span class="math display">\[
\log\pi_\theta(a\mid s)
\;=\;
\log\mathcal N\!\big(u;\mu_\theta(s),\sigma_\theta^2(s)\big)
\;-\;\sum_i \log\!\big(1-\tanh^2(u_i)\big)
\;+\;\text{constant}.
\]</span>
The intuition here is that the <code>tanh</code> function is a nonlinear transformation that distorts the original Gaussian distribution. This “tanh correction” is crucial for stable training.</p>
<p><strong>Critic Update.</strong> Maintain two critics <span class="math inline">\(Q_{\psi_1},Q_{\psi_2}\)</span> and their target copies
<span class="math inline">\(Q_{\bar\psi_1},Q_{\bar\psi_2}\)</span>.
For a replay minibatch <span class="math inline">\((s,a,r,s&#39;,d)\)</span>, form the target by drawing a next action from the current policy:
<span class="math display" id="eq:cont-target">\[\begin{equation}
a&#39; \sim \pi_\theta(\cdot\mid s&#39;),\qquad
y \;=\; r \;+\; \gamma(1-d)\,\Big(
\min\nolimits_{j=1,2} Q_{\bar\psi_j}(s&#39;,a&#39;) \;-\; \alpha \log\pi_\theta(a&#39;\mid s&#39;)
\Big).
\tag{3.67}
\end{equation}\]</span>
Each critic minimizes the squared error to <span class="math inline">\(y\)</span> (with stop-grad on <span class="math inline">\(y\)</span>):
<span class="math display">\[
\mathcal L_Q(\psi_j) \;=\; \mathbb{E}\big[(Q_{\psi_j}(s,a)-y)^2\big],\quad j=1,2,
\]</span>
where the expectation is taken over the distribution in the replay buffer.</p>
<p><strong>Actor Update.</strong> The actor minimizes the soft objective under the replay state distribution:
<span class="math display" id="eq:cont-actor">\[\begin{equation}
J_\pi(\theta)
\;=\;
\mathbb{E}_{s\sim\mathcal D,\,\varepsilon}\left[
\alpha\,\log\pi_\theta \big(f_\theta(s,\varepsilon)\mid s\big)
-\min\nolimits_{j=1,2} Q_{\psi_j} \big(s,f_\theta(s,\varepsilon)\big)
\right].
\tag{3.68}
\end{equation}\]</span>
By reparameterization, the gradient flows through both the explicit <span class="math inline">\(\log\pi_\theta\)</span> term and the path <span class="math inline">\(a=f_\theta(s,\varepsilon)\)</span>. Particularly, denote <span class="math inline">\(Q_\psi(\cdot, \cdot) = \min_{j=1,2} Q_{\psi_j} (\cdot, \cdot)\)</span>, we have that
<span class="math display">\[
\nabla_\theta J_\pi(\theta) = \mathbb{E}_{s,\varepsilon} \left[ \alpha \nabla_\theta \log \pi_\theta (a \mid s) + \left( \alpha \nabla_a \log \pi_\theta(a \mid s) - \nabla_a Q_\psi (s,a) \right) \nabla_\theta f_\theta (s, \varepsilon) \right]_{a=f_\theta(s,\varepsilon)}.
\]</span>
In code, you typically just write the loss
<span class="math display">\[
\mathbb{E}_{s, \varepsilon}\left[ \alpha \log \pi_\theta (a \mid s) - Q_\psi (s, a) \right], \quad a = f_\theta(s, \varepsilon),
\]</span>
and autodiff will automatically compute the correct gradient.</p>
<p><strong>Tuning <span class="math inline">\(\alpha\)</span> (Temperature).</strong> <span class="math inline">\(\alpha\)</span> trades off reward pursuit vs. policy entropy. A fixed <span class="math inline">\(\alpha\)</span> is problem-dependent. SAC treats <span class="math inline">\(\alpha\)</span> as a dual variable to enforce a target entropy <span class="math inline">\(\bar{\mathcal H}\)</span> (often <span class="math inline">\(-\text{dim}(\mathcal A)\)</span>):
<span class="math display">\[
J(\alpha)\;=\;\mathbb{E}_{s\sim\mathcal D,\,a\sim\pi_\theta}\!\Big[-\alpha\big(\log \pi_\theta(a\mid s)+\bar{\mathcal H}\big)\Big],
\quad
\log\alpha \leftarrow \log\alpha - \alpha_\alpha \nabla_{\log\alpha} J(\alpha).
\]</span>
This adapts exploration automatically across tasks and training phases.</p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>Soft Actor–Critic (Continuous Actions, Twin Critics + Twin Targets)</strong>
</div>
<p><strong>Inputs:</strong> replay buffer <span class="math inline">\(\mathcal{D}\)</span>; policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span> reparameterized by <span class="math inline">\(a=f_\theta(s,\varepsilon)\)</span> with tanh-squashed Gaussian; twin critics <span class="math inline">\(Q_{\psi_1},Q_{\psi_2}\)</span>; twin target critics with params <span class="math inline">\(\bar\psi_1,\bar\psi_2\)</span>; discount <span class="math inline">\(\gamma\)</span>; temperature <span class="math inline">\(\alpha\)</span> (learned or fixed); Polyak <span class="math inline">\(\tau\in(0,1]\)</span>; batch size <span class="math inline">\(B\)</span>; stepsizes <span class="math inline">\(\alpha_\theta,\alpha_\psi,\alpha_\alpha\)</span>.</p>
<p><strong>Initialize:</strong> <span class="math inline">\(\bar\psi_j \leftarrow \psi_j\)</span> for <span class="math inline">\(j\in\{1,2\}\)</span>.</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Interaction.</strong><br />
Observe <span class="math inline">\(s_t\)</span>. Sample <span class="math inline">\(\varepsilon_t\sim\mathcal N(0,I)\)</span>, set <span class="math inline">\(a_t=f_\theta(s_t,\varepsilon_t)\)</span>; step env to get <span class="math inline">\((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\)</span>; push to <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p><strong>Sample minibatch.</strong><br />
Draw <span class="math inline">\(B\)</span> transitions <span class="math inline">\(\{(s,a,r,s&#39;,d)\}\)</span> from <span class="math inline">\(\mathcal{D}\)</span>.</p></li>
<li><p><strong>Target computation (twin targets, reparameterized next action).</strong></p>
<ul>
<li>Sample <span class="math inline">\(\varepsilon&#39;\sim\mathcal N(0,I)\)</span>, set <span class="math inline">\(a&#39;=f_\theta(s&#39;,\varepsilon&#39;)\)</span>.</li>
<li>Compute <span class="math inline">\(\log\pi_\theta(a&#39;\mid s&#39;)\)</span> with <strong>tanh correction</strong>.</li>
<li>Evaluate target critics <span class="math inline">\(Q_{\bar\psi_1}(s&#39;,a&#39;)\)</span>, <span class="math inline">\(Q_{\bar\psi_2}(s&#39;,a&#39;)\)</span>; let <span class="math inline">\(Q_{\min}(s&#39;,a&#39;)=\min\{Q_{\bar\psi_1},Q_{\bar\psi_2}\}\)</span>.</li>
<li><strong>Bellman target:</strong><br />
<span class="math display">\[
y \leftarrow r + \gamma(1-d)\,\big(Q_{\min}(s&#39;,a&#39;) - \alpha\,\log\pi_\theta(a&#39;\mid s&#39;)\big).
\]</span>
<em>(Stop gradient through <span class="math inline">\(y\)</span>.)</em></li>
</ul></li>
<li><p><strong>Critic updates (both heads).</strong><br />
<span class="math display">\[
\psi_j \leftarrow \psi_j - \alpha_\psi \nabla_{\psi_j}\;\frac{1}{B}\sum (Q_{\psi_j}(s,a)-y)^2,\quad j=1,2.
\]</span></p></li>
<li><p><strong>Actor update (reparameterized).</strong></p>
<ul>
<li>For each <span class="math inline">\(s\)</span> in the batch, sample <span class="math inline">\(\varepsilon\)</span>, set <span class="math inline">\(a=f_\theta(s,\varepsilon)\)</span>.</li>
<li><strong>Actor objective:</strong><br />
<span class="math display">\[
J_\pi(\theta)=\frac{1}{B}\sum_s \Big(\alpha\,\log\pi_\theta(a\mid s)-\min\nolimits_j Q_{\psi_j}(s,a)\Big).
\]</span></li>
<li>Update:
<span class="math display">\[
\theta \leftarrow \theta - \alpha_\theta \nabla_\theta J_\pi(\theta).
\]</span></li>
</ul></li>
<li><p><strong>Temperature (optional).</strong><br />
With target entropy <span class="math inline">\(\bar{\mathcal H}\)</span> and parameter <span class="math inline">\(\log\alpha\)</span>:
<span class="math display">\[
J(\alpha)=\frac{1}{B}\sum_s \big[-\alpha(\log\pi_\theta(a\mid s)+\bar{\mathcal H})\big],\quad
\log\alpha \leftarrow \log\alpha - \alpha_\alpha \nabla_{\log\alpha} J(\alpha),\quad
\alpha\leftarrow e^{\log\alpha}.
\]</span></p></li>
<li><p><strong>Target critics (Polyak).</strong> For <span class="math inline">\(j=1,2\)</span>:
<span class="math display">\[
\bar\psi_j \;\leftarrow\; \tau\,\psi_j \;+\; (1-\tau)\,\bar\psi_j.
\]</span></p></li>
</ol>
</div>
<p>The next example applies SAC to Inverted Pendulum.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:pendulum-sac" class="example"><strong>Example 3.8  (SAC for Inverted Pendulum) </strong></span>Fig. <a href="policy-gradient.html#fig:pendulum-learning-curve-sac">3.19</a> plots the learning curve.</p>
<p>Fig. <a href="policy-gradient.html#fig:pendulum-policy-rollout-sac">3.20</a> visualizes two sample rollouts of the policy.</p>
<p>Code can be found <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_sac.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-learning-curve-sac"></span>
<img src="images/Policy-Gradients/pendulum_learning_curve_sac.png" alt="Learning curve (Soft Actor--Critic)." width="60%" />
<p class="caption">
Figure 3.19: Learning curve (Soft Actor–Critic).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-policy-rollout-sac"></span>
<img src="images/Policy-Gradients/pendulum_rollout_sac-1.gif" alt="Policy rollout (Soft Actor--Critic)." width="40%" /><img src="images/Policy-Gradients/pendulum_rollout_sac-2.gif" alt="Policy rollout (Soft Actor--Critic)." width="40%" />
<p class="caption">
Figure 3.20: Policy rollout (Soft Actor–Critic).
</p>
</div>
</div>
</div>
</div>
</div>
<div id="deterministic-policy-gradient" class="section level3 hasAnchor" number="3.4.9">
<h3><span class="header-section-number">3.4.9</span> Deterministic Policy Gradient<a href="policy-gradient.html#deterministic-policy-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In continuous-control tasks, sampling or integrating over actions inside policy gradients is costly and noisy. The Deterministic Policy Gradient (DPG) framework <span class="citation">(<a href="#ref-silver2014deterministic">Silver et al. 2014</a>)</span> replaces the stochastic policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span> with a <em>deterministic</em> actor
<span class="math display">\[
a = \mu_\theta(s)\in\mathbb{R}^m.
\]</span>
Its state–action value and discounted state visitation measure are
<span class="math display">\[
Q^{\mu_\theta}(s,a) \;=\; \mathbb{E}\!\left[\sum_{t=0}^\infty \gamma^t R(s_t,a_t)\,\middle|\,s_0=s,\ a_0=a,\ a_{t&gt;0}=\mu_\theta(s_t)\right],
\]</span>
<span class="math display">\[
\rho^{\pi}(s) \;=\; \sum_{t=0}^{\infty}\gamma^t \Pr(s_t=s \mid a_t\sim \pi(\cdot\mid s_t)),\qquad
\rho^{\mu_\theta}\equiv \rho^{\pi=\mu_\theta}.
\]</span></p>
<p>We consider two objectives:</p>
<ul>
<li><p><strong>On-policy objective</strong>:
<span class="math display" id="eq:DPG-obj-on">\[\begin{equation}
J(\theta)\;=\;\mathbb{E}_{s\sim \rho^{\mu_\theta}}\big[\,Q^{\mu_\theta}(s,\mu_\theta(s))\,\big].
\tag{3.69}
\end{equation}\]</span></p></li>
<li><p><strong>Off-policy surrogate</strong> (with behavior policy <span class="math inline">\(\beta\)</span>):
<span class="math display" id="eq:DPG-obj-off">\[\begin{equation}
J_\beta(\theta)\;=\;\mathbb{E}_{s\sim \rho^{\beta}} \big[\,Q^{\mu_\theta}(s,\mu_\theta(s))\,\big].
\tag{3.70}
\end{equation}\]</span></p></li>
</ul>
<p>The on-policy objective <a href="policy-gradient.html#eq:DPG-obj-on">(3.69)</a> is the usual RL objective in policy gradient methods, as <span class="math inline">\(Q^{\mu_\theta}(s, \mu_\theta(s)) = V^{\mu_\theta}(s)\)</span> by definition.</p>
<p>A key result in Deterministic Policy Gradient is that under mild conditions, optimizating the surrogate off-policy objective <a href="policy-gradient.html#eq:DPG-obj-off">(3.70)</a> is the same as optimizating the original on-policy objective.</p>
<p>To see this, assume</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R\)</span> and <span class="math inline">\(P(\cdot\mid s,a)\)</span> (the transition dynamics) are bounded/measurable; <span class="math inline">\(Q^{\mu_\theta}\)</span> exists and is continuously differentiable in <span class="math inline">\(a\)</span>;</li>
<li><span class="math inline">\(\mu_\theta(s)\)</span> is continuously differentiable in <span class="math inline">\(\theta\)</span>;</li>
<li>Interchange of integration and differentiation is valid (e.g., dominated convergence).</li>
</ol>
<p>Then, the on-policy and off-policy deterministic policy gradients (DPG) are:</p>
<ul>
<li><p><strong>On-policy DPG.</strong>
<span class="math display" id="eq:DPG-grad-on">\[\begin{equation}
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{s\sim \rho^{\mu_\theta}} \left[
\nabla_\theta \mu_\theta(s)\;
\nabla_a Q^{\mu_\theta}(s,a)\big|_{a=\mu_\theta(s)}
\right].
\tag{3.71}
\end{equation}\]</span></p></li>
<li><p><strong>Off-policy DPG.</strong> For any behavior policy <span class="math inline">\(\beta\)</span> with visitation <span class="math inline">\(\rho^\beta\)</span>,
<span class="math display" id="eq:DPG-grad-off">\[\begin{equation}
\nabla_\theta J_\beta(\theta)
\;=\;
\mathbb{E}_{s\sim \rho^{\beta}} \left[
\nabla_\theta \mu_\theta(s)\;
\nabla_a Q^{\mu_\theta}(s,a)\big|_{a=\mu_\theta(s)}
\right].
\tag{3.72}
\end{equation}\]</span></p></li>
</ul>
<p>In particular, the off-policy DPG <a href="policy-gradient.html#eq:DPG-grad-off">(3.72)</a> can be estimated from replay sampled under <span class="math inline">\(\beta\)</span> without action-importance ratios; only the state weighting changes.</p>
<p>The following result states that the on-policy and off-policy objectives share the same stationary points.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:dpg-common-foc" class="theorem"><strong>Theorem 3.9  (Common First-Order Optima) </strong></span>Let
<span class="math display" id="eq:per-state-gradient">\[\begin{equation}
g(s;\theta)\;:=\;\nabla_\theta \mu_\theta(s)\,\nabla_a Q^{\mu_\theta}(s,a)\big|_{a=\mu_\theta(s)}\in\mathbb{R}^{d},
\tag{3.73}
\end{equation}\]</span>
where <span class="math inline">\(d\)</span> is the dimension of <span class="math inline">\(\theta\)</span>.</p>
<p>Suppose <span class="math inline">\(\rho^\beta\)</span> has coverage of the on-policy support, i.e.,
<span class="math display">\[
\text{supp}(\rho^{\mu_\theta})\ \subseteq\ \text{supp}(\rho^\beta),\quad
\text{and}\quad \rho^\beta(s)&gt;0\ \text{a.e. on }\text{supp}(\rho^{\mu_\theta}).
\]</span>
If <span class="math inline">\(g(s;\theta^\star)=0\)</span> for <span class="math inline">\(\rho^{\mu_{\theta^\star}}\)</span>-almost every <span class="math inline">\(s\)</span> (in particular, if <span class="math inline">\(\mu_{\theta^\star}\)</span> is greedy w.r.t. <span class="math inline">\(Q^{\mu_{\theta^\star}}\)</span>, so <span class="math inline">\(\nabla_a Q^{\mu_{\theta^\star}}(s,a)|_{a=\mu_{\theta^\star}(s)}=0\)</span> for all <span class="math inline">\(s\)</span>), then
<span class="math display">\[
\nabla_\theta J(\theta^\star)=0
\quad\text{and}\quad
\nabla_\theta J_\beta(\theta^\star)=0.
\]</span>
Thus any deterministic policy satisfying the first-order optimality condition (greedy w.r.t. its own <span class="math inline">\(Q\)</span>) is a <em>stationary point of both</em> <span class="math inline">\(J\)</span> and <span class="math inline">\(J_\beta\)</span>, regardless of the (covered) state weighting.</p>
<p>If additionally <span class="math inline">\(\text{supp}(\rho^{\mu_\theta})=\text{supp}(\rho^\beta)\)</span> and both are strictly positive on that support, then
<span class="math display">\[
\nabla_\theta J(\theta)=0 \ \Longleftrightarrow\ \nabla_\theta J_\beta(\theta)=0.
\]</span></p>
</div>
</div>
<p><strong>Remarks.</strong></p>
<ul>
<li><p>The off-policy objective <span class="math inline">\(J_\beta\)</span> changes only the <strong>weights</strong> over states; the <strong>per-state improvement direction</strong> <span class="math inline">\(g(s;\theta)\)</span> is identical. With sufficient coverage, ascent on <span class="math inline">\(J_\beta\)</span> improves <span class="math inline">\(J\)</span> and shares its stationary points.</p></li>
<li><p>In practice, DDPG uses exploration noise to expand support of <span class="math inline">\(\rho^\beta\)</span> and target networks to stabilize <span class="math inline">\(Q^{\mu_\theta}\)</span>, making the off-policy gradient estimate reliable.</p></li>
</ul>
<p><strong>From DPG to DDPG (Deep DPG).</strong> DDPG <span class="citation">(<a href="#ref-lillicrap2015continuous">Lillicrap et al. 2015</a>)</span> implements DPG with deep networks + standard stabilizers:</p>
<ul>
<li><strong>Replay buffer</strong> <span class="math inline">\(\mathcal D\)</span> for off-policy sample efficiency.</li>
<li><strong>Target networks</strong> <span class="math inline">\(\mu_{\bar\theta}, Q_{\bar\psi}\)</span> with Polyak averaging to stabilize TD targets.</li>
<li><strong>Exploration noise</strong> added to the deterministic action: <span class="math inline">\(a_t = \mu_\theta(s_t) + \varepsilon_t\)</span> (original paper used Ornstein–Uhlenbeck noise; Gaussian works well too).</li>
</ul>
<p><strong>High-Level Algorithm (DDPG).</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Interact off-policy.</strong> Act with exploration: <span class="math inline">\(a_t=\mu_\theta(s_t)+\varepsilon_t\)</span>. Store <span class="math inline">\((s_t,a_t,r_t,s_{t+1},\text{done}_t)\)</span> in <span class="math inline">\(\mathcal D\)</span>.</p></li>
<li><p><strong>Critic TD(0).</strong> For a minibatch from <span class="math inline">\(\mathcal D\)</span>,
<span class="math display">\[
y = r + \gamma(1-\text{done})\,Q_{\bar\psi} \big(s&#39;,\,\mu_{\bar\theta}(s&#39;)\big),\qquad
\min_\psi\ \frac{1}{B}\sum (Q_\psi(s,a)-y)^2.
\]</span></p></li>
<li><p><strong>Actor DPG step.</strong>
<span class="math display">\[
\max_\theta\ \frac{1}{B}\sum Q_\psi\big(s,\,\mu_\theta(s)\big)
\quad\Longleftrightarrow\quad
\nabla_\theta J \approx \frac{1}{B}\sum \nabla_\theta \mu_\theta(s)\,\nabla_a Q_\psi(s,a)\big|_{a=\mu_\theta(s)}.
\]</span></p></li>
<li><p><strong>Targets Polyak update.</strong>
<span class="math display">\[
\bar\theta \leftarrow \tau\,\theta + (1-\tau)\bar\theta,\ \ \bar\psi \leftarrow \tau\,\psi + (1-\tau)\bar\psi.
\]</span></p></li>
</ol>
<p><strong>Remarks.</strong></p>
<ul>
<li>No entropy bonus or log-probabilities (in contrast to SAC). Exploration comes from additive noise.<br />
</li>
<li>Overestimation and sensitivity to hyperparameters can appear; target networks, small actor steps, and proper normalization help.</li>
</ul>
<p>The following pseudocode implements DDPG.</p>
<div class="highlightbox">
<div style="text-align:center;">
<strong>Deep Deterministic Policy Gradient (DDPG)</strong>
</div>
<p><strong>Inputs:</strong> replay buffer <span class="math inline">\(\mathcal{D}\)</span>; deterministic actor <span class="math inline">\(\mu_\theta(s)\)</span>; critic <span class="math inline">\(Q_\psi(s,a)\)</span>; target networks <span class="math inline">\(\mu_{\bar\theta}, Q_{\bar\psi}\)</span>; discount <span class="math inline">\(\gamma\in[0,1)\)</span>; Polyak <span class="math inline">\(\tau\in(0,1]\)</span>; batch size <span class="math inline">\(B\)</span>; stepsizes <span class="math inline">\(\alpha_\theta,\alpha_\psi\)</span>; exploration noise process <span class="math inline">\(\varepsilon_t\sim \mathcal N(0,\sigma^2 I)\)</span> (or Ornstein–Uhlenbeck).</p>
<p><strong>Initialize:</strong> <span class="math inline">\(\bar\theta\leftarrow\theta,\ \bar\psi\leftarrow\psi\)</span>. Fill <span class="math inline">\(\mathcal D\)</span> with a short random warm-up.</p>
<p>For iterations <span class="math inline">\(k=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Interaction (off-policy).</strong><br />
Observe <span class="math inline">\(s_t\)</span>. Compute action with noise<br />
<span class="math display">\[
a_t \leftarrow \text{clip}\big(\mu_\theta(s_t) + \varepsilon_t,\ a_{\min}, a_{\max}\big).
\]</span>
Step env to get <span class="math inline">\((s_t,a_t,r_t,s_{t+1},\mathrm{done}_t)\)</span>. Push into <span class="math inline">\(\mathcal D\)</span>.</p></li>
<li><p><strong>Sample minibatch.</strong><br />
Draw <span class="math inline">\(B\)</span> transitions <span class="math inline">\(\{(s,a,r,s&#39;,d)\}\)</span> from <span class="math inline">\(\mathcal D\)</span>.</p></li>
<li><p><strong>Critic target.</strong><br />
<span class="math display">\[
a&#39; \leftarrow \mu_{\bar\theta}(s&#39;),\qquad
y \leftarrow r + \gamma(1-d)\,Q_{\bar\psi}(s&#39;,a&#39;).
\]</span>
<em>(Stop gradient through <span class="math inline">\(y\)</span>.)</em></p></li>
<li><p><strong>Critic update.</strong><br />
<span class="math display">\[
\psi \leftarrow \psi - \alpha_\psi\,\nabla_\psi\ \frac{1}{B}\sum (Q_\psi(s,a)-y)^2.
\]</span></p></li>
<li><p><strong>Actor update (DPG).</strong><br />
<span class="math display">\[
\theta \leftarrow \theta + \alpha_\theta\ \frac{1}{B}\sum
\Big[\ \nabla_\theta \mu_\theta(s)\ \nabla_a Q_\psi(s,a)\big|_{a=\mu_\theta(s)}\ \Big].
\]</span>
<em>(Equivalently, ascend <span class="math inline">\(\frac{1}{B}\sum Q_\psi(s,\mu_\theta(s))\)</span> by backprop.)</em></p></li>
<li><p><strong>Target networks (Polyak).</strong><br />
<span class="math display">\[
\bar\theta \leftarrow \tau\,\theta + (1-\tau)\,\bar\theta,\qquad
\bar\psi \leftarrow \tau\,\psi + (1-\tau)\,\bar\psi.
\]</span></p></li>
</ol>
</div>
<p>The next example applies DDPG to Inverted Pendulum.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:pendulum-ddpg" class="example"><strong>Example 3.9  (DDPG for Inverted Pendulum) </strong></span>Fig. <a href="policy-gradient.html#fig:pendulum-learning-curve-ddpg">3.21</a> plots the learning curve of DDPG.</p>
<p>Fig. <a href="policy-gradient.html#fig:pendulum-policy-rollout-ddpg">3.22</a> visualizes sample rollouts of the learned policy.</p>
<p>Code can be found <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_ddpg.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-learning-curve-ddpg"></span>
<img src="images/Policy-Gradients/pendulum_learning_curve_ddpg.png" alt="Learning curve (DDPG)." width="60%" />
<p class="caption">
Figure 3.21: Learning curve (DDPG).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-policy-rollout-ddpg"></span>
<img src="images/Policy-Gradients/pendulum_rollout_ddpg-1.gif" alt="Policy rollout (DDPG)." width="30%" /><img src="images/Policy-Gradients/pendulum_rollout_ddpg-2.gif" alt="Policy rollout (DDPG)." width="30%" /><img src="images/Policy-Gradients/pendulum_rollout_ddpg-3.gif" alt="Policy rollout (DDPG)." width="30%" />
<p class="caption">
Figure 3.22: Policy rollout (DDPG).
</p>
</div>
</div>
</div>
</div>
</div>
<div id="model-based-policy-optimization" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Model-based Policy Optimization<a href="policy-gradient.html#model-based-policy-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Model-based policy optimization (MBPO) <span class="citation">(<a href="#ref-janner2019trust">Janner et al. 2019</a>)</span> sits between pure model-free methods (high variance, data hungry) and “plan-only” model-based control (sensitive to model bias, to be introduced in Chapter <a href="model-based-plan-optimize.html#model-based-plan-optimize">4</a>). The key idea is to learn a dynamics model and then use only <em>short</em> rollouts from that model to create extra training data for a strong off-policy learner (usually SAC).</p>
<p>Consider an MDP with unknown dynamics <span class="math inline">\(s_{t+1} \sim P( \cdot \mid s_t,a_t)\)</span>. MBPO learns an ensemble <span class="math inline">\(\{\hat f_{\psi_k}\}_{k=1}^K\)</span> that predicts the next state (often the <em>delta-state</em> <span class="math inline">\(\Delta s\)</span>). Rather than planning far ahead inside the learned model, MBPO:</p>
<ol style="list-style-type: decimal">
<li><p>Collects real transitions <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span> by interacting with the environment.</p></li>
<li><p>Fits the dynamics ensemble on <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span>.</p></li>
<li><p>Periodically generates short rollouts (e.g., horizon <span class="math inline">\(H=1\ldots5\)</span>) starting from <em>real</em> states by simulating with a <em>random</em> member of the ensemble, producing model transitions <span class="math inline">\(\mathcal{D}_{\text{model}}\)</span>.</p></li>
<li><p>Trains an off-policy actor-critic (e.g., SAC) on a <em>mixture</em> of <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span> and <span class="math inline">\(\mathcal{D}_{\text{model}}\)</span>, typically with a high fraction of model data but <em>short</em> <span class="math inline">\(H\)</span> to limit bias.</p></li>
</ol>
<p>This yields the sample-efficiency benefits of model-based learning while maintaining the robustness of model-free policy optimization.</p>
<p>The following pseudocode implements MBPO with SAC as the off-policy learner.</p>
<div class="highlightbox">
<div style="text-align: center;">
<p><strong>Model-based Policy Optimization (SAC as Off-Policy Learner)</strong></p>
</div>
<p><strong>Inputs:</strong> environment <span class="math inline">\(\mathcal{E}\)</span>, policy <span class="math inline">\(\pi_\theta(a\mid s)\)</span>, twin critics <span class="math inline">\(Q_{\phi_1},Q_{\phi_2}\)</span> with targets, temperature <span class="math inline">\(\alpha\)</span> (auto-tuned), dynamics ensemble <span class="math inline">\(\{\hat f_{\psi_k}\}_{k=1}^K\)</span>, real buffer <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span>, model buffer <span class="math inline">\(\mathcal{D}_{\text{model}}\)</span>. Rollout horizon <span class="math inline">\(H\)</span>, model ratio <span class="math inline">\(p_{\text{model}}\in[0,1]\)</span>, update counts <span class="math inline">\(G_{\text{dyn}}, G_{\text{rl}}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><strong>Warm-up &amp; data collection.</strong>
<ul>
<li>Interact with <span class="math inline">\(\mathcal{E}\)</span> using <span class="math inline">\(\pi_\theta\)</span> (or random for a short warm-up).</li>
<li>Store <span class="math inline">\((s,a,r,s&#39;)\)</span> in <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span>.</li>
</ul></li>
<li><strong>Fit dynamics.</strong> For <span class="math inline">\(G_{\text{dyn}}\)</span> steps:
<ul>
<li>Sample minibatch <span class="math inline">\(B\subset\mathcal{D}_{\text{env}}\)</span>.</li>
<li>Update each <span class="math inline">\(\psi_k\)</span> to predict <span class="math inline">\(\Delta s = s&#39;-s\)</span> (and optionally <span class="math inline">\(r\)</span>) by minimizing, e.g, mean squared error.</li>
</ul></li>
<li><strong>Short model rollouts (data generation).</strong>
<ul>
<li>Sample seed states <span class="math inline">\(S_0\)</span> from recent <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span>.</li>
<li>For each <span class="math inline">\(s\in S_0\)</span>
<ul>
<li>for <span class="math inline">\(h=1\ldots H\)</span>:
<ul>
<li>Sample <span class="math inline">\(a\sim \pi_\theta(\cdot\mid s)\)</span>.</li>
<li>Pick random ensemble member <span class="math inline">\(k\)</span>; predict <span class="math inline">\(\hat\Delta s \leftarrow \hat f_{\psi_k}(s,a)\)</span>; set <span class="math inline">\(\hat s&#39; = s + \hat\Delta s\)</span>.</li>
<li>Compute <span class="math inline">\(r\)</span> via a learned reward model or a known formula (when available).</li>
<li>Push <span class="math inline">\((s,a,r,\hat s&#39;,\texttt{done}=0)\)</span> into <span class="math inline">\(\mathcal{D}_{\text{model}}\)</span>.</li>
<li>Set <span class="math inline">\(s\leftarrow\hat s&#39;\)</span>; break if time-limit reached.</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Off-policy RL updates (SAC).</strong> For <span class="math inline">\(G_{\text{rl}}\)</span> steps:
<ul>
<li>Form a minibatch by drawing a fraction <span class="math inline">\(p_{\text{model}}\)</span> from <span class="math inline">\(\mathcal{D}_{\text{model}}\)</span> and <span class="math inline">\(1-p_{\text{model}}\)</span> from <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span>.</li>
<li><strong>Critic targets:</strong>
<span class="math display">\[
y=r+\gamma(1-d)\big[\min_j Q_{\phi_j^-}(s&#39;,a&#39;)-\alpha\log\pi_\theta(a&#39;\mid s&#39;)\big],
\]</span>
where <span class="math inline">\(a&#39;\sim\pi_\theta(\cdot\mid s&#39;)\)</span>.</li>
<li><strong>Critic update:</strong> regress <span class="math inline">\(Q_{\phi_j}\)</span> to <span class="math inline">\(y\)</span> (both heads).</li>
<li><strong>Actor update:</strong> minimize <span class="math inline">\(J_\pi = \mathbb{E}_s\!\big[\alpha\log\pi_\theta(a\mid s) - \min_j Q_{\phi_j}(s,a)\big]\)</span>, with <span class="math inline">\(a\sim\pi_\theta\)</span>.</li>
<li><strong>Temperature update (optional):</strong> adjust <span class="math inline">\(\alpha\)</span> towards target entropy.</li>
<li>Soft-update target critics.</li>
</ul></li>
<li><strong>Repeat</strong> steps 1–4 until convergence or iteration limits.</li>
</ol>
</div>
<p><strong>Notes.</strong></p>
<ul>
<li><p>Ensembles capture epistemic uncertainty; random-member rollouts implicitly regularize toward pessimism.</p></li>
<li><p>Keeping <span class="math inline">\(H\)</span> <em>short</em> (e.g., <span class="math inline">\(1\!\!-\!\!5\)</span>) is crucial to prevent model error explosion.</p></li>
<li><p>Use recent real states as rollout seeds to stay on-distribution.</p></li>
</ul>
<p>The next example applies MBPO to Inverted Pendulum.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:pendulum-mbpo" class="example"><strong>Example 3.10  (MBPO for Inverted Pendulum) </strong></span>Fig. <a href="policy-gradient.html#fig:pendulum-learning-curve-mbpo">3.23</a> plots the learning curve.</p>
<p>Fig. <a href="policy-gradient.html#fig:pendulum-policy-rollout-mbpo">3.24</a> visualizes sample rollouts of the policy.</p>
<p>Code can be found <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_mbpo.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-learning-curve-mbpo"></span>
<img src="images/Policy-Gradients/pendulum_learning_curve_mbpo.png" alt="Learning curve (MBPO)." width="60%" />
<p class="caption">
Figure 3.23: Learning curve (MBPO).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-policy-rollout-mbpo"></span>
<img src="images/Policy-Gradients/pendulum_rollout_mbpo-1.gif" alt="Policy rollout (MBPO)." width="30%" /><img src="images/Policy-Gradients/pendulum_rollout_mbpo-2.gif" alt="Policy rollout (MBPO)." width="30%" /><img src="images/Policy-Gradients/pendulum_rollout_mbpo-3.gif" alt="Policy rollout (MBPO)." width="30%" />
<p class="caption">
Figure 3.24: Policy rollout (MBPO).
</p>
</div>
</div>
</div>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-barto2012neuronlike" class="csl-entry">
Barto, Andrew G, Richard S Sutton, and Charles W Anderson. 2012. <span>“Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, no. 5: 834–46.
</div>
<div id="ref-garrigos2023handbook" class="csl-entry">
Garrigos, Guillaume, and Robert M Gower. 2023. <span>“Handbook of Convergence Theorems for (Stochastic) Gradient Methods.”</span> <em>arXiv Preprint arXiv:2301.11235</em>.
</div>
<div id="ref-haarnoja2018soft" class="csl-entry">
Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. <span>“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</span> In <em>International Conference on Machine Learning</em>, 1861–70. Pmlr.
</div>
<div id="ref-janner2019trust" class="csl-entry">
Janner, Michael, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. <span>“When to Trust Your Model: Model-Based Policy Optimization.”</span> <em>Advances in Neural Information Processing Systems</em> 32.
</div>
<div id="ref-kakade2001natural" class="csl-entry">
Kakade, Sham M. 2001. <span>“A Natural Policy Gradient.”</span> <em>Advances in Neural Information Processing Systems</em> 14.
</div>
<div id="ref-lillicrap2015continuous" class="csl-entry">
Lillicrap, Timothy P, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. <span>“Continuous Control with Deep Reinforcement Learning.”</span> <em>arXiv Preprint arXiv:1509.02971</em>.
</div>
<div id="ref-nesterov2018lectures" class="csl-entry">
Nesterov, Yurii. 2018. <em>Lectures on Convex Optimization</em>. Vol. 137. Springer.
</div>
<div id="ref-schulman2015trust" class="csl-entry">
Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. <span>“Trust Region Policy Optimization.”</span> In <em>International Conference on Machine Learning</em>, 1889–97. PMLR.
</div>
<div id="ref-schulman2015high" class="csl-entry">
Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. <span>“High-Dimensional Continuous Control Using Generalized Advantage Estimation.”</span> <em>arXiv Preprint arXiv:1506.02438</em>.
</div>
<div id="ref-schulman2017proximal" class="csl-entry">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal Policy Optimization Algorithms.”</span> <em>arXiv Preprint arXiv:1707.06347</em>.
</div>
<div id="ref-silver2014deterministic" class="csl-entry">
Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. <span>“Deterministic Policy Gradient Algorithms.”</span> In <em>International Conference on Machine Learning</em>, 387–95. Pmlr.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="value-rl.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-based-plan-optimize.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/03-policy-gradient.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
