<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Policy Gradient Methods | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-09-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="value-rl.html"/>
<link rel="next" href="appconvex.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.1.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.1.4" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.1.4</b> Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.2</b> Infinite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mdp.html"><a href="mdp.html#value-functions"><i class="fa fa-check"></i><b>1.2.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation-1"><i class="fa fa-check"></i><b>1.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.2.3" data-path="mdp.html"><a href="mdp.html#principle-of-optimality"><i class="fa fa-check"></i><b>1.2.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.2.4" data-path="mdp.html"><a href="mdp.html#policy-improvement"><i class="fa fa-check"></i><b>1.2.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="1.2.5" data-path="mdp.html"><a href="mdp.html#policy-iteration"><i class="fa fa-check"></i><b>1.2.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="1.2.6" data-path="mdp.html"><a href="mdp.html#value-iteration"><i class="fa fa-check"></i><b>1.2.6</b> Value Iteration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="value-rl.html"><a href="value-rl.html"><i class="fa fa-check"></i><b>2</b> Value-based Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="value-rl.html"><a href="value-rl.html#tabular-methods"><i class="fa fa-check"></i><b>2.1</b> Tabular Methods</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-2"><i class="fa fa-check"></i><b>2.1.1</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.1.2" data-path="value-rl.html"><a href="value-rl.html#value-rl-convergence-td"><i class="fa fa-check"></i><b>2.1.2</b> Convergence Proof of TD Learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control"><i class="fa fa-check"></i><b>2.1.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.1.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control"><i class="fa fa-check"></i><b>2.1.4</b> Off-Policy Control</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="value-rl.html"><a href="value-rl.html#function-approximation"><i class="fa fa-check"></i><b>2.2</b> Function Approximation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="value-rl.html"><a href="value-rl.html#basics-of-continuous-mdp"><i class="fa fa-check"></i><b>2.2.1</b> Basics of Continuous MDP</a></li>
<li class="chapter" data-level="2.2.2" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-3"><i class="fa fa-check"></i><b>2.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.2.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control-1"><i class="fa fa-check"></i><b>2.2.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.2.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control-1"><i class="fa fa-check"></i><b>2.2.4</b> Off-Policy Control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="policy-gradient.html"><a href="policy-gradient.html"><i class="fa fa-check"></i><b>3</b> Policy Gradient Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-optimization"><i class="fa fa-check"></i><b>3.1</b> Gradient-based Optimization</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="policy-gradient.html"><a href="policy-gradient.html#basic-setup"><i class="fa fa-check"></i><b>3.1.1</b> Basic Setup</a></li>
<li class="chapter" data-level="3.1.2" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-ascent-and-descent"><i class="fa fa-check"></i><b>3.1.2</b> Gradient Ascent and Descent</a></li>
<li class="chapter" data-level="3.1.3" data-path="policy-gradient.html"><a href="policy-gradient.html#stochastic-gradients"><i class="fa fa-check"></i><b>3.1.3</b> Stochastic Gradients</a></li>
<li class="chapter" data-level="3.1.4" data-path="policy-gradient.html"><a href="policy-gradient.html#beyond-vanilla-gradient-methods"><i class="fa fa-check"></i><b>3.1.4</b> Beyond Vanilla Gradient Methods</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="policy-gradient" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Policy Gradient Methods<a href="policy-gradient.html#policy-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="value-rl.html#value-rl">2</a>, we relaxed two key assumptions of the MDP introduced in Chapter <a href="mdp.html#mdp">1</a>:</p>
<ul>
<li><strong>Unknown dynamics</strong>: the transition function <span class="math inline">\(P\)</span> was no longer assumed to be known.<br />
</li>
<li><strong>Continuous states</strong>: the state space <span class="math inline">\(\mathcal{S}\)</span> was extended from finite to continuous.</li>
</ul>
<p>When only the dynamics are unknown but the MDP remains tabular, we introduced generalized versions of policy iteration (e.g., SARSA) and value iteration (e.g., Q-learning). These algorithms can recover near-optimal value functions with strong convergence guarantees.</p>
<p>When both the dynamics are unknown and the state space is continuous, tabular methods become infeasible. In this setting, we employed function approximation to represent value functions, and generalized SARSA and Q-learning accordingly. We also introduced stabilization techniques such as experience replay and target networks to ensure more reliable learning.</p>
<hr />
<p>In this chapter, we relax a third assumption: the action space <span class="math inline">\(\mathcal{A}\)</span> is also continuous. This setting captures many important real-world systems, such as autonomous vehicles and robots. Handling continuous actions requires a departure from the value-based methods of Chapter <a href="value-rl.html#value-rl">2</a>. The key difficulty is that even if we had access to a near-optimal action-value function <span class="math inline">\(Q(s,a)\)</span>, selecting the control action requires solving
<span class="math display">\[
\max_a Q(s,a),
\]</span>
which is often computationally expensive and can lead to suboptimal solutions.</p>
<p>To address this challenge, we introduce a new paradigm: policy gradient methods. Rather than learning value functions to derive policies indirectly, we directly optimize parameterized policies using gradient-based methods.</p>
<p>We begin this chapter by reviewing the fundamentals of gradient-based optimization, and then build upon them to develop algorithms for searching optimal policies via policy gradients.</p>
<div id="gradient-optimization" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Gradient-based Optimization<a href="policy-gradient.html#gradient-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gradient-based optimization is the workhorse behind most modern machine learning algorithms, including policy gradient methods. The central idea is to iteratively update the parameters of a model in the direction that most improves an objective function.</p>
<div id="basic-setup" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Basic Setup<a href="policy-gradient.html#basic-setup" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a differentiable objective function <span class="math inline">\(J(\theta)\)</span>, where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> represents the parameter vector. The goal is to find
<span class="math display">\[
\theta^\star \in \arg\max_\theta J(\theta).
\]</span></p>
<p>The gradient of the objective with respect to the parameters,
<span class="math display">\[
\nabla_\theta J(\theta) =
\begin{bmatrix}
\frac{\partial J}{\partial \theta_1} &amp;
\frac{\partial J}{\partial \theta_2} &amp;
\cdots &amp;
\frac{\partial J}{\partial \theta_d}
\end{bmatrix}^\top,
\]</span>
provides the local direction of steepest ascent. Gradient-based optimization uses this direction to iteratively update the parameters. Note that modern machine learning software tools such as PyTorch allow the user to conveniently query the gradient of any function <span class="math inline">\(J\)</span> defined by neural networks.</p>
</div>
<div id="gradient-ascent-and-descent" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Gradient Ascent and Descent<a href="policy-gradient.html#gradient-ascent-and-descent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The simplest method is <strong>gradient ascent</strong> (for maximization):
<span class="math display">\[
\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k),
\]</span>
where <span class="math inline">\(\alpha &gt; 0\)</span> is the learning rate.<br />
For minimization, the update rule uses <strong>gradient descent</strong>:
<span class="math display">\[
\theta_{k+1} = \theta_k - \alpha \nabla_\theta J(\theta_k).
\]</span></p>
<p>The choice of learning rate <span class="math inline">\(\alpha\)</span> is critical:</p>
<ul>
<li>Too large <span class="math inline">\(\alpha\)</span> can cause divergence.<br />
</li>
<li>Too small <span class="math inline">\(\alpha\)</span> leads to slow convergence.</li>
</ul>
<div id="convergence-guarantees" class="section level4 hasAnchor" number="3.1.2.1">
<h4><span class="header-section-number">3.1.2.1</span> Convergence Guarantees<a href="policy-gradient.html#convergence-guarantees" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For convex functions <span class="math inline">\(J(\theta)\)</span>, gradient descent (or ascent) can be shown to converge to the <strong>global optimum</strong> under appropriate conditions on the learning rate.</p>
<p>For non-convex functions—which are common in reinforcement learning—gradient methods may only find so-called <strong>first-order stationary points</strong>, i.e., points <span class="math inline">\(\theta\)</span> at which the gradient <span class="math inline">\(\nabla_\theta J(\theta) = 0\)</span>. Nevertheless, they remain effective in practice.</p>
<p><span class="red">TODO: graph different stationary points</span></p>
<p>We now formalize the convergence speed of Gradient Descent (GD) for minimizing a smooth convex function. We switch to the minimization convention and write the objective as <span class="math inline">\(f:\mathbb{R}^d \to \mathbb{R}\)</span> (to avoid sign confusions with <span class="math inline">\(J\)</span> used for maximization). We assume exact gradients <span class="math inline">\(\nabla f(\theta)\)</span> are available.</p>
<p><strong>Setup and Assumptions.</strong></p>
<ul>
<li><p>(<strong>Convexity</strong>) For all <span class="math inline">\(\theta,\vartheta\in\mathbb{R}^d\)</span>,
<span class="math display" id="eq:PG-GO-convexity">\[\begin{equation}
f(\vartheta) \;\ge\; f(\theta) + \nabla f(\theta)^\top(\vartheta-\theta).
\tag{3.1}
\end{equation}\]</span></p></li>
<li><p>(<strong><span class="math inline">\(L\)</span>-smoothness</strong>) The gradient is <span class="math inline">\(L\)</span>-Lipschitz: for all <span class="math inline">\(\theta,\vartheta\)</span>,
<span class="math display" id="eq:PG-GO-Lsmooth">\[\begin{equation}
\|\nabla f(\vartheta)-\nabla f(\theta)\| \;\le\; L\|\vartheta-\theta\|.
\tag{3.2}
\end{equation}\]</span>
Equivalently (the <strong>descent lemma</strong>), for all <span class="math inline">\(\theta,\Delta\)</span>,
<span class="math display" id="eq:PG-GO-descent-lemma">\[\begin{equation}
f(\theta+\Delta) \;\le\; f(\theta) + \nabla f(\theta)^\top \Delta + \frac{L}{2}\|\Delta\|^2.
\tag{3.3}
\end{equation}\]</span></p></li>
</ul>
<p>Consider Gradient Descent with a constant stepsize <span class="math inline">\(\alpha&gt;0\)</span>:
<span class="math display">\[
\theta_{k+1} \;=\; \theta_k \;-\; \alpha\, \nabla f(\theta_k).
\]</span></p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:gd-convex-smooth" class="theorem"><strong>Theorem 3.1  (GD on smooth convex function) </strong></span>Let <span class="math inline">\(f\)</span> be convex and <span class="math inline">\(L\)</span>-smooth with a minimizer
<span class="math display">\[
\theta^\star\in\arg\min_\theta f(\theta).
\]</span>
If <span class="math inline">\(0&lt;\alpha\le \frac{1}{L}\)</span>, then the GD iterates satisfy for all <span class="math inline">\(k\ge 1\)</span>:
<span class="math display">\[
f(\theta_k) - f(\theta^\star)
\;\le\;
\frac{\|\theta_0-\theta^\star\|^2}{2\alpha\,k}
\;\le\;
\frac{L\,\|\theta_0-\theta^\star\|^2}{2\,k}.
\]</span>
In particular, choosing <span class="math inline">\(\alpha=\frac{1}{L}\)</span> yields the canonical <span class="math inline">\(O(1/k)\)</span> convergence rate in suboptimality.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>Using the descent lemma with <span class="math inline">\(\Delta=-\alpha \nabla f(\theta_k)\)</span>,
<span class="math display">\[
f(\theta_{k+1}) \;\le\; f(\theta_k) - \alpha \|\nabla f(\theta_k)\|^2 + \frac{L\alpha^2}{2}\|\nabla f(\theta_k)\|^2
= f(\theta_k) - \alpha\Bigl(1-\frac{L\alpha}{2}\Bigr)\|\nabla f(\theta_k)\|^2.
\]</span>
For <span class="math inline">\(0&lt;\alpha\le \frac{1}{L}\)</span>, the coefficient is at least <span class="math inline">\(\frac{\alpha}{2}\)</span>, so
<span class="math display">\[
f(\theta_k) - f(\theta_{k+1}) \;\ge\; \frac{\alpha}{2}\,\|\nabla f(\theta_k)\|^2.
\]</span>
By convexity,
<span class="math display">\[
f(\theta_k)-f(\theta^\star) \;\le\; \nabla f(\theta_k)^\top(\theta_k-\theta^\star)
\;\le\; \|\nabla f(\theta_k)\|\,\|\theta_k-\theta^\star\|.
\]</span>
Also, expand the squared distance after one GD step:
<span class="math display">\[
\|\theta_{k+1}-\theta^\star\|^2
= \|\theta_k-\theta^\star\|^2 - 2\alpha\,\nabla f(\theta_k)^\top(\theta_k-\theta^\star)
+ \alpha^2\|\nabla f(\theta_k)\|^2.
\]</span>
Combining the convexity bound with Young’s inequality (or substituting
<span class="math inline">\(\nabla f(\theta_k)^\top(\theta_k-\theta^\star)\ge f(\theta_k)-f(\theta^\star)\)</span>)
and then eliminating <span class="math inline">\(\|\nabla f(\theta_k)\|^2\)</span> using the descent inequality above yields
<span class="math display">\[
\|\theta_{k+1}-\theta^\star\|^2
\;\le\; \|\theta_k-\theta^\star\|^2 - 2\alpha\bigl(f(\theta_k)-f(\theta^\star)\bigr)
+ 2\alpha\bigl(f(\theta_k)-f(\theta_{k+1})\bigr).
\]</span>
Telescoping from <span class="math inline">\(t=0\)</span> to <span class="math inline">\(k-1\)</span> gives
<span class="math display">\[
2\alpha\,\sum_{t=0}^{k-1} \bigl(f(\theta_t)-f(\theta^\star)\bigr)
\;\le\; \|\theta_0-\theta^\star\|^2.
\]</span>
By convexity of <span class="math inline">\(f\)</span> and monotonicity of GD (with <span class="math inline">\(\alpha\le 1/L\)</span>), <span class="math inline">\(f(\theta_k)\)</span> is nonincreasing, so
<span class="math display">\[
k\bigl(f(\theta_k)-f(\theta^\star)\bigr)
\;\le\; \sum_{t=0}^{k-1} \bigl(f(\theta_t)-f(\theta^\star)\bigr)
\;\le\; \frac{\|\theta_0-\theta^\star\|^2}{2\alpha},
\]</span>
which yields the stated bound. Setting <span class="math inline">\(\alpha=1/L\)</span> gives the explicit <span class="math inline">\(\frac{L\|\theta_0-\theta^\star\|^2}{2k}\)</span> rate.</p>
</div>
</div>
<p><strong>Strongly Convex Case (Linear Rate).</strong> If, in addition, <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex (<span class="math inline">\(\mu&gt;0\)</span>), i.e., for all <span class="math inline">\(\theta,\vartheta\in\mathbb{R}^d\)</span>,
<span class="math display" id="eq:PG-GO-strongly-convex">\[\begin{equation}
f(\vartheta)\;\ge\; f(\theta) + \nabla f(\theta)^\top(\vartheta-\theta) \;+\; \frac{\mu}{2}\,\|\vartheta-\theta\|^2.
\tag{3.4}
\end{equation}\]</span>
Then, GD with <span class="math inline">\(0&lt;\alpha\le \frac{1}{L}\)</span> enjoys a <strong>linear</strong> (geometric) rate:</p>
<div class="theorembox">
<div class="corollary">
<p><span id="cor:gd-strongly-convex" class="corollary"><strong>Corollary 3.1  </strong></span>If <span class="math inline">\(f\)</span> is <span class="math inline">\(L\)</span>-smooth and <span class="math inline">\(\mu\)</span>-strongly convex, then for <span class="math inline">\(\alpha=\frac{1}{L}\)</span>,
<span class="math display">\[
\|\theta_k-\theta^\star\|
\;\le\;
\Bigl(1-\frac{\mu}{L}\Bigr)^k \,\|\theta_0-\theta^\star\|,
\qquad
f(\theta_k)-f(\theta^\star)
\;\le\;
\frac{L}{2}\Bigl(1-\frac{\mu}{L}\Bigr)^{2k}\|\theta_0-\theta^\star\|^2.
\]</span></p>
</div>
</div>
<p><strong>Practical Notes.</strong></p>
<ul>
<li><p>The step size <span class="math inline">\(\alpha=\frac{1}{L}\)</span> is <strong>optimal among fixed stepsizes</strong> for the above worst-case bounds on smooth convex <span class="math inline">\(f\)</span>.</p></li>
<li><p>In practice, backtracking line search or adaptive schedules can approach similar behavior without knowing <span class="math inline">\(L\)</span>.</p></li>
<li><p>For policy gradients (which maximize <span class="math inline">\(J\)</span>), apply the results to <span class="math inline">\(f=-J\)</span> and flip the update sign (gradient ascent). The smooth/convex assumptions rarely hold globally in RL, but these results calibrate expectations about step sizes and motivate variance reduction and curvature-aware methods used later.</p></li>
</ul>
</div>
</div>
<div id="stochastic-gradients" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Stochastic Gradients<a href="policy-gradient.html#stochastic-gradients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In reinforcement learning and other large-scale machine learning problems, computing the exact gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span> is often infeasible. Instead, we use an unbiased estimator <span class="math inline">\(\hat{\nabla}_\theta J(\theta)\)</span> computed from a subset of data (or trajectories in RL). The update becomes
<span class="math display">\[
\theta_{k+1} = \theta_k + \alpha \hat{\nabla}_\theta J(\theta_k).
\]</span></p>
<p>This approach, known as <strong>stochastic gradient ascent/descent (SGD)</strong>, trades off exactness for computational efficiency. Variance in the gradient estimates plays an important role in convergence speed and stability.</p>
<div id="convergence-guarantees-1" class="section level4 hasAnchor" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> Convergence Guarantees<a href="policy-gradient.html#convergence-guarantees-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We now turn to the convergence guarantees of stochastic gradient methods, which replace exact gradients with unbiased noisy estimates. Throughout this section we consider the minimization problem <span class="math inline">\(\min_\theta f(\theta)\)</span> and assume <span class="math inline">\(\nabla f\)</span> is available only through a stochastic oracle.</p>
<p><strong>Setup and Assumptions.</strong></p>
<p>Let <span class="math inline">\(f:\mathbb{R}^d\!\to\!\mathbb{R}\)</span> be differentiable. At iterate <span class="math inline">\(\theta_k\)</span>, we observe a random vector <span class="math inline">\(g_k\)</span> such that
<span class="math display">\[
\mathbb{E}[\,g_k \mid \theta_k\,] = \nabla f(\theta_k)
\quad\text{and}\quad
\mathbb{E}\!\left[\|g_k-\nabla f(\theta_k)\|^2 \mid \theta_k\right] \le \sigma^2.
\]</span>
We will also use one of the following standard regularity conditions:</p>
<ul>
<li>(<strong>Convex + <span class="math inline">\(L\)</span>-smooth</strong>) <span class="math inline">\(f\)</span> is convex and the gradient is <span class="math inline">\(L\)</span>-Lipschitz.<br />
</li>
<li>(<strong>Strongly convex + <span class="math inline">\(L\)</span>-smooth</strong>) <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex and <span class="math inline">\(L\)</span>-smooth.</li>
</ul>
<p>We consider the SGD update
<span class="math display">\[
\theta_{k+1} \;=\; \theta_k - \alpha_k\, g_k,
\]</span>
and define the <strong>averaged iterate</strong>
<span class="math display">\[
\bar\theta_K := \frac{1}{K}\sum_{k=0}^{K-1}\theta_k.
\]</span></p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:sgd-convex-rate" class="theorem"><strong>Theorem 3.2  (SGD on smooth convex function) </strong></span>Assume <span class="math inline">\(f\)</span> is convex and <span class="math inline">\(L\)</span>-smooth. Suppose there exists <span class="math inline">\(G\!&gt;\!0\)</span> with <span class="math inline">\(\mathbb{E}\|g_k\|^2 \le G^2\)</span> for all <span class="math inline">\(k\)</span>. Choose stepsizes <span class="math inline">\(\alpha_k = \frac{\eta}{\sqrt{k+1}}\)</span> with <span class="math inline">\(\eta&gt;0\)</span>. Then for all <span class="math inline">\(K\!\ge\!1\)</span>,
<span class="math display">\[
\mathbb{E}\big[f(\bar\theta_K)\big] - f(\theta^\star)
\;\le\;
\frac{\|\theta_0-\theta^\star\|^2}{2\,\eta\,\sqrt{K}}
\;+\;
\frac{\eta\,G^2}{2\,\sqrt{K}},
\]</span>
so choosing <span class="math inline">\(\eta = \frac{\|\theta_0-\theta^\star\|}{G}\)</span> yields
<span class="math display">\[
\mathbb{E}\big[f(\bar\theta_K)\big] - f(\theta^\star)
\;\le\;
\frac{\|\theta_0-\theta^\star\|\,G}{\sqrt{K}}
\;=\; O\!\left(\frac{1}{\sqrt{K}}\right).
\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span><em>Sketch.</em> Combine (i) the smoothness-based descent inequality for one noisy step, (ii) convexity to relate function error to gradient inner products, and (iii) the unbiasedness and bounded second moment of <span class="math inline">\(g_k\)</span>, then sum over <span class="math inline">\(k\)</span> and apply Jensen’s inequality to the average <span class="math inline">\(\bar\theta_K\)</span>. The <span class="math inline">\(1/\sqrt{K}\)</span> rate follows from the harmonic growth of <span class="math inline">\(\sum_k \alpha_k\)</span> and boundedness of <span class="math inline">\(\sum_k \alpha_k^2\)</span>.</p>
</div>
</div>
<p><strong>Remarks.</strong></p>
<ul>
<li><p>The bound is on the <em>averaged</em> iterate <span class="math inline">\(\bar\theta_K\)</span> (the last iterate may be worse by constants without further assumptions).</p></li>
<li><p>Replacing the second-moment bound by a variance bound <span class="math inline">\(\sigma^2\)</span> yields the same rate with <span class="math inline">\(G^2\)</span> replaced by <span class="math inline">\(\sigma^2 + \sup_k\|\nabla f(\theta_k)\|^2\)</span>.</p></li>
</ul>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:sgd-strong-rate" class="theorem"><strong>Theorem 3.3  (SGD on smooth strongly convex function) </strong></span>Assume <span class="math inline">\(f\)</span> is <span class="math inline">\(\mu\)</span>-strongly convex and <span class="math inline">\(L\)</span>-smooth, and <span class="math inline">\(\mathbb{E}\!\left[\|g_k-\nabla f(\theta_k)\|^2 \mid \theta_k\right]\le \sigma^2\)</span>.<br />
With stepsizes <span class="math inline">\(\alpha_k = \frac{1}{\mu(k+1)}\)</span>, the SGD iterates satisfy for all <span class="math inline">\(K\!\ge\!1\)</span>,
<span class="math display">\[
\mathbb{E}\,\|\theta_K - \theta^\star\|^2
\;\le\;
\frac{C}{K}
\quad\text{and}\quad
\mathbb{E}\big[f(\theta_K)\big] - f(\theta^\star)
\;\le\;
\frac{L}{2}\,\frac{C}{K}
\;=\; O\!\left(\frac{1}{K}\right),
\]</span>
where <span class="math inline">\(C := \max\!\Big\{\|\theta_0-\theta^\star\|^2,\; \frac{\sigma^2}{\mu^2}\Big\}\)</span>.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span><em>Sketch.</em> Strong convexity gives a contraction in expected squared distance after one step; smoothness converts distance to function error. Summing the resulting supermartingale inequality and using <span class="math inline">\(\sum_k \alpha_k = \infty\)</span> and <span class="math inline">\(\sum_k \alpha_k^2 &lt; \infty\)</span> yields the stated <span class="math inline">\(1/K\)</span> decay with explicit constants.</p>
</div>
</div>
<p>With a constant stepsize, SGD converges linearly up to a neighborhood set by the gradient noise.</p>
<div class="theorembox">
<div class="corollary">
<p><span id="cor:sgd-const-steps" class="corollary"><strong>Corollary 3.2  (Noise-dominated steady state) </strong></span>Under the assumptions of Theorem <a href="policy-gradient.html#thm:sgd-strong-rate">3.3</a>, if <span class="math inline">\(0&lt;\alpha \le \frac{1}{2L}\)</span> is constant, then
<span class="math display">\[
\mathbb{E}\,\|\theta_k-\theta^\star\|^2
\;\le\;
(1-\mu\alpha)^k\,\|\theta_0-\theta^\star\|^2
\;+\;
\frac{\alpha\,\sigma^2}{\mu},
\]</span>
and consequently, by <span class="math inline">\(L\)</span>-smoothness,
<span class="math display">\[
\mathbb{E}\big[f(\theta_k)\big]-f(\theta^\star)
\;\le\;
\frac{L}{2}(1-\mu\alpha)^k\,\|\theta_0-\theta^\star\|^2
\;+\;
\frac{L\,\alpha\,\sigma^2}{2\mu}.
\]</span>
Thus the error contracts geometrically until it reaches an <span class="math inline">\(O(\alpha\,\sigma^2)\)</span> noise floor.</p>
</div>
</div>
<p><strong>Practical Takeaways for Policy Gradients.</strong></p>
<ul>
<li><p>Use <strong>diminishing stepsizes</strong> for theoretical convergence (<span class="math inline">\(\alpha_k \propto 1/\sqrt{k}\)</span> for general convex, <span class="math inline">\(\alpha_k \propto 1/k\)</span> for strongly convex surrogates).</p></li>
<li><p>With <strong>constant stepsizes</strong>, expect fast initial progress down to a variance-limited plateau; lowering variance (e.g., via baselines/advantage estimation) is as important as tuning <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
<!-- - **Averaging** (Polyak–Ruppert) typically improves last-iterate performance in the convex regime and is widely used in practice. -->
<p><span class="red">TODO: graph the different trajectories between minimizing a convex function using GD and SGD.</span></p>
</div>
</div>
<div id="beyond-vanilla-gradient-methods" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Beyond Vanilla Gradient Methods<a href="policy-gradient.html#beyond-vanilla-gradient-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Several refinements to basic gradient updates are widely used:</p>
<ul>
<li><strong>Momentum methods</strong>: incorporate past gradients to smooth updates and accelerate convergence.</li>
<li><strong>Adaptive learning rates (Adam, RMSProp, AdaGrad)</strong>: adjust the learning rate per parameter based on historical gradient magnitudes.</li>
<li><strong>Second-order methods</strong>: approximate or use curvature information (the Hessian) for more informed updates, though often impractical in high dimensions.</li>
</ul>
<!-- ## Policy Gradients

Policy gradients optimize a **parameterized stochastic policy** directly, without requiring an explicit action-value maximization step. They are applicable to both **finite** and **continuous** action spaces and are especially useful when actions are continuous or when argmax over $Q(s,a)$ is costly or ill-posed.

### Setup: Objective and Policy Models

We consider a Markov decision process (MDP) with (possibly continuous) state space $\mathcal{S}$, action space $\mathcal{A}$, unknown dynamics $P$, reward $r(s,a)$, and discount factor $\gamma\in[0,1)$. Let $\pi_\theta(a\mid s)$ be a differentiable stochastic policy with parameters $\theta\in\mathbb{R}^d$.

- **Trajectory view.** A trajectory is $\tau=(s_0,a_0,s_1,a_1,\dots,s_{T})$ with probability density/mass
  \[
  p_\theta(\tau) = \rho(s_0)\prod_{t=0}^{T-1} \pi_\theta(a_t\mid s_t)\,P(s_{t+1}\mid s_t,a_t),
  \]
  where $\rho$ is the initial state distribution and $T$ is the (random or fixed) episode length.
- **Return.** Define the (discounted) return
  \[
  R(\tau) \;=\; \sum_{t=0}^{T-1}\gamma^t r(s_t,a_t), 
  \qquad
  \text{and the return-to-go}\quad
  G_t \;=\; \sum_{t'=t}^{T-1}\gamma^{t'-t} r(s_{t'},a_{t'}).
  \]
- **Optimization objective.** The goal is to maximize the expected return
  \[
  J(\theta) \;\equiv\; \mathbb{E}_{\tau\sim p_\theta}\!\left[R(\tau)\right]
  \;=\; \mathbb{E}\!\left[\sum_{t=0}^{T-1}\gamma^t r(s_t,a_t)\right].
  \]

#### Policy models

- **Finite action spaces ($\mathcal{A}$ discrete).** A common choice is a **softmax (categorical) policy** over a score (logit) function $f_\theta(s,a)$:
  \[
  \pi_\theta(a\mid s)
  \;=\;
  \frac{\exp\{f_\theta(s,a)\}}{\sum_{a'\in\mathcal{A}}\exp\{f_\theta(s,a')\}}.
  \]
  Typically $f_\theta$ is a neural network or a linear function over features.

- **Continuous action spaces ($\mathcal{A}\subseteq\mathbb{R}^m$).** A standard choice is a **Gaussian policy**:
  \[
  \pi_\theta(a\mid s) \;=\; \mathcal{N}\!\big(a;\;\mu_\theta(s),\,\Sigma_\theta(s)\big),
  \]
  where $\mu_\theta(s)$ and (often diagonal) covariance $\Sigma_\theta(s)$ are differentiable functions (e.g., neural networks). Other choices include squashed Gaussians (e.g., $\tanh$) or Beta distributions for bounded actions.

### The Policy Gradient Lemma

The policy gradient has an elegant form that **does not** involve the derivative of the environment dynamics $P$.

::: {.theorem #policy-gradient-lemma}
**Lemma (Policy gradient / likelihood-ratio form).**  
Let $J(\theta)=\mathbb{E}_{\tau\sim p_\theta}[R(\tau)]$. Then:
\[
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\tau\sim p_\theta}\!\Big[R(\tau)\,\nabla_\theta \log p_\theta(\tau)\Big]
\;=\;
\mathbb{E}_{\tau\sim p_\theta}\!\Bigg[\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t\mid s_t)\;R(\tau)\Bigg].
\]
By **causality** (future rewards do not affect past actions), the full return can be replaced by return-to-go:
\[
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{\tau\sim p_\theta}\!\Bigg[\sum_{t=0}^{T-1}
\nabla_\theta \log \pi_\theta(a_t\mid s_t)\;G_t\Bigg].
\]
Equivalently, using value functions,
\[
\nabla_\theta J(\theta)
\;=\;
\mathbb{E}_{s\sim d_\theta,\;a\sim\pi_\theta}\!\Big[\nabla_\theta \log \pi_\theta(a\mid s)\,Q^{\pi_\theta}(s,a)\Big],
\]
where $d_\theta$ is the (discounted) on-policy state visitation distribution.
:::

::: {.proof}
*Sketch.* Differentiate under the integral sign and apply the log-derivative trick:
$\nabla_\theta p_\theta(\tau) = p_\theta(\tau)\nabla_\theta\log p_\theta(\tau)$.
The $\theta$-dependence in $p_\theta(\tau)$ is only through $\pi_\theta(a_t\mid s_t)$ (the dynamics $P$ do not depend on $\theta$), hence $\log p_\theta(\tau)=\sum_t \log \pi_\theta(a_t\mid s_t)+\text{const}$.
Causality follows by dropping reward terms $r_{t'}$ for $t'<t$ whose expectation against $\nabla\log\pi_\theta(a_t\mid s_t)$ is zero. The $Q^{\pi_\theta}$ form is obtained by conditioning on $(s_t,a_t)$ and using the law of total expectation.  
:::

#### Baselines and Variance Reduction

A key variance-reduction device is a **baseline** $b:\mathcal{S}\to\mathbb{R}$:
\[
\mathbb{E}\!\left[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,b(s_t)\right] = 0
\quad\Rightarrow\quad
\nabla_\theta J(\theta) =
\mathbb{E}\!\Big[\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\big(Q^{\pi_\theta}(s_t,a_t)-b(s_t)\big)\Big].
\]
A common choice is $b(s)=V^{\pi_\theta}(s)$, yielding the **advantage** $A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)$.

### REINFORCE: Monte Carlo Policy Gradient

REINFORCE is the canonical on-policy Monte Carlo estimator of the policy gradient (Williams, 1992). It uses sampled returns to form an unbiased gradient estimate.

**Estimator (episodic form).**
Given a batch of $N$ trajectories $\{\tau^{(i)}\}_{i=1}^N$ collected by $\pi_\theta$, define for each timestep the return-to-go
\[
G_t^{(i)} = \sum_{t'=t}^{T^{(i)}-1} \gamma^{t'-t} r\!\left(s_{t'}^{(i)},a_{t'}^{(i)}\right).
\]
An unbiased gradient estimator with a baseline $b$ is
\[
\widehat{\nabla_\theta J}
\;=\;
\frac{1}{N}\sum_{i=1}^N \sum_{t=0}^{T^{(i)}-1}
\nabla_\theta \log \pi_\theta\!\big(a_t^{(i)}\mid s_t^{(i)}\big)
\;\Big(G_t^{(i)} - b\!\big(s_t^{(i)}\big)\Big).
\]

**Algorithm (one update):**
1. **Collect data.** Roll out $\pi_\theta$ to obtain $N$ episodes $\{\tau^{(i)}\}$.
2. **Compute returns.** For each episode and timestep, compute $G_t^{(i)}$ by backward recursion.
3. **(Optional) Baseline.** Choose $b(s)$ (e.g., running average of returns per state, or a learned value function; see actor–critic later).
4. **Gradient estimate.** Form $\widehat{\nabla_\theta J}$ as above.
5. **Update.** Perform a gradient ascent step
   \[
   \theta \leftarrow \theta + \alpha\, \widehat{\nabla_\theta J}.
   \]

**Practical details.**
- **Normalization.** Normalize advantages $G_t - b(s_t)$ within a batch to stabilize updates.
- **Entropy regularization.** Add $\beta\,\mathbb{E}_{s\sim d_\theta}\!\big[\mathcal{H}(\pi_\theta(\cdot\mid s))\big]$ to $J(\theta)$ for exploration; the gradient adds $+\beta\,\nabla_\theta \mathbb{E}[\log \pi_\theta]$.
- **Continuous actions.** For Gaussian policies,
  \[
  \nabla_\theta \log \pi_\theta(a\mid s)
  = \nabla_\theta\!\left[-\tfrac{1}{2}(a-\mu_\theta(s))^\top \Sigma_\theta(s)^{-1}(a-\mu_\theta(s)) - \tfrac{1}{2}\log\det\Sigma_\theta(s) \right],
  \]
  which is tractable via automatic differentiation.
- **Sample efficiency.** REINFORCE is unbiased but high variance; later sections (actor–critic, GAE, trust region methods) reduce variance and improve stability.

**Connection to Gradient-Based Optimization.**  
REINFORCE performs **stochastic gradient ascent** on $J(\theta)$ using the score-function (likelihood-ratio) estimator. The baseline preserves unbiasedness while reducing variance, aligning with the SGD convergence insights discussed in Section \@ref(sgd-convergence). -->

</div>
</div>
</div>



</div>
            </section>

          </div>
        </div>
      </div>
<a href="value-rl.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appconvex.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/03-policy-gradient.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
