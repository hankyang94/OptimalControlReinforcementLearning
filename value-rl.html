<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Value-based Reinforcement Learning | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Value-based Reinforcement Learning | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Value-based Reinforcement Learning | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-09-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mdp.html"/>
<link rel="next" href="policy-gradient.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.1.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.1.4" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.1.4</b> Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.2</b> Infinite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mdp.html"><a href="mdp.html#value-functions"><i class="fa fa-check"></i><b>1.2.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation-1"><i class="fa fa-check"></i><b>1.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.2.3" data-path="mdp.html"><a href="mdp.html#principle-of-optimality"><i class="fa fa-check"></i><b>1.2.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.2.4" data-path="mdp.html"><a href="mdp.html#policy-improvement"><i class="fa fa-check"></i><b>1.2.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="1.2.5" data-path="mdp.html"><a href="mdp.html#policy-iteration"><i class="fa fa-check"></i><b>1.2.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="1.2.6" data-path="mdp.html"><a href="mdp.html#value-iteration"><i class="fa fa-check"></i><b>1.2.6</b> Value Iteration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="value-rl.html"><a href="value-rl.html"><i class="fa fa-check"></i><b>2</b> Value-based Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="value-rl.html"><a href="value-rl.html#tabular-methods"><i class="fa fa-check"></i><b>2.1</b> Tabular Methods</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-2"><i class="fa fa-check"></i><b>2.1.1</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.1.2" data-path="value-rl.html"><a href="value-rl.html#convergence-proof-of-td-learning"><i class="fa fa-check"></i><b>2.1.2</b> Convergence Proof of TD Learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control"><i class="fa fa-check"></i><b>2.1.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.1.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control"><i class="fa fa-check"></i><b>2.1.4</b> Off-Policy Control</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="value-rl.html"><a href="value-rl.html#function-approximation"><i class="fa fa-check"></i><b>2.2</b> Function Approximation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="value-rl.html"><a href="value-rl.html#basics-of-continuous-mdp"><i class="fa fa-check"></i><b>2.2.1</b> Basics of Continuous MDP</a></li>
<li class="chapter" data-level="2.2.2" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-3"><i class="fa fa-check"></i><b>2.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.2.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control-1"><i class="fa fa-check"></i><b>2.2.3</b> On-Policy Control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="policy-gradient.html"><a href="policy-gradient.html"><i class="fa fa-check"></i><b>3</b> Policy Gradients</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="value-rl" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Value-based Reinforcement Learning<a href="value-rl.html#value-rl" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="mdp.html#mdp">1</a>, we introduced algorithms for policy evaluation, policy improvement, and computing optimal policies in the tabular setting when the model is known. These dynamic-programming methods are grounded in Bellman consistency and optimality and come with strong convergence guarantees.</p>
<p>A key limitation of the methods in Chapter <a href="mdp.html#mdp">1</a> is that they require the transition dynamics <span class="math inline">\(P(s&#39; \mid s, a)\)</span> to be known. While in some applications modeling the dynamics is feasible (e.g., the inverted pendulum), in many others it is costly or impractical to obtain an accurate model of the environment (e.g., a humanoid robot interacting with everyday objects).</p>
<p>This motivates relaxing the known-dynamics assumption and asking whether we can design algorithms that learn purely from interaction—i.e., by collecting data through environment interaction. This brings us to <strong>model-free reinforcement learning</strong>.</p>
<p>In this chapter we focus on <strong>value-based</strong> RL methods. The central idea is to learn the value functions—<span class="math inline">\(V(s)\)</span> and <span class="math inline">\(Q(s,a)\)</span>—from interaction with the environment and then leverage these estimates to derive (approximately) optimal policies. We begin with tabular methods and then move to function-approximation approaches (e.g., neural networks) for problems where a tabular representation is intractable.</p>
<div id="tabular-methods" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Tabular Methods<a href="value-rl.html#tabular-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider an infinite-horizon Markov decision process (MDP)<br />
<span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma),
\]</span>
with a discount factor <span class="math inline">\(\gamma \in [0,1)\)</span>. We focus on the <em>tabular setting</em> where both the state space <span class="math inline">\(\mathcal{S}\)</span> and the action space <span class="math inline">\(\mathcal{A}\)</span> are finite, with cardinalities <span class="math inline">\(|\mathcal{S}|\)</span> and <span class="math inline">\(|\mathcal{A}|\)</span>, respectively.</p>
<p>A policy is a stationary stochastic mapping<br />
<span class="math display">\[
\pi: \mathcal{S} \to \Delta(\mathcal{A}),
\]</span>
where <span class="math inline">\(\pi(a \mid s)\)</span> denotes the probability of selecting action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>.</p>
<p>Unlike in Chapter <a href="mdp.html#mdp">1</a>, here we do not assume knowledge of the transition dynamics <span class="math inline">\(P\)</span> or the reward function <span class="math inline">\(R\)</span> (other than that <span class="math inline">\(R\)</span> is deterministic). Instead, we assume we can interact with the environment and obtain <em>trajectories</em> of the form<br />
<span class="math display">\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots),
\]</span>
by following a policy <span class="math inline">\(\pi\)</span>.</p>
<div id="policy-evaluation-2" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Policy Evaluation<a href="value-rl.html#policy-evaluation-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We first consider the problem of estimating the value function of a given policy <span class="math inline">\(\pi\)</span>. Recall the definition of the state-value function associated with <span class="math inline">\(\pi\)</span> is:
<span class="math display" id="eq:InfiniteHorizonStateValueRestate">\[\begin{equation}
V^{\pi}(s) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \mid s_0 = s \right],
\tag{2.1}
\end{equation}\]</span>
where the expectation is taken over the randomness of both the policy <span class="math inline">\(\pi\)</span> and the transition dynamics <span class="math inline">\(P\)</span>.</p>
<div id="monte-carlo-estimation" class="section level4 hasAnchor" number="2.1.1.1">
<h4><span class="header-section-number">2.1.1.1</span> Monte Carlo Estimation<a href="value-rl.html#monte-carlo-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The basic idea of Monte Carlo (MC) estimation is to approximate the value function <span class="math inline">\(V^\pi\)</span> by averaging <em>empirical returns</em> observed from sampled trajectories generated under policy <span class="math inline">\(\pi\)</span>. Since the return is defined as the discounted sum of future rewards, MC methods replace the expectation in the definition of <span class="math inline">\(V^\pi\)</span> with an average over sampled trajectories.</p>
<p><strong>Episodic Assumption.</strong> To make Monte Carlo methods well-defined, we restrict attention to the <em>episodic setup</em>, where each trajectory terminates upon reaching a terminal state (and the rewards thereafter are always zero). This ensures that the return is finite and can be computed exactly for each trajectory. Concretely, if an episode terminates at time <span class="math inline">\(T\)</span>, the return starting from time <span class="math inline">\(t\)</span> is
<span class="math display" id="eq:return-MC">\[\begin{equation}
g_t = \sum_{k=0}^{T-t-1} \gamma^k r_{t+k} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^{T-t-1} r_{T-1}.
\tag{2.2}
\end{equation}\]</span></p>
<p><strong>Algorithmic Form.</strong> Let <span class="math inline">\(\mathcal{D}(s)\)</span> denote the set of all time indices at which state <span class="math inline">\(s\)</span> is visited across sampled episodes. Then the Monte Carlo estimate of the value function is
<span class="math display" id="eq:StateValueMCEstimate">\[\begin{equation}
\hat{V}(s) = \frac{1}{|\mathcal{D}(s)|} \sum_{t \in \mathcal{D}(s)} g_t.
\tag{2.3}
\end{equation}\]</span></p>
<p>There are two common variants:</p>
<ul>
<li><strong>First-visit MC:</strong> use only the first occurrence of <span class="math inline">\(s\)</span> in each episode.<br />
</li>
<li><strong>Every-visit MC:</strong> use all occurrences of <span class="math inline">\(s\)</span> within an episode.</li>
</ul>
<p>Both variants converge to the same value function in the limit of infinitely many episodes.</p>
<p><strong>Incremental Implementation.</strong> Monte Carlo can be written as an incremental stochastic-approximation update that uses the return <span class="math inline">\(g_t\)</span> as the <em>target</em> and a <em>diminishing step size</em>. Let <span class="math inline">\(N(s)\)</span> be the number of (first- or every-) visits to state <span class="math inline">\(s\)</span> that have been used to update <span class="math inline">\(\hat V(s)\)</span> so far, and let <span class="math inline">\(g_t\)</span> be the return computed at a particular visit time <span class="math inline">\(t\in\mathcal{D}(s)\)</span>. Then the MC update is
<span class="math display" id="eq:mc-incremental">\[\begin{equation}
\hat V(s) \;\leftarrow\; \hat V(s) + \alpha_{N(s)}\,\big( g_t - \hat V(s) \big),
\qquad \alpha_{N(s)} &gt; 0 \text{ diminishing.}
\tag{2.4}
\end{equation}\]</span>
A canonical choice is the <em>sample-average</em> step size <span class="math inline">\(\alpha_{N(s)} = 1/N(s)\)</span>, which yields the recurrence
<span class="math display">\[\begin{align}
\hat V_{N}(s) = \hat V_{N-1}(s) + \tfrac{1}{N}\big(g_t - \hat V_{N-1}(s)\big)
&amp; = \Big(1-\tfrac{1}{N}\Big)\hat V_{N-1}(s) + \tfrac{1}{N}\, g_t \\
&amp; = \frac{N-1}{N} \frac{1}{N-1} \sum_{i=1}^{N-1} g_{t,i} + \frac{1}{N} g_t \\
&amp; = \frac{1}{N} \sum_{i=1}^N g_{t,i}
\end{align}\]</span>
so that <span class="math inline">\(\hat V_{N}(s)\)</span> equals the average of the <span class="math inline">\(N\)</span> observed returns for <span class="math inline">\(s\)</span> (i.e., Eq. <a href="value-rl.html#eq:StateValueMCEstimate">(2.3)</a>). In the above equation, I have used <span class="math inline">\(g_{t,i}\)</span> to denote the <span class="math inline">\(i\)</span>-th return before <span class="math inline">\(g_t\)</span> was collected (and <span class="math inline">\(g_t = g_{t,N}\)</span>). More generally, any diminishing schedule satisfying
<span class="math display">\[
\sum_{n=1}^\infty \alpha_n = \infty, \qquad \sum_{n=1}^\infty \alpha_n^2 &lt; \infty
\]</span>
(e.g., <span class="math inline">\(\alpha_n = c/(n+t_0)^p\)</span> with <span class="math inline">\(1/2 &lt; p \le 1\)</span>) also ensures consistency in the tabular setting. In first-visit MC, <span class="math inline">\(N(s)\)</span> increases by one per episode at most; in every-visit MC, <span class="math inline">\(N(s)\)</span> increases at each occurrence of <span class="math inline">\(s\)</span> within an episode.</p>
<p><strong>Theoretical Guarantees.</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Unbiasedness:</strong> For any state <span class="math inline">\(s\)</span>, the return <span class="math inline">\(g_t\)</span> is an unbiased sample of <span class="math inline">\(V^\pi(s)\)</span>.<br />
<span class="math display">\[
\mathbb{E}[g_t \mid s_t = s] = V^\pi(s).
\]</span></li>
<li><strong>Consistency:</strong> By the law of large numbers, as the number of episodes grows,
<span class="math display">\[
\hat{V}(s) \xrightarrow{\text{a.s.}} V^\pi(s).
\]</span></li>
<li><strong>Asymptotic Normality:</strong> The MC estimator converges at rate <span class="math inline">\(O(1/\sqrt{N})\)</span>, where <span class="math inline">\(N\)</span> is the number of episodes used for the estimation.</li>
</ol>
<p><strong>Limitations.</strong> Despite its conceptual simplicity, MC estimation suffers from several drawbacks:</p>
<ul>
<li><p>It requires <em>episodes to terminate</em>, making it unsuitable for continuing tasks without artificial truncation.</p></li>
<li><p>It can only update value estimates <em>after an episode ends</em>, which is data-inefficient.</p></li>
<li><p>While unbiased, MC estimates often have <em>high variance</em>, leading to slow convergence.</p></li>
</ul>
<p>These limitations motivate the study of <em>Temporal-Difference (TD) learning</em>, which updates value estimates online and can handle continuing tasks.</p>
</div>
<div id="temporal-difference-learning" class="section level4 hasAnchor" number="2.1.1.2">
<h4><span class="header-section-number">2.1.1.2</span> Temporal-Difference Learning<a href="value-rl.html#temporal-difference-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>While Monte Carlo methods estimate value functions by averaging full returns from complete episodes, Temporal-Difference (TD) learning provides an alternative approach that updates value estimates <em>incrementally</em> after each step of interaction with the environment. The key idea is to combine the sampling of Monte Carlo with the <em>bootstrapping</em> of dynamic programming.</p>
<p><strong>High-Level Intuition.</strong> TD learning avoids waiting until the end of an episode by using the Bellman consistency equation as a basis for updates. Recall that for any policy <span class="math inline">\(\pi\)</span>, the Bellman consistency equation reads:
<span class="math display" id="eq:InfiniteHorizonBellmanConsistencyRestate">\[\begin{equation}
V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot \mid s)} \left[ R(s,a) + \gamma \mathbb{E}_{s&#39; \sim P(s&#39; \mid s, a)} V(s&#39;)  \right].
\tag{2.5}
\end{equation}\]</span>
At a high level, TD learning turns the expectation in Bellman equation into sampling.
At each step, it updates the current estimate of the value function toward a <em>one-step bootstrap target</em>: the immediate reward plus the discounted value of the next state. This makes TD methods more data-efficient and applicable to continuing tasks without terminal states.</p>
<p><strong>Algorithmic Form.</strong> Suppose the agent is in state <span class="math inline">\(s_t\)</span>, takes action <span class="math inline">\(a_t \sim \pi(\cdot \mid s_t)\)</span>, receives reward <span class="math inline">\(r_t\)</span>, and transitions to <span class="math inline">\(s_{t+1}\)</span>. The TD(0) update rule is
<span class="math display" id="eq:TDZeroUpdate">\[\begin{equation}
\hat{V}(s_t) \;\leftarrow\; \hat{V}(s_t) + \alpha \big[ r_t + \gamma \hat{V}(s_{t+1}) - \hat{V}(s_t) \big],
\tag{2.6}
\end{equation}\]</span>
where <span class="math inline">\(\alpha \in (0,1]\)</span> is the learning rate.</p>
<p>The term inside the brackets,
<span class="math display" id="eq:TDError">\[\begin{equation}
\delta_t = r_t + \gamma \hat{V}(s_{t+1}) - \hat{V}(s_t),
\tag{2.7}
\end{equation}\]</span>
is called the TD error. It measures the discrepancy between the current value estimate and the bootstrap target. The algorithm updates <span class="math inline">\(\hat{V}(s_t)\)</span> in the direction of reducing this error.</p>
<p><strong>Theoretical Guarantees.</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Convergence in the Tabular Case:</strong> If each state is visited infinitely often and the learning rate sequence satisfies
<span class="math display">\[
\sum_t \alpha_t = \infty, \; \sum_t \alpha_t^2 &lt; \infty
\]</span>
then TD(0) converges almost surely to the true value function <span class="math inline">\(V^\pi\)</span>. For example, choosing <span class="math inline">\(\alpha_t = 1/(t+1)\)</span> satisfies this condition.</p></li>
<li><p><strong>Bias–Variance Tradeoff:</strong></p>
<ul>
<li><p>The TD target uses the current estimate <span class="math inline">\(\hat{V}(s_{t+1})\)</span> rather than the true value, which introduces <em>bias</em>.</p></li>
<li><p>However, it has significantly <em>lower variance</em> than Monte Carlo estimates, often leading to faster convergence in practice.</p></li>
</ul>
<p>To see this, note that for TD(0), the target is a one-step bootstrap:
<span class="math display">\[
Y_t = r_t + \gamma \hat{V}(s_{t+1}).
\]</span>
This replaces the true value <span class="math inline">\(V^\pi(s_{t+1})\)</span> with the <em>current estimate</em> <span class="math inline">\(\hat{V}(s_{t+1})\)</span>. As a result, <span class="math inline">\(Y_t\)</span> is <em>biased</em> relative to the true return. However, since it depends only on the immediate reward and the next state, the variance of <span class="math inline">\(Y_t\)</span> is <em>much lower</em> than that of the Monte Carlo target.</p></li>
</ol>
<p><strong>Limitations.</strong></p>
<ul>
<li><p>TD(0) relies on bootstrapping, which introduces bias relative to Monte Carlo methods.</p></li>
<li><p>Convergence can be slow if the learning rate is not chosen carefully.</p></li>
</ul>
<p>In summary, Temporal-Difference learning addresses the major limitations of Monte Carlo estimation: it works in <em>continuing tasks</em>, updates <em>online</em> at each step, and is generally more <em>sample-efficient</em>. However, it trades away unbiasedness for bias–variance efficiency, motivating further extensions such as multi-step TD and TD(<span class="math inline">\(\lambda\)</span>).</p>
</div>
<div id="multi-step-td-learning" class="section level4 hasAnchor" number="2.1.1.3">
<h4><span class="header-section-number">2.1.1.3</span> Multi-Step TD Learning<a href="value-rl.html#multi-step-td-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Monte Carlo methods use the <em>full return</em> <span class="math inline">\(g_t\)</span>, while TD(0) uses a <em>one-step bootstrap</em>. Multi-step TD learning generalizes these two extremes by using <span class="math inline">\(n\)</span>-step returns as targets. In this way, multi-step TD interpolates between Monte Carlo and TD(0).</p>
<p><strong>High-Level Intuition.</strong> The motivation is to balance the high variance of Monte Carlo with the bias of TD(0). Instead of waiting for a full return (MC) or using only one step of bootstrapping (TD(0)), multi-step TD uses partial returns spanning <span class="math inline">\(n\)</span> steps of real rewards, followed by a bootstrap. This provides a flexible tradeoff between bias and variance.</p>
<p><strong>Algorithmic Form.</strong> The <span class="math inline">\(n\)</span>-step return starting from time <span class="math inline">\(t\)</span> is defined as
<span class="math display" id="eq:nStepReturn">\[\begin{equation}
g_t^{(n)} = r_t + \gamma r_{t+1} + \dots + \gamma^{n-1} r_{t+n-1} + \gamma^n \hat{V}(s_{t+n}).
\tag{2.8}
\end{equation}\]</span></p>
<p>The <span class="math inline">\(n\)</span>-step TD update is
<span class="math display" id="eq:nStepTDUpdate">\[\begin{equation}
\hat{V}(s_t) \;\leftarrow\; \hat{V}(s_t) + \alpha \big[ g_t^{(n)} - \hat{V}(s_t) \big],
\tag{2.9}
\end{equation}\]</span>
where <span class="math inline">\(g_t^{(n)}\)</span> replaces the one-step target in TD(0) <a href="value-rl.html#eq:TDZeroUpdate">(2.6)</a>.</p>
<ul>
<li><p>For <span class="math inline">\(n=1\)</span>: the method reduces to TD(0).</p></li>
<li><p>For <span class="math inline">\(n=T-t\)</span> (the full episode length): the method reduces to Monte Carlo.</p></li>
</ul>
<p><strong>Theoretical Guarantees.</strong></p>
<ol style="list-style-type: decimal">
<li><p><strong>Convergence in the Tabular Case:</strong> With suitable learning rates and sufficient exploration, <span class="math inline">\(n\)</span>-step TD converges to <span class="math inline">\(V^\pi\)</span>.</p></li>
<li><p><strong>Bias–Variance Tradeoff:</strong></p>
<ul>
<li><p>Larger <span class="math inline">\(n\)</span>: lower bias, higher variance (closer to Monte Carlo).</p></li>
<li><p>Smaller <span class="math inline">\(n\)</span>: higher bias, lower variance (closer to TD(0)).</p></li>
<li><p>Intermediate <span class="math inline">\(n\)</span> provides a balance that often yields faster learning in practice.</p></li>
</ul></li>
</ol>
<p><strong>Limitations.</strong></p>
<ul>
<li><p>Choosing the right <span class="math inline">\(n\)</span> is problem-dependent: too small and bias dominates; too large and variance grows.</p></li>
<li><p>Requires storing <span class="math inline">\(n\)</span>-step reward sequences before updating, which can increase memory and computation.</p></li>
</ul>
<p>In summary, multi-step TD unifies Monte Carlo and TD(0) by introducing <span class="math inline">\(n\)</span>-step returns. It allows practitioners to <em>tune the bias–variance tradeoff</em> by selecting <span class="math inline">\(n\)</span>. Later, we will see how TD(<span class="math inline">\(\lambda\)</span>) averages over all <span class="math inline">\(n\)</span>-step returns in a principled way, further smoothing this tradeoff.</p>
</div>
<div id="eligibility-traces-and-tdlambda" class="section level4 hasAnchor" number="2.1.1.4">
<h4><span class="header-section-number">2.1.1.4</span> Eligibility Traces and TD(<span class="math inline">\(\lambda\)</span>)<a href="value-rl.html#eligibility-traces-and-tdlambda" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>So far, we have seen that Monte Carlo methods use <em>full returns</em> <span class="math inline">\(g_t\)</span>, while TD(0) uses a <em>one-step bootstrap</em>. Multi-step TD methods generalize between these two extremes by using <span class="math inline">\(n\)</span>-step returns. However, a natural question arises: <em>can we combine information from all possible <span class="math inline">\(n\)</span>-step returns in a principled way?</em></p>
<p>This motivates TD(<span class="math inline">\(\lambda\)</span>), which blends multi-step TD methods into a single algorithm using <em>eligibility traces</em>.</p>
<p><strong>High-Level Intuition.</strong> TD(<span class="math inline">\(\lambda\)</span>) introduces a parameter <span class="math inline">\(\lambda \in [0,1]\)</span> that controls the weighting of <span class="math inline">\(n\)</span>-step returns:</p>
<ul>
<li><p><span class="math inline">\(\lambda = 0\)</span>: reduces to TD(0), relying only on one-step bootstrapping.</p></li>
<li><p><span class="math inline">\(\lambda = 1\)</span>: reduces to Monte Carlo, relying on full returns.</p></li>
<li><p><span class="math inline">\(0 &lt; \lambda &lt; 1\)</span>: interpolates smoothly between these two extremes by averaging all <span class="math inline">\(n\)</span>-step returns with exponentially decaying weights.</p></li>
</ul>
<p>Formally, the <span class="math inline">\(\lambda\)</span>-return is
<span class="math display" id="eq:LambdaReturn">\[\begin{equation}
g_t^{(\lambda)} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} g_t^{(n)},
\tag{2.10}
\end{equation}\]</span>
where <span class="math inline">\(g_t^{(n)}\)</span> is the <span class="math inline">\(n\)</span>-step return defined in <a href="value-rl.html#eq:nStepReturn">(2.8)</a>.</p>
<div class="exercisebox">
<div class="remark">
<p><span id="unlabeled-div-9" class="remark"><em>Remark</em>. </span>To make the <span class="math inline">\(\lambda\)</span>-return well defined, we consider two cases.</p>
<p><strong>Episodic Case: Well-posed.</strong> If an episode terminates at time <span class="math inline">\(T\)</span>, let <span class="math inline">\(N=T-t\)</span> be the remaining steps. Then
<span class="math display" id="eq:lambda-return-episodic">\[\begin{equation}
\begin{split}
g_t^{(\lambda)}
&amp; =
(1-\lambda)\sum_{n=1}^{N-1}\lambda^{\,n-1} \, g_t^{(n)}
\;+\;
\lambda^{\,N-1}\, g_t^{(N)}, \\
&amp; = (1-\lambda)\sum_{n=1}^{N}\lambda^{\,n-1} \, g_t^{(n)}
\;+\;
\lambda^{N}\, g_t^{(N)},
\end{split}
\tag{2.11}
\end{equation}\]</span>
where <span class="math inline">\(g_t^{(n)}\)</span> is the <span class="math inline">\(n\)</span>-step return (Eq. <a href="value-rl.html#eq:nStepReturn">(2.8)</a>) and <span class="math inline">\(g_t^{(N)}\)</span> is the <em>full</em> Monte Carlo return (Eq. <a href="value-rl.html#eq:return-MC">(2.2)</a>).</p>
<p>This expression is well-defined for all <span class="math inline">\(\lambda\in[0,1]\)</span>. Note that the weights form a convex combination:
<span class="math display">\[
(1-\lambda)\sum_{n=1}^{N-1}\lambda^{n-1} + \lambda^{N-1}
= 1-\lambda^{N-1}+\lambda^{N-1} = 1.
\]</span></p>
<p><strong>Continuing Case: Limit.</strong> Taking <span class="math inline">\(\lambda\uparrow 1\)</span> in <a href="value-rl.html#eq:lambda-return-episodic">(2.11)</a> gives
<span class="math display">\[
\lim_{\lambda\uparrow 1} g_t^{(\lambda)}
= g_t^{(N)} = g_t,
\]</span>
so the <span class="math inline">\(\lambda\)</span>-return <em>reduces to the Monte Carlo return</em> at <span class="math inline">\(\lambda=1\)</span>. For continuing tasks (no terminal <span class="math inline">\(T\)</span>), <span class="math inline">\(\lambda=1\)</span> is conventionally defined by this same limiting argument, yielding the infinite-horizon discounted return when <span class="math inline">\(\gamma&lt;1\)</span>.</p>
</div>
</div>
<p><strong>Eligibility Traces.</strong> Naively computing <span class="math inline">\(g_t^{(\lambda)}\)</span> would require storing and combining infinitely many <span class="math inline">\(n\)</span>-step returns, which is impractical. Instead, TD(<span class="math inline">\(\lambda\)</span>) uses eligibility traces to implement this efficiently online.</p>
<p>An eligibility trace is a temporary record that tracks how much each state is “eligible” for updates based on how recently and frequently it has been visited. Specifically, for each state <span class="math inline">\(s\)</span>, we maintain a trace <span class="math inline">\(z_t(s)\)</span> that evolves as
<span class="math display" id="eq:EligibilityTrace">\[\begin{equation}
z_t(s) = \gamma \lambda z_{t-1}(s) + \mathbf{1}\{s_t = s\},
\tag{2.12}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{1}\{s_t = s\}\)</span> is an indicator that equals 1 if state <span class="math inline">\(s\)</span> is visited at time <span class="math inline">\(t\)</span>, and 0 otherwise.</p>
<p><strong>TD(<span class="math inline">\(\lambda\)</span>) Update Rule.</strong> At each time step <span class="math inline">\(t\)</span>, we compute the TD error
<span class="math display">\[
\delta_t = r_t + \gamma \hat{V}(s_{t+1}) - \hat{V}(s_t),
\]</span>
as in <a href="value-rl.html#eq:TDError">(2.7)</a>. Then, for each state <span class="math inline">\(s\)</span>, we update
<span class="math display" id="eq:TDLambdaUpdate">\[\begin{equation}
\hat{V}(s) \;\leftarrow\; \hat{V}(s) + \alpha \, \delta_t \, z_t(s).
\tag{2.13}
\end{equation}\]</span></p>
<p>Thus, all states with nonzero eligibility traces are updated simultaneously, with the magnitude of the update determined by both the TD error and the eligibility trace. See Proposition <a href="value-rl.html#prp:ForwardBackwardEquivalence">2.1</a> below for a justification.</p>
<p><strong>Theoretical Guarantees.</strong></p>
<ol style="list-style-type: decimal">
<li><p>In the tabular case, TD(<span class="math inline">\(\lambda\)</span>) converges almost surely to the true value function <span class="math inline">\(V^\pi\)</span> under the usual stochastic approximation conditions (sufficient exploration, decaying step sizes).</p></li>
<li><p>The parameter <span class="math inline">\(\lambda\)</span> directly controls the bias–variance tradeoff:</p>
<ul>
<li><p>Smaller <span class="math inline">\(\lambda\)</span>: more bootstrapping, more bias but lower variance.</p></li>
<li><p>Larger <span class="math inline">\(\lambda\)</span>: less bootstrapping, less bias but higher variance.</p></li>
</ul></li>
<li><p>TD(<span class="math inline">\(\lambda\)</span>) can be shown to converge to the fixed point of the <span class="math inline">\(\lambda\)</span>-operator, which is itself a contraction mapping.</p></li>
</ol>
<p>In summary, eligibility traces provide an elegant mechanism to combine the advantages of Monte Carlo and TD learning. TD(<span class="math inline">\(\lambda\)</span>) introduces a spectrum of algorithms: at one end TD(0), at the other Monte Carlo, and in between a family of methods balancing bias and variance. In practice, intermediate values such as <span class="math inline">\(\lambda \approx 0.9\)</span> often work well.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:ForwardBackwardEquivalence" class="proposition"><strong>Proposition 2.1  (Forward–Backward Equivalence) </strong></span>Consider one episode <span class="math inline">\(s_0,a_0,r_0,\ldots,s_T\)</span> with <span class="math inline">\(\hat V(s_T)=0\)</span>. Let the <strong>forward view</strong> apply updates at the end of the episode:
<span class="math display">\[
\hat V(s_t) \leftarrow \hat V(s_t) + \alpha \big[g_t^{(\lambda)}-\hat V(s_t)\big],
\quad t=0,\ldots,T-1,
\]</span>
where <span class="math inline">\(g_t^{(\lambda)}\)</span> is the <span class="math inline">\(\lambda\)</span>-return in <a href="value-rl.html#eq:LambdaReturn">(2.10)</a> with the <span class="math inline">\(n\)</span>-step returns <span class="math inline">\(g_t^{(n)}\)</span> from <a href="value-rl.html#eq:nStepReturn">(2.8)</a>, and where <span class="math inline">\(\hat V\)</span> is kept fixed while computing all <span class="math inline">\(g_t^{(\lambda)}\)</span>.</p>
<p>Let the <strong>backward view</strong> run through the episode once, using the TD error <span class="math inline">\(\delta_t\)</span> from <a href="value-rl.html#eq:TDError">(2.7)</a> and eligibility traces <span class="math inline">\(z_t(s)\)</span> from <a href="value-rl.html#eq:EligibilityTrace">(2.12)</a>, and then apply the cumulative update
<span class="math display">\[
\Delta_{\text{back}} \hat V(s) \;=\; \alpha \sum_{t=0}^{T-1} \delta_t\, z_t(s).
\]</span></p>
<p>Then, for every state <span class="math inline">\(s\)</span>,
<span class="math display">\[
\Delta_{\text{back}} \hat V(s)
\;=\;
\alpha \sum_{t:\, s_t=s}\big[g_t^{(\lambda)}-\hat V(s_t)\big],
\]</span>
i.e., the net parameter change produced by <a href="value-rl.html#eq:TDLambdaUpdate">(2.13)</a> equals that of the <span class="math inline">\(\lambda\)</span>-return updates.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span>Fix a state <span class="math inline">\(s\)</span>. Using <a href="value-rl.html#eq:EligibilityTrace">(2.12)</a>,
<span class="math display">\[
z_t(s)=\sum_{k=0}^{t}(\gamma\lambda)^{\,t-k}\,\mathbf{1}\{s_k=s\}.
\]</span>
Hence
<span class="math display">\[
\sum_{t=0}^{T-1}\delta_t z_t(s)
=\sum_{t=0}^{T-1}\delta_t \sum_{k=0}^{t}(\gamma\lambda)^{\,t-k}\mathbf{1}\{s_k=s\}
=\sum_{k:\,s_k=s}\; \sum_{t=k}^{T-1} (\gamma\lambda)^{\,t-k}\delta_t .
\tag{1}
\]</span></p>
<p>Write <span class="math inline">\(\delta_t=r_t+\gamma\hat V(s_{t+1})-\hat V(s_t)\)</span> and split the inner sum:
<span class="math display">\[
\sum_{t=k}^{T-1} (\gamma\lambda)^{t-k}\delta_t
= \underbrace{\sum_{t=k}^{T-1} \gamma^{t-k}\lambda^{t-k} r_t}_{\text{(A)}}
+ \underbrace{\sum_{t=k}^{T-1}\gamma^{t-k}\lambda^{t-k}(\gamma\hat V(s_{t+1})-\hat V(s_t))}_{\text{(B)}}.
\]</span></p>
<p>Term (B) telescopes. Shifting index in the first part of (B),
<span class="math display">\[
\sum_{t=k}^{T-1}\gamma^{t-k}\lambda^{t-k}\gamma \hat V(s_{t+1})
= \sum_{t=k+1}^{T}\gamma^{t-k}\lambda^{t-1-k}\hat V(s_t).
\]</span>
Therefore
<span class="math display">\[
\text{(B)}=
-\hat V(s_k)
+ \sum_{t=k+1}^{T-1}\gamma^{t-k}\lambda^{t-1-k}(1-\lambda)\hat V(s_t)
+ \underbrace{\gamma^{T-k}\lambda^{T-1-k}\hat V(s_T)}_{=\,0}.
\tag{2}
\]</span></p>
<p>Combining (A) and (2), and reindexing with <span class="math inline">\(n=t-k\)</span>,
<span class="math display">\[
\sum_{t=k}^{T-1} (\gamma\lambda)^{t-k}\delta_t
= -\hat V(s_k)
+ \sum_{n=0}^{T-1-k}\gamma^{n}\lambda^{n} r_{k+n}
+ (1-\lambda)\sum_{n=1}^{T-1-k}\gamma^{n}\lambda^{n-1}\hat V(s_{k+n}).
\tag{3}
\]</span></p>
<p>On the other hand, expanding the <span class="math inline">\(\lambda\)</span>-return <a href="value-rl.html#eq:LambdaReturn">(2.10)</a>,
<span class="math display">\[
\begin{aligned}
g_k^{(\lambda)}
&amp;=(1-\lambda)\sum_{n=1}^{T-k}\lambda^{n-1}
\Bigg(\sum_{m=0}^{n-1}\gamma^{m} r_{k+m} + \gamma^{n}\hat V(s_{k+n})\Bigg) + \lambda^{T-k} g_k^{(T-k)}\\
&amp;= \sum_{n=0}^{T-1-k}\gamma^{n}\lambda^{n} r_{k+n}
+ (1-\lambda)\sum_{n=1}^{T-1-k}\gamma^{n}\lambda^{n-1}\hat V(s_{k+n}),
\end{aligned}
\tag{4}
\]</span>
where we used that <span class="math inline">\(\hat V(s_T)=0\)</span>. Comparing (3) and (4) yields
<span class="math display">\[
\sum_{t=k}^{T-1} (\gamma\lambda)^{t-k}\delta_t
= g_k^{(\lambda)} - \hat V(s_k).
\tag{5}
\]</span></p>
<p>Substituting (5) into (1) and multiplying by <span class="math inline">\(\alpha\)</span> completes the proof.</p>
</div>
</div>
<!-- **Remarks.**
1. The equivalence above is **exact** when the value function \(\hat V\) is held fixed throughout the episode (offline/episodic view).  
2. With *online* updates (changing \(\hat V\) within the episode), the standard accumulating-trace TD(\(\lambda\)) is only approximately forward-equivalent; the discrepancy is \(O(\alpha)\). An exactly online-equivalent variant is known as **true online TD(\(\lambda\))** (not covered here). -->
<div class="examplebox">
<div class="example">
<p><span id="exm:PolicyEvaluationRandomWalk" class="example"><strong>Example 2.1  (Policy Evaluation (MC and TD Family)) </strong></span>We consider the classic random-walk MDP with terminal states:</p>
<ul>
<li><strong>States:</strong> <span class="math inline">\(\{0,1,2,3,4,5,6\}\)</span>, where <span class="math inline">\(0\)</span> and <span class="math inline">\(6\)</span> are terminal; nonterminal states are <span class="math inline">\(1{:}5\)</span>.</li>
<li><strong>Actions:</strong> <span class="math inline">\(\{-1,+1\}\)</span> (“Left”/“Right”).</li>
<li><strong>Dynamics:</strong> From a nonterminal state <span class="math inline">\(s\in\{1,\dots,5\}\)</span>, action <span class="math inline">\(-1\)</span> moves to <span class="math inline">\(s-1\)</span>, and action <span class="math inline">\(+1\)</span> moves to <span class="math inline">\(s+1\)</span>.</li>
<li><strong>Rewards:</strong> Transitioning into state <span class="math inline">\(6\)</span> yields reward <span class="math inline">\(+1\)</span>; all other transitions yield <span class="math inline">\(0\)</span>.</li>
<li><strong>Discount:</strong> <span class="math inline">\(\gamma=1\)</span> (episodic task). Episodes start at state <span class="math inline">\(s_0=3\)</span> and terminate upon reaching <span class="math inline">\(\{0,6\}\)</span>.</li>
</ul>
<p>We evaluate the <em>equiprobable policy</em> <span class="math inline">\(\pi\)</span> that chooses Left/Right with probability <span class="math inline">\(1/2\)</span> each at every nonterminal state. Under this policy, the true state-value function on nonterminal states <span class="math inline">\(s\in\{1,\dots,5\}\)</span> is
<span class="math display" id="eq:trueV-rw">\[\begin{equation}
V^\pi(s) \;=\; \frac{s}{6}.
\tag{2.14}
\end{equation}\]</span></p>
<p>We compare four <em>tabular policy-evaluation</em> methods:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Monte Carlo (MC), first-visit</strong> — using full returns as target.</p></li>
<li><p><strong>TD(0)</strong> — one-step bootstrap.</p></li>
<li><p><strong><span class="math inline">\(n\)</span>-step TD</strong> — here we use <span class="math inline">\(n=3\)</span> (intermediate between MC and TD(0)).</p></li>
<li><p><strong>TD(<span class="math inline">\(\lambda\)</span>)</strong> — accumulating eligibility traces (we illustrate with <span class="math inline">\(\lambda=0.9\)</span>).</p></li>
</ol>
<p>All methods estimate <span class="math inline">\(V^\pi\)</span> from trajectories generated by <span class="math inline">\(\pi\)</span>.</p>
<p><strong>Error Metric.</strong>
We report the <em>mean-squared error (MSE)</em> over nonterminal states after each episode:
<span class="math display" id="eq:mse-metric">\[\begin{equation}
\mathrm{MSE}_t \;=\; \frac{1}{5}\sum_{s=1}^{5}\big(\hat V_t(s)-V^\pi(s)\big)^2,
\tag{2.15}
\end{equation}\]</span>
where <span class="math inline">\(V^\pi\)</span> is given by <a href="value-rl.html#eq:trueV-rw">(2.14)</a>. Curves are averaged over multiple random seeds.</p>
<p><strong>Fixed Step Sizes.</strong>
We first use a fixed step size <span class="math inline">\(\alpha=0.1\)</span> for all methods. Fig. <a href="value-rl.html#fig:policy-evaluation-random-walk-fixed-step-size">2.1</a> shows the trajectories of MSE versus number of episodes. We can see that, when using a constant step size, these methods do not converge to exactly the true value function, but to a small neighborhood. In addition, if the algorithm initially decays very fast, then the final variance is larger. For example, MC initially decays very fast, but has a higher variance, whereas TD(0) initially decays slower, but has a lower final variance. This agrees with the theoretical analysis in <span class="citation">(<a href="#ref-kearns2000bias">Kearns and Singh 2000</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:policy-evaluation-random-walk-fixed-step-size"></span>
<img src="images/Value-RL/mc_td_comparison_constant_step.png" alt="Policy Evaluation, MC versus TD Family, Fixed Step Size" width="90%" />
<p class="caption">
Figure 2.1: Policy Evaluation, MC versus TD Family, Fixed Step Size
</p>
</div>
<p><strong>Diminishing Step Sizes.</strong>
We then use a diminishing step size for the TD family:
<span class="math display" id="eq:per-state-decay">\[\begin{equation}
\alpha_t(s) \;=\; \frac{c}{\big(N_t(s)+t_0\big)^p},
\qquad \tfrac{1}{2} &lt; p \le 1,
\tag{2.16}
\end{equation}\]</span>
where <span class="math inline">\(N_t(s)\)</span> counts how many times <span class="math inline">\(V(s)\)</span> has been updated up to time <span class="math inline">\(t\)</span>. A common choice is <span class="math inline">\(p=1\)</span> with moderate <span class="math inline">\(c&gt;0\)</span> and <span class="math inline">\(t_0&gt;0\)</span>.</p>
<p>Fig. <a href="value-rl.html#fig:policy-evaluation-random-walk-diminishing-step-size">2.2</a> shows the MSE versus episodes for MC, TD(0), 3-step TD, and TD(<span class="math inline">\(\lambda\)</span>) under the diminishing step-size. Observe that all algorithms converge to the true value function under the diminishing step size schedule.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:policy-evaluation-random-walk-diminishing-step-size"></span>
<img src="images/Value-RL/mc_td_comparison_diminishing_step.png" alt="Policy Evaluation, MC versus TD Family, Diminishing Step Size" width="90%" />
<p class="caption">
Figure 2.2: Policy Evaluation, MC versus TD Family, Diminishing Step Size
</p>
</div>
<p>You are encouraged to play with the parameters of these algorithms in the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/policy_evaluation_mc_td.py">here</a>.</p>
</div>
</div>
</div>
</div>
<div id="convergence-proof-of-td-learning" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Convergence Proof of TD Learning<a href="value-rl.html#convergence-proof-of-td-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>TBD.</p>
</div>
<div id="on-policy-control" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> On-Policy Control<a href="value-rl.html#on-policy-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Monte Carlo (MC) estimation and the TD family evaluate policies directly from interaction—no model required. We now turn evaluation into control via generalized policy iteration (GPI): repeatedly (i) evaluate the current policy from data and (ii) improve it by acting greedily with respect to the new estimates. We first cover on-policy control methods, which estimate and improve the same (typically <span class="math inline">\(\varepsilon\)</span>-greedy) policy, and then off-policy methods, which learn about a target policy while behaving with a different one.</p>
<div id="monte-carlo-control" class="section level4 hasAnchor" number="2.1.3.1">
<h4><span class="header-section-number">2.1.3.1</span> Monte Carlo Control<a href="value-rl.html#monte-carlo-control" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>High-level Intuition.</strong></p>
<ul>
<li><p><strong>Goal.</strong> Learn an (approximately) optimal policy by alternating <em>policy evaluation</em> and <em>policy improvement</em> using only sampled episodes.</p></li>
<li><p><strong>Why action-values?</strong> Estimating <span class="math inline">\(Q^\pi(s,a)\)</span> lets us improve the policy <em>without a model</em> by choosing “<span class="math inline">\(\arg\max_a Q(s,a)\)</span>”.</p></li>
<li><p><strong>Exploration.</strong> Pure greedy improvement can get stuck. MC control keeps the policy <em><span class="math inline">\(\varepsilon\)</span>-soft</em> (e.g., <span class="math inline">\(\varepsilon\)</span>-greedy) so that every action has nonzero probability and all state-action pairs continue to be sampled. An <span class="math inline">\(\varepsilon\)</span>-soft policy is one that never rules out any action: in every state <span class="math inline">\(s\)</span>, each action <span class="math inline">\(a\)</span> gets at least a small fraction of probability. Formally, in the tabular setup, we have that a policy <span class="math inline">\(\pi\)</span> is <span class="math inline">\(\varepsilon\)</span>-soft if and only if
<span class="math display" id="eq:epsilon-soft-policy">\[\begin{equation}
\forall s, \forall a: \quad \pi(a \mid s) \geq \frac{\varepsilon}{|\mathcal{A}(s)|}, \quad \varepsilon \in (0,1],
\tag{2.17}
\end{equation}\]</span>
where <span class="math inline">\(\mathcal{A}(s)\)</span> denotes the set of actions the agent can select at state <span class="math inline">\(s\)</span>.</p></li>
<li><p><strong>Coverage mechanisms.</strong> Classic guarantees use either:</p>
<ol style="list-style-type: decimal">
<li><strong>Exploring starts (ES):</strong> start each episode from a randomly chosen <span class="math inline">\((s,a)\)</span> with nonzero probability; or<br />
</li>
<li><strong><span class="math inline">\(\varepsilon\)</span>-soft / GLIE (Greedy in the Limit with Infinite Exploration):</strong> use <span class="math inline">\(\varepsilon\)</span>-greedy behavior with <span class="math inline">\(\varepsilon_t \downarrow 0\)</span> so every <span class="math inline">\((s,a)\)</span> is visited infinitely often while the policy becomes greedy in the limit.</li>
</ol></li>
</ul>
<p><strong>Algorithmic Form.</strong>
We maintain tabular action-value estimates <span class="math inline">\(Q(s,a)\)</span> and an <em><span class="math inline">\(\varepsilon\)</span>-soft</em> policy <span class="math inline">\(\pi\)</span> (<span class="math inline">\(\varepsilon\)</span>-greedy w.r.t. <span class="math inline">\(Q\)</span>). After each episode we update <span class="math inline">\(Q\)</span> from <em>empirical returns</em> and then improve <span class="math inline">\(\pi\)</span>.</p>
<p><strong>Return from time <span class="math inline">\(t\)</span>:</strong>
<span class="math display">\[
g_t = r_t + \gamma r_{t+1} + \dots + \gamma^{T-t} r_T = \sum_{k=0}^{T-t} \gamma^{k} r_{t+k}.
\]</span></p>
<p><strong>First-visit MC update (common choice):</strong>
<span class="math display" id="eq:MCControl-QUpdate">\[\begin{equation}
Q(s_t,a_t) \;\leftarrow\; Q(s_t,a_t) + \alpha_{N(s_t,a_t)}\!\left(g_t - Q(s_t,a_t)\right),
\tag{2.18}
\end{equation}\]</span>
applied only on the first occurrence of <span class="math inline">\((s_t,a_t)\)</span> in the episode. <em>Sample-average</em> learning uses <span class="math inline">\(\alpha_n = 1/n\)</span> per pair; more generally, use diminishing stepsizes.</p>
<p><strong>Policy improvement (<span class="math inline">\(\varepsilon\)</span>-greedy):</strong>
<span class="math display" id="eq:MCControl-PI">\[\begin{equation}
\pi(a|s) \;=\;
\begin{cases}
1-\varepsilon + \dfrac{\varepsilon}{|\mathcal{A}(s)|}, &amp; a \in \arg\max_{a&#39;} Q(s,a&#39;), \\
\dfrac{\varepsilon}{|\mathcal{A}(s)|}, &amp; \text{otherwise}.
\end{cases}
\tag{2.19}
\end{equation}\]</span></p>
<p><strong>Theoretical Guarantees.</strong></p>
<p>Assume a tabular episodic MDP and <span class="math inline">\(\gamma \in [0,1)\)</span>.</p>
<ul>
<li><p><strong>Convergence with Exploring Starts.</strong> If every state–action pair has nonzero probability of being the <em>first</em> pair of an episode (using ES), and each <span class="math inline">\(Q(s,a)\)</span> is updated toward the true mean return from <span class="math inline">\((s,a)\)</span> (e.g., via sample averages), then repeated policy evaluation and greedy improvement converge with probability 1 to an optimal deterministic policy. (If one uses an <span class="math inline">\(\varepsilon\)</span>-greedy improvement, then it converges to an optimal <span class="math inline">\(\varepsilon\)</span>-soft policy.)</p></li>
<li><p><strong>Convergence with <span class="math inline">\(\varepsilon\)</span>-soft GLIE behavior.</strong> If the behavior policy is GLIE—every <span class="math inline">\((s,a)\)</span> is visited infinitely often and <span class="math inline">\(\epsilon_t \to 0\)</span>—and the stepsizes for each <span class="math inline">\((s,a)\)</span> satisfy the Robbins–Monro conditions <span class="math inline">\(\sum_{t} \alpha_t(s,a) = \infty,\sum_{t} \alpha_t(s,a)^2 &lt; \infty\)</span>, then <span class="math inline">\(Q(s,a)\)</span> converges to <span class="math inline">\(Q^\star(s,a)\)</span> for all pairs visited infinitely often, and the <span class="math inline">\(\varepsilon\)</span>-greedy policy converges almost surely to an optimal policy.</p></li>
</ul>
<div class="remark">
<p><span id="unlabeled-div-11" class="remark"><em>Remark</em>. </span><strong>Unbiased but high-variance.</strong> MC targets <span class="math inline">\(g_t\)</span> are unbiased estimates of action values under the current policy, but can have high variance—especially for long horizons—so convergence can be slower than TD methods. Keeping <span class="math inline">\(\varepsilon&gt;0\)</span> ensures exploration but limits asymptotic optimality to the best <span class="math inline">\(\varepsilon\)</span>-soft policy; hence <span class="math inline">\(\varepsilon_t \downarrow 0\)</span> (GLIE) is recommended for optimality.</p>
</div>
</div>
<div id="sarsa-on-policy-td-control" class="section level4 hasAnchor" number="2.1.3.2">
<h4><span class="header-section-number">2.1.3.2</span> SARSA (On-Policy TD Control)<a href="value-rl.html#sarsa-on-policy-td-control" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>High-level Intuition.</strong></p>
<ul>
<li><strong>Goal.</strong> Turn evaluation into control by updating action values online and improving the same policy that generates data.</li>
<li><strong>Key idea.</strong> Replace Monte Carlo returns with a bootstrapped target. After taking action <span class="math inline">\(a_t\)</span> in state <span class="math inline">\(s_t\)</span> and observing <span class="math inline">\(r_{t}, s_{t+1}\)</span>, sample the next action <span class="math inline">\(a_{t+1}\)</span> from the current policy and update toward <span class="math inline">\(r_{t} + \gamma Q(s_{t+1}, a_{t+1})\)</span>.</li>
<li><strong>On-policy nature.</strong> SARSA evaluates the behavior policy itself, typically an <span class="math inline">\(\varepsilon\)</span>-greedy policy w.r.t. <span class="math inline">\(Q\)</span>.</li>
<li><strong>Exploration.</strong> Use <span class="math inline">\(\varepsilon\)</span>-soft behavior so every action keeps nonzero probability. For optimality, let <span class="math inline">\(\varepsilon_t \downarrow 0\)</span> to obtain GLIE (Greedy in the Limit with Infinite Exploration).</li>
</ul>
<p><strong>Algorithmic Form.</strong></p>
<p>Let <span class="math inline">\(Q\)</span> be a tabular action-value function and <span class="math inline">\(\pi_t\)</span> be <span class="math inline">\(\varepsilon_t\)</span>-greedy w.r.t. <span class="math inline">\(Q_t\)</span>.</p>
<p><strong>TD target and error:</strong>
<span class="math display" id="eq:SARSA-TDTarget">\[\begin{equation}
y_t = r_{t} + \gamma Q(s_{t+1}, a_{t+1}), \qquad
\delta_t = y_t - Q(s_t, a_t).
\tag{2.20}
\end{equation}\]</span></p>
<p><strong>SARSA update (one-step):</strong>
<span class="math display" id="eq:SARSA-QUpdate">\[\begin{equation}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha_t(s_t,a_t)\, \delta_t.
\tag{2.21}
\end{equation}\]</span></p>
<p><strong><span class="math inline">\(\varepsilon\)</span>-greedy policy improvement:</strong>
<span class="math display" id="eq:SARSA-PolicyUpdate">\[\begin{equation}
\pi_{t+1}(a\mid s) =
\begin{cases}
1-\varepsilon_{t+1} + \dfrac{\varepsilon_{t+1}}{|\mathcal A(s)|}, &amp; a \in \arg\max_{a&#39;} Q_{t+1}(s,a&#39;),\\
\dfrac{\varepsilon_{t+1}}{|\mathcal A(s)|}, &amp; \text{otherwise.}
\end{cases}
\tag{2.22}
\end{equation}\]</span></p>
<!-- ### Algorithm 2 — SARSA (tabular, \(\varepsilon\)-greedy) {#alg:sarsa}
1. Initialize \(Q(s,a)\) arbitrarily. Choose an exploration schedule \(\varepsilon_t\).
2. For each episode:
   1. Sample \(a_0 \sim \pi_0(\cdot\mid s_0)\) (\(\varepsilon_0\)-greedy w.r.t. \(Q\)).
   2. For \(t=0,1,\dots\) until terminal:
      - Take \(a_t\), observe \(r_{t+1}, s_{t+1}\).
      - Sample \(a_{t+1} \sim \pi_t(\cdot\mid s_{t+1})\) (\(\varepsilon_t\)-greedy w.r.t. \(Q\)).
      - Compute \(\delta_t = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\).
      - Update \(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha_t(s_t,a_t)\, \delta_t\).
      - Improve policy at \(s_t\) to be \(\varepsilon_t\)-greedy w.r.t. updated \(Q\).
   3. Optionally decay \(\varepsilon_{t}\) according to a GLIE schedule. -->
<p><strong>Variants.</strong></p>
<ul>
<li><p><strong>Expected SARSA</strong> replaces the sampled <span class="math inline">\(a_{t+1}\)</span> by its expectation under <span class="math inline">\(\pi_t\)</span> for lower variance:
<span class="math display" id="eq:ExpectedSARSA-Target">\[\begin{equation}
y_t = r_{t} + \gamma \sum_a \pi_t(a\mid s_{t+1}) Q(s_{t+1}, a).
\tag{2.23}
\end{equation}\]</span></p></li>
<li><p><strong><span class="math inline">\(n\)</span>-step SARSA</strong> and <strong>SARSA(<span class="math inline">\(\lambda\)</span>)</strong> blend multi-step targets; these trade bias and variance similarly to MC vs TD.</p></li>
</ul>
<p><strong>Convergence Guarantees.</strong></p>
<p>Assume a finite MDP, <span class="math inline">\(\gamma \in [0,1)\)</span>, asynchronous updates, and that each state–action pair is visited infinitely often.</p>
<ul>
<li><strong>GLIE convergence to optimal policy.</strong> If the behavior is GLIE, i.e., <span class="math inline">\(\varepsilon_t \downarrow 0\)</span> while ensuring infinite exploration, and stepsizes satisfy the Robbins–Monro conditions, then <span class="math inline">\(Q_t \to Q^\star\)</span> almost surely and the <span class="math inline">\(\varepsilon_t\)</span>-greedy behavior becomes greedy in the limit, yielding an optimal policy almost surely.</li>
</ul>
</div>
</div>
<div id="off-policy-control" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Off-Policy Control<a href="value-rl.html#off-policy-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Off-policy methods learn about a <em>target</em> policy <span class="math inline">\(\pi\)</span> while following a (potentially different) <em>behavior</em> policy <span class="math inline">\(b\)</span> to gather data. This decoupling is useful when:</p>
<ul>
<li><p>you want to <em>reuse logged data</em> collected by some <span class="math inline">\(b\)</span> (e.g., a rule-based controller or a past system),</p></li>
<li><p>you need <em>safer exploration</em> by restricting behavior <span class="math inline">\(b\)</span> while aiming to evaluate or improve a different <span class="math inline">\(\pi\)</span>,</p></li>
<li><p>you want to learn about the <em>greedy</em> policy without executing it, which motivates algorithms like Q-learning.</p></li>
</ul>
<p>In this section we first cover off-policy policy evaluation with <em>importance sampling</em>, then show how it can be used to construct an off-policy <em>Monte Carlo control</em> scheme in the tabular case. Finally, we present Q-learning.</p>
<div id="importance-sampling-for-policy-evaluation" class="section level4 hasAnchor" number="2.1.4.1">
<h4><span class="header-section-number">2.1.4.1</span> Importance Sampling for Policy Evaluation<a href="value-rl.html#importance-sampling-for-policy-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Motivation.</strong> Suppose we have episodes generated by a behavior policy <span class="math inline">\(b\)</span>, but we want the value of a different target policy <span class="math inline">\(\pi\)</span>. For a state value this is <span class="math inline">\(V^\pi(s) = \mathbb{E}_\pi[g_t \mid s_t=s]\)</span>, and for action values <span class="math inline">\(Q^\pi(s,a) = \mathbb{E}_\pi[g_t \mid s_t=s, a_t=a]\)</span>, where
<span class="math display">\[
g_t = \sum_{k=0}^{T-t} \gamma^{k} r_{t+k}.
\]</span>
Because the data come from <span class="math inline">\(b\)</span>, the naive sample average is biased. Importance sampling (IS) reweights returns so that expectations under <span class="math inline">\(b\)</span> equal those under <span class="math inline">\(\pi\)</span>.</p>
<p>A basic <em>support condition</em> is required:
<span class="math display" id="eq:IS-SupportCondition">\[\begin{equation}
\text{If } \pi(a\mid s) &gt; 0 \text{ then } b(a\mid s) &gt; 0 \quad \text{for all visited } (s,a).
\tag{2.24}
\end{equation}\]</span>
This ensures that <span class="math inline">\(\pi\)</span> is absolutely continuous with respect to <span class="math inline">\(b\)</span> on the experienced trajectories.</p>
<p><strong>Importance Sampling (episode-wise).</strong> Consider a trajectory starting at time <span class="math inline">\(t\)</span>:
<span class="math display">\[
\tau_t = (s_t, a_t, r_t, s_{t+1}, a_{t+1}, \dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T).
\]</span>
The probability of observing this trajectory conditioned on <span class="math inline">\(s_t = s\)</span>, under policy <span class="math inline">\(\pi\)</span>, is
<span class="math display">\[
\mathbb{P}_{\pi}[\tau_t \mid s_t = s] = \pi(a_t \mid s_t) P(s_{t+1} \mid s_t, a_t) \pi(a_{t+1} \mid s_{t+1}) \cdots \pi(a_{T-1} \mid s_{T-1}) P(s_T \mid s_{T-1}, a_{T-1}).
\]</span>
The probability of observing the same trajectory conditioned on <span class="math inline">\(s_t = s\)</span>, under policy <span class="math inline">\(b\)</span>, is
<span class="math display">\[
\mathbb{P}_{b}[\tau_t \mid s_t = s] = b(a_t \mid s_t) P(s_{t+1} \mid s_t, a_t) b(a_{t+1} \mid s_{t+1}) \cdots b(a_{T-1} \mid s_{T-1}) P(s_T \mid s_{T-1}, a_{T-1}).
\]</span>
Since the return <span class="math inline">\(g_t\)</span> is a deterministic function of <span class="math inline">\(\tau_t\)</span>, i.e., applying the reward function <span class="math inline">\(R\)</span> to state-action pairs, we have that
<span class="math display" id="eq:IS-Trajectory-LikelihoodRatio">\[\begin{equation}
\begin{split}
V^\pi (s) &amp; = \mathbb{E}_{\pi}[g_t \mid s_t = s] = \sum_{\tau_t} g_t \mathbb{P}_\pi [\tau_t \mid s_t = s] \\
&amp; = \sum_{\tau_t} g_t \mathbb{P}_b[\tau_t \mid s_t = s] \left(\frac{\mathbb{P}_\pi [\tau_t \mid s_t = s]}{\mathbb{P}_b[\tau_t \mid s_t = s]} \right) \\
&amp; = \sum_{\tau_t} \left( \frac{\pi(a_t \mid s_t) \pi(a_{t+1} \mid s_{t+1}) \cdots \pi(a_{T-1} \mid s_{T-1}) }{b(a_t \mid s_t)  b(a_{t+1} \mid s_{t+1}) \cdots b(a_{T-1} \mid s_{T-1})} \right) g_t \mathbb{P}_b [\tau_t \mid s_t = s]
\end{split}
\tag{2.25}
\end{equation}\]</span>
Therefore, define the <em>likelihood ratio</em>
<span class="math display" id="eq:IS-LikelihoodRatio">\[\begin{equation}
\rho_{t:T-1} = \prod_{k=t}^{T-1} \frac{\pi(a_k \mid s_k)}{b(a_k \mid s_k)},
\tag{2.26}
\end{equation}\]</span>
we have
<span class="math display" id="eq:IS-Value">\[\begin{equation}
V^\pi(s) = \mathbb{E}_b\left[\rho_{t:T-1} g_t \mid s_t=s\right].
\tag{2.27}
\end{equation}\]</span>
Similarly, we have
<span class="math display" id="eq:IS-ActionValue">\[\begin{equation}
Q^\pi(s,a) =  \mathbb{E}_b\!\left[\rho_{t:T-1} g_t \mid s_t=s, a_t=a\right].
\tag{2.28}
\end{equation}\]</span>
Given <span class="math inline">\(n\)</span> episodes, the ordinary IS estimator for <span class="math inline">\(Q^\pi\)</span> at the first visit of <span class="math inline">\((s,a)\)</span> is
<span class="math display">\[
\hat Q_n^{\text{IS}}(s,a) = \frac{1}{N_n(s,a)} \sum_{i=1}^n \mathbf{1}\{(s,a)\text{ visited}\}\, \rho_{t_i:T_i-1}^{(i)}\, g_{t_i}^{(i)},
\]</span>
where <span class="math inline">\(N_n(s,a)\)</span> counts the number of first visits of <span class="math inline">\((s,a)\)</span>. In words, to estimate the <span class="math inline">\(Q\)</span> value of the target policy <span class="math inline">\(\pi\)</span> using trajectories of the behavior policy <span class="math inline">\(b\)</span>, we need to reweight the return <span class="math inline">\(g_t\)</span> by the likelihood ratio <span class="math inline">\(\rho_{t:T-1}\)</span>. Note that the likelihood ratio does not require knowledge about the transition dynamics.</p>
<!-- ### Weighted (self-normalized) importance sampling

To reduce variance, use **weighted IS**
\[
\hat Q_n^{\text{WIS}}(s,a) = \frac{\sum_{i=1}^n w_i\, G_{t_i}^{(i)}}{\sum_{i=1}^n w_i}, \quad
w_i = \mathbf{1}\{(s,a)\text{ visited}\}\, \rho_{t_i:T_i-1}^{(i)}.
\]
This estimator is **biased** for finite \(n\) but typically has much lower variance and is **consistent** as \(n\to\infty\) under the support condition.

### Per-decision importance sampling

Variance can be further reduced by reweighting **each reward term** with the prefix ratio
\[
\rho_{t:k} = \prod_{j=t}^{k} \frac{\pi(a_j \mid s_j)}{b(a_j \mid s_j)}.
\]
The per-decision target for state values is
\[
\tilde G_t^{\text{PD}} = \sum_{k=t}^{T-1} \gamma^{k-t} \rho_{t:k} r_{k+1},
\]
and analogously for action values when conditioning on \((s_t,a_t)\). These can be used in either ordinary or weighted form.
!-->
<p><strong>Algorithmic Form: Off-policy Monte Carlo Policy Evaluation.</strong></p>
<p><strong>Input:</strong> behavior <span class="math inline">\(b\)</span>, target <span class="math inline">\(\pi\)</span>, episodes from <span class="math inline">\(b\)</span><br />
<strong>For each episode</strong> <span class="math inline">\((s_0,a_0,r_0,s_1,\dots,s_{T-1},a_{T-1},r_{T-1},s_T)\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>For <span class="math inline">\(t=T-1,\dots,0\)</span> compute episode-wise likelihood ratio <span class="math inline">\(\rho_{t:T-1}\)</span> and return <span class="math inline">\(g_t\)</span>,</p></li>
<li><p>For first visits of <span class="math inline">\((s_t,a_t)\)</span>, update
<span class="math display">\[
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha_{N(s_t,a_t)}\big(\rho_{t:T-1} g_t - Q(s_t,a_t)\big).
\]</span>
Use sample averages <span class="math inline">\(\alpha_n=1/n\)</span> or Robbins-Monro stepsizes.</p></li>
</ol>
<p><strong>Guarantees.</strong> Under the support condition and finite variance assumptions, ordinary IS is <em>unbiased</em> and converges almost surely to <span class="math inline">\(Q^\pi\)</span>.</p>
</div>
<div id="off-policy-monte-carlo-control" class="section level4 hasAnchor" number="2.1.4.2">
<h4><span class="header-section-number">2.1.4.2</span> Off-Policy Monte Carlo Control<a href="value-rl.html#off-policy-monte-carlo-control" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>High-level Intuition.</strong> We wish to improve a target policy <span class="math inline">\(\pi\)</span> toward optimality while behaving with a different exploratory policy <span class="math inline">\(b\)</span>. We evaluate <span class="math inline">\(Q^\pi\)</span> off-policy using IS on data from <span class="math inline">\(b\)</span>, then set <span class="math inline">\(\pi\)</span> greedy with respect to the updated <span class="math inline">\(Q\)</span>. Keep <span class="math inline">\(b\)</span> sufficiently exploratory (for coverage), for example <span class="math inline">\(\varepsilon\)</span>-greedy with a fixed <span class="math inline">\(\varepsilon&gt;0\)</span> or a GLIE schedule.</p>
<p><strong>Algorithmic Form.</strong></p>
<ol style="list-style-type: decimal">
<li><p>Initialize <span class="math inline">\(Q(s,a)\)</span> arbitrarily. Set target <span class="math inline">\(\pi\)</span> to be greedy w.r.t. <span class="math inline">\(Q\)</span>. Choose an exploratory behavior <span class="math inline">\(b\)</span> that ensures coverage, e.g., <span class="math inline">\(\varepsilon\)</span>-greedy w.r.t. <span class="math inline">\(Q\)</span> with <span class="math inline">\(\varepsilon&gt;0\)</span>.</p></li>
<li><p>Loop over iterations <span class="math inline">\(i=0,1,2,\dots\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Data collection under <span class="math inline">\(b\)</span>: generate a batch of episodes using <span class="math inline">\(b\)</span>.</li>
<li>Off-policy evaluation of <span class="math inline">\(\pi\)</span>: for each episode, compute IS targets for first visits of <span class="math inline">\((s_t,a_t)\)</span> and update <span class="math inline">\(Q\)</span> using either ordinary IS</li>
<li>Policy improvement: set for all states
<span class="math display">\[
\pi_{i+1}(s) \in \arg\max_{a} Q(s,a).
\]</span></li>
<li>Optionally update <span class="math inline">\(b\)</span> to remain exploratory, for example <span class="math inline">\(b\)</span> <span class="math inline">\(\leftarrow\)</span> <span class="math inline">\(\varepsilon\)</span>-greedy w.r.t. <span class="math inline">\(Q\)</span> with a chosen <span class="math inline">\(\varepsilon\)</span> or a GLIE decay.</li>
</ol></li>
</ol>
<!-- ::: {.remark}
You may separate time scales: use many episodes to evaluate \(\pi\) before each improvement step, or interleave smaller updates more frequently.
::: -->
<p><strong>Convergence Guarantees.</strong></p>
<ul>
<li><p><strong>Evaluation step:</strong> With the support condition and appropriate stepsizes, off-policy MC prediction converges almost surely to <span class="math inline">\(Q^\pi\)</span> when using ordinary IS.</p></li>
<li><p><strong>Control in the batch GPI limit:</strong> If each evaluation step produces estimates that converge to the exact <span class="math inline">\(Q^{\pi_i}\)</span> before improvement, then by the policy improvement theorem the sequence of greedy target policies <span class="math inline">\(\pi_i\)</span> converges to an optimal policy in finite MDPs.</p></li>
</ul>
<!-- - **Practical interleaving:** When evaluation and improvement are interleaved with finite samples, high variance of IS can slow or destabilize progress. There is **no general convergence guarantee** for incremental off-policy MC control beyond the evaluation consistency. This motivates off-policy TD control methods such as **Q-learning** and **Expected Q-learning**, which avoid explicit IS. -->
<div class="remark">
<p><span id="unlabeled-div-12" class="remark"><em>Remark</em>. </span><strong>Choice of <span class="math inline">\(b\)</span>.</strong> A common and simple choice is an <span class="math inline">\(\varepsilon\)</span>-greedy behavior <span class="math inline">\(b\)</span> w.r.t. current <span class="math inline">\(Q\)</span> that maintains <span class="math inline">\(\varepsilon&gt;0\)</span> for coverage or uses GLIE so that <span class="math inline">\(\varepsilon_t \downarrow 0\)</span> while all pairs are still visited infinitely often.</p>
</div>
</div>
<div id="q-learning" class="section level4 hasAnchor" number="2.1.4.3">
<h4><span class="header-section-number">2.1.4.3</span> Q-Learning<a href="value-rl.html#q-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>High-Level Intuition.</strong></p>
<ul>
<li><p><strong>What it learns.</strong> Q-Learning seeks the fixed point of the Bellman optimality operator
<span class="math display">\[
(\mathcal T^\star Q)(s,a) = \mathbb E\big[ r_{t} + \gamma \max_{a&#39;} Q(s_{t+1}, a&#39;) \mid s_t=s, a_t=a \big],
\]</span>
whose unique fixed point is <span class="math inline">\(Q^\star\)</span>. Because <span class="math inline">\(\mathcal T^\star\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction in <span class="math inline">\(\|\cdot\|_\infty\)</span>, repeatedly applying it converges to <span class="math inline">\(Q^\star\)</span> in the tabular case.</p></li>
<li><p><strong>Why off-policy.</strong> We can behave with any sufficiently exploratory policy <span class="math inline">\(b\)</span> (e.g., <span class="math inline">\(\varepsilon\)</span>-greedy w.r.t. current <span class="math inline">\(Q\)</span>) but learn from the greedy target <span class="math inline">\(\max_{a&#39;} Q(s&#39;,a&#39;)\)</span>. No importance sampling is needed.</p></li>
</ul>
<!-- - **Bias–variance profile.** The max target bootstraps strongly (low variance) but introduces maximization bias when estimates are noisy; Double Q-Learning** remedies this. -->
<p><strong>Algorithmic Form.</strong> Let <span class="math inline">\(Q\)</span> be a tabular action-value function. At each step observe a transition <span class="math inline">\((s_t, a_t, r_{t}, s_{t+1})\)</span> generated by a behavior policy <span class="math inline">\(b_t\)</span> (typically <span class="math inline">\(\varepsilon_t\)</span>-greedy w.r.t. <span class="math inline">\(Q_t\)</span>).</p>
<ul>
<li><p><strong>Target and TD error</strong>
<span class="math display">\[
y_t = r_{t} + \gamma \max_{a&#39;} Q(s_{t+1}, a&#39;), \qquad
\delta_t = y_t - Q(s_t, a_t).
\]</span></p></li>
<li><p><strong>Update</strong>
<span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha_t(s_t,a_t)\, \delta_t.
\]</span></p></li>
<li><p><strong>Behavior (exploration)</strong>
Use <span class="math inline">\(\varepsilon_t\)</span>-greedy with <span class="math inline">\(\varepsilon_t\)</span> decaying (GLIE) or any scheme that ensures each <span class="math inline">\((s,a)\)</span> is updated infinitely often.</p></li>
</ul>
<!-- **Algorithm 3 — Q-Learning (tabular, off-policy) {#alg:qlearning}**

1. Initialize \(Q(s,a)\) arbitrarily and choose stepsizes \(\alpha_t(s,a)\).
2. For each episode:
   1. Set \(s \leftarrow s_0\).
   2. Repeat until terminal:
      - Choose \(a \sim b_t(\cdot \mid s)\) (e.g., \(\varepsilon_t\)-greedy w.r.t. \(Q\)).
      - Take \(a\), observe \(r, s'\).
      - \(Q(s,a) \leftarrow Q(s,a) + \alpha_t(s,a)\big[r + \gamma \max_{a'} Q(s',a') - Q(s,a)\big]\).
      - \(s \leftarrow s'\). -->
<!-- **Variants and Practical Tweaks**

- **Double Q-Learning.** Maintain \(Q^A, Q^B\) and alternate:
  \[
  Q^A(s,a) \leftarrow Q^A(s,a) + \alpha\big[r + \gamma Q^B(s', \arg\max_{a'} Q^A(s',a')) - Q^A(s,a)\big],
  \]
  and symmetrically for \(Q^B\). This reduces overestimation bias.

- **Expected Q-Learning.** Replace \(\max\) with the expectation under a target policy (e.g., \(\varepsilon\)-greedy) for variance reduction. -->
<p><strong>Convergence.</strong> In a finite MDP with <span class="math inline">\(\gamma \in [0,1)\)</span>, if each <span class="math inline">\((s,a)\)</span> is updated infinitely often (sufficient exploration) and stepsizes satisfy Robbins-Monro conditions, then Q-Learning converges to <span class="math inline">\(Q^\star\)</span> with probability 1.</p>
</div>
<div id="double-q-learning" class="section level4 hasAnchor" number="2.1.4.4">
<h4><span class="header-section-number">2.1.4.4</span> Double Q-Learning<a href="value-rl.html#double-q-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Motivation.</strong> Max operators tend to be optimistically biased when action values are noisy. Consider an example where in state <span class="math inline">\(s\)</span> one can take two actions <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. The estimated Q function <span class="math inline">\(\hat{Q}(s, \cdot)\)</span> has two values <span class="math inline">\(+1\)</span> and <span class="math inline">\(-1\)</span> with equal probability. In this case we have <span class="math inline">\(Q(s,1) = Q(s,2) = \mathbb{E}[\hat{Q}(s,\cdot)] = 0\)</span>. Therefore, <span class="math inline">\(\max Q(s,a) = 0\)</span>. However, the noisy estimated <span class="math inline">\(\hat{Q}(s,\cdot)\)</span> has four outcomes with equal probabilities:
<span class="math display">\[
(+1,-1), (+1,+1), (-1, +1), (-1,-1).
\]</span>
Therefore, we have
<span class="math display">\[
\mathbb{E}[\max_a \hat{Q}(s,a)] = \frac{1}{4} (1 + 1 + 1 -1) = 1/2 &gt; \max_a Q(s,a),
\]</span>
which overestimates the max <span class="math inline">\(Q\)</span> value. In general, we have
<span class="math display">\[
\mathbb{E}[\max_a \hat{Q}(s,a)] \geq \max_a \mathbb{E} [\hat{Q}(s,a)] = \max_a Q(s,a),
\]</span>
where the estimates <span class="math inline">\(\hat{Q}\)</span> are noisy (try to prove this on your own).
In Q-Learning the target
<span class="math display">\[
y_t = r_{t} + \gamma \max_{a&#39;} Q(s_{t+1}, a&#39;)
\]</span>
can therefore overestimate action values and slow learning or push policies toward risky actions.</p>
<p>Double Q-Learning reduces this bias by decoupling selection from evaluation: maintain two independent estimators <span class="math inline">\(Q^A\)</span> and <span class="math inline">\(Q^B\)</span>. Use one to select the greedy action and the other to evaluate it, and alternate which table you update. This weakens the statistical coupling that creates overestimation.</p>
<p><strong>Algorithmic Form.</strong> Keep two tables <span class="math inline">\(Q^A, Q^B\)</span>. Use an <span class="math inline">\(\varepsilon\)</span>-greedy behavior policy with respect to a combined estimate, e.g., <span class="math inline">\(Q^{\text{avg}}= \tfrac12(Q^A+Q^B)\)</span> or <span class="math inline">\(Q^A+Q^B\)</span>.</p>
<p>At each step observe <span class="math inline">\((s_t, a_t, r_{t}, s_{t+1})\)</span>. With probability <span class="math inline">\(1/2\)</span> update <span class="math inline">\(Q^A\)</span>, else update <span class="math inline">\(Q^B\)</span>.</p>
<ul>
<li><p><strong>Update <span class="math inline">\(Q^A\)</span>:</strong>
<span class="math display">\[
a^\star = \arg\max_{a&#39;} Q^A(s_{t+1}, a&#39;),\qquad
y_t = r_{t} + \gamma\, Q^B(s_{t+1}, a^\star),
\]</span>
<span class="math display">\[
Q^A(s_t, a_t) \leftarrow Q^A(s_t, a_t) + \alpha_t(s_t,a_t)\big[y_t - Q^A(s_t, a_t)\big].
\]</span></p></li>
<li><p><strong>Update <span class="math inline">\(Q^B\)</span>:</strong>
<span class="math display">\[
a^\star = \arg\max_{a&#39;} Q^B(s_{t+1}, a&#39;),\qquad
y_t = r_{t} + \gamma\, Q^A(s_{t+1}, a^\star),
\]</span>
<span class="math display">\[
Q^B(s_t, a_t) \leftarrow Q^B(s_t, a_t) + \alpha_t(s_t,a_t)\big[y_t - Q^B(s_t, a_t)\big].
\]</span></p></li>
<li><p><strong>Behavior policy (<span class="math inline">\(\varepsilon\)</span>-greedy):</strong> choose <span class="math inline">\(a_t \sim \varepsilon\)</span>-greedy with respect to <span class="math inline">\(Q^{\text{avg}}(s_t,\cdot)\)</span>.
A GLIE schedule <span class="math inline">\(\varepsilon_t \downarrow 0\)</span> is standard.</p></li>
<li><p><strong>Acting and planning:</strong> for greedy actions or plotting a single estimate, use <span class="math inline">\(Q^{\text{avg}} = \tfrac12(Q^A+Q^B)\)</span>.</p></li>
</ul>
<p><strong>Convergence.</strong></p>
<ul>
<li><strong>Tabular setting.</strong> In a finite MDP with <span class="math inline">\(\gamma \in [0,1)\)</span>, bounded rewards, sufficient exploration so that every <span class="math inline">\((s,a)\)</span> is updated infinitely often, and Robbins–Monro stepsizes for each pair. Double Q-Learning converges with probability 1 to <span class="math inline">\(Q^\star\)</span>.</li>
</ul>
<!-- - **Bias reduction, not elimination.** Decoupling selection and evaluation markedly reduces overestimation; it does not guarantee a perfectly unbiased max estimate at all times, but empirically it stabilizes and often improves learning compared to standard Q-Learning. -->
<!-- **Practical Notes**

- **Tie handling.** Break \(\arg\max\) ties uniformly at random to avoid systematic bias.
- **Which estimate to act on.** Use \(Q^{\text{avg}}\) for action selection; alternatives like \(Q^A\) or \(Q^B\) alone also work but can add variance.
- **Expected variants.** You can combine Double Q with **Expected** targets by replacing the evaluation term with an expectation under a target policy at \(s_{t+1}\).
- **Function approximation.** Double Q reduces overestimation in deep RL (e.g., Double DQN) when paired with target networks and replay, but it does not by itself prevent the off-policy bootstrapping instabilities of the deadly triad. -->
<div class="examplebox">
<div class="example">
<p><span id="exm:GridWorldMCControl" class="example"><strong>Example 2.2  (Value-based RL for Grid World) </strong></span>Consider the following <span class="math inline">\(5 \times 5\)</span> grid with <span class="math inline">\((0,4)\)</span> being the goal and the terminal state. At every state, the agent can take four actions: left, right, up, and down. There is a wall in the gray area shown in Fig. <a href="value-rl.html#fig:grid-world">2.3</a>. Upon hitting the wall, the agent stays in the original cell. Every action incurs a reward of <span class="math inline">\(-1\)</span>. Once the agent arrives at the goal state, reward stays at 0.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grid-world"></span>
<img src="images/Value-RL/grid-world.png" alt="Grid World" width="40%" />
<p class="caption">
Figure 2.3: Grid World
</p>
</div>
<p>We run Generalized Policy Iteration (GPI) with Monte Carlo (on-policy), SARSA, Expected SARSA, Q-Learning, and Double Q-Learning on this problem with diminishing learning rates.</p>
<p>Fig. <a href="value-rl.html#fig:grid-world-Q-convergence">2.4</a> plots the error between the estimated Q values (of different algorithms) and the ground-truth optimal Q value (obtained from value iteration with known transition dynamics). Except Monte Carlo control which converges slowly, the other methods converge fast.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grid-world-Q-convergence"></span>
<img src="images/Value-RL/Q-Value-Convergence.png" alt="Convergence of Estimated Q Values." width="90%" />
<p class="caption">
Figure 2.4: Convergence of Estimated Q Values.
</p>
</div>
<p>From the final estimated Q value, we can extract a greedy policy, visualized below.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/GPI_GridWorld.py">here</a>.</p>
<pre><code>MC Control:
 &gt; &gt; &gt; &gt; G
^ # ^ ^ ^
v # ^ ^ ^
v # &gt; ^ ^
&gt; &gt; &gt; &gt; ^
SARSA:
 &gt; &gt; &gt; &gt; G
^ # &gt; &gt; ^
^ # ^ ^ ^
^ # ^ ^ ^
&gt; &gt; ^ ^ ^
Expected SARSA:
 &gt; &gt; &gt; &gt; G
^ # &gt; &gt; ^
^ # ^ ^ ^
^ # &gt; &gt; ^
&gt; &gt; ^ ^ ^
Q-Learning:
 &gt; &gt; &gt; &gt; G
^ # ^ ^ ^
^ # ^ ^ ^
^ # ^ ^ ^
&gt; &gt; ^ ^ ^
Double Q-Learning:
 &gt; &gt; &gt; &gt; G
^ # &gt; ^ ^
^ # &gt; ^ ^
^ # ^ ^ ^
&gt; &gt; &gt; &gt; ^</code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="function-approximation" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Function Approximation<a href="value-rl.html#function-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many reinforcement learning problems have continuous state spaces–think of mechanical systems like robot arms, legged locomotion, drones, and autonomous vehicles. In these domains the state <span class="math inline">\(s\)</span> (e.g., joint angles/velocities, poses) lives in <span class="math inline">\(\mathbb{R}^n\)</span>, which makes a tabular representation of the value functions impossible. In this case, we must approximate values with parameterized functions.</p>
<div id="basics-of-continuous-mdp" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Basics of Continuous MDP<a href="value-rl.html#basics-of-continuous-mdp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a continuous MDP, at least one of the state space or the action space is a continuous space. Suppose <span class="math inline">\(\mathcal{S} \subseteq \mathbb{R}^n\)</span> and <span class="math inline">\(\mathcal{A} \subseteq \mathbb{R}^{m}\)</span> are both continuous spaces.</p>
<p>The environment kernel <span class="math inline">\(P(\cdot \mid s, a)\)</span> is a Markov kernel from <span class="math inline">\(\mathcal{S} \times \mathcal{A}\)</span> to <span class="math inline">\(\mathcal{S}\)</span>: for each state-action pair <span class="math inline">\((s,a)\)</span>, <span class="math inline">\(P(\cdot \mid s,a)\)</span> is a probability measure on <span class="math inline">\(\mathcal{S}\)</span>. For each Borel set <span class="math inline">\(B \subseteq \mathcal{S}\)</span>, the map <span class="math inline">\((s,a) \mapsto P(B \mid s, a)\)</span> is measurable. For example, <span class="math inline">\(P(\mathcal{S} \mid s, a) = 1\)</span> for any <span class="math inline">\((s,a)\)</span>.</p>
<p>The policy kernel <span class="math inline">\(\pi(\cdot \mid s)\)</span> is a stochastic kernel from <span class="math inline">\(\mathcal{S}\)</span> to <span class="math inline">\(\mathcal{A}\)</span>: for each <span class="math inline">\(s\)</span>, <span class="math inline">\(\pi(\cdot \mid s)\)</span> is a probability measure on <span class="math inline">\(\mathcal{A}\)</span>.</p>
<p><strong>Induced State-Transition Kernel.</strong> For notational convenience, given a policy and the environment kernel <span class="math inline">\(P\)</span>, we define a state-only Markov kernel
<span class="math display" id="eq:StateOnlyMarkovKernel">\[\begin{equation}
P^\pi(B \mid s) := \int_{\mathcal{A}} P(B \mid s, a) \pi(da \mid s), \quad B \subseteq \mathcal{S}.
\tag{2.29}
\end{equation}\]</span>
In words, <span class="math inline">\(P^\pi(B \mid s)\)</span> measures the probability of landing at a set <span class="math inline">\(B\)</span> starting from state <span class="math inline">\(s\)</span>, under all actions possible for the policy <span class="math inline">\(\pi\)</span>.</p>
<p>If densities exist, i.e., <span class="math inline">\(P(ds&#39; \mid s, a) = p(s&#39; \mid s, a) ds&#39;\)</span> and <span class="math inline">\(\pi(da \mid s) = \pi(a \mid s) da\)</span>, then,
<span class="math display" id="eq:StateOnlyMarkovKernel-density">\[\begin{equation}
p^\pi(s&#39; \mid s) := \int_{\mathcal{A}} p(s&#39; \mid s, a) \pi(a \mid s) da \quad\text{and}\quad P^{\pi}(d s&#39; \mid s) = p^\pi(s&#39; \mid s) ds&#39;.
\tag{2.30}
\end{equation}\]</span></p>
<p><strong>Stationary State Distribution.</strong> A probability measure <span class="math inline">\(\mu^\pi\)</span> on <span class="math inline">\(\mathcal{S}\)</span> is called <em>stationary</em> for the state-transition kernel <span class="math inline">\(P^\pi\)</span> if and only if
<span class="math display" id="eq:StationaryDistribution-definition">\[\begin{equation}
\mu^{\pi}(B) = \int_{\mathcal{S}} P^\pi(B \mid s) \mu^{\pi}(ds), \quad \forall B \subseteq \mathcal{S}.
\tag{2.31}
\end{equation}\]</span>
If a density <span class="math inline">\(\mu^\pi(s)\)</span> exists, then the above equation is the followng condition
<span class="math display" id="eq:StationaryDistribution-definition-density">\[\begin{equation}
\mu^{\pi}(s&#39;) = \int_{\mathcal{S}} p^\pi(s&#39; \mid s) \mu^{\pi}(s) ds.
\tag{2.32}
\end{equation}\]</span>
In words, the state distribution <span class="math inline">\(\mu^\pi\)</span> does not change under the state-transition kernel <span class="math inline">\(P^\pi\)</span> (e.g., if a state <span class="math inline">\(A\)</span> has probability <span class="math inline">\(0.1\)</span> of being visited at time <span class="math inline">\(t\)</span>, the probability of visiting <span class="math inline">\(A\)</span> in the next time step remains <span class="math inline">\(0.1\)</span>, under policy <span class="math inline">\(\pi\)</span>).
Under standard ergodicity assumptions, this stationary state distribution <span class="math inline">\(\mu^\pi\)</span> exists and is unique (after sufficient steps, the initial state distribution does not matter and the state distribution follows <span class="math inline">\(\mu^\pi\)</span>). Moreover, the empirical state distribution converge to <span class="math inline">\(\mu^\pi\)</span>.</p>
</div>
<div id="policy-evaluation-3" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Policy Evaluation<a href="value-rl.html#policy-evaluation-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For simplicity, let us first relax the state space to be a continuous space <span class="math inline">\(\mathcal{S} \subseteq \mathbb{R}^n\)</span>. We assume the action space <span class="math inline">\(\mathcal{A}\)</span> is still finite with <span class="math inline">\(|\mathcal{A}|\)</span> elements. We first consider the problem of policy evaluation, i.e., estimate the value functions associated with a policy <span class="math inline">\(\pi\)</span> from interaction data with the environment.</p>
<p><strong>Bellman Consistency.</strong> Given a policy <span class="math inline">\(\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})\)</span>, its associated state-value function <span class="math inline">\(V^\pi\)</span> must satisfy the following Bellman Consistency equation
<span class="math display" id="eq:BellmanConsistencyContinuousStateFiniteAction">\[\begin{equation}
V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \left[ R(s, a) + \gamma \int_{\mathcal{S}} V(s&#39;) P(d s&#39; \mid s, a)  \right].
\tag{2.33}
\end{equation}\]</span>
Notice that since <span class="math inline">\(\mathcal{S}\)</span> is a continuous space, we need to replace “<span class="math inline">\(\sum_{s&#39; \in \mathcal{S}}\)</span>” with “<span class="math inline">\(\int_{\mathcal{S}}\)</span>”. If <span class="math inline">\(P(d s&#39; \mid s, a)\)</span> has a density <span class="math inline">\(p(s&#39; \mid s, a)\)</span>, the above Bellman consistency equation also reads
<span class="math display" id="eq:BellmanConsistencyContinuousStateFiniteAction-1">\[\begin{equation}
V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a \mid s) \left[ R(s, a) + \gamma \int_{\mathcal{S}} V(s&#39;) p(s&#39; \mid s, a) ds&#39;  \right].
\tag{2.34}
\end{equation}\]</span></p>
<p><strong>Bellman Operator.</strong> Define the Bellman operator <span class="math inline">\(T^\pi\)</span> acting on any bounded measurable function <span class="math inline">\(V:\mathcal{S}\to\mathbb{R}\)</span> by
<span class="math display" id="eq:BellmanOperator">\[\begin{equation}
(T^\pi V)(s) = \sum_{a\in\mathcal{A}} \pi(a\mid s)\left[ R(s,a) + \gamma \int_{\mathcal{S}} V(s&#39;) P(ds&#39;\mid s,a)\right].
\tag{2.35}
\end{equation}\]</span>
Then <span class="math inline">\(V^\pi\)</span> is the unique fixed point of <span class="math inline">\(T^\pi\)</span>, i.e., <span class="math inline">\(V^\pi = T^\pi V^\pi\)</span>. Moreover, when rewards are uniformly bounded and <span class="math inline">\(\gamma\in[0,1)\)</span>, <span class="math inline">\(T^\pi\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction under the sup-norm and is monotone.</p>
<p><strong>Approximate Value Function.</strong> In large/continuous state spaces we restrict attention to a parametric family <span class="math inline">\({V(\cdot;\theta): \theta\in\mathbb{R}^d}\)</span> and learn <span class="math inline">\(\theta\)</span> from data. We use <span class="math inline">\(\nabla_\theta V(s;\theta) \in \mathbb{R}^d\)</span> to denote the gradient of <span class="math inline">\(V\)</span> with respect to <span class="math inline">\(\theta\)</span> at state <span class="math inline">\(s\)</span>.</p>
<p>A special and very important case is linear function approximation
<span class="math display" id="eq:LinearV">\[\begin{equation}
V(s;\theta) = \theta^\top \phi(s),
\tag{2.36}
\end{equation}\]</span>
where <span class="math inline">\(\phi(s) = [\phi_1(s),\ldots,\phi_d(s)]^\top\)</span> are fixed basis functions (e.g., neural network last-layer features). When <span class="math inline">\(V(s;\theta) = \theta^{\top} \phi(s)\)</span>, we have
<span class="math display">\[
\nabla_\theta V(s;\theta) = \phi(s).
\]</span>
When we restrict the value function to a function class (e.g., linear features or a neural network), it is generally not guaranteed that the unique fixed point of the Bellman operator <a href="value-rl.html#eq:BellmanOperator">(2.35)</a>, namely <span class="math inline">\(V^\pi\)</span>, belongs to that class. This misspecification (or realizability gap) means we typically cannot recover <span class="math inline">\(V^\pi\)</span> exactly; instead, we seek its <em>best approximation</em> according to a chosen criterion.</p>
<!-- For analysis and algorithm design it is convenient to fix a weighting distribution $d$ over $\mathcal{S}$ (e.g., the on-policy stationary visitation density under $\pi$) and define the inner product $\langle f,g\rangle_d := \mathbb{E}_{s\sim d}[f(s)g(s)]$ with norm $|f|_d := \sqrt{\langle f,f\rangle_d}$. The mean-squared value error (MSVE) is
\begin{equation}
J(\theta) ;=; \tfrac12 , | V(\cdot;\theta) - V^\pi(\cdot)|d^2 ;=; \tfrac12,\mathbb{E}{s\sim d}!\left[ \big(V(s;\theta)-V^\pi(s)\big)^2\right].
\tag{#eq:MSVE}
\end{equation}
Let $\mathcal{V} := { V(\cdot;\theta) : \theta\in\mathbb{R}^d }$ denote the function class. The $d$-orthogonal projection $\Pi$ onto $\mathcal{V}$ is
\begin{equation}
(\Pi f) ;:=; \underset{\hat V \in \mathcal{V}}{\arg\min};| \hat V - f|_d .
\tag{#eq:Projection}
\end{equation}
A central object for TD methods is the Projected Bellman Equation (PBE)
\begin{equation}
V(\cdot;\theta^\star) ;=; \Pi , T^\pi , V(\cdot;\theta^\star),
\tag{#eq:PBE}
\end{equation}
whose unique solution (under standard assumptions) is the TD fixed point within $\mathcal{V}$. -->
<div id="monte-carlo-estimation-1" class="section level4 hasAnchor" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> Monte Carlo Estimation<a href="value-rl.html#monte-carlo-estimation-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Given an episode <span class="math inline">\((s_t,a_t,r_t,\dots,s_T)\)</span> collected by policy <span class="math inline">\(\pi\)</span>, its discounted return
<span class="math display">\[
g_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^{T-t} r_T
\]</span>
is an unbiased estimate of the value at <span class="math inline">\(s_t\)</span>, i.e., <span class="math inline">\(V^{\pi}(s_t)\)</span>.</p>
<p>Therefore, Monte Carlo estimation follows the intuitive idea to make the approximate value function <span class="math inline">\(V(\cdot, \theta)\)</span> fit the returns from these episodes as close as possible:
<span class="math display" id="eq:MCEstimationGoal">\[\begin{equation}
\min_{\theta} \frac{1}{\mathcal{D}} \sum_{t \in \mathcal{D}} \frac{1}{2} (g_t - V(s_t, \theta))^2,
\tag{2.37}
\end{equation}\]</span>
where <span class="math inline">\(\mathcal{D}\)</span> denotes the dataset of episodes collected under policy <span class="math inline">\(\pi\)</span>. The formulation <a href="value-rl.html#eq:MCEstimationGoal">(2.37)</a> is a formulation in the sense that it waits until all episodes are collected before performing the optimization.</p>
<p>In an online formulation, we can optimize after every episode the objective function
<span class="math display">\[
\min_{\theta} \frac{1}{2} (g_t - V(s_t, \theta))^2,
\]</span>
which leads to one step of gradient descent:
<span class="math display" id="eq:MCOneStepGD">\[\begin{equation}
\theta \ \ \leftarrow \ \ \theta + \alpha_t (g_t - V(s_t;\theta)) \nabla_\theta V(s_t; \theta).
\tag{2.38}
\end{equation}\]</span>
To connect the above update back to the MC update <a href="value-rl.html#eq:mc-incremental">(2.4)</a> in the tabular case, we see that the term <span class="math inline">\(g_t - V(s_t;\theta\)</span> is similar as before the difference between the target and the current estimate. However, in the case of function approximation, the error is multiplied by the gradient <span class="math inline">\(\nabla_\theta V (s_t; \theta)\)</span>.</p>
<p>It is worth noting that when using function approximation, the update on <span class="math inline">\(\theta\)</span> caused by one episode <span class="math inline">\((s_t,\dots)\)</span> will affect the values at all other states even if the policy only visited state <span class="math inline">\(s_t\)</span>.</p>
<p><strong>Convergence Guarantees.</strong> Assume on-policy sampling under <span class="math inline">\(\pi\)</span>, bounded rewards, and step sizes <span class="math inline">\(\alpha_t\)</span> satisfying Robbins–Monro conditions.</p>
<ul>
<li><p>For <em>linear</em> <span class="math inline">\(V(s;\theta)=\theta^\top\phi(s)\)</span> with full-rank features, i.e.,
<span class="math display">\[
\mathbb{E}_{s \sim \mu^\pi} \left[ \phi(s) \phi(s)^\top \right] \succ 0,
\]</span>
and <span class="math inline">\(\mathbb{E}_{s \sim \mu^\pi}\|\phi(s)\|^2&lt;\infty\)</span>, the iterates converge almost surely to the unique global minimizer of the convex objective
<span class="math display" id="eq:convergence-of-MC">\[\begin{equation}
\theta_{\text{MC}}^\star \in \arg\min_\theta \;  \frac{1}{2}\mathbb{E}_{s_t \sim \mu^\pi}\!\left[ \big(V(s_t;\theta)-V^\pi(s_t)\big)^2 \right],
\tag{2.39}
\end{equation}\]</span>
where the expectation is with respect to stationary state distribution <span class="math inline">\(\mu^\pi\)</span> under <span class="math inline">\(\pi\)</span>.</p></li>
<li><p>For <em>nonlinear</em> differentiable function classes with bounded gradients, the iterates converge almost surely to a stationary point of the same objective.</p></li>
</ul>
</div>
<div id="semi-gradient-td0" class="section level4 hasAnchor" number="2.2.2.2">
<h4><span class="header-section-number">2.2.2.2</span> Semi-Gradient TD(0)<a href="value-rl.html#semi-gradient-td0" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We know from previous discussion that MC uses the full return <span class="math inline">\(g_t\)</span> as the target and thus can have high variance. A straightforward idea is to replace the MC target <span class="math inline">\(g_t\)</span> in the update <a href="value-rl.html#eq:MCOneStepGD">(2.38)</a> by the one-step bootstrap target
<span class="math display">\[
r_t + \gamma V(s_{t+1};\theta),
\]</span>
which yields the <em>semi-gradient TD(0)</em> update
<span class="math display" id="eq:SemiGradientTD0">\[\begin{equation}
\theta \ \leftarrow\ \theta \;+\; \alpha_t \,\big(r_t + \gamma V(s_{t+1};\theta) - V(s_t;\theta)\big)\, \nabla_\theta V(s_t;\theta).
\tag{2.40}
\end{equation}\]</span>
(At terminal <span class="math inline">\(s_{t+1}\)</span>, use <span class="math inline">\(V(s_{t+1};\theta)=0\)</span> or equivalently set <span class="math inline">\(\gamma=0\)</span> for that step.)</p>
<p><strong>Why call it “semi-gradient”?</strong> Let the TD error be
<span class="math display">\[
\delta_t(\theta) \;:=\; r_t + \gamma V(s_{t+1};\theta) - V(s_t;\theta).
\]</span>
Consider the per-sample squared TD error objective
<span class="math display">\[
\min_{\theta}\; \frac{1}{2} \,\delta_t(\theta)^2.
\]</span>
Its <strong>true gradient</strong> (a.k.a. the <em>residual gradient</em>) is
<span class="math display">\[
\nabla_\theta \frac{1}{2} \delta_t(\theta)^2
\;=\;
\delta_t(\theta)\,\big(\gamma \nabla_\theta V(s_{t+1};\theta) - \nabla_\theta V(s_t;\theta)\big).
\]</span>
Thus a <strong>true-gradient (residual-gradient) TD(0)</strong> step would be
<span class="math display" id="eq:ResidualGradientTD0">\[\begin{equation}
\theta \ \leftarrow\ \theta \;-\; \alpha_t \,\delta_t(\theta)\,\big(\gamma \nabla_\theta V(s_{t+1};\theta) - \nabla_\theta V(s_t;\theta)\big).
\tag{2.41}
\end{equation}\]</span></p>
<p>By contrast, the semi-gradient TD(0) step in <a href="value-rl.html#eq:SemiGradientTD0">(2.40)</a> ignores the dependence of the target on <span class="math inline">\(\theta\)</span> (i.e., it drops the <span class="math inline">\(\gamma \nabla_\theta V(s_{t+1};\theta)\)</span> term) and treats the target <span class="math inline">\(r_t+\gamma V(s_{t+1};\theta)\)</span> as a <em>constant</em> when differentiating. Concretely,
<span class="math display">\[
\nabla_\theta \frac{1}{2} \big( \text{target} - V(s_t;\theta)\big)^2
\;\approx\;
-\big(\text{target} - V(s_t;\theta)\big)\,\nabla_\theta V(s_t;\theta).
\]</span>
This approximation yields the simpler update <a href="value-rl.html#eq:SemiGradientTD0">(2.40)</a>.</p>
<p><strong>Convergence Guarantees.</strong> When using linear approximation, the Monte Carlo estimator converges to <span class="math inline">\(\theta^\star_{\text{MC}}\)</span> in <a href="value-rl.html#eq:convergence-of-MC">(2.39)</a>. We now study what the semi-gradient TD(0) updates <a href="value-rl.html#eq:SemiGradientTD0">(2.40)</a> converge to.</p>
<p><strong>Projected Bellman Operator.</strong> Fix a weighting/visitation distribution <span class="math inline">\(\mu\)</span> on <span class="math inline">\(\mathcal S\)</span> (e.g., the stationary distribution <span class="math inline">\(\mu^\pi\)</span>) and the associated inner product
<span class="math display">\[
\langle f,g\rangle_\mu := \mathbb{E}_{s\sim \mu}[\,f(s)g(s)\,],
\qquad
\|f\|_\mu := \sqrt{\langle f,f\rangle_\mu}.
\]</span>
Let <span class="math inline">\(\mathcal V := \{V(s;\theta)=\theta^\top\phi(s)\;:\;\theta\in\mathbb{R}^d\}\)</span> be the linear function class spanned by features <span class="math inline">\(\phi:\mathcal S\to\mathbb{R}^d\)</span>. The <em><span class="math inline">\(\mu\)</span>-orthogonal projection</em> <span class="math inline">\(\Pi_\mu:\mathcal{F}\to\mathcal V\)</span> is
<span class="math display">\[
\Pi_\mu f \;:=\; \arg\min_{V\in\mathcal V}\, \| V - f\|_\mu .
\]</span>
In words, given any function <span class="math inline">\(f \in \mathcal{F}: \mathcal{S} \mapsto \mathbb{R}\)</span>, <span class="math inline">\(\Pi_\mu f\)</span> returns the closest function <span class="math inline">\(V\)</span> to <span class="math inline">\(f\)</span> that belongs to the subset of linearly representable functions <span class="math inline">\(\mathcal{V}\)</span>, where the “closest” is defined by the weighting distribution <span class="math inline">\(\mu\)</span>.
The Projected Bellman Operator is the composition
<span class="math display" id="eq:ProjectedBellmanOperator">\[\begin{equation}
\mathcal{T}^\pi_{\!\text{proj}} \;:=\; \Pi_\mu \, T^\pi,
\qquad\text{i.e.,}\qquad
\big(\mathcal{T}^\pi_{\!\text{proj}} V\big)(\cdot) \;=\; \Pi_\mu \!\left[\, T^\pi V \,\right](\cdot).
\tag{2.42}
\end{equation}\]</span></p>
<ul>
<li><p><span class="math inline">\(T^\pi\)</span> is the Bellman operator defined in <a href="value-rl.html#eq:BellmanOperator">(2.35)</a>.</p></li>
<li><p><span class="math inline">\(\Pi_\mu\)</span> projects any function onto <span class="math inline">\(\mathcal V\)</span> using the <span class="math inline">\(\mu\)</span>-weighted <span class="math inline">\(L^2\)</span> norm.</p></li>
<li><p>In discrete <span class="math inline">\(\mathcal S\)</span>, write <span class="math inline">\(\Phi\in\mathbb{R}^{|\mathcal S|\times d}\)</span> with rows <span class="math inline">\(\phi(s)^\top\)</span> and <span class="math inline">\(D=\mathrm{diag}(\mu(s))\)</span>. Then
<span class="math display">\[
\Pi_\mu f \;=\; \Phi\,(\Phi^\top D \Phi)^{-1}\Phi^\top D f.
\]</span></p></li>
<li><p><span class="math inline">\(T^\pi\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction under <span class="math inline">\(\|\cdot\|_\mu\)</span>, and <span class="math inline">\(\Pi_\mu\)</span> is nonexpansive under <span class="math inline">\(\|\cdot\|_\mu\)</span>, hence <span class="math inline">\(\mathcal{T}^\pi_{\!\text{proj}}\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction:
<span class="math display">\[
\|\Pi_\mu T^\pi V - \Pi_\mu T^\pi U\|_\mu \;\le\; \|T^\pi V - T^\pi U\|_\mu \;\le\; \gamma \|V-U\|_\mu.
\]</span></p></li>
</ul>
<p>Therefore, <a href="value-rl.html#eq:ProjectedBellmanOperator">(2.42)</a>, the projected Bellman equation (PBE), has a unique fixed point <span class="math inline">\(V_{\text{TD}}^\star\in\mathcal V\)</span> satisfying
<span class="math display" id="eq:PBEFixedPoint">\[\begin{equation}
V_{\text{TD}}^\star \;=\; \Pi_\mu\, T^\pi V_{\text{TD}}^\star.
\tag{2.43}
\end{equation}\]</span></p>
<p><strong>Semi-gradient TD(0) Converges to the PBE Fixed Point (linear case).</strong> Assume on-policy sampling under an ergodic chain, bounded second moments, Robbins–Monro stepsizes, and full-rank features under <span class="math inline">\(\mu = \mu^\pi\)</span>. In the linear case <span class="math inline">\(V(s;\theta)=\theta^\top\phi(s)\)</span>, define
<span class="math display">\[
\delta_t(\theta) := r_t + \gamma \theta^\top \phi(s_{t+1}) - \theta^\top \phi(s_t).
\]</span>
The semi-gradient TD(0) update <a href="value-rl.html#eq:SemiGradientTD0">(2.40)</a> becomes
<span class="math display">\[
\theta \leftarrow \theta + \alpha_t\, \delta_t(\theta)\,\phi(s_t).
\]</span>
Taking conditional expectation w.r.t. the stationary visitation (and using the Markov property) yields the mean update:
<span class="math display">\[
\mathbb{E}\!\left[\delta_t(\theta)\,\phi(s_t)\right] \;=\; b \;-\; A\theta,
\]</span>
with the standard TD system
<span class="math display" id="eq:TDLinearSystem-again">\[\begin{equation}
A \;:=\; \mathbb{E}_{\mu}\!\big[\phi(s_t)\big(\phi(s_t)-\gamma \phi(s_{t+1})\big)^\top\big],
\qquad
b \;:=\; \mathbb{E}_{\mu}\!\big[r_{t}\,\phi(s_t)\big].
\tag{2.44}
\end{equation}\]</span>
Thus, in expectation, TD(0) performs a stochastic approximation to the ODE
<span class="math display">\[
\dot\theta \;=\; b - A\theta,
\]</span>
whose unique globally asymptotically stable equilibrium is
<span class="math display">\[
\theta^\star_{\text{TD}} \;=\; A^{-1} b,
\]</span>
provided the symmetric part of <span class="math inline">\(A\)</span> is positive definite (guaranteed on-policy with full-rank features). Standard stochastic approximation theory then gives
<span class="math display">\[
\theta_t \xrightarrow{\text{a.s.}} \theta^\star_{\text{TD}}.
\]</span>
Finally, one can show the equivalence with the PBE: <span class="math inline">\(V(\cdot; \theta) \in\mathcal V\)</span> satisfies <span class="math inline">\(V(\cdot; \theta)=\Pi_\mu T^\pi V(\cdot; \theta)\)</span> if and only if <span class="math inline">\(A\theta \;=\; b\)</span> (see a proof below).
Hence the almost-sure limit <span class="math inline">\(V(\cdot; \theta^\star_{\text{TD}})\)</span> is exactly the fixed point <a href="value-rl.html#eq:PBEFixedPoint">(2.43)</a>.</p>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>The projected Bellman equation reads
<span class="math display">\[
V(\cdot;\theta) = \Pi_\mu T^\pi V(\cdot; \theta).
\]</span>
Since <span class="math inline">\(V(\cdot; \theta) = \theta^\top \phi(\cdot)\)</span> is the orthogonal projection of <span class="math inline">\(T^\pi V(\cdot; \theta)\)</span> onto <span class="math inline">\(\mathcal V\)</span> weighted by <span class="math inline">\(\mu\)</span>, we have that
<span class="math display">\[
\mathbb{E}_\mu \left[ \phi(s_t) (T^\pi V(s_t;\theta) - V(s_t;\theta)) \right] = 0 = \mathbb{E}_\mu \left[ \phi(s_t) (r_t + \gamma \theta^\top \phi(s_{t+1}) - \theta^\top \phi(s_t) )\right],
\]</span>
which reduces to <span class="math inline">\(A \theta = b\)</span> with <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span> defined in <a href="value-rl.html#eq:TDLinearSystem-again">(2.44)</a>.</p>
</div>
</div>
<p><strong>What does convergence to the PBE fixed point imply?</strong></p>
<ul>
<li><p><strong>Best fixed point in the feature subspace (good).</strong> <span class="math inline">\(V_{\text{TD}}^\star\)</span> is the unique function in <span class="math inline">\(\mathcal V\)</span> whose Bellman update <span class="math inline">\(T^\pi V\)</span> projects back to itself under <span class="math inline">\(\Pi_\mu\)</span>. If <span class="math inline">\(V^\pi\in\mathcal V\)</span> (realisable case), then <span class="math inline">\(V_{\text{TD}}^\star=V^\pi\)</span>.</p></li>
<li><p><strong>Different target than least squares (mixed).</strong> TD(0) solves the Projected Bellman Equation <a href="value-rl.html#eq:ProjectedBellmanOperator">(2.42)</a>; Monte Carlo least-squares solves
<span class="math display">\[
\min_{V\in\mathcal V} \frac{1}{2} \|V - V^\pi\|_\mu^2.
\]</span>
When <span class="math inline">\(V^\pi\notin\mathcal V\)</span>, these solutions generally differ. Either can have lower <span class="math inline">\(\mu\)</span>-weighted prediction error depending on features and dynamics; in practice TD often wins due to lower variance and online bootstrapping.</p></li>
</ul>
</div>
</div>
<div id="on-policy-control-1" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> On-Policy Control<a href="value-rl.html#on-policy-control-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="semi-gradient-sarsa0" class="section level4 hasAnchor" number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> Semi-Gradient SARSA(0)<a href="value-rl.html#semi-gradient-sarsa0" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>High-level Intuition.</strong> Semi-gradient SARSA(0) is an on-policy value-based control method. It learns an action-value function <span class="math inline">\(Q(s,a;\theta)\)</span> by bootstrapping one step ahead and using the next action actually selected by the current behavior policy (e.g., <span class="math inline">\(\varepsilon\)</span>-greedy). Because the target uses <span class="math inline">\(Q(s_{t+1},a_{t+1};\theta)\)</span>, SARSA trades some bias for substantially lower variance than Monte Carlo, updates online from each transition, and naturally couples policy evaluation (of the current policy) with policy improvement (by making the policy greedy/soft-greedy w.r.t. the current <span class="math inline">\(Q\)</span>).</p>
<p><strong>Algorithmic Form (On-policy, Finite <span class="math inline">\(\mathcal A\)</span>).</strong> Let the behavior policy at time <span class="math inline">\(t\)</span> be <span class="math inline">\(\pi_t(\cdot\mid s)\)</span> (e.g., <span class="math inline">\(\varepsilon_t\)</span>-greedy w.r.t. <span class="math inline">\(Q(\cdot,\cdot;\theta_t)\)</span>). For each step:</p>
<ol style="list-style-type: decimal">
<li><p>Given <span class="math inline">\(s_t\)</span>, pick <span class="math inline">\(a_t \sim \pi_t(\cdot \mid s_t)\)</span>; observe <span class="math inline">\(r_t\)</span> and <span class="math inline">\(s_{t+1}\)</span>.</p></li>
<li><p>Pick the next action <span class="math inline">\(a_{t+1} \sim \pi_t(\cdot\mid s_{t+1})\)</span>.</p></li>
<li><p>Form the TD error
<span class="math display" id="eq:SARSA-TDerror">\[\begin{equation}
\delta_t \;=\; r_t \;+\; \gamma\, Q(s_{t+1},a_{t+1};\theta) \;-\; Q(s_t,a_t;\theta).
\tag{2.45}
\end{equation}\]</span></p></li>
<li><p>Update parameters with a semi-gradient step
<span class="math display" id="eq:SARSA-Update">\[\begin{equation}
\theta \;\leftarrow\; \theta \;+\; \alpha_t\, \delta_t \,\nabla_\theta Q(s_t,a_t;\theta).
\tag{2.46}
\end{equation}\]</span>
For terminal <span class="math inline">\(s_{t+1}\)</span>, use <span class="math inline">\(Q(s_{t+1},a_{t+1};\theta)=0\)</span> (equivalently, set <span class="math inline">\(\gamma=0\)</span> on terminal transitions).</p></li>
</ol>
<ul>
<li><p><strong>Linear special case.</strong> If <span class="math inline">\(Q(s,a;\theta)=\theta^\top \phi(s,a)\)</span>, then <span class="math inline">\(\nabla_\theta Q(s_t,a_t;\theta)=\phi(s_t,a_t)\)</span> and the update becomes
<span class="math display">\[
\theta \leftarrow \theta + \alpha_t\, \delta_t\, \phi(s_t,a_t).
\]</span></p></li>
<li><p><strong>Expected SARSA (variance reduction).</strong> Replace the sample bootstrap by its expectation under <span class="math inline">\(\pi_t\)</span>:
<span class="math display" id="eq:ExpectedSARSA">\[\begin{equation}
\delta_t^{\text{exp}} \;=\; r_t \;+\; \gamma \sum_{a&#39;\in\mathcal A} \pi_t(a&#39;\mid s_{t+1})\, Q(s_{t+1},a&#39;;\theta) \;-\; Q(s_t,a_t;\theta),
\tag{2.47}
\end{equation}\]</span>
then update <span class="math inline">\(\theta \leftarrow \theta + \alpha_t\, \delta_t^{\text{exp}}\, \nabla_\theta Q(s_t,a_t;\theta)\)</span>.</p></li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Update the next policy to be <span class="math inline">\(\varepsilon_{t+1}\)</span>-greedy w.r.t. the new Q value <span class="math inline">\(Q(\cdot, \cdot; \theta_{t+1})\)</span>. (<span class="math inline">\(\varepsilon_t\)</span> follows GLIE.)</li>
</ol>
<!-- ::: {.examplebox}
::: {.example #MountainCarSemiGradientSARSA name="Semi-Gradient SARSA for Mountain Car"}
Mountain car



:::
::: -->
<!-- **Convergence guarantees.**

- **Prediction (fixed policy, linear function approximation, on-policy).**  
  Assume an ergodic Markov chain under a fixed policy \(\pi\), bounded rewards/features, Robbins–Monro stepsizes \(\sum_t \alpha_t=\infty\), \(\sum_t \alpha_t^2<\infty\), and **full-rank features** under the visitation distribution \(d^\pi\). Let
  \[
  A \;:=\; \mathbb{E}_{d^\pi}\!\Big[\phi(S_t,A_t)\big(\phi(S_t,A_t) - \gamma\,\phi(S_{t+1},A_{t+1})\big)^\top\Big],\quad
  b \;:=\; \mathbb{E}_{d^\pi}\!\big[ R_t\, \phi(S_t,A_t)\big],
  \tag{\#eq:SARSA-LinearSystem}
  \]
  where \(A_t\sim\pi(\cdot\mid S_t)\) and \(A_{t+1}\sim\pi(\cdot\mid S_{t+1})\). Then semi-gradient SARSA(0) is a stochastic approximation to \(\dot\theta = b - A\theta\) and
  \[
  \theta_t \xrightarrow{\text{a.s.}} \ \theta^\star \;=\; A^{-1}b,
  \]
  so \(Q(\cdot,\cdot;\theta^\star)\) is the unique solution of the **Projected Bellman Equation** for action-values:
  \[
  Q(\cdot,\cdot;\theta^\star) \;=\; \Pi_d \, T^\pi Q(\cdot,\cdot;\theta^\star),
  \]
  where \(\Pi_d\) is the \(d^\pi\)-orthogonal projection onto the linear span of \(\phi\), and \(T^\pi\) is the action-value Bellman operator.

- **Control (changing policy via \(\varepsilon\)-greedy).**  
  In the **tabular** case, with GLIE exploration and suitable stepsizes, SARSA(0) converges to an **optimal policy**. With **function approximation**, general convergence to the globally optimal policy is **not guaranteed**; however, on-policy semi-gradient SARSA is typically **stable** and converges to a fixed point of the projected Bellman operator for the (slowly varying/limiting) policies. Using **Expected SARSA** often improves stability by reducing target variance.

- **Nonlinear approximation.**  
  For general nonlinear \(Q(s,a;\theta)\), global convergence guarantees are lacking; the algorithm may converge to a local fixed point or may diverge. In practice, standard stabilizers (target networks, replay, regularization) are often employed.

**Remarks.**  
- SARSA is **on-policy**: it evaluates and improves the *same* behavior policy. This avoids the classic off-policy instability (“deadly triad”) seen in Q-learning with function approximation.  
- Compared to Monte Carlo control, SARSA learns **online** with **lower variance** per update; compared to off-policy Q-learning, it is usually more **stable** with function approximation but may converge more slowly to near-greedy behavior. -->

</div>
</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-kearns2000bias" class="csl-entry">
Kearns, Michael J, and Satinder Singh. 2000. <span>“Bias-Variance Error Bounds for Temporal Difference Updates.”</span> In <em>COLT</em>, 142–47.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mdp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="policy-gradient.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/02-value-rl.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
