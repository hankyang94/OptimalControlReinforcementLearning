# Markov Decision Process {#mdp}

Optimal control (OC) and reinforcement learning (RL) address the problem of making **optimal decisions** in the presence of a **dynamic environment**.  

- In **optimal control**, this dynamic environment is often referred to as a *plant* or a *dynamical system*.  
- In **reinforcement learning**, it is modeled as a *Markov decision process* (MDP).  

The goal in both fields is to evaluate and design decision-making strategies that optimize long-term performance:  

- **RL** typically frames this as maximizing a long-term *reward*.  
- **OC** often formulates it as minimizing a long-term *cost*.  

The emphasis on **long-term** evaluation is crucial. Because the environment evolves over time, decisions that appear beneficial in the short term may lead to poor long-term outcomes and thus be suboptimal.  

---

With this motivation, we now formalize the framework of Markov Decision Processes (MDPs), which are discrete-time stochastic dynamical systems. 


## Finite-Horizon MDP {#FiniteHorizonMDP}

We begin with finite-horizon MDPs and introduce infinite-horizon MDPs in the following section. An abstract definition of the finite-horizon case will be presented first, followed by illustrative examples.

A finite-horizon MDP is given by the following tuple:
$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, T),
$$
where

- $\mathcal{S}$: state space (set of all possible states)
- $\mathcal{A}$: action space (set of all possible actions)
- $P(s' \mid s, a)$: probability of transitioning to state $s'$ from state $s$ under action $a$ (i.e., dynamics)
- $R(s,a)$: reward of taking action $a$ in state $s$
- $T$: horizon, a positive integer

For now, let us assume both the state space and the action space are discrete and have a finite number of elements. In particular, denote the number of elements in $\mathcal{S}$ as $|\mathcal{S}|$, and the number of elements in $\mathcal{A}$ as $|\mathcal{A}|$. This is also referred to as a _tabular MDP_. 

**Policy**. Decision-making in MDPs is represented by policies. A policy is a function that, given any state, outputs a distribution of actions: $\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})$. That is, $\pi(a \mid s)$ returns the probability of taking action $a$ in state $s$. In finite-horizon MDPs, we consider a tuple of policies:
\begin{equation}
\pi = (\pi_0, \dots, \pi_t, \dots, \pi_{T-1}),
(\#eq:policy-tuple)
\end{equation}
where each $\pi_t$ denotes the policy at step $t \in [0,T-1]$.


**Trajectory and Return**. Given an initial state $s_0 \in \mathcal{S}$ and a policy $\pi$, the MDP will evolve as

1. Start at state $s_0$
2. Take action $a_0 \sim \pi_0(a \mid s_0)$ following policy $\pi_0$
3. Collect reward $r_0 = R(s_0, a_0)$ (assume $R$ is deterministic)
4. Transition to state $s_1 \sim P(s' \mid s_0, a_0)$ following the dynamics
5. Go to step 2 and continue until reaching state $s_T$

This evolution generates a trajectory of states, actions, and rewards:
$$
\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T).
$$
The cumulative reward of this trajectory is $g_0 = \sum_{t=0}^{T-1} r_t$, which is called the _return_ of the trajectory. Clearly, $g_0$ is a random variable due to the stochasticity of both the policy and the dynamics. Similarly, if the state at time $t$ is $s_t$, we denote:
$$
g_t = r_t + \dots + r_{T-1}
$$
as the return of the policy starting at $s_t$. 

### Value Functions {#FiniteHorizonMDP-Value}

**State-Value Function**. Given a policy $\pi$ as in \@ref(eq:policy-tuple), which states are preferable at time $t$? The (time-indexed) state-value function assigns to each $s\in\mathcal{S}$ the expected return from $t$ onward when starting in $s$ and following $\pi$ thereafter. Formally, define
\begin{equation}
V_t^\pi(s) := \mathbb{E} \left[g_t \mid s_t=s\right]
= \mathbb{E} \left[\sum_{i=t}^{T-1} R(s_i,a_i) \middle| s_t=s,a_i\sim \pi_i(\cdot\mid s_i), s_{i+1}\sim P(\cdot\mid s_i,a_i)\right].
(\#eq:FiniteHorizonMDP-state-value)
\end{equation}
The expectation is over the randomness induced by both the policy and the dynamics. Thus, if $V_t^\pi(s_1)>V_t^\pi(s_2)$, then at time $t$ under policy $\pi$ it is better in expectation to be in $s_1$ than in $s_2$ because the former yields a larger expected return.

::: {.highlightbox}
$V^{\pi}_t(s)$: given policy $\pi$, how good is it to start in state $s$ at time $t$?
:::

**Action-Value Function**. Similarly, the action-value function assigns to each state-action pair $(s,a)\in\mathcal{S}\times\mathcal{A}$ the expected return obtained by starting in state $s$, taking action $a$ first, and then following policy $\pi$ thereafter:
\begin{equation}
\begin{split}
Q_t^\pi(s,a) := & \mathbb{E} \left[R(s,a) + g_{t+1} \mid s_{t+1} \sim P(\cdot \mid s,a)\right] \\
= & \mathbb{E} \left[R(s,a) + \sum_{i=t+1}^{T-1} R(s_i, a_i) \middle| s_{t+1} \sim P(\cdot \mid s,a) \right].
\end{split}
(\#eq:FiniteHorizonMDP-action-value)
\end{equation}
The key distinction is that the action-value function evaluates the return when the first action may deviate from policy $\pi$, whereas the state-value function assumes strict adherence to $\pi$. This flexibility makes the action-value function central to improving $\pi$, since it reveals whether alternative actions can yield higher returns.

::: {.highlightbox}
$Q^{\pi}_t(s,a)$: At time $t$, how good is it to take action $a$ in state $s$, then follow the policy $\pi$?
:::

It is easy to verify that the state-value function and the action-value function satisfy:
\begin{align}
V_t^{\pi}(s) & = \sum_{a \in \mathcal{A}} \pi_t(a \mid s) Q_t^{\pi}(s,a), (\#eq:FiniteHorizonMDP-state-value-from-action-value) \\
Q_t^{\pi}(s,a) & = R(s,a) + \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^{\pi}_{t+1} (s'). (\#eq:FiniteHorizonMDP-action-value-from-state-value)
\end{align}

From these two equations, we can derive the Bellman Consistency equations.

::: {.theorembox}
::: {.proposition #BellmanConsistency name="Bellman Consistency"}
The state-value function $V^{\pi}_t(\cdot)$ in \@ref(eq:FiniteHorizonMDP-state-value) satisfies the following recursion:
\begin{equation}
\begin{split}
V^{\pi}_t(s) & = \sum_{a \in \mathcal{A}} \pi_t(a\mid s) \left( R(s,a) + \sum_{s' \in \mathcal{S}} P(s' \mid s, a) V^{\pi}_{t+1} (s') \right) \\
    & =: \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s' \sim P(\cdot \mid s, a)} [V^{\pi}_{t+1}(s')] \right].
\end{split}
(\#eq:BellmanConsistency-State-Value)
\end{equation}

Similarly, the action-value function $Q^{\pi}_t(s,a)$ in \@ref(eq:FiniteHorizonMDP-action-value) satisfies the following recursion:
\begin{equation}
\begin{split}
Q^{\pi}_t (s, a) & = R(s,a) + \sum_{s' \in \mathcal{S}} P(s' \mid s, a) \left( \sum_{a' \in \mathcal{A}} \pi_{t+1}(a' \mid s') Q^{\pi}_{t+1}(s', a')\right) \\
& =: R(s, a) + \mathbb{E}_{s' \sim P(\cdot \mid s, a)} \left[\mathbb{E}_{a' \sim \pi_{t+1}(\cdot \mid s')} [Q^{\pi}_{t+1}(s', a')] \right].
\end{split}
(\#eq:BellmanConsistency-Action-Value)
\end{equation}
:::
:::

### Policy Evaluation

The Bellman consistency result in Proposition \@ref(prp:BellmanConsistency) is fundamental because it directly yields an algorithm for evaluating a given policy $\pi$---that is, for computing its state-value and action-value functions---provided the transition dynamics of the MDP are known.  

Policy evaluation for the state-value function proceeds as follows:

- **Initialization:** set $V^{\pi}_T(s) = 0$ for all $s \in \mathcal{S}$.  
- **Backward recursion:** for $t = T-1, T-2, \dots, 0$, update each $s \in \mathcal{S}$ by
\[
V^{\pi}_{t}(s) = \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s' \sim P(\cdot \mid s, a)} \big[ V^{\pi}_{t+1}(s') \big] \right].
\]

Similarly, policy evaluation for the action-value function is given by:

- **Initialization:** set $Q^{\pi}_T(s,a) = 0$ for all $s \in \mathcal{S}, a \in \mathcal{A}$.  
- **Backward recursion:** for $t = T-1, T-2, \dots, 0$, update each $(s,a) \in \mathcal{S}\times\mathcal{A}$ by
\[
Q^{\pi}_t(s,a) = R(s, a) + \mathbb{E}_{s' \sim P(\cdot \mid s, a)} \left[ \mathbb{E}_{a' \sim \pi_{t+1}(\cdot \mid s')} \big[ Q^{\pi}_{t+1}(s', a') \big] \right].
\]

The essential feature of this algorithm is its backward-in-time recursion: the value functions are first set at the terminal horizon $T$, and then propagated backward step by step through the Bellman consistency equations.

::: {.examplebox}
::: {.example #MDPExampleGraph name="MDP, Transition Graph, and Policy Evaluation"}
It is often useful to visualize small MDPs as transition graphs, where states are represented by nodes and actions are represented by directed edges connecting those nodes.

As a simple illustrative example, consider a robot navigating on a two-state grid. At each step, the robot can either Stay in its current state or Move to the other state. This finite-horizon MDP is fully specified by the tuple of states, actions, transition dynamics, rewards, and horizon:

- States: $\mathcal{S} = \{\alpha, \beta \}$

- Actions: $\mathcal{A} = \{\text{Move} , \text{Stay} \}$

- Transition dynamics: we can specify the transition dynamics in the following table

| State $s$ | Action $a$ | Next State $s'$ | Probability $P(s' \mid s, a)$ |
|:-----:|:------:|:----------:|:------:|
| $\alpha$    | Stay    |    $\alpha$      |  1     |
| $\alpha$    | Move    |  $\beta$         | 1    |
| $\beta$    | Stay    |  $\beta$          | 1      |
| $\beta$    | Move    |  $\alpha$         | 1     |

- Reward: $R(s,a)=1$ if $a = \text{Move}$ and $R(s,a)=0$ if $a = \text{Stay}$

- Horizon: $T=2$.

This MDP can be represented by the transition graph in Fig. \@ref(fig:mdp-robot-transition-graph). Note that for this MDP, the transition dynamics is deterministic. We will see a stochastic MDP soon.

```{r mdp-robot-transition-graph, out.width='90%', fig.show='hold', fig.cap='A Simple Transition Graph.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/MDP/robot-mdp-graph.png')
```

At time $t=0$, if the robot starts at $s_0 = \alpha$, first chooses action $a_0 = \text{Move}$, and then chooses action $a_1 = \text{Stay}$, the resulting trajectory is
$$
\tau = (\alpha, \text{Move}, +1, \beta, \text{Stay}, 0,  \beta).
$$
The return of this trajectory is:
$$
g_0 = +1 + 0 = +1.
$$

**Policy Evaluation**. Given a policy
\begin{equation}
\pi = (\pi_0, \pi_1), \quad \pi_0(a \mid s) = \begin{cases}
0.5 & a = \text{Move} \\
0.5 & a = \text{Stay}
\end{cases}, 
\quad 
\pi_1( a \mid s) = \begin{cases}
0.8 & a = \text{Move} \\
0.2 & a = \text{Stay}
\end{cases}.
\end{equation}
We can use the Bellman consistency equations to compute the state-value function. We first initialize:
$$
V^{\pi}_2 = \begin{bmatrix}
0 \\ 0
\end{bmatrix},
$$
where the first row contains the value at $s = \alpha$ and the second row contains the value at $s = \beta$.
We then perform the backward recursion for $t=1$. For $s = \alpha$, we have
\begin{equation}
V^{\pi}_1(\alpha) = \begin{bmatrix}
\pi_1(\text{Move} \mid \alpha) \\
\pi_1(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_2(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_2(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8 
\end{equation}
For $s = \beta$, we have
\begin{equation}
V^{\pi}_1(\beta) = \begin{bmatrix}
\pi_1(\text{Move} \mid \beta) \\
\pi_1(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_2(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_2(\beta)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8.
\end{equation}
Therefore, we have
$$
V^{\pi}_1 = \begin{bmatrix}
0.8 \\ 0.8
\end{bmatrix}.
$$
We then proceed to the backward recursion for $t=0$:
\begin{align}
V_0^{\pi}(\alpha) & = \begin{bmatrix}
\pi_0(\text{Move} \mid \alpha) \\
\pi_0(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_1(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_1(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3. \\
V_0^{\pi}(\beta) & = \begin{bmatrix}
\pi_0(\text{Move} \mid \beta) \\
\pi_0(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_0(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_0(\beta)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3.
\end{align}
Therefore, the state-value function at $t=0$ is
$$
V^{\pi}_0 = \begin{bmatrix}
1.3 \\ 1.3
\end{bmatrix}.
$$
You are encouraged to carry out the similar calculations for the action-value function.

---

The toy example was small enough to carry out policy evaluation by hand; in realistic MDPs, we will need the help from computers.

Consider now an MDP whose transition graph is shown in Fig. \@ref(fig:mdp-hangover-transition-graph). This example is adapted from [here](https://github.com/upb-lea/reinforcement_learning_course_materials/blob/master/exercises/solutions/ex02/Ex2.ipynb).

```{r mdp-hangover-transition-graph, out.width='90%', fig.show='hold', fig.cap='Hangover Transition Graph.', fig.align='center', echo=FALSE}
knitr::include_graphics('images/MDP/hangover-graph.png')
```

This MDP has six states:
$$
\mathcal{S} = \{\text{Hangover}, \text{Sleep}, \text{More Sleep}, \text{Visit Lecture}, \text{Study}, \text{Pass Exam} \},
$$
and two actions:
$$
\mathcal{A} = \{\text{Lazy}, \text{Productive} \}.
$$
The stochastic transition dynamics are labeled in the transition graph. For example, at state "Hangover", taking action "Productive" will lead to state "Visit Lecture" with probability $0.3$ and state "Hangover" with probability $0.7$.
The rewards of the MDP are defined as:
$$
R(s,a) = \begin{cases}
+1 & s = \text{Pass Exam} \\
-1 & \text{otherwise}.
\end{cases}.
$$

**Policy Evaluation**. Consider a time-invariant random policy
$$
\pi = \{\pi_0,\dots,\pi_{T-1} \}, \quad \pi_t(a \mid s) = \begin{cases}
\alpha & a = \text{Lazy} \\
1 - \alpha & a = \text{Productive}
\end{cases},
$$
that takes "Lazy" with probability $\alpha$ and "Productive" with probability $1-\alpha$. 

The following Python code performs policy evaluation for this MDP, with $T=10$ and $\alpha = 0.4$.

```Python
# Finite-horizon policy evaluation for the Hangover MDP

from collections import defaultdict
from typing import Dict, List, Tuple

State = str
Action = str

# --- MDP spec ---------------------------------------------------------------

S: List[State] = [
    "Hangover", "Sleep", "More Sleep", "Visit Lecture", "Study", "Pass Exam"
]
A: List[Action] = ["Lazy", "Productive"]

# P[s, a] -> list of (s_next, prob)
P: Dict[Tuple[State, Action], List[Tuple[State, float]]] = {
    # Hangover
    ("Hangover", "Lazy"):       [("Sleep", 1.0)],
    ("Hangover", "Productive"): [("Visit Lecture", 0.3), ("Hangover", 0.7)],

    # Sleep
    ("Sleep", "Lazy"):          [("More Sleep", 1.0)],
    ("Sleep", "Productive"):    [("Visit Lecture", 0.6), ("More Sleep", 0.4)],

    # More Sleep
    ("More Sleep", "Lazy"):       [("More Sleep", 1.0)],
    ("More Sleep", "Productive"): [("Study", 0.5), ("More Sleep", 0.5)],

    # Visit Lecture
    ("Visit Lecture", "Lazy"):       [("Study", 0.8), ("Pass Exam", 0.2)],
    ("Visit Lecture", "Productive"): [("Study", 1.0)],

    # Study
    ("Study", "Lazy"):         [("More Sleep", 1.0)],
    ("Study", "Productive"):   [("Pass Exam", 0.9), ("Study", 0.1)],

    # Pass Exam (absorbing)
    ("Pass Exam", "Lazy"):       [("Pass Exam", 1.0)],
    ("Pass Exam", "Productive"): [("Pass Exam", 1.0)],
}

def R(s: State, a: Action) -> float:
    """Reward: +1 in Pass Exam, -1 otherwise."""
    return 1.0 if s == "Pass Exam" else -1.0

# --- Policy: time-invariant, state-independent ------------------------------

def pi(a: Action, s: State, alpha: float) -> float:
    """π(a|s): Lazy with prob α, Productive with prob 1-α."""
    return alpha if a == "Lazy" else (1.0 - alpha)

# --- Policy evaluation -------------------------------------------------------

def policy_evaluation(T: int, alpha: float):
    """
    Compute {V_t(s)} and {Q_t(s,a)} for t=0..T with terminal condition V_T = Q_T = 0.
    Returns:
        V: Dict[int, Dict[State, float]]
        Q: Dict[int, Dict[Tuple[State, Action], float]]
    """
    assert T >= 0
    # sanity: probabilities sum to 1 for each (s,a)
    for key, rows in P.items():
        total = sum(p for _, p in rows)
        if abs(total - 1.0) > 1e-9:
            raise ValueError(f"Probabilities for {key} sum to {total}, not 1.")

    V: Dict[int, Dict[State, float]] = defaultdict(dict)
    Q: Dict[int, Dict[Tuple[State, Action], float]] = defaultdict(dict)

    # Terminal boundary
    for s in S:
        V[T][s] = 0.0
        for a in A:
            Q[T][(s, a)] = 0.0

    # Backward recursion
    for t in range(T - 1, -1, -1):
        for s in S:
            # First compute Q_t(s,a)
            for a in A:
                exp_next = sum(p * V[t + 1][s_next] for s_next, p in P[(s, a)])
                Q[t][(s, a)] = R(s, a) + exp_next
            # Then V_t(s) = E_{a~π}[Q_t(s,a)]
            V[t][s] = sum(pi(a, s, alpha) * Q[t][(s, a)] for a in A)

    return V, Q

# --- Example run -------------------------------------------------------------

if __name__ == "__main__":
    T = 10        # horizon
    alpha = 0.4  # probability of choosing Lazy
    V, Q = policy_evaluation(T=T, alpha=alpha)

    # Print V_0
    print(f"V_0(s) with T={T}, alpha={alpha}:")
    for s in S:
        print(f"  {s:13s}: {V[0][s]: .3f}")
```

The code returns the following state values at $t=0$:
\begin{equation}
V^{\pi}_0 = \begin{bmatrix}
-3.582 \\ -2.306 \\ -2.180 \\ 1.757 \\ 2.939 \\ 10
\end{bmatrix},
(\#eq:HangoverRandomValueFunction)
\end{equation}
where the ordering of the states follows that defined in $\mathcal{S}$.

You can find the code [here](https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/hangover_policy_evaluation.py).
:::
:::


## Principle of Optimality {#optimality}

Every policy $\pi$ induces a value function $V_0^{\pi}$ that can be evaluated by policy evaluation (assuming the transition dynamics are known). The goal of reinforcement learning is to find an optimal policy that maximizes the value function with respect to a given initial state distribution:
\begin{equation}
V^\star_0 = \max_{\pi}\; \mathbb{E}_{s_0 \sim \mu(\cdot)} \big[ V_0^{\pi}(s_0) \big],
(\#eq:FiniteHorizonMDPRLProblem)
\end{equation}
where we have used the superscript "$\star$" to denote the optimality of the value function. $V^\star_0$ is often known as the _optimal value function_.

At first glance, \@ref(eq:FiniteHorizonMDPRLProblem) appears daunting: a naive approach would enumerate all stochastic policies $\pi$, evaluate their value functions, and select the best. A central result in reinforcement learning and optimal control—rooted in the principle of optimality—is that the _optimal_ value functions satisfy a Bellman-style recursion, analogous to Proposition \@ref(prp:BellmanConsistency). This Bellman optimality recursion enables backward computation of the optimal value functions without enumerating policies.


::: {.theorembox}
::: {.theorem #FiniteHorizonMDPBellmanOptimality name="Bellman Optimality (Finite Horizon, State-Value)"}

Consider a finite-horizon MDP $\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,T)$ with finite state and action sets and bounded rewards.
Define the optimal value functions $\{V_t^\star\}_{t=0}^{T}$ by the following _Bellman optimality_ recursion
\begin{equation}
\begin{split}
V_T^\star(s)& \equiv 0, \\
V_t^\star(s)& = \max_{a\in\mathcal{A}}\Big\{ R(s,a)\;+\;\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^\star(s')\Big\},\ t=T-1,\ldots,0.
\end{split}
(\#eq:FiniteHorizonMDPBellmanOptimality)
\end{equation}
Then, the optimal value functions are optimal in the sense of _statewise dominance_:
\begin{equation}
V_t^{\star}(s)\;\ge\; V_t^{\pi}(s)
\quad\text{for all policies }\pi,\; s\in\mathcal{S},\; t=0,\ldots,T.
(\#eq:FiniteHorizonMDPStatewiseDominance)
\end{equation}

Moreover, the deterministic policy 
$\pi^\star=(\pi^\star_0,\ldots,\pi^\star_{T-1})$ with
\begin{equation}
\begin{split}
\pi_t^\star(s)\;\in\;\arg\max_{a\in\mathcal{A}}
\Big\{ R(s,a)\;+\;\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^\star(s')\Big\}, \\
\text{for any } s\in\mathcal{S},\; t=0,\dots,T-1
\end{split}
(\#eq:FiniteHorizonMDPOptimalPolicy)
\end{equation}
is optimal, where ties can be broken by any fixed rule.
:::
:::

::: {.proofbox}
::: {.proof}
We first show that the value functions defined by the Bellman optimality recursion \@ref(eq:FiniteHorizonMDPBellmanOptimality) are _optimal_ in the sense that they dominate the value functions of any other policy. The proof proceeds by backward induction.

**Base case ($t=T$).**
For every $s\in\mathcal{S}$,
\[
V^\star_T(s)\;=\;0\;=\;V_T^{\pi}(s),
\]
so $V^\star_T(s)\ge V_T^{\pi}(s)$ holds trivially.

**Inductive step.**
Assume $V^\star_{t+1}(s)\ge V^{\pi}_{t+1}(s)$ for all $s\in\mathcal{S}$. Then, for any $s\in\mathcal{S}$,
\begin{align*}
V_t^{\pi}(s)
&= \sum_{a\in\mathcal{A}} \pi_t(a\mid s)\!\left(R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^{\pi}(s')\right) \\
&\le \sum_{a\in\mathcal{A}} \pi_t(a\mid s)\!\left(R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^{\star}(s')\right) \\
&\le \max_{a\in\mathcal{A}} \left(R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^{\star}(s')\right)
\;=\; V_t^\star(s),
\end{align*}
where the first inequality uses the induction hypothesis and the second uses that an expectation is bounded above by a maximum. Hence $V_t^\star(s)\ge V_t^{\pi}(s)$ for all $s$, completing the induction. Therefore, $\{V_t^\star\}_{t=0}^T$ dominates the value functions attainable by any policy.


Next, we show that $\{V_t^\star\}$ is _attainable_ by some policy. Since $\mathcal{A}$ is finite (tabular setting), the maximizer in the Bellman optimality operator exists for every $(t,s)$; thus we can define a (deterministic) greedy policy
\[
\pi_t^\star(s)\;\in\;\arg\max_{a\in\mathcal{A}}
\Big\{ R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^\star(s') \Big\}.
\]
A simple backward induction then shows $V_t^{\pi^\star}(s)=V_t^\star(s)$ for all $t$ and $s$: at $t=T$ both are $0$, and if $V_{t+1}^{\pi^\star}=V_{t+1}^\star$, then by construction of $\pi_t^\star$ the Bellman equality yields $V_t^{\pi^\star}=V_t^\star$. Consequently, the optimal value functions are achieved by the greedy (deterministic) policy $\pi^\star$.
:::
:::



::: {.theorembox}
::: {.corollary #FiniteHorizonMDPBellmanOptimalityActionValue name="Bellman Optimality (Finite Horizon, Action-Value)"}
Given the optimal (state-)value functions $V^{\star}_{t},t=0,\dots,T$, define the optimal action-value function
\begin{equation}
Q_t^\star(s,a)\;=\;R(s,a)+\sum_{s'\in\mathcal{S}} P(s'\mid s,a)\,V_{t+1}^\star(s'), \quad t=0,\dots,T-1.
(\#eq:FiniteHorizonMDPOptimalActionValue)
\end{equation}

Then we have 
\begin{equation}
V_t^\star(s)=\max_{a\in\mathcal{A}} Q_t^\star(s,a),\qquad
\pi_t^\star(s)\in\arg\max_{a\in\mathcal{A}} Q_t^\star(s,a).
(\#eq:FiniteHorizonMDPStateValueActionValue)
\end{equation}

The optimal action-value functions satisfy:
\begin{equation}
\begin{split}
Q_T^\star(s,a) & \equiv 0,\\
Q_t^\star(s,a)
& = R(s,a) \;+\; \mathbb{E}_{s'\sim P(\cdot\mid s,a)}
\!\left[ \max_{a'\in\mathcal{A}} Q_{t+1}^\star(s',a') \right],
\quad t=T-1,\ldots,0.
\end{split}
(\#eq:FiniteHorizonMDPBellmanRecursionActionValue)
\end{equation}
:::
:::


## Dynamic Programming {#dp}

The principle of optimality in Theorem \@ref(thm:FiniteHorizonMDPBellmanOptimality) yields a constructive procedure to compute the optimal value functions and an associated deterministic optimal policy. This backward-induction procedure is the *dynamic programming* (DP) algorithm.

**Dynamic programming (finite horizon).**

- **Initialization.** Set $V_T^\star(s) = 0$ for all $s \in \mathcal{S}$.

- **Backward recursion.** For $t = T-1, T-2, \dots, 0$:

  - *Optimal value:* for each $s \in \mathcal{S}$,
  $$
  V_t^\star(s)
  = \max_{a \in \mathcal{A}}
  \left\{
    R(s,a) + \mathbb{E}_{s' \sim P(\cdot \mid s,a)} \big[ V_{t+1}^\star(s') \big]
  \right\}.
  $$

  - *Greedy policy (deterministic):* for each $s \in \mathcal{S}$,
  $$
  \pi_t^\star(s) \in \arg\max_{a \in \mathcal{A}}
  \left\{
    R(s,a) + \mathbb{E}_{s' \sim P(\cdot \mid s,a)} \big[ V_{t+1}^\star(s') \big]
  \right\}.
  $$


::: {.exercisebox}
::: {.exercise}
How does dynamic programming look like when applied to the action-value function? 
:::
:::

::: {.exercisebox}
::: {.exercise}
What is the computational complexity of dynamic programming? 
:::
:::


Let us try dynamic programming for the Hangover MDP presented before.

::: {.examplebox}
::: {.example #HangoverDynamicProgramming name="Dynamic Programming for Hangover MDP"}
Consider the Hangover MDP defined by the transition graph shown in Fig. \@ref(fig:mdp-hangover-transition-graph). With slight modification to the policy evaluation code, we can find the optimal value functions and optimal policies.

```Python
# Dynamic programming (finite-horizon optimal control) for the Hangover MDP

from collections import defaultdict
from typing import Dict, List, Tuple

State = str
Action = str

# --- MDP spec ---------------------------------------------------------------

S: List[State] = [
    "Hangover", "Sleep", "More Sleep", "Visit Lecture", "Study", "Pass Exam"
]
A: List[Action] = ["Lazy", "Productive"]

# P[s, a] -> list of (s_next, prob)
P: Dict[Tuple[State, Action], List[Tuple[State, float]]] = {
    # Hangover
    ("Hangover", "Lazy"):       [("Sleep", 1.0)],
    ("Hangover", "Productive"): [("Visit Lecture", 0.3), ("Hangover", 0.7)],

    # Sleep
    ("Sleep", "Lazy"):          [("More Sleep", 1.0)],
    ("Sleep", "Productive"):    [("Visit Lecture", 0.6), ("More Sleep", 0.4)],

    # More Sleep
    ("More Sleep", "Lazy"):       [("More Sleep", 1.0)],
    ("More Sleep", "Productive"): [("Study", 0.5), ("More Sleep", 0.5)],

    # Visit Lecture
    ("Visit Lecture", "Lazy"):       [("Study", 0.8), ("Pass Exam", 0.2)],
    ("Visit Lecture", "Productive"): [("Study", 1.0)],

    # Study
    ("Study", "Lazy"):         [("More Sleep", 1.0)],
    ("Study", "Productive"):   [("Pass Exam", 0.9), ("Study", 0.1)],

    # Pass Exam (absorbing)
    ("Pass Exam", "Lazy"):       [("Pass Exam", 1.0)],
    ("Pass Exam", "Productive"): [("Pass Exam", 1.0)],
}

def R(s: State, a: Action) -> float:
    """Reward: +1 in Pass Exam, -1 otherwise."""
    return 1.0 if s == "Pass Exam" else -1.0

# --- Dynamic programming (Bellman optimality) -------------------------------

def dynamic_programming(T: int):
    """
    Compute optimal finite-horizon tables:
      - V[t][s] = V_t^*(s)
      - Q[t][(s,a)] = Q_t^*(s,a)
      - PI[t][s] = optimal action at (t,s)
    with terminal condition V_T^* = 0.
    """
    assert T >= 0

    # sanity: probabilities sum to 1 for each (s,a)
    for key, rows in P.items():
        total = sum(p for _, p in rows)
        if abs(total - 1.0) > 1e-9:
            raise ValueError(f"Probabilities for {key} sum to {total}, not 1.")

    V: Dict[int, Dict[State, float]] = defaultdict(dict)
    Q: Dict[int, Dict[Tuple[State, Action], float]] = defaultdict(dict)
    PI: Dict[int, Dict[State, Action]] = defaultdict(dict)

    # Terminal boundary
    for s in S:
        V[T][s] = 0.0
        for a in A:
            Q[T][(s, a)] = 0.0

    # Backward recursion (Bellman optimality)
    for t in range(T - 1, -1, -1):
        for s in S:
            # compute Q*_t(s,a)
            for a in A:
                exp_next = sum(p * V[t + 1][s_next] for s_next, p in P[(s, a)])
                Q[t][(s, a)] = R(s, a) + exp_next

            # greedy action and optimal value
            # tie-breaking is deterministic by the order in A
            best_a = max(A, key=lambda a: Q[t][(s, a)])
            PI[t][s] = best_a
            V[t][s] = Q[t][(s, best_a)]

    return V, Q, PI

# --- Example run -------------------------------------------------------------

if __name__ == "__main__":
    T = 10  # horizon
    V, Q, PI = dynamic_programming(T=T)

    print(f"Optimal V_0(s) with T={T}:")
    for s in S:
        print(f"  {s:13s}: {V[0][s]: .3f}")

    print("\nGreedy policy at t=0:")
    for s in S:
        print(f"  {s:13s}: {PI[0][s]}")

    print("\nAction value at t=0:")
    for s in S:
        print(f"  {s:13s}: {Q[0][s, A[0]]: .3f}, {Q[0][s, A[1]]: .3f}")
```

The optimal value function at $t=0$ is:
\begin{equation}
V^\star_0 = \begin{bmatrix}
1.259 \\
3.251 \\
3.787 \\
6.222 \\
7.778 \\
10
\end{bmatrix}.
(\#eq:HangoverOptimalValueFunction)
\end{equation}
Clearly, the optimal value function dominates the value function shown in \@ref(eq:HangoverRandomValueFunction) of the random policy at every state. 

The optimal actions at $t=0$ are:
\begin{equation}
\begin{split}
\text{Hangover} & : \text{Lazy} \\
\text{Sleep}        & : \text{Productive} \\
\text{More Sleep}   & : \text{Productive} \\
\text{Visit Lecture} & : \text{Lazy} \\
\text{Study}      & : \text{Productive} \\
\text{Pass Exam}    & : \text{Lazy}
\end{split}.
\end{equation}

You can play with the code [here](https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/hangover_dynamic_programming.py).
:::
:::

## Infinite-Horizon MDP {#InfiniteHorizonMDP}












