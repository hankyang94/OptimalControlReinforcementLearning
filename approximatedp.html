<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Approximate Optimal Control | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Approximate Optimal Control | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Approximate Optimal Control | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-03-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="exactdp.html"/>
<link rel="next" href="stability.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="formulation.html"><a href="formulation.html"><i class="fa fa-check"></i><b>1</b> The Optimal Control Formulation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="formulation.html"><a href="formulation.html#the-basic-problem"><i class="fa fa-check"></i><b>1.1</b> The Basic Problem</a></li>
<li class="chapter" data-level="1.2" data-path="formulation.html"><a href="formulation.html#dynamic-programming-and-principle-of-optimality"><i class="fa fa-check"></i><b>1.2</b> Dynamic Programming and Principle of Optimality</a></li>
<li class="chapter" data-level="1.3" data-path="formulation.html"><a href="formulation.html#infinite-horizon"><i class="fa fa-check"></i><b>1.3</b> Infinite-horizon Formulation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exactdp.html"><a href="exactdp.html"><i class="fa fa-check"></i><b>2</b> Exact Dynamic Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path="exactdp.html"><a href="exactdp.html#lqr"><i class="fa fa-check"></i><b>2.1</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="exactdp.html"><a href="exactdp.html#infinite-horizon-lqr"><i class="fa fa-check"></i><b>2.1.1</b> Infinite-Horizon LQR</a></li>
<li class="chapter" data-level="2.1.2" data-path="exactdp.html"><a href="exactdp.html#lqr-with-constraints"><i class="fa fa-check"></i><b>2.1.2</b> LQR with Constraints</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="exactdp.html"><a href="exactdp.html#mdp-exact-dp"><i class="fa fa-check"></i><b>2.2</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="exactdp.html"><a href="exactdp.html#bellman-optimality-equations"><i class="fa fa-check"></i><b>2.2.1</b> Bellman Optimality Equations</a></li>
<li class="chapter" data-level="2.2.2" data-path="exactdp.html"><a href="exactdp.html#value-iteration"><i class="fa fa-check"></i><b>2.2.2</b> Value Iteration</a></li>
<li class="chapter" data-level="2.2.3" data-path="exactdp.html"><a href="exactdp.html#value-iteration-with-barycentric-interpolation"><i class="fa fa-check"></i><b>2.2.3</b> Value Iteration with Barycentric Interpolation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="approximatedp.html"><a href="approximatedp.html"><i class="fa fa-check"></i><b>3</b> Approximate Optimal Control</a>
<ul>
<li class="chapter" data-level="3.1" data-path="approximatedp.html"><a href="approximatedp.html#fitted-value-iteration"><i class="fa fa-check"></i><b>3.1</b> Fitted Value Iteration</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="approximatedp.html"><a href="approximatedp.html#linear-features"><i class="fa fa-check"></i><b>3.1.1</b> Linear Features</a></li>
<li class="chapter" data-level="3.1.2" data-path="approximatedp.html"><a href="approximatedp.html#neural-network-features"><i class="fa fa-check"></i><b>3.1.2</b> Neural Network Features</a></li>
<li class="chapter" data-level="3.1.3" data-path="approximatedp.html"><a href="approximatedp.html#fitted-q-value-iteration"><i class="fa fa-check"></i><b>3.1.3</b> Fitted Q-value Iteration</a></li>
<li class="chapter" data-level="3.1.4" data-path="approximatedp.html"><a href="approximatedp.html#deep-q-network"><i class="fa fa-check"></i><b>3.1.4</b> Deep Q Network</a></li>
<li class="chapter" data-level="3.1.5" data-path="approximatedp.html"><a href="approximatedp.html#deep-shallow"><i class="fa fa-check"></i><b>3.1.5</b> Deep + Shallow</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="approximatedp.html"><a href="approximatedp.html#trajectory-optimization"><i class="fa fa-check"></i><b>3.2</b> Trajectory Optimization</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="approximatedp.html"><a href="approximatedp.html#direct-single-shooting"><i class="fa fa-check"></i><b>3.2.1</b> Direct Single Shooting</a></li>
<li class="chapter" data-level="3.2.2" data-path="approximatedp.html"><a href="approximatedp.html#direct-multiple-shooting"><i class="fa fa-check"></i><b>3.2.2</b> Direct Multiple Shooting</a></li>
<li class="chapter" data-level="3.2.3" data-path="approximatedp.html"><a href="approximatedp.html#direct-collocation"><i class="fa fa-check"></i><b>3.2.3</b> Direct Collocation</a></li>
<li class="chapter" data-level="3.2.4" data-path="approximatedp.html"><a href="approximatedp.html#direct-orthogonal-collocation"><i class="fa fa-check"></i><b>3.2.4</b> Direct Orthogonal Collocation</a></li>
<li class="chapter" data-level="3.2.5" data-path="approximatedp.html"><a href="approximatedp.html#failure-of-open-loop-control"><i class="fa fa-check"></i><b>3.2.5</b> Failure of Open-Loop Control</a></li>
<li class="chapter" data-level="3.2.6" data-path="approximatedp.html"><a href="approximatedp.html#lqr-trajectory-tracking"><i class="fa fa-check"></i><b>3.2.6</b> LQR Trajectory Tracking</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="approximatedp.html"><a href="approximatedp.html#model-predictive-control"><i class="fa fa-check"></i><b>3.3</b> Model Predictive Control</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="approximatedp.html"><a href="approximatedp.html#turn-trajectory-optimization-into-feedback-control"><i class="fa fa-check"></i><b>3.3.1</b> Turn Trajectory Optimization into Feedback Control</a></li>
<li class="chapter" data-level="3.3.2" data-path="approximatedp.html"><a href="approximatedp.html#controllability-reachability-and-invariance"><i class="fa fa-check"></i><b>3.3.2</b> Controllability, Reachability, and Invariance</a></li>
<li class="chapter" data-level="3.3.3" data-path="approximatedp.html"><a href="approximatedp.html#basic-formulation-for-linear-systems"><i class="fa fa-check"></i><b>3.3.3</b> Basic Formulation for Linear Systems</a></li>
<li class="chapter" data-level="3.3.4" data-path="approximatedp.html"><a href="approximatedp.html#persistent-feasibility"><i class="fa fa-check"></i><b>3.3.4</b> Persistent Feasibility</a></li>
<li class="chapter" data-level="3.3.5" data-path="approximatedp.html"><a href="approximatedp.html#mpc-stability"><i class="fa fa-check"></i><b>3.3.5</b> Stability</a></li>
<li class="chapter" data-level="3.3.6" data-path="approximatedp.html"><a href="approximatedp.html#explicit-mpc"><i class="fa fa-check"></i><b>3.3.6</b> Explicit MPC</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="approximatedp.html"><a href="approximatedp.html#policy-gradient"><i class="fa fa-check"></i><b>3.4</b> Policy Gradient</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stability.html"><a href="stability.html"><i class="fa fa-check"></i><b>4</b> Stability Analysis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stability.html"><a href="stability.html#autonomous-systems"><i class="fa fa-check"></i><b>4.1</b> Autonomous Systems</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="stability.html"><a href="stability.html#concepts-of-stability"><i class="fa fa-check"></i><b>4.1.1</b> Concepts of Stability</a></li>
<li class="chapter" data-level="4.1.2" data-path="stability.html"><a href="stability.html#stability-by-linearization"><i class="fa fa-check"></i><b>4.1.2</b> Stability by Linearization</a></li>
<li class="chapter" data-level="4.1.3" data-path="stability.html"><a href="stability.html#lyapunov-analysis"><i class="fa fa-check"></i><b>4.1.3</b> Lyapunov Analysis</a></li>
<li class="chapter" data-level="4.1.4" data-path="stability.html"><a href="stability.html#invariant-set-theorem"><i class="fa fa-check"></i><b>4.1.4</b> Invariant Set Theorem</a></li>
<li class="chapter" data-level="4.1.5" data-path="stability.html"><a href="stability.html#computing-lyapunov-certificates"><i class="fa fa-check"></i><b>4.1.5</b> Computing Lyapunov Certificates</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="stability.html"><a href="stability.html#controlled-systems"><i class="fa fa-check"></i><b>4.2</b> Controlled Systems</a></li>
<li class="chapter" data-level="4.3" data-path="stability.html"><a href="stability.html#non-autonomous-systems"><i class="fa fa-check"></i><b>4.3</b> Non-autonomous Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="psets.html"><a href="psets.html"><i class="fa fa-check"></i><b>5</b> Problem Sets</a></li>
<li class="chapter" data-level="" data-path="acknowledgement.html"><a href="acknowledgement.html"><i class="fa fa-check"></i>Acknowledgement</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra and Differential Equations</a>
<ul>
<li class="chapter" data-level="A.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#linear-algebra"><i class="fa fa-check"></i><b>A.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#matrix-exponential"><i class="fa fa-check"></i><b>A.1.1</b> Matrix Exponential</a></li>
<li class="chapter" data-level="A.1.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#gradients"><i class="fa fa-check"></i><b>A.1.2</b> Gradients</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#solving-an-ordinary-differential-equation"><i class="fa fa-check"></i><b>A.2</b> Solving an Ordinary Differential Equation</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#separation-of-variables"><i class="fa fa-check"></i><b>A.2.1</b> Separation of Variables</a></li>
<li class="chapter" data-level="A.2.2" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#first-order-linear-ode"><i class="fa fa-check"></i><b>A.2.2</b> First-order Linear ODE</a></li>
<li class="chapter" data-level="A.2.3" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#gronwall-inequality"><i class="fa fa-check"></i><b>A.2.3</b> Gronwall Inequality</a></li>
<li class="chapter" data-level="A.2.4" data-path="linear-algebra-and-differential-equations.html"><a href="linear-algebra-and-differential-equations.html#matlab"><i class="fa fa-check"></i><b>A.2.4</b> Matlab</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>B</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>B.1</b> Theory</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>B.1.1</b> Sets</a></li>
<li class="chapter" data-level="B.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>B.1.2</b> Convex function</a></li>
<li class="chapter" data-level="B.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>B.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="B.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>B.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>B.2</b> Practice</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>B.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="B.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>B.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="B.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>B.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="B.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>B.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="B.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>B.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="B.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>B.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="B.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>B.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>C</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="C.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>C.1</b> Stability</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>C.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="C.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>C.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="C.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis-1"><i class="fa fa-check"></i><b>C.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>C.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>C.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="C.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>C.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="C.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>C.2.3</b> Duality</a></li>
<li class="chapter" data-level="C.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>C.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>C.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>C.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="C.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>C.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html"><i class="fa fa-check"></i><b>D</b> Algebraic Techniques and Sum-of-Squares</a>
<ul>
<li class="chapter" data-level="D.1" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#algebra"><i class="fa fa-check"></i><b>D.1</b> Algebra</a>
<ul>
<li class="chapter" data-level="D.1.1" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#polynomials"><i class="fa fa-check"></i><b>D.1.1</b> Polynomials</a></li>
<li class="chapter" data-level="D.1.2" data-path="algebraic-techniques-and-sum-of-squares.html"><a href="algebraic-techniques-and-sum-of-squares.html#representation-of-nonnegative-polynomial-univariate-case"><i class="fa fa-check"></i><b>D.1.2</b> Representation of nonnegative polynomial: Univariate case</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="the-kalman-yakubovich-lemma.html"><a href="the-kalman-yakubovich-lemma.html"><i class="fa fa-check"></i><b>E</b> The Kalman-Yakubovich Lemma</a></li>
<li class="chapter" data-level="F" data-path="feedbacklinearization.html"><a href="feedbacklinearization.html"><i class="fa fa-check"></i><b>F</b> Feedback Linearization</a></li>
<li class="chapter" data-level="G" data-path="slidingcontrol.html"><a href="slidingcontrol.html"><i class="fa fa-check"></i><b>G</b> Sliding Control</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="approximatedp" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Approximate Optimal Control<a href="approximatedp.html#approximatedp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Thanks to <a href="https://jrli.org/">Jiarui Li</a> for contributing to this Chapter.</em></p>
<p>In Chapter <a href="exactdp.html#exactdp">2</a>, we have studied two cases where dynamic programming (DP) can be executed exactly. Both cases are interesting yet limiting.</p>
<p>In the linear quadratic regulator (LQR) case, the optimal controller is a linear controller and it does not require discretization of the state and control space. However, LQR only handles systems with linear dynamics.</p>
<p>In the Markov Decision Process (MDP) case, value iteration can obtain the optimal cost-to-go (or the optimal <span class="math inline">\(Q\)</span>-value function) usually in a finite number of iterations. With barycentric interpolation, value iteration also leads to a practical controller for swinging up the pendulum (see Example <a href="exactdp.html#exm:pendulumvalueiterationbarycentric">2.3</a>), at least starting from some initial states. However, value iteration suffers from the <em>curse of dimensionality</em>, i.e., the amount of memory and computation needed to compute the optimal cost-to-go grows exponentially with the number of grids used to discretize the state space and control space.</p>
<p>In this Chapter, we will study approximate dynamic programming, which aims to not find the optimal controller (simply because it is too demanding), but only a suboptimal controller that is perhaps good enough. It is worth noting that there exists a large amount of algorithms and literature for approximate DP, most of which are closely related to reinforcement learning, and studying all of them is beyond the scope of this course. In the following we will only go through several algorithmic frameworks that are representative.</p>
<!-- Although we see in Example \@ref(exm:pendulumvalueiterationbarycentric) that value iteration with barycentric interpolation does stabilize the pendulum, we also realize that DP comes with 

The limitations of classical deterministic dynamic programming (DP) were mentioned in Chapter \@ref(formulation), particularly its inefficiency in fields such as robotics where both the state and control spaces are typically large and continuous. The process of discretization in such contexts is not only challenging but also costly. Even when discretization is achievable, the resultant state and control spaces tend to be extraordinarily vast and often high-dimensional, leading to prohibitive computational demands. This issue, commonly called the _curse of dimensionality_, renders the use of classical DP unfeasible. <span style="color:red">Add time complexity analysis here</span>. 
To circumvent the constraints of traditional DP algorithms in such contexts, a pragmatic approach involves the adoption of a suboptimal control scheme. This compromises between the ease of implementation and adequate performance. The principal objective of this chapter is to find such suboptimal control. In this chapter, we will spend most of the time discussing finite horizon problems with discrete state and control space, which is the classical scenario. We will also mention the infinite horizon problem and continuous state and control spaces scenario later. 

Broadly, two categories of approximation are used in the context of DP-based suboptimal control. The first is _approximation in value space_, where we aim to approximate the optimal cost function or the cost function of a given policy. The second is _approximation in policy space_, where we select the policy by using optimization over a suitable class of policies.  -->
<div id="fitted-value-iteration" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Fitted Value Iteration<a href="approximatedp.html#fitted-value-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us consider the infinite-horizon optimal control problem introduced in Chapter <a href="formulation.html#infinite-horizon">1.3</a>
<span class="math display">\[
\min_{\pi} \mathbb{E} \left\{ \sum_{k=0}^\infty g(x_k, \pi(x_k)) \right\}.
\]</span>
Under some technical conditions, we know that the optimal policy is a deterministic and stationary policy and the optimal cost-to-go satisfies the following Bellman Optimality Equation
<span class="math display" id="eq:bellmanoptimality-fvi">\[\begin{equation}
J^\star(x) = \min_{u \in \mathbb{U}} \mathbb{E}_w \left\{ g(x,u) + J^\star(f(x,u,w)) \right\}, \quad \forall x \in \mathbb{X}.
\tag{3.1}
\end{equation}\]</span>
From the plots in Example <a href="exactdp.html#exm:pendulumvalueiterationbarycentric">2.3</a>, we observe that even for a “simple” problem like pendulum swing-up, the optimal cost-to-go <span class="math inline">\(J^\star(x)\)</span> does not look simple at all.</p>
<p>So here comes a very natural idea. What if we parametrize a cost-to-go function <span class="math inline">\(\tilde{J}(x,r)\)</span> by a vector of unknown coefficients <span class="math inline">\(r \in \mathbb{R}^{d}\)</span> and ask <span class="math inline">\(\tilde{J}(x,r)\)</span> to satisfy <a href="approximatedp.html#eq:bellmanoptimality-fvi">(3.1)</a> as closely as possible?</p>
<p>This indeed leads to a valid algoithm called fitted value iteration (FVI).</p>
<p>In FVI, we initialize the parameters <span class="math inline">\(r\)</span> as <span class="math inline">\(r_0\)</span> in the first step. Then, at the <span class="math inline">\(k\)</span>-th iteration, we perform two subroutines.</p>
<p><strong>Value update</strong>. Firstly, we sample a large amount of points in the state space <span class="math inline">\(\{x_k^s \}_{s=1}^q\)</span>, and for each <span class="math inline">\(x_k^s\)</span>, we solve the minimization problem on the right-hand side of <a href="approximatedp.html#eq:bellmanoptimality-fvi">(3.1)</a> using <span class="math inline">\(J(x,r^{(k)})\)</span> as the cost-to-go. Formally, this is
<span class="math display" id="eq:fvi-step-one">\[\begin{equation}
\beta_k^s \leftarrow \min_{u \in \mathbb{U}} \mathbb{E}_w \left\{ g(x_k^s, u) + \tilde{J}(f(x_k^s,u,w),r^{(k)}) \right\}, \quad \forall x_k^s, s= 1,\dots,q.
\tag{3.2}
\end{equation}\]</span>
This step gives us a list of scalars <span class="math inline">\(\{ \beta_k^s \}_{s=1}^q\)</span>. If <span class="math inline">\(\tilde{J}(x,r)\)</span> indeed satisfies the Bellman optimality equation, then we should have
<span class="math display">\[
\tilde{J}(x_k^s,r^{(k)}) = \beta_k^s, \quad \forall s = 1,\dots,q.
\]</span>
This is certainly not true when the parameter <span class="math inline">\(r\)</span> is imperfect.</p>
<p><strong>Parameter update</strong>. Therefore, we will see the best parameter that minimizes the violation of the above equation
<span class="math display" id="eq:fvi-step-two">\[\begin{equation}
r^{(k+1)} \leftarrow \arg\min_{r \in \mathbb{R}^d} \sum_{s=1}^q \left(\tilde{J}(x_k^s,r) -  \beta_k^s\right)^2.
\tag{3.3}
\end{equation}\]</span>
FVI essentially carries out <a href="approximatedp.html#eq:fvi-step-one">(3.2)</a> and <a href="approximatedp.html#eq:fvi-step-two">(3.3)</a> until some convergence metric is met.</p>
<p>Two challenges immediately show up:</p>
<ul>
<li><p>How to perform the minimization over <span class="math inline">\(u\)</span> in the first step <a href="approximatedp.html#eq:fvi-step-one">(3.2)</a>?</p></li>
<li><p>How to find the best parameter update in the second step <a href="approximatedp.html#eq:fvi-step-two">(3.3)</a>?</p></li>
</ul>
<p>To solve the first challenge, we will assume that <span class="math inline">\(\mathbb{U}\)</span> is a finite set so that minimization over <span class="math inline">\(u\)</span> can be solved exactly. Note that this assumption is not strictly necessary. In fact, as long as <span class="math inline">\(\mathbb{U}\)</span> is a convex set and the objective in <a href="approximatedp.html#eq:fvi-step-one">(3.2)</a> is also convex, then the minimization can also be solved exactly <span class="citation">(<a href="#ref-yang23arxiv-value">A. Yang and Boyd 2023</a>)</span>. However, we assume <span class="math inline">\(\mathbb{U}\)</span> is finite to simplify our presentation.</p>
<p>To solve the second challenge, we will assume <span class="math inline">\(\tilde{J}(x,r)\)</span> is <em>linear</em> in the parameters <span class="math inline">\(r\)</span>, as we will study further in the following.</p>
<div id="linear-features" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Linear Features<a href="approximatedp.html#linear-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We parameterize <span class="math inline">\(\tilde{J}(x,r)\)</span> as follows:
<span class="math display">\[\begin{equation}
\tilde{J}(x,r) = \sum_{i=1}^d r_i \cdot \phi_i(x) = \begin{bmatrix} \phi_1(x) &amp; \cdots &amp; \phi_d(x) \end{bmatrix} r = \phi(x)^T r,
\end{equation}\]</span>
where <span class="math inline">\(\phi_1(x),\dots,\phi_d(x)\)</span> are a set of known basis functions, or <em>features</em>. Common examples of features include polynomials and radial basis functions.
With this parametrization, the optimization in <a href="approximatedp.html#eq:fvi-step-two">(3.3)</a> becomes
<span class="math display" id="eq:fvi-step-two-least-squares">\[\begin{equation}
\min_{r \in \mathbb{R}^d} \sum_{s=1}^q \left( \phi(x_k^s)^T r - \beta_k^s \right)^2,
\tag{3.4}
\end{equation}\]</span>
which is a least-squares problem that can be solved in closed form. In particular, we can compute the gradient of its objective with respect to <span class="math inline">\(r\)</span> and set it to zero, leading to
<span class="math display">\[\begin{align}
0 = \sum_{s=1}^q 2(\phi(x_k^s)^T r - \beta_k^s ) \phi(x_k^s)  \Longrightarrow \\
r = \left( \sum_{s=1}^q \phi(x_k^s) \phi(x_k^s)^T \right)^{-1} \left( \sum_{s=1}^q \beta_k^s \phi(x_k^s) \right).
\end{align}\]</span></p>
<p>Let us apply FVI with linear features to a linear system for which we know the optimal cost-to-go.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:feature-double-integrator" class="example"><strong>Example 3.1  (Fitted Value Iteration for Double Integrator) </strong></span>Consider the double integrator
<span class="math display">\[
\ddot{q} = u.
\]</span>
Take <span class="math inline">\(x = [q;\dot{q}]\)</span>, we can write the dynamics in state-space form
<span class="math display" id="eq:double-integrator-dynamics">\[\begin{equation}
    \dot{x} = \begin{bmatrix}
    \dot{q} \\
    u
    \end{bmatrix} =
    \begin{bmatrix}
        0 &amp; 1 \\
        0 &amp; 0
    \end{bmatrix} x + \begin{bmatrix}
        0 \\
        1 \end{bmatrix} u
    \tag{3.5}
\end{equation}\]</span>
We use a constant time <span class="math inline">\(h=0.01\)</span> differentiation to convert the continuous-time dynamics into discrete-time:
<span class="math display">\[
x_{k+1} = h \cdot \dot{x}_k + x_k = \begin{bmatrix} 1 &amp; h \\ 0 &amp; 1 \end{bmatrix} x_k + \begin{bmatrix} 0 \\ h \end{bmatrix} u_k.
\]</span></p>
<p>The goal is to regulate the system at <span class="math inline">\((0,0)\)</span>. To do so, we use a quadratic cost
<span class="math display">\[
J(x) = \min \sum_{k=0}^{\infty} x_k^T Q x_k + u_k^T R u_k, \quad x_0 = x.
\]</span>
with <span class="math inline">\(Q = 0.1I_2\)</span> and <span class="math inline">\(R = 1\)</span>.</p>
<p>In Chapter <a href="exactdp.html#exactdp">2</a>, we learned how to use LQR to precisely calculate the cost-to-go function of these systems using the <em>Algebraic Recatti Equation</em>. Solving the ARE, we obtain
<span class="math display">\[
S_{\mathrm{LQR}} = \begin{bmatrix} 27.1640 &amp; 31.7584 \\ 31.7584 &amp; 85.9510 \end{bmatrix}.
\]</span></p>
<p>Now we want to investigate if FVI can find the same matrix <span class="math inline">\(S\)</span>. To do so, we parametrize
<span class="math display">\[
\tilde{J}(x) = x^T S x = x^T \begin{bmatrix} S_1 &amp; S_2 \\ S_2 &amp; S_3 \end{bmatrix} x = \begin{bmatrix} x_1^2 &amp; 2 x_1 x_2 &amp; x_2^2 \end{bmatrix} \begin{bmatrix} S_1 \\ S_2 \\ S_3 \end{bmatrix},
\]</span>
where <span class="math inline">\(S\)</span> only has three independent variables due to being symmetric. Do note that <span class="math inline">\(\tilde{J}(x)\)</span> is <em>linear</em> in <span class="math inline">\(S\)</span>, so the parameter update step can be solved in closed form.</p>
<p>We now show that the value update step can also be solved in closed form. Suppose we choose <span class="math inline">\(x_k\)</span> as a sample, then the right-hand side of <a href="approximatedp.html#eq:fvi-step-one">(3.2)</a> reads:
<span class="math display">\[
\min_{u} x_k^T Q x_k + u^2 + (A x_k + B u)^T S^{(k)} (A x_k + B u),
\]</span>
where <span class="math inline">\(S^{(k)}\)</span> is the value of <span class="math inline">\(S\)</span> at the <span class="math inline">\(k\)</span>-th iteration. Clearly, the optimization problem above is a convex quadratic optimization and can be solved in closed form:
<span class="math display">\[
u = - (1 + B^T S^{(k)} B) ^{-1} B^T S^{(k)} A x_k.
\]</span></p>
<p>Applying fitted value iteration on randomly sampled points mentioned above, we obtain the fitted <span class="math inline">\(S\)</span>
<span class="math display">\[
S_{\mathrm{FVI}} = \begin{bmatrix} 27.1640 &amp; 31.7583 \\ 31.7584 &amp; 85.9509 \end{bmatrix}.
\]</span></p>
<p>We can see that <span class="math inline">\(S_{\mathrm{FVI}}\)</span> almost exactly matches the groundtruth LQR solution <span class="math inline">\(S_{\mathrm{LQR}}\)</span>.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/LQR_FVI_Example.m">here</a>.</p>
<!-- the cost-to-go in Fig. \@ref(fig:double-integrator-feature-example). The value iteration converges in 3000 iterations.
<div class="figure" style="text-align: center">
<img src="images/approxdp-feature-FVI.png" alt="Comparison between the result of feature-based FVI and ground truth" width="90%" />
<p class="caption">(\#fig:double-integrator-feature-example)Comparison between the result of feature-based FVI and ground truth</p>
</div>
The S matrix calculated by the result of FVI is $S_1$, and $S_2$ is the solution to the Recatti Equation. We have
\begin{equation}
    S_1 = \begin{bmatrix}
        27.164022 & 31.758386 \\ 
        31.758386 & 85.950968 \end{bmatrix}, \quad
    S_2 = \begin{bmatrix}
        27.164022 & 31.758386 \\
        31.758386 & 85.950968 \end{bmatrix}
    \end{equation}
$S_1$ and $S_2$ are almost the same, which means that we obtained the optimal cost-to-go function using fitted value iteration. 
It is worth noting that the cost-to-go function in Linear Quadratic Regulator is also using the linear architecture mentioned above.
\begin{equation}
    J(x,S)=x^T S x = \operatorname{tr}(S x^T x)
\end{equation}
It is quadratic in $x$ but linear in $S$.  -->
</div>
</div>
</div>
<div id="neural-network-features" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Neural Network Features<a href="approximatedp.html#neural-network-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have seen that simple quadratic features work for the LQR problem. For more complicated problems, we will need more powerful function approximators like neural networks.</p>
<p>When we parameterize <span class="math inline">\(\tilde{J}(x,r)\)</span> as a neural network, <span class="math inline">\(r\)</span> will be the weights of the neural network. We may still be able to solve the value update step if <span class="math inline">\(u\)</span> lives in a finite space. However, the parameter update step usually cannot be solved exactly and will need to rely on numerical algoithms such as gradient descent.</p>
<p>Let us try neural FVI on the same double integrator problem.</p>
<!-- The selection of features is frequently manually crafted, relying on human intellect, intuition, or experience, and can pose considerable challenges. The utilization of a neural network as the approximation architecture has emerged as a popular approach in recent years. In this context, the parameter $r_k$ may correspond to the weights of the neural networks. A diverse range of machine learning techniques can then be implemented to manage the training problem, steering the approximation toward the optimal value.

However, the optimization process of the weights is more difficult due to the non-convexity. According to the equation \@ref(eq:apprinv-dp), the $J_{k+1}$ is non-convex which makes the entire equation hard to optimize.  -->
<div class="examplebox">
<div class="example">
<p><span id="exm:NNbased-double-tegrator" class="example"><strong>Example 3.2  (Neural Fitted Value Iteration for the Double Integrator) </strong></span>In this example, we will use neural network as the approximation of the cost-to-go function and conduct neural FVI on a double integrator. The dynamics of the double integrator has been introduced in example <a href="approximatedp.html#exm:feature-double-integrator">3.1</a>. We use a positive definite network to model the cost-to-go function
<span class="math display">\[\begin{equation}
    J(x) = N(x)^T N(x),
\end{equation}\]</span>
where <span class="math inline">\(N(x)\)</span> is a 3-layer Multi-Layer-Perceptron with ReLU activation.</p>
Using mini-batch learning plus Adam optimizer, we obtain the cost-to-go in Fig. <a href="approximatedp.html#fig:double-integrator-NN-example">3.1</a> after 300 epochs of training.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:double-integrator-NN-example"></span>
<img src="images/approxdp-NN-FVI.png" alt="Comparison between the result of NN-based FVI and ground truth" width="90%" />
<p class="caption">
Figure 3.1: Comparison between the result of NN-based FVI and ground truth
</p>
</div>
<p>The figure shows that the approximation performance of NN is pretty good. Simulation experiments also shows that the corresponding controller could successfully regulate the system at <span class="math inline">\((0,0)\)</span>.</p>
<p>You can see the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/tree/main/Neural-FVI">here</a>.</p>
</div>
</div>
</div>
<div id="fitted-q-value-iteration" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Fitted Q-value Iteration<a href="approximatedp.html#fitted-q-value-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From the MDP Chapter we know there is an equivalent representation of the Bellman Optimality Equation by replacing the <span class="math inline">\(J\)</span> value function in <a href="approximatedp.html#eq:bellmanoptimality-fvi">(3.1)</a> with the <span class="math inline">\(Q\)</span>-value function <span class="math inline">\(Q(x,u)\)</span>. In particular, with
<span class="math display">\[
J^\star(x) = \min_{u \in \mathbb{U}} Q^\star(x,u)
\]</span>
substituted into <a href="approximatedp.html#eq:bellmanoptimality-fvi">(3.1)</a>, we obtain the Bellman Optimality Equation in <span class="math inline">\(Q^\star(x,u)\)</span>:
<span class="math display" id="eq:bellmanoptimality-fqi">\[\begin{equation}
Q^\star(x,u) =  g(x,u) + \mathbb{E}_w \left\{ \min_{u&#39; \in \mathbb{U}} Q^\star(f(x,u,w),u&#39;) \right\}.
\tag{3.6}
\end{equation}\]</span>
We can then use the same idea to approximate <span class="math inline">\(Q^\star(x,u)\)</span> as
<span class="math display">\[
\tilde{Q}(x,u,r)
\]</span>
with <span class="math inline">\(r \in \mathbb{R}^d\)</span> a parameter vector. By iteratively evaluating the right-hand side of <a href="approximatedp.html#eq:bellmanoptimality-fqi">(3.6)</a>, we obtain the algoithm known as <em>fitted <span class="math inline">\(Q\)</span>-value iteration</em> (FQI).</p>
<p>For example, we can similarly adopt the linear feature parameterization and set
<span class="math display">\[
\tilde{Q}(x,u,r) = \phi(x,u)^T r,
\]</span>
where <span class="math inline">\(\phi(x,u)\)</span> is a known pre-selected feature vector. Then at the <span class="math inline">\(k\)</span>-th iteration of FQI, we perform two subroutines.</p>
<p><strong>Value update</strong>. We sample a number of state-control pairs <span class="math inline">\(\{(x_k^s,u_k^s) \}_{s=1}^q\)</span> and evaluate the right-hand side of the Bellman optimality equation <a href="approximatedp.html#eq:bellmanoptimality-fqi">(3.6)</a>:
<span class="math display" id="eq:fqi-value-update">\[\begin{equation}
\beta_k^s \leftarrow g(x_k^s,u_k^s) + \mathbb{E}_w \left\{ \min_{u&#39; \in \mathbb{U}} \tilde{Q}(f(x_k^s,u_k^s,w),u&#39;,r^{(k)}) \right\}, \quad \forall s = 1,\dots,q.
\tag{3.7}
\end{equation}\]</span>
Again, if <span class="math inline">\(u\)</span> lives in a finite space, or the minimization is convex, the above value update can be solved exactly.</p>
<p><strong>Parameter update</strong>. We update the parameter vector using the updated values <span class="math inline">\(\{ \beta_k^s \}_{s=1}^q\)</span>:
<span class="math display" id="eq:fqi-parameter-update">\[\begin{equation}
r^{(k+1)} \leftarrow \arg\min_{r \in \mathbb{R}^d} \sum_{s=1}^q \left( \tilde{Q}(x_k^s,u_k^s,r) - \beta_k^s \right)^2,
\tag{3.8}
\end{equation}\]</span>
which is a least squares problem that can be solved in closed form.</p>
<p>Let us apply FQI to the same double integrator example.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:fqidoubleintegrator" class="example"><strong>Example 3.3  (Fitted Q-value Iteration for Double Integrator) </strong></span>Consider the same double integrator dynamics in Example <a href="approximatedp.html#exm:feature-double-integrator">3.1</a>.</p>
<p>From the LQR solution we know the optimal <span class="math inline">\(Q\)</span>-value function is
<span class="math display">\[\begin{align}
Q^\star(x,u) &amp;= x^T Q x + u^T R u + (Ax + Bu)^T S (Ax + Bu) \\
&amp;= \begin{bmatrix} x \\ u \end{bmatrix}^T
\underbrace{\begin{bmatrix} A^T S A + Q &amp; A^T S B \\
B^T S A &amp; B^T S B + R \end{bmatrix}}_{M_{\mathrm{LQR}}} \begin{bmatrix} x \\ u \end{bmatrix},
\end{align}\]</span>
where <span class="math inline">\(M_{\mathrm{LQR}}\)</span> is
<span class="math display">\[
M_{\mathrm{LQR}} = \begin{bmatrix}
27.2640 &amp; 32.0300 &amp; 0.3176 \\
32.0300 &amp; 86.6889 &amp; 0.8627 \\
0.3176 &amp; 0.8627 &amp; 1.0086
\end{bmatrix}.
\]</span></p>
<p>Let us apply FQI to see if we get the same solution. We parametrize
<span class="math display">\[\begin{align}
\tilde{Q}(x,u) &amp;= \begin{bmatrix} x \\ u \end{bmatrix}^T \begin{bmatrix}
M_1 &amp; M_2 &amp; M_3 \\
M_2 &amp; M_4 &amp; M_5 \\
M_3 &amp; M_5 &amp; M_6
\end{bmatrix} \begin{bmatrix} x \\ u \end{bmatrix} \\
&amp;= \begin{bmatrix} x_1^2 &amp; 2 x_1 x_2 &amp; 2x_1 u &amp; x_2^2 &amp; 2x_2 u &amp; u^2 \end{bmatrix} \begin{bmatrix} M_1 \\ M_2 \\ M_3 \\ M_4 \\ M_5 \\ M_6 \end{bmatrix}.
\end{align}\]</span></p>
<p>At the <span class="math inline">\(k\)</span>-th FQI iteration, we are given <span class="math inline">\(M^{(k)}\)</span>. Suppose we sample <span class="math inline">\((x_k, u_k)\)</span>, then the value update step needs to solve <a href="approximatedp.html#eq:fqi-value-update">(3.7)</a>, which reads:
<span class="math display">\[
\beta_k = g(x_k, u_k) + \min_{u&#39;} \underbrace{\begin{bmatrix} A x_k + B u_k \\ u&#39; \end{bmatrix}^T M^{(k)} \begin{bmatrix} A x_k + B u_k \\ u&#39; \end{bmatrix}}_{\psi(u&#39;)}.
\]</span>
The objective function <span class="math inline">\(\psi(u&#39;)\)</span> can be shown to be quadratic:
<span class="math display">\[
\psi(u&#39;) = (Lu&#39; + a_k)^T M^{(k)} (Lu&#39; + a_k), \quad L = \begin{bmatrix} 0 \\ 1 \end{bmatrix}, a_k = \begin{bmatrix} A x_k + B u_k \\ 0 \end{bmatrix}.
\]</span>
Therefore, we can solve <span class="math inline">\(u&#39;\)</span> in closed-form
<span class="math display">\[
u&#39; = - (L^T M^{(k)} L)^{-1} L^T M^{(k)} a_k.
\]</span></p>
<p>Applying FQI with the closed-form update above, we get
<span class="math display">\[
M_{\mathrm{FQI}} = \begin{bmatrix}
27.2640 &amp; 32.0300 &amp; 0.3176 \\
32.0300 &amp; 86.6889 &amp; 0.8627 \\
0.3176 &amp; 0.8627 &amp; 1.0086
\end{bmatrix},
\]</span>
which is exactly the same as the solution obtained from LQR.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/LQR_FQI_Example.m">here</a>.</p>
</div>
</div>
</div>
<div id="deep-q-network" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Deep Q Network<a href="approximatedp.html#deep-q-network" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Although we have only tested FVI and FQI on the simple double integrator linear system, these algoithms are really not so different from the state-of-the-art reinforcement learning algoithms. For example, the core of the seminal work Deep <span class="math inline">\(Q\)</span>-Network (DQN) <span class="citation">(<a href="#ref-mnih15nature-dqn">Mnih et al. 2015</a>)</span> is to use a deep neural network to parameterize the <span class="math inline">\(Q\)</span>-value function. Of course the DQN work has used other clever ideas to make it work in practice, such as experience replay, but the essence is fitted <span class="math inline">\(Q\)</span>-value iteration.</p>
<p>You can find a good explanation of DQN in this <a href="https://www.tensorflow.org/agents/tutorials/0_intro_rl">tutorial</a>, and a practical step-by-step Pytorch implementation that <a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">applies DQN on the cart-pole system</a>.</p>
</div>
<div id="deep-shallow" class="section level3 hasAnchor" number="3.1.5">
<h3><span class="header-section-number">3.1.5</span> Deep + Shallow<a href="approximatedp.html#deep-shallow" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Combine the rich features of DQN with the stable learning of FQI <span class="citation">(<a href="#ref-levine17neurips-shallow">Levine et al. 2017</a>)</span>.</p>
</div>
</div>
<div id="trajectory-optimization" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Trajectory Optimization<a href="approximatedp.html#trajectory-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a continuous-time optimal control problem (OCP) in full generality
<span class="math display" id="eq:constrained-optimal-control">\[\begin{equation}
\begin{split}
\min_{u(t), t \in [0,T]} &amp; \quad  g_T(x(T)) + \int_{t = 0}^T g(x(t),u(t)) dt \\
\text{subject to} &amp; \quad \dot{x} = f(x(t),u(t)) \\
&amp; \quad x(0) = x_0 \\
&amp; \quad (x(t),u(t)) \in \mathcal{X} \times \mathcal{U} \\
&amp; \quad \phi_i (x(t),u(t)) \geq 0, i=1,\dots,q.
\end{split}
\tag{3.9}
\end{equation}\]</span>
where <span class="math inline">\(g_T\)</span> the terminal cost, <span class="math inline">\(g\)</span> the running cost, <span class="math inline">\(x_0\)</span> the initial condition, <span class="math inline">\(\mathcal{X}\)</span> the state constraint set, <span class="math inline">\(\mathcal{U}\)</span> the control constraint set, and <span class="math inline">\(\{ \phi_i \}_{i=1}^q\)</span> are general state-control constraints. We assume that the state constraint set <span class="math inline">\(\mathcal{X}\)</span> and control constraint set <span class="math inline">\(\mathcal{U}\)</span> can be described by a finite set of inequalities, i.e.,
<span class="math display">\[
\mathcal{X} = \{x \in \mathbb{R}^n \mid c^x_i(x) \geq 0, i =1,\dots,q_x\}, \quad \mathcal{U} = \{u \in \mathbb{R}^m \mid c^u_i(u) \geq 0, i = 1,\dots,q_u \}.
\]</span>
Assume that all functions are differentiable.</p>
<p>Our goal is to “transcribe” the infinite-dimensional continuous-time optimization <a href="approximatedp.html#eq:constrained-optimal-control">(3.9)</a> into a <em>nonlinear programming problem</em> (NLP) of the following form
<span class="math display" id="eq:ocp-nonlinear-programming">\[\begin{equation}
\begin{split}
\min_{v \in \mathbb{R}^{n_v}} &amp; \quad c(v) \\
\text{subject to} &amp; \quad c_{\text{eq},i}(v) = 0, i =1,\dots, n_{\text{eq}} \\
&amp; \quad c_{\text{ineq},i}(v) \geq 0, i=1,\dots,n_{\text{ineq}},
\end{split}
\tag{3.10}
\end{equation}\]</span>
where <span class="math inline">\(v\)</span> is a finite-dimensional vector variable to be optimized, <span class="math inline">\(c, c_{\text{eq},i}, c_{\text{ineq},i}\)</span> are (continuously differentiable) objective and constraint functions. Once we have done the transcription from <a href="approximatedp.html#eq:constrained-optimal-control">(3.9)</a> to <a href="approximatedp.html#eq:ocp-nonlinear-programming">(3.10)</a>, then we have a good number of numerical optimization algorithms at our disposal. For example, the Matlab <a href="https://www.mathworks.com/help/optim/ug/fmincon.html"><code>fmincon</code></a> provides a nice interface to many such algorithms, e.g., interior-point method and sequential quadratic programming. You have already played with a simple example of <code>fmincon</code> in Exercise <a href="psets.html#exr:polygoninsidecircle">5.1</a>. Other well-implemented NLP algorithms include <a href="https://github.com/coin-or/Ipopt">IPOPT</a> and <a href="http://www.sbsi-sol-optimize.com/asp/sol_product_snopt.htm">SNOPT</a>. However, usually the Matlab <code>fmincon</code> gives a decent start point for trying out different algorithms before moving to commercial solvers such as SNOPT. For a more comprehensive understanding about how NLP algoithms work under the hood, I suggest reading <span class="citation">(<a href="#ref-nocedal99book-numerical">Nocedal and Wright 1999</a>)</span>.</p>
<p>Before we talk about how to transcribe the OCP into an NLP, let us use a simple example to get a taste of <code>fmincon</code>.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:fmincon-example" class="example"><strong>Example 3.4  (Fire a Canon with fmincon) </strong></span>On a 2D plane, suppose we have a canon at the origin <span class="math inline">\((0,0)\)</span>, and we want to fire the canon, with control over the initial velocity <span class="math inline">\((v_1, v_2)\)</span>, so that the canon ball hits the target location <span class="math inline">\((10,10)\)</span>.</p>
<p>From our basic physics knowledge, we know the trajectory of the canon ball is described by
<span class="math display">\[
\begin{cases}
x_1(t) = v_1 t \\
x_2(t) = v_2 t - \frac{1}{2}g t^2,
\end{cases}
\]</span>
where <span class="math inline">\(g\)</span> is the gravity constant.</p>
<p>With the trajectory of the canon ball written above, we can formulate our nonlinear programming problem (NLP) as:
<span class="math display" id="eq:fire-canon">\[\begin{equation}
\begin{split}
\min_{v_1, v_2, T} &amp; \quad \frac{1}{2} (v_1^2 + v_2^2) \\
\text{subject to} &amp; \quad v_1, v_2, T \geq 0 \\
&amp; \quad x_1(T) = v_1 T = 10 \\
&amp; \quad x_2(T) = v_2 T - \frac{1}{2}g T^2 = 10
\end{split}
\tag{3.11}
\end{equation}\]</span>
where our unknown variables are the initial velocities and the time <span class="math inline">\(T\)</span> at which the canon ball hits the target. In the NLP <a href="approximatedp.html#eq:fire-canon">(3.11)</a>, the first constraint asks all our varaibles to be nonnegative, the second and third constraints enforce the canon ball to hit the target.</p>
<p>The following script formulates the NLP <a href="approximatedp.html#eq:fire-canon">(3.11)</a> in matlab.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb1-1"><a href="approximatedp.html#cb1-1" tabindex="-1"></a><span class="va">clc</span><span class="op">;</span> <span class="va">clear</span><span class="op">;</span> <span class="va">close</span> <span class="va">all</span><span class="op">;</span></span>
<span id="cb1-2"><a href="approximatedp.html#cb1-2" tabindex="-1"></a><span class="va">g</span> <span class="op">=</span> <span class="fl">9.8</span><span class="op">;</span></span>
<span id="cb1-3"><a href="approximatedp.html#cb1-3" tabindex="-1"></a><span class="va">x0</span> <span class="op">=</span> [<span class="fl">0</span><span class="op">;</span><span class="fl">0</span><span class="op">;</span><span class="fl">1</span>]<span class="op">;</span> <span class="co">% initial guess of the solution</span></span>
<span id="cb1-4"><a href="approximatedp.html#cb1-4" tabindex="-1"></a><span class="co">% define objective function</span></span>
<span id="cb1-5"><a href="approximatedp.html#cb1-5" tabindex="-1"></a><span class="va">obj</span> <span class="op">=</span> <span class="op">@</span>(<span class="va">x</span>) <span class="va">objective</span>(<span class="va">x</span>)<span class="op">;</span></span>
<span id="cb1-6"><a href="approximatedp.html#cb1-6" tabindex="-1"></a><span class="co">% define nonlinear constraints</span></span>
<span id="cb1-7"><a href="approximatedp.html#cb1-7" tabindex="-1"></a><span class="va">nonlincon</span> <span class="op">=</span> <span class="op">@</span>(<span class="va">x</span>) <span class="va">nonlinear_con</span>(<span class="va">x</span><span class="op">,</span><span class="va">g</span>)<span class="op">;</span></span>
<span id="cb1-8"><a href="approximatedp.html#cb1-8" tabindex="-1"></a><span class="co">% define options for fmincon</span></span>
<span id="cb1-9"><a href="approximatedp.html#cb1-9" tabindex="-1"></a><span class="va">options</span> <span class="op">=</span> <span class="va">optimoptions</span>(<span class="ss">&#39;fmincon&#39;</span><span class="op">,</span><span class="ss">&#39;Algorithm&#39;</span><span class="op">,</span><span class="ss">&#39;interior-point&#39;</span><span class="op">,...</span></span>
<span id="cb1-10"><a href="approximatedp.html#cb1-10" tabindex="-1"></a>    <span class="ss">&#39;SpecifyObjectiveGradient&#39;</span><span class="op">,</span><span class="va">true</span><span class="op">,</span><span class="ss">&#39;SpecifyConstraintGradient&#39;</span><span class="op">,</span><span class="va">true</span><span class="op">,...</span></span>
<span id="cb1-11"><a href="approximatedp.html#cb1-11" tabindex="-1"></a>    <span class="ss">&#39;checkGradients&#39;</span><span class="op">,</span><span class="va">false</span>)<span class="op">;</span></span>
<span id="cb1-12"><a href="approximatedp.html#cb1-12" tabindex="-1"></a><span class="co">% call fmincon</span></span>
<span id="cb1-13"><a href="approximatedp.html#cb1-13" tabindex="-1"></a>[<span class="va">xopt</span><span class="op">,</span><span class="va">fopt</span><span class="op">,~,</span><span class="va">out</span>] <span class="op">=</span> <span class="va">fmincon</span>(<span class="va">obj</span><span class="op">,</span><span class="va">x0</span><span class="op">,...</span> <span class="co">% objective and initial guess</span></span>
<span id="cb1-14"><a href="approximatedp.html#cb1-14" tabindex="-1"></a>    <span class="op">-</span><span class="va">eye</span>(<span class="fl">3</span>)<span class="op">,</span><span class="va">zeros</span>(<span class="fl">3</span><span class="op">,</span><span class="fl">1</span>)<span class="op">,...</span> <span class="co">% linear inequality constraints</span></span>
<span id="cb1-15"><a href="approximatedp.html#cb1-15" tabindex="-1"></a>    []<span class="op">,</span>[]<span class="op">,...</span> <span class="co">% no linear equality constraints</span></span>
<span id="cb1-16"><a href="approximatedp.html#cb1-16" tabindex="-1"></a>    []<span class="op">,</span>[]<span class="op">,...</span> <span class="co">% no upper and lower bounds</span></span>
<span id="cb1-17"><a href="approximatedp.html#cb1-17" tabindex="-1"></a>    <span class="va">nonlincon</span><span class="op">,...</span> <span class="co">% nonlinear constraints</span></span>
<span id="cb1-18"><a href="approximatedp.html#cb1-18" tabindex="-1"></a>    <span class="va">options</span>)<span class="op">;</span></span>
<span id="cb1-19"><a href="approximatedp.html#cb1-19" tabindex="-1"></a><span class="va">fprintf</span>(<span class="st">&quot;Maximum constraint violation: %3.2f.\n&quot;</span><span class="op">,</span><span class="va">out</span>.<span class="va">constrviolation</span>)<span class="op">;</span></span>
<span id="cb1-20"><a href="approximatedp.html#cb1-20" tabindex="-1"></a><span class="va">fprintf</span>(<span class="st">&quot;Objective: %3.2f.\n&quot;</span><span class="op">,</span><span class="va">fopt</span>)<span class="op">;</span></span>
<span id="cb1-21"><a href="approximatedp.html#cb1-21" tabindex="-1"></a><span class="co">% plot the solution</span></span>
<span id="cb1-22"><a href="approximatedp.html#cb1-22" tabindex="-1"></a><span class="va">T</span> <span class="op">=</span> <span class="va">xopt</span>(<span class="fl">3</span>)<span class="op">;</span></span>
<span id="cb1-23"><a href="approximatedp.html#cb1-23" tabindex="-1"></a><span class="va">t</span> <span class="op">=</span> <span class="fl">0</span><span class="op">:</span><span class="fl">0.01</span><span class="op">:</span><span class="va">T</span><span class="op">;</span></span>
<span id="cb1-24"><a href="approximatedp.html#cb1-24" tabindex="-1"></a><span class="va">xt</span> <span class="op">=</span> <span class="va">xopt</span>(<span class="fl">1</span>)<span class="op">*</span><span class="va">t</span><span class="op">;</span></span>
<span id="cb1-25"><a href="approximatedp.html#cb1-25" tabindex="-1"></a><span class="va">yt</span> <span class="op">=</span> <span class="va">xopt</span>(<span class="fl">2</span>)<span class="op">*</span><span class="va">t</span> <span class="op">-</span> <span class="fl">0.5</span><span class="op">*</span><span class="va">g</span><span class="op">*</span>(<span class="va">t</span><span class="op">.^</span><span class="fl">2</span>)<span class="op">;</span></span>
<span id="cb1-26"><a href="approximatedp.html#cb1-26" tabindex="-1"></a><span class="va">figure</span><span class="op">;</span></span>
<span id="cb1-27"><a href="approximatedp.html#cb1-27" tabindex="-1"></a><span class="va">plot</span>(<span class="va">xt</span><span class="op">,</span><span class="va">yt</span><span class="op">,</span><span class="ss">&#39;LineWidth&#39;</span><span class="op">,</span><span class="fl">2</span>)<span class="op">;</span></span>
<span id="cb1-28"><a href="approximatedp.html#cb1-28" tabindex="-1"></a><span class="va">hold</span> <span class="va">on</span></span>
<span id="cb1-29"><a href="approximatedp.html#cb1-29" tabindex="-1"></a><span class="va">scatter</span>(<span class="fl">10</span><span class="op">,</span><span class="fl">10</span><span class="op">,</span><span class="fl">100</span><span class="op">,</span><span class="st">&quot;red&quot;</span><span class="op">,</span><span class="ss">&#39;filled&#39;</span><span class="op">,</span><span class="ss">&#39;diamond&#39;</span>)<span class="op">;</span></span>
<span id="cb1-30"><a href="approximatedp.html#cb1-30" tabindex="-1"></a><span class="va">axis</span> <span class="va">equal</span><span class="op">;</span> <span class="va">grid</span> <span class="va">on</span><span class="op">;</span></span>
<span id="cb1-31"><a href="approximatedp.html#cb1-31" tabindex="-1"></a><span class="va">xlabel</span>(<span class="ss">&#39;$x_1(t)$&#39;</span><span class="op">,</span><span class="ss">&#39;FontSize&#39;</span><span class="op">,</span><span class="fl">24</span><span class="op">,</span><span class="ss">&#39;Interpreter&#39;</span><span class="op">,</span><span class="ss">&#39;latex&#39;</span>)<span class="op">;</span></span>
<span id="cb1-32"><a href="approximatedp.html#cb1-32" tabindex="-1"></a><span class="va">ylabel</span>(<span class="ss">&#39;$x_2(t)$&#39;</span><span class="op">,</span><span class="ss">&#39;FontSize&#39;</span><span class="op">,</span><span class="fl">24</span><span class="op">,</span><span class="ss">&#39;Interpreter&#39;</span><span class="op">,</span><span class="ss">&#39;latex&#39;</span>)<span class="op">;</span></span>
<span id="cb1-33"><a href="approximatedp.html#cb1-33" tabindex="-1"></a><span class="va">ax</span> <span class="op">=</span> <span class="va">gca</span><span class="op">;</span> <span class="va">ax</span>.<span class="va">FontSize</span> <span class="op">=</span> <span class="fl">20</span><span class="op">;</span></span>
<span id="cb1-34"><a href="approximatedp.html#cb1-34" tabindex="-1"></a><span class="co">%% helper functions</span></span>
<span id="cb1-35"><a href="approximatedp.html#cb1-35" tabindex="-1"></a><span class="kw">function</span> [<span class="va">f</span><span class="op">,</span><span class="va">df</span>] <span class="op">=</span> <span class="va">objective</span>(<span class="va">x</span>)</span>
<span id="cb1-36"><a href="approximatedp.html#cb1-36" tabindex="-1"></a><span class="co">% x is our decision variable: [v1, v2, T]</span></span>
<span id="cb1-37"><a href="approximatedp.html#cb1-37" tabindex="-1"></a><span class="co">% objective function</span></span>
<span id="cb1-38"><a href="approximatedp.html#cb1-38" tabindex="-1"></a><span class="va">f</span> <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="va">x</span>(<span class="fl">1</span>)<span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">x</span>(<span class="fl">2</span>)<span class="op">^</span><span class="fl">2</span>)<span class="op">;</span></span>
<span id="cb1-39"><a href="approximatedp.html#cb1-39" tabindex="-1"></a><span class="co">% gradient of the objective function (optional)</span></span>
<span id="cb1-40"><a href="approximatedp.html#cb1-40" tabindex="-1"></a><span class="va">df</span> <span class="op">=</span> [<span class="va">x</span>(<span class="fl">1</span>)<span class="op">;</span> <span class="va">x</span>(<span class="fl">2</span>)<span class="op">;</span> <span class="fl">0</span>]<span class="op">;</span> <span class="co">% column vector</span></span>
<span id="cb1-41"><a href="approximatedp.html#cb1-41" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb1-42"><a href="approximatedp.html#cb1-42" tabindex="-1"></a></span>
<span id="cb1-43"><a href="approximatedp.html#cb1-43" tabindex="-1"></a><span class="kw">function</span> [<span class="va">c</span><span class="op">,</span><span class="va">ceq</span><span class="op">,</span><span class="va">dc</span><span class="op">,</span><span class="va">dceq</span>] <span class="op">=</span> <span class="va">nonlinear_con</span>(<span class="va">x</span><span class="op">,</span><span class="va">g</span>)</span>
<span id="cb1-44"><a href="approximatedp.html#cb1-44" tabindex="-1"></a><span class="co">% no inequality constraints</span></span>
<span id="cb1-45"><a href="approximatedp.html#cb1-45" tabindex="-1"></a><span class="va">c</span> <span class="op">=</span> []<span class="op">;</span> <span class="va">dc</span> <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb1-46"><a href="approximatedp.html#cb1-46" tabindex="-1"></a><span class="co">% two equality constraints</span></span>
<span id="cb1-47"><a href="approximatedp.html#cb1-47" tabindex="-1"></a><span class="va">ceq</span> <span class="op">=</span> [<span class="va">x</span>(<span class="fl">1</span>)<span class="op">*</span><span class="va">x</span>(<span class="fl">3</span>)<span class="op">-</span><span class="fl">10</span><span class="op">;</span></span>
<span id="cb1-48"><a href="approximatedp.html#cb1-48" tabindex="-1"></a>       <span class="va">x</span>(<span class="fl">2</span>)<span class="op">*</span><span class="va">x</span>(<span class="fl">3</span>)<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span><span class="va">g</span><span class="op">*</span><span class="va">x</span>(<span class="fl">3</span>)<span class="op">^</span><span class="fl">2</span><span class="op">-</span><span class="fl">10</span>]<span class="op">;</span></span>
<span id="cb1-49"><a href="approximatedp.html#cb1-49" tabindex="-1"></a><span class="co">% explicit gradient of ceq</span></span>
<span id="cb1-50"><a href="approximatedp.html#cb1-50" tabindex="-1"></a><span class="va">dceq</span> <span class="op">=</span> [<span class="va">x</span>(<span class="fl">3</span>)<span class="op">,</span> <span class="fl">0</span><span class="op">;</span></span>
<span id="cb1-51"><a href="approximatedp.html#cb1-51" tabindex="-1"></a>        <span class="fl">0</span><span class="op">,</span> <span class="va">x</span>(<span class="fl">3</span>)<span class="op">;</span></span>
<span id="cb1-52"><a href="approximatedp.html#cb1-52" tabindex="-1"></a>        <span class="va">x</span>(<span class="fl">1</span>)<span class="op">,</span> <span class="va">x</span>(<span class="fl">2</span>)<span class="op">-</span><span class="va">g</span><span class="op">*</span><span class="va">x</span>(<span class="fl">3</span>)]<span class="op">;</span></span>
<span id="cb1-53"><a href="approximatedp.html#cb1-53" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>Running the code above, we get the following trajectory</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fire-canon-ball"></span>
<img src="images/canon_ball.png" alt="Trajectory of the canon ball obtained from fmincon." width="70%" />
<p class="caption">
Figure 3.2: Trajectory of the canon ball obtained from fmincon.
</p>
</div>
<p>What if you change the initial guess to <code>fmincon</code>, or add more constraints? Feel free to play with the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/fire_canon_ball_fmincon.m">here</a>.</p>
</div>
</div>
<p>It turns out there are multiple different ways to transcribe the optimal control problem <a href="approximatedp.html#eq:constrained-optimal-control">(3.9)</a> as the nonlinear programming problem <a href="approximatedp.html#eq:ocp-nonlinear-programming">(3.10)</a>. In the following, we will introduce three different formulations, direct single shooting, direct multiple shooting, and direct collocation.</p>
<div id="direct-single-shooting" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Direct Single Shooting<a href="approximatedp.html#direct-single-shooting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In direct single shooting, we transcribe the OCP to the NLP by only optimizing a sequence of control values, with the intuition being that the state values are functions of the controls by invoking the system dynamics. In particular, we follow the procedure below.</p>
<p><strong>Time discretization</strong>. We first discretize the total time window <span class="math inline">\([0,T]\)</span> into a set of <span class="math inline">\(N\)</span> intervals:
<span class="math display">\[
0 = t_0 \leq t_1 \leq \dots \leq t_k \leq t_{k+1} \leq \dots \leq t_N = T.
\]</span>
We denote
<span class="math display">\[
h_k = t_{k+1} - t_k
\]</span>
as the length of the <span class="math inline">\(k\)</span>-th time interval.</p>
<p><strong>Piece-wise constant control</strong>. We will approximate the continuous-time control signal <span class="math inline">\(u(t)\)</span> as a piece-wise constant function, i.e.,
<span class="math display">\[
u(t) = u_k, \forall t \in [t_k, t_{k+1}).
\]</span>
This is shown in Fig. <a href="approximatedp.html#fig:direct-single-shooting">3.3</a>. Let us collect all the constant control values as our decision variable to be optimized
<span class="math display" id="eq:direct-single-shooting-v">\[\begin{equation}
v = \begin{bmatrix} u_0 \\ u_1 \\ \vdots \\ u_{N-1} \end{bmatrix} \in \mathbb{R}^{N m}.
\tag{3.12}
\end{equation}\]</span>
This <span class="math inline">\(v\)</span> will be our variable in the NLP <a href="approximatedp.html#eq:ocp-nonlinear-programming">(3.10)</a>. Note that <span class="math inline">\(v\)</span> has dimension <span class="math inline">\(N m\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:direct-single-shooting"></span>
<img src="images/direct_single_shooting.png" alt="Direct single shooting." width="60%" />
<p class="caption">
Figure 3.3: Direct single shooting.
</p>
</div>
<p><strong>Dynamics integration</strong>. It is clear that once <span class="math inline">\(v\)</span> in <a href="approximatedp.html#eq:direct-single-shooting-v">(3.12)</a>, i.e., the set of controls, is determined, then the state trajectory <span class="math inline">\(x(t)\)</span> is also uniquely determined by the initial condition <span class="math inline">\(x(0) = x_0\)</span> and the dynamics
<span class="math display">\[
\dot{x}(t) = f(x(t),u(t)).
\]</span>
In order to enforce the state constraint <span class="math inline">\(x(t) \in \mathcal{X}\)</span>, we will enforce the values of <span class="math inline">\(x\)</span> at <span class="math inline">\(t_0,t_1,\dots,t_N\)</span>, i.e., <span class="math inline">\(x_k = x(t_k)\)</span>, to lie in the constraint set <span class="math inline">\(\mathcal{X}\)</span>. To do so, we need to integrate the dynamics. When the dynamics is linear, this integration can be done in closed form:
<span class="math display">\[
x_{k+1} = x_k + \int_{\tau=t_k}^{t_k + h_k} A x(\tau) + B u(\tau) d\tau = e^{Ah_k} x_k + A^{-1}(e^{Ah_k} - I) B u_k.
\]</span>
By running the above equation from <span class="math inline">\(k=0\)</span> to <span class="math inline">\(k=N-1\)</span>, we obtain the values of <span class="math inline">\(x_k,k=1,\dots,N\)</span> as functions of the control vectors <span class="math inline">\(v\)</span>, shown in Fig. <a href="approximatedp.html#fig:direct-single-shooting">3.3</a>. When the dynamics is nonlinear, we will need to perform the dynamics integration using numerical integration. One of the most well known family of integrators is called <a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#Adaptive_Runge%E2%80%93Kutta_methods">“Runge-Kutta methods”</a>. Among the family of methods, RK45 is perhaps the most popular one. We now briefly describe how a fourth-order RK integrator, i.e., RK4, works. Suppose we have already computed <span class="math inline">\(x_k\)</span>, and we want to integrate the dynamics to obtain <span class="math inline">\(x_{k+1}\)</span>. The RK4 integrator first computes the time derivatives at a sequence of four points:
<span class="math display">\[\begin{align}
\alpha_1 &amp;= f(x_k, u_k) \\
\alpha_2 &amp;= f\left( x_k + \frac{1}{2} h_k \alpha_1, u_k \right) \\
\alpha_3 &amp;= f\left( x_k + \frac{1}{2} h_k \alpha_2, u_k \right) \\
\alpha_4 &amp;= f\left( x_k + h_k \alpha_3, u_k \right).
\end{align}\]</span>
Then we can obtain <span class="math inline">\(x_{k+1}\)</span> via a weighted sum of the <span class="math inline">\(\alpha_i\)</span>’s
<span class="math display" id="eq:RK4-integrator">\[\begin{equation}
x_{k+1} = x_k + \frac{1}{6} h_k \left( \alpha_1 + 2 \alpha_2 + 2 \alpha_3 + \alpha_4 \right) = \text{RK4}(x_k, u_k).
\tag{3.13}
\end{equation}\]</span>
Notice that a naive integrator would just perform <span class="math inline">\(x_{k+1} = x_k + h_k \alpha_1\)</span> without querying the gradients at three other points. The nice property of RK4 is that it leads to better accuracy than the naive integration. Writing the RK4 integrator recursively, we obtain
<span class="math display">\[
x_{k+1} = \text{RK4}(x_k, u_k) = \text{RK4}(\text{RK4}(x_{k-1},u_{k-1}),u_k) = \text{RK4}(\text{RK4}(...(x_0,u_0))...u_k),
\]</span>
where RK4 is invoked for <span class="math inline">\(k+1\)</span> times. Clearly, each <span class="math inline">\(x_k\)</span> is a complicated function of the sequence of controls <span class="math inline">\(v\)</span> and the initial state <span class="math inline">\(x_0\)</span>. We will write <span class="math inline">\(x_k(v)\)</span> to make this explict, as shown in Fig. <a href="approximatedp.html#fig:direct-single-shooting">3.3</a>.</p>
<p><strong>Objective approximation</strong>. We can approximate the objective of the OCP <a href="approximatedp.html#eq:constrained-optimal-control">(3.9)</a> using trapezoidal integration:
<span class="math display">\[
g_T(x(T)) + \int_{t=0}^T g(x(t),u(t)) dt \approx g_T(x_N(v)) + \sum_{k=0}^{N-1} \frac{h_k}{2}(g(x_k(v),u_k) + g(x_{k+1}(v),u_{k+1})).
\]</span></p>
<p><strong>Summary</strong>. In summary, the transcribed NLP for the optimal control problem using direct single shooting would be
<span class="math display" id="eq:ocp-direct-single-shooting">\[\begin{equation}
\begin{split}
\min_{v = (u_0,\dots,u_{N-1})} &amp; \quad g_T(x_N(v)) + \sum_{k=0}^{N-1} \frac{g(x_k(v),u_k) + g(x_{k+1}(v), u_{k+1})}{2} h_k \\
\text{subject to} &amp; \quad c^u_i(u_k) \geq 0, i=1,\dots,q_u, \quad k=0,\dots,N-1 \\
&amp; \quad c^x_i(x_k(v)) \geq 0, i=1,\dots,q_x, \quad k=1,\dots,N \\
&amp; \quad \phi_i(x_k(v),u_k) \geq 0, i=1,\dots, q, \quad k=0,\dots,N.
\end{split}
\tag{3.14}
\end{equation}\]</span></p>
<p><strong>Pros and Cons</strong>. The advantage of using direct single shooting to transcribe the OCP is clear: it leads to fewer variables because the only decision variables in <a href="approximatedp.html#eq:ocp-direct-single-shooting">(3.14)</a> are the sequence of controls. In other transcription methods, the decision variable of the NLP typically involves the states as well. The disadvantage of direct single shooting is the complication of the function <span class="math inline">\(x_k(v)\)</span> caused by numerical integrators such as RK4. Even though the original continuous-time dynamics <span class="math inline">\(f(x,u)\)</span> may be simple, the RK4 function can be highly complicated due to evaluating <span class="math inline">\(\dot{x}\)</span> at multiple other locations. Moreover, the recursion of the RK4 operator makes this complication even worse.</p>
<p>Let us try the RK4 simulator for a simple example.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:RK4-simulation" class="example"><strong>Example 3.5  (Propagation of Nonlinearities) </strong></span>Consider the following dynamics
<span class="math display">\[
x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}, \quad
\dot{x} = \begin{bmatrix}
10(x_2 - x_1) \\
x_1(u - x_3) - x_2 \\
x_1 x_2 - 3 x_3
\end{bmatrix}
\]</span>
where <span class="math inline">\(u\)</span> is a single scalar control. We are interested in running the RK4 simulator for <span class="math inline">\(100\)</span> seconds (with <span class="math inline">\(h_k = 0.01\)</span> for all <span class="math inline">\(k\)</span>) at <span class="math inline">\(x_0 = [1,0,0]^T\)</span> with <span class="math inline">\(u\)</span> varying from <span class="math inline">\(0\)</span> to <span class="math inline">\(100\)</span>, and see how the nonlinearities propagate.</p>
<p>Fig. <a href="approximatedp.html#fig:RK4-simulate">3.4</a> plots <span class="math inline">\(x\)</span> as a function of <span class="math inline">\(u\)</span>. We can clearly see the nonlinearities getting worse due to RK4.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/RK4_example.m">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RK4-simulate"></span>
<img src="images/RK4-sim.png" alt="RK4 simulation." width="60%" />
<p class="caption">
Figure 3.4: RK4 simulation.
</p>
</div>
</div>
</div>
</div>
<div id="direct-multiple-shooting" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Direct Multiple Shooting<a href="approximatedp.html#direct-multiple-shooting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:direct-multiple-shooting"></span>
<img src="images/direct_multiple_shooting.png" alt="Direct multiple shooting." width="60%" />
<p class="caption">
Figure 3.5: Direct multiple shooting.
</p>
</div>
<p>Propagation of the nonlinearities through numerical integrators motivates using <em>direct multiple shooting</em> to transcribe the OCP problem into a nonlinear programming problem.</p>
<p>Instead of only optimizing the controls, direct multiple shooting optimizes both the controls and states. The decision variable in the NLP becomes
<span class="math display">\[
v = \begin{bmatrix}
u_0 \\ u_1 \\ \vdots \\ u_{N-1} \\ x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix} \in \mathbb{R}^{N(n+m)}.
\]</span>
With the state variables also introduced in the optimization, we no longer need to recursively run the RK4 integrators. Instead, we just need to run it once and enforce
<span class="math display">\[
x_{k+1} = \text{RK4}(x_k, u_k),
\]</span>
i.e., the current state and the next state satisfy the dynamics constraint. This is shown in Fig. <a href="approximatedp.html#fig:direct-multiple-shooting">3.5</a>.</p>
<p>In summary, the transcribed NLP for the OCP problem using direct multiple shooting becomes
<span class="math display" id="eq:ocp-direct-multiple-shooting">\[\begin{equation}
\begin{split}
\min_{v = (u_0,\dots,u_{N-1},x_1,\dots,x_N)} &amp; \quad g_T(x_N) + \sum_{k=0}^{N-1} \frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k \\
\text{subject to} &amp; \quad x_{k+1} = \text{RK4}(x_k, u_k), \quad k=0,\dots,N-1 \\
&amp; \quad c^u_i (u_k) \geq 0, i=1,\dots,q_u, \quad k=0,\dots,N-1 \\
&amp; \quad c^x_i(x_k) \geq 0, i=1,\dots,q_x, \quad k=1,\dots,N \\
&amp; \quad \phi_i(x_k, u_k) \geq 0, i=1,\dots,q, \quad k=0,\dots,N.
\end{split}
\tag{3.15}
\end{equation}\]</span></p>
<p>Direct multiple shooting avoids the recursion of numerical integrators such as RK4, but at the expense of introducing additional state variables in the NLP.</p>
<p>Let us try direct multiple shooting on the double integrator.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:multiple-shooting-double-integrator" class="example"><strong>Example 3.6  (Direct Multiple Shooting for Minimum-Time Double Integrator) </strong></span>The double integrator has continuous-time dynamics (which you have already seen in Exercise <a href="psets.html#exr:lqrconstraints">5.2</a>):
<span class="math display">\[
\ddot{q} = u.
\]</span>
In standard state-space form, the dynamics is linear
<span class="math display">\[
x = \begin{bmatrix} q \\ \dot{q} \end{bmatrix}, \quad
\dot{x} = \begin{bmatrix} 0 &amp; 1 \\ 0 &amp; 0 \end{bmatrix} x + \begin{bmatrix} 0 \\ 1 \end{bmatrix} u.
\]</span></p>
<p>Let us consider the minimum-time optimal control problem
<span class="math display">\[\begin{equation}
\begin{split}
\min_{u(t),t \in [0,T]} &amp; \quad T \\
\text{subject to} &amp; \quad \dot{x} = A x + Bu, \quad x(0) = x_0 \\
&amp; \quad x(T) = 0 \\
&amp; \quad u(t) \in [-1,1], \forall t \in [0,T].
\end{split}
\end{equation}\]</span>
which seeks to get from the initial condition <span class="math inline">\(x_0\)</span> to the origin as fast as possible.</p>
<p>You should know from our physics intuition that the optimal controller is bang-bang, i.e., the optimal control first accelerates (deaccelerates) using the maximum control and then reverses using the opposite maximum control.</p>
<p>Let’s see if we can obtain the optimal controller using direct multiple shooting.</p>
<p>In direct multiple shooting, we will optimize the control trajectory, the state trajectory, and the final time <span class="math inline">\(T\)</span>. We fix the number of intervals <span class="math inline">\(N\)</span>, and discretize <span class="math inline">\([0,T]\)</span> evenly into <span class="math inline">\(N\)</span> intervals, which leads to variables for the NLP as
<span class="math display">\[
v=(T,x_0,\dots,x_{N},u_0,\dots,u_N).
\]</span>
We can easily enforce the control constraints:
<span class="math display">\[
u_k \in [-1,1],k=0,\dots,N,
\]</span>
and the initial / terminal constraints
<span class="math display">\[
x_0 = x_0, \quad x_N = 0.
\]</span>
The only nontrivial constraint is the dynamics constraint:
<span class="math display">\[
x_{k+1} = \text{RK4}(x_k,u_k),k=0,\dots,N-1.
\]</span></p>
<p>The following script shows how to use the Matlab integrator <code>ode45</code> to enforce the dynamics constraint.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb2-1"><a href="approximatedp.html#cb2-1" tabindex="-1"></a><span class="kw">function</span> <span class="va">dx</span> <span class="op">=</span> <span class="va">double_integrator</span>(<span class="va">t</span><span class="op">,</span><span class="va">states</span><span class="op">,</span><span class="va">v</span>)</span>
<span id="cb2-2"><a href="approximatedp.html#cb2-2" tabindex="-1"></a><span class="co">% return xdot at the selected times t and states, using information from v</span></span>
<span id="cb2-3"><a href="approximatedp.html#cb2-3" tabindex="-1"></a><span class="co">% assume the controls in v define piece-wise constant control signal</span></span>
<span id="cb2-4"><a href="approximatedp.html#cb2-4" tabindex="-1"></a><span class="va">T</span> <span class="op">=</span> <span class="va">v</span>(<span class="fl">1</span>)<span class="op">;</span> <span class="co">% final time</span></span>
<span id="cb2-5"><a href="approximatedp.html#cb2-5" tabindex="-1"></a><span class="va">N</span> <span class="op">=</span> (<span class="va">length</span>(<span class="va">v</span>) <span class="op">-</span> <span class="fl">1</span>) <span class="op">/</span> <span class="fl">3</span><span class="op">;</span> <span class="co">% number of knot points</span></span>
<span id="cb2-6"><a href="approximatedp.html#cb2-6" tabindex="-1"></a><span class="va">u_grid</span> <span class="op">=</span> <span class="va">v</span>(<span class="fl">2</span><span class="op">+</span><span class="fl">2</span><span class="op">*</span><span class="va">N</span><span class="op">:</span><span class="fl">3</span><span class="op">*</span><span class="va">N</span><span class="op">+</span><span class="fl">1</span>)<span class="op">;</span> <span class="co">% N controls</span></span>
<span id="cb2-7"><a href="approximatedp.html#cb2-7" tabindex="-1"></a><span class="va">t_grid</span> <span class="op">=</span> <span class="va">linspace</span>(<span class="fl">0</span><span class="op">,</span><span class="fl">1</span><span class="op">,</span><span class="va">N</span>)<span class="op">*</span><span class="va">T</span><span class="op">;</span></span>
<span id="cb2-8"><a href="approximatedp.html#cb2-8" tabindex="-1"></a><span class="va">u_t</span> <span class="op">=</span> <span class="va">interp1</span>(<span class="va">t_grid</span><span class="op">,</span><span class="va">u_grid</span><span class="op">,</span><span class="va">t</span><span class="op">,</span><span class="ss">&#39;previous&#39;</span>)<span class="op">;</span> <span class="co">% piece-wise constant</span></span>
<span id="cb2-9"><a href="approximatedp.html#cb2-9" tabindex="-1"></a><span class="va">A</span> <span class="op">=</span> [<span class="fl">0</span> <span class="fl">1</span><span class="op">;</span> <span class="fl">0</span> <span class="fl">0</span>]<span class="op">;</span></span>
<span id="cb2-10"><a href="approximatedp.html#cb2-10" tabindex="-1"></a><span class="va">B</span> <span class="op">=</span> [<span class="fl">0</span><span class="op">;</span> <span class="fl">1</span>]<span class="op">;</span></span>
<span id="cb2-11"><a href="approximatedp.html#cb2-11" tabindex="-1"></a><span class="va">dx</span> <span class="op">=</span> <span class="va">A</span> <span class="op">*</span> <span class="va">states</span> <span class="op">+</span> <span class="va">B</span> <span class="op">*</span> <span class="va">u_t</span><span class="op">;</span></span>
<span id="cb2-12"><a href="approximatedp.html#cb2-12" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb2-13"><a href="approximatedp.html#cb2-13" tabindex="-1"></a></span>
<span id="cb2-14"><a href="approximatedp.html#cb2-14" tabindex="-1"></a><span class="kw">function</span> [<span class="va">c</span><span class="op">,</span><span class="va">ceq</span>] <span class="op">=</span> <span class="va">double_integrator_nonlincon</span>(<span class="va">v</span><span class="op">,</span><span class="va">initial_state</span>)</span>
<span id="cb2-15"><a href="approximatedp.html#cb2-15" tabindex="-1"></a><span class="co">% enforce x_k+1 = RK45(x_k, u_k); integration done using ode45</span></span>
<span id="cb2-16"><a href="approximatedp.html#cb2-16" tabindex="-1"></a><span class="va">T</span> <span class="op">=</span> <span class="va">v</span>(<span class="fl">1</span>)<span class="op">;</span> <span class="co">% final time</span></span>
<span id="cb2-17"><a href="approximatedp.html#cb2-17" tabindex="-1"></a><span class="va">N</span> <span class="op">=</span> (<span class="va">length</span>(<span class="va">v</span>) <span class="op">-</span> <span class="fl">1</span>) <span class="op">/</span> <span class="fl">3</span><span class="op">;</span> <span class="co">% number of knot points</span></span>
<span id="cb2-18"><a href="approximatedp.html#cb2-18" tabindex="-1"></a><span class="va">t_grid</span> <span class="op">=</span> <span class="va">linspace</span>(<span class="fl">0</span><span class="op">,</span><span class="fl">1</span><span class="op">,</span><span class="va">N</span>)<span class="op">*</span><span class="va">T</span><span class="op">;</span></span>
<span id="cb2-19"><a href="approximatedp.html#cb2-19" tabindex="-1"></a><span class="va">x1</span> <span class="op">=</span> <span class="va">v</span>(<span class="fl">2</span><span class="op">:</span><span class="va">N</span><span class="op">+</span><span class="fl">1</span>)<span class="op">;</span> <span class="co">% position </span></span>
<span id="cb2-20"><a href="approximatedp.html#cb2-20" tabindex="-1"></a><span class="va">x2</span> <span class="op">=</span> <span class="va">v</span>(<span class="fl">2</span><span class="op">+</span><span class="va">N</span><span class="op">:</span><span class="fl">2</span><span class="op">*</span><span class="va">N</span><span class="op">+</span><span class="fl">1</span>)<span class="op">;</span> <span class="co">% velocity</span></span>
<span id="cb2-21"><a href="approximatedp.html#cb2-21" tabindex="-1"></a><span class="co">% u = v(2+2*N:3*N+1); % controls</span></span>
<span id="cb2-22"><a href="approximatedp.html#cb2-22" tabindex="-1"></a><span class="co">% no inequality constraints</span></span>
<span id="cb2-23"><a href="approximatedp.html#cb2-23" tabindex="-1"></a><span class="va">c</span> <span class="op">=</span> []<span class="op">;</span> </span>
<span id="cb2-24"><a href="approximatedp.html#cb2-24" tabindex="-1"></a><span class="co">% equality constraints</span></span>
<span id="cb2-25"><a href="approximatedp.html#cb2-25" tabindex="-1"></a><span class="va">ceq</span> <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb2-26"><a href="approximatedp.html#cb2-26" tabindex="-1"></a><span class="kw">for</span> <span class="va">i</span> <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>(<span class="va">N</span><span class="op">-</span><span class="fl">1</span>)</span>
<span id="cb2-27"><a href="approximatedp.html#cb2-27" tabindex="-1"></a>    <span class="va">ti</span> <span class="op">=</span> <span class="va">t_grid</span>(<span class="va">i</span>)<span class="op">;</span></span>
<span id="cb2-28"><a href="approximatedp.html#cb2-28" tabindex="-1"></a>    <span class="va">tip1</span> <span class="op">=</span> <span class="va">t_grid</span>(<span class="va">i</span><span class="op">+</span><span class="fl">1</span>)<span class="op">;</span></span>
<span id="cb2-29"><a href="approximatedp.html#cb2-29" tabindex="-1"></a>    <span class="va">xi</span> <span class="op">=</span> [<span class="va">x1</span>(<span class="va">i</span>)<span class="op">;</span><span class="va">x2</span>(<span class="va">i</span>)]<span class="op">;</span></span>
<span id="cb2-30"><a href="approximatedp.html#cb2-30" tabindex="-1"></a>    <span class="va">xip1</span> <span class="op">=</span> [<span class="va">x1</span>(<span class="va">i</span><span class="op">+</span><span class="fl">1</span>)<span class="op">;</span><span class="va">x2</span>(<span class="va">i</span><span class="op">+</span><span class="fl">1</span>)]<span class="op">;</span></span>
<span id="cb2-31"><a href="approximatedp.html#cb2-31" tabindex="-1"></a>    <span class="co">% integrate system dynamics starting from xi in [ti,tip1]</span></span>
<span id="cb2-32"><a href="approximatedp.html#cb2-32" tabindex="-1"></a>    <span class="va">tt</span> <span class="op">=</span> <span class="va">ti</span><span class="op">:</span>(<span class="va">tip1</span><span class="op">-</span><span class="va">ti</span>)<span class="op">/</span><span class="fl">20</span><span class="op">:</span><span class="va">tip1</span><span class="op">;</span> <span class="co">% fine-grained time discretization</span></span>
<span id="cb2-33"><a href="approximatedp.html#cb2-33" tabindex="-1"></a>    [<span class="op">~,</span><span class="va">sol_int</span>] <span class="op">=</span> <span class="va">ode45</span>(<span class="op">@</span>(<span class="va">t</span><span class="op">,</span><span class="va">y</span>) <span class="va">double_integrator</span>(<span class="va">t</span><span class="op">,</span><span class="va">y</span><span class="op">,</span><span class="va">v</span>)<span class="op">,</span><span class="va">tt</span><span class="op">,</span><span class="va">xi</span>)<span class="op">;</span></span>
<span id="cb2-34"><a href="approximatedp.html#cb2-34" tabindex="-1"></a>    <span class="va">xip1_int</span> <span class="op">=</span> <span class="va">sol_int</span>(<span class="kw">end</span><span class="op">,:</span>)<span class="op">;</span></span>
<span id="cb2-35"><a href="approximatedp.html#cb2-35" tabindex="-1"></a>    <span class="co">% enforce them to be the same</span></span>
<span id="cb2-36"><a href="approximatedp.html#cb2-36" tabindex="-1"></a>    <span class="va">ceq</span> <span class="op">=</span> [<span class="va">ceq</span><span class="op">;</span></span>
<span id="cb2-37"><a href="approximatedp.html#cb2-37" tabindex="-1"></a>           <span class="va">xip1_int</span>(<span class="fl">1</span>) <span class="op">-</span> <span class="va">xip1</span>(<span class="fl">1</span>)<span class="op">;</span></span>
<span id="cb2-38"><a href="approximatedp.html#cb2-38" tabindex="-1"></a>           <span class="va">xip1_int</span>(<span class="fl">2</span>) <span class="op">-</span> <span class="va">xip1</span>(<span class="fl">2</span>)]<span class="op">;</span></span>
<span id="cb2-39"><a href="approximatedp.html#cb2-39" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb2-40"><a href="approximatedp.html#cb2-40" tabindex="-1"></a><span class="co">% add initial state constraint</span></span>
<span id="cb2-41"><a href="approximatedp.html#cb2-41" tabindex="-1"></a><span class="va">ceq</span> <span class="op">=</span> [<span class="va">ceq</span><span class="op">;</span></span>
<span id="cb2-42"><a href="approximatedp.html#cb2-42" tabindex="-1"></a>       <span class="va">x1</span>(<span class="fl">1</span>) <span class="op">-</span> <span class="va">initial_state</span>(<span class="fl">1</span>)<span class="op">;</span></span>
<span id="cb2-43"><a href="approximatedp.html#cb2-43" tabindex="-1"></a>       <span class="va">x2</span>(<span class="fl">1</span>) <span class="op">-</span> <span class="va">initial_state</span>(<span class="fl">2</span>)]<span class="op">;</span></span>
<span id="cb2-44"><a href="approximatedp.html#cb2-44" tabindex="-1"></a></span>
<span id="cb2-45"><a href="approximatedp.html#cb2-45" tabindex="-1"></a><span class="co">% add terminal state constraint: land at origin</span></span>
<span id="cb2-46"><a href="approximatedp.html#cb2-46" tabindex="-1"></a><span class="va">ceq</span> <span class="op">=</span> [<span class="va">ceq</span><span class="op">;</span></span>
<span id="cb2-47"><a href="approximatedp.html#cb2-47" tabindex="-1"></a>       <span class="va">x1</span>(<span class="kw">end</span>)<span class="op">;</span></span>
<span id="cb2-48"><a href="approximatedp.html#cb2-48" tabindex="-1"></a>       <span class="va">x2</span>(<span class="kw">end</span>)]<span class="op">;</span> </span>
<span id="cb2-49"><a href="approximatedp.html#cb2-49" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>I first define a function <code>double_integrator</code> that returns the continuous-time dynaimcs, and then in the nonlinear constraints function <code>double_integrator_nonlincon</code> I use <code>ode45</code> to simulate the dynamics starting at <span class="math inline">\(x_k\)</span> from <span class="math inline">\(t_k\)</span> to <span class="math inline">\(t_{k+1}\)</span>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb3-1"><a href="approximatedp.html#cb3-1" tabindex="-1"></a>[<span class="op">~,</span><span class="va">sol_int</span>] <span class="op">=</span> <span class="va">ode45</span>(<span class="op">@</span>(<span class="va">t</span><span class="op">,</span><span class="va">y</span>) <span class="va">double_integrator</span>(<span class="va">t</span><span class="op">,</span><span class="va">y</span><span class="op">,</span><span class="va">v</span>)<span class="op">,</span><span class="va">tt</span><span class="op">,</span><span class="va">xi</span>)<span class="op">;</span></span></code></pre></div>
<p>The solution I got from <code>ode45</code> should be equal to my decision state variable.</p>
<p>Running the <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/double_integrator_multiple_shooting.m">complete code</a> with <span class="math inline">\(x_0 = [-10;0]\)</span> and <span class="math inline">\(N=51\)</span>, I obtain the control signal in Fig. <a href="approximatedp.html#fig:min-time-di-u-ms">3.6</a>, which is bang-bang.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:min-time-di-u-ms"></span>
<img src="images/min_time_di_u.png" alt="Optimal control signal by direct multiple shooting." width="70%" />
<p class="caption">
Figure 3.6: Optimal control signal by direct multiple shooting.
</p>
</div>
<p>Using <code>ode45</code> to integrate the double integrator dynamics from <span class="math inline">\(x_0\)</span> with the controller in Fig. <a href="approximatedp.html#fig:min-time-di-u-ms">3.6</a>, we obtain the following state trajectory. Notice that the final state does not exactly land at the origin. This is expected due to our time discretization and imperfect dynamics integration using <code>ode45</code>. Make sure you play with the code, e.g., by changing the initial state <span class="math inline">\(x_0\)</span> and see what happens.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:min-time-di-x-ms"></span>
<img src="images/min_time_di_x.png" alt="ODE45 integration with the optimal control signal found by direct multiple shooting." width="70%" />
<p class="caption">
Figure 3.7: ODE45 integration with the optimal control signal found by direct multiple shooting.
</p>
</div>
</div>
</div>
</div>
<div id="direct-collocation" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Direct Collocation<a href="approximatedp.html#direct-collocation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In direct multiple shooting, we still need to rely on the numerical integrator RK4, which complicates the original nonlinear dynamics. In direct collocation, we will remove our dependency of RK4.</p>
<p>The key idea of direct collocation is to approximate the state trajectory <span class="math inline">\(x(t)\)</span> and the control trajectory <span class="math inline">\(u(t)\)</span> as piece-wise polynomial functions. In the following, we will describe the Hermite-Simpson collocation method.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:direct-collocation"></span>
<img src="images/direct_collocation.png" alt="Direct collocation." width="60%" />
<p class="caption">
Figure 3.8: Direct collocation.
</p>
</div>
<p><strong>Time discretization</strong>. We first discretize the total time window <span class="math inline">\([0,T]\)</span> into a set of <span class="math inline">\(N\)</span> intervals:
<span class="math display">\[
0 = t_0 \leq t_1 \leq \dots \leq t_k \leq t_{k+1} \leq \dots \leq t_N = T.
\]</span>
We denote
<span class="math display">\[
h_k = t_{k+1} - t_k
\]</span>
as the length of the <span class="math inline">\(k\)</span>-th time interval. As we will see, the length of the time interval does not need to be fixed, and instead they can themselves be unknown variables to be optimized (in which case the final time <span class="math inline">\(T\)</span> also becomes flexible).</p>
<p><strong>Knot variables</strong>. At each of the timestamps <span class="math inline">\(t_0,\dots,t_N\)</span>, we assign <em>knot variables</em>, which are unknown state and control varaibles that need to be optimized. In particular, we have state knot variables
<span class="math display">\[
x_k = x(t_k), k=1,\dots,N,
\]</span>
and control knot variables
<span class="math display">\[
u_k = u(t_k),k=0,\dots,N-1.
\]</span>
As a result, the entire set of knot varaibles to be optimized is
<span class="math display" id="eq:knot-variables">\[\begin{equation}
v = \begin{bmatrix} u_0 \\ u_1 \\ \vdots \\ u_{N-1} \\ x_1 \\ x_2 \\ \vdots \\ x_{N} \end{bmatrix}.
\tag{3.16}
\end{equation}\]</span>
If the time-discretization is also optimized, then <span class="math inline">\(v\)</span>
<!-- $$
v = \begin{bmatrix} u_0 \\ u_1 \\ \vdots \\ u_{N-1} \\ x_1 \\ x_2 \\ \vdots \\ x_{N} \\ h_0 \\ \vdots \\ h_{N-1} \end{bmatrix}
$$ -->
includes the time intervals as well.</p>
<p><strong>Transcribe dynamics</strong>. The most important step is to transcribe the nonlinear dynamics <span class="math inline">\(\dot{x}=f(x(t),u(t))\)</span> as constraints on the knot varaibles. In direct collocation, the way this is done is to enforce the dynamics equation at the set of <em>collocation points</em> that are the mid-points between each consecutive pair of knot variables <span class="math inline">\((x_k, x_{k+1})\)</span>.</p>
<p>Specifically, in each subinterval <span class="math inline">\([t_k, t_{k+1}]\)</span>, we approximate the state trajectory as a cubic polynomial
<span class="math display" id="eq:state-polynomial">\[\begin{equation}
x(t) = p_{k,0} + p_{k,1}(t-t_k) + p_{k,2}(t-t_k)^2 + p_{k,3}(t - t_k)^3, \quad t \in [t_k, t_{k+1}],
\tag{3.17}
\end{equation}\]</span>
where <span class="math inline">\(p_{k,0},p_{k,1},p_{k,2},p_{k,3} \in \mathbb{R}^n\)</span> are the coefficients of the polynomial. You would think that we would need to optimize the coefficients as well, but actually we won’t need to, as will be shown soon.
With this parameterization, we can obtain the time derivative of <span class="math inline">\(x(t)\)</span> as
<span class="math display" id="eq:state-polynomial-derivative">\[\begin{equation}
\dot{x}(t) = p_{k,1} + 2 p_{k,2}(t-t_k) + 3p_{k,3}(t - t_k)^2, \quad t \in [t_k, t_{k+1}].
\tag{3.18}
\end{equation}\]</span>
Now the key step is to write the coefficients <span class="math inline">\(p_{k,0},p_{k,1},p_{k,2},p_{k,3}\)</span> using our knot variables <a href="approximatedp.html#eq:knot-variables">(3.16)</a>. To do so, we can invoke <a href="approximatedp.html#eq:state-polynomial">(3.17)</a> and <a href="approximatedp.html#eq:state-polynomial-derivative">(3.18)</a> to obtain
<span class="math display">\[
\begin{bmatrix}
x_k \\
\dot{x}_k = f(x_k,u_k) \\
x_{k+1} \\
\dot{x}_{k+1} = f(x_{k+1}, u_{k+1})
\end{bmatrix}
= \begin{bmatrix}
I &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; I &amp; 0 &amp; 0 \\
I &amp; h_k I &amp; h_k^2 I &amp; h_k^3 I \\
0 &amp; I &amp; 2 h_k I &amp; 3 h_k^2 I \end{bmatrix}
\begin{bmatrix}
p_{k,0}\\
p_{k,1}\\
p_{k,2}\\
p_{k,3}
\end{bmatrix}.
\]</span>
Solving the above equation, we get
<span class="math display" id="eq:polynomial-coefficients-closed-form">\[\begin{equation}
\begin{bmatrix}
p_{k,0}\\
p_{k,1}\\
p_{k,2}\\
p_{k,3}
\end{bmatrix} =
\begin{bmatrix}
I &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; I &amp; 0 &amp; 0 \\
- \frac{3}{h_k^2} I &amp; - \frac{2}{h_k} I &amp; \frac{3}{h_k^2} I &amp; -\frac{1}{h_k} I \\
\frac{2}{h_k^3} I &amp; \frac{1}{h_k^2} I &amp; -\frac{2}{h_k^3} I &amp; \frac{1}{h_k^2} I
\end{bmatrix}
\begin{bmatrix}
x_k \\
f(x_k,u_k) \\
x_{k+1} \\
f(x_{k+1}, u_{k+1})
\end{bmatrix}.
\tag{3.19}
\end{equation}\]</span>
Equation <a href="approximatedp.html#eq:polynomial-coefficients-closed-form">(3.19)</a> implies, using the knot variables <span class="math inline">\(v\)</span> in <a href="approximatedp.html#eq:knot-variables">(3.16)</a>, we can query the value of <span class="math inline">\(x(t)\)</span> and <span class="math inline">\(\dot{x}(t)\)</span> at any time <span class="math inline">\(t \in [0,T]\)</span>. In particular, we will query the values of <span class="math inline">\(x(t)\)</span> and <span class="math inline">\(\dot{x}(t)\)</span> at the midpoints to obtain
<span class="math display">\[
x_k^c = x\left( t_k + \frac{h_k}{2} \right) = \frac{1}{2} (x_k + x_{k+1}) + \frac{h_k}{8} (f(x_k,u_k) - f(x_{k+1},u_{k+1})),
\]</span>
and
<span class="math display">\[
\dot{x}_k^c = \dot{x} \left( t_k + \frac{h_k}{2} \right) = - \frac{3}{2h_k} (x_k - x_{k+1}) - \frac{1}{4} \left( f(x_k,u_k) + f(x_{k+1},u_{k+1}) \right).
\]</span>
At the midpoint, we assume the control is
<span class="math display">\[
u_k^c = \frac{1}{2}(u_k + u_{k+1}).
\]</span>
Therefore, we can enforce the dynamics constraint at the midpoint as
<span class="math display" id="eq:collocation-constraint">\[\begin{equation}
\dot{x}^c_k = f(x_k^c, u_k^c).
\tag{3.20}
\end{equation}\]</span></p>
<p><strong>Transcribe other constraints</strong>. The other constraints in the continuous-time formulation <a href="approximatedp.html#eq:constrained-optimal-control">(3.9)</a> can be transcribed to the knot variables in a straightforward way:
<span class="math display">\[\begin{align}
x_k \in \mathcal{X} \Rightarrow c_i^x(x_k)\geq 0, i=1,\dots,q_x, \quad k=1,\dots,N \\
u_k \in \mathcal{X} \Rightarrow c^u_i(u_k) \geq 0, i=1,\dots,q_u, \quad k=0,\dots,N-1\\
\phi_i(x_k, u_k) \geq 0, i=1,\dots,q, \quad k=0,\dots,N.
\end{align}\]</span></p>
<p><strong>Transcribe the objective</strong>. We can write the objective as
<span class="math display">\[
g_T(x_N) + \sum_{k=0}^{N-1} \frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k.
\]</span></p>
<p><strong>Summary</strong>. In summary, the final optimization problem becomes</p>
<p><span class="math display" id="eq:constrained-optimal-control-nlp">\[\begin{equation}
\begin{split}
\min_{u_0,\dots,u_{N-1},x_1,\dots,x_N} &amp; \quad  g_T(x_N) + \sum_{k=0}^{N-1} \frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k \\
\text{subject to} &amp; \quad \dot{x}_k^c = f(x_k^c,u_k^c), \quad k=0,\dots,N-1 \\
&amp; \quad c_i^x(x_k) \geq 0,i=1,\dots,q_x, \quad k=1,\dots,N \\
&amp; \quad c_i^u(u_k) \geq 0,i=1,\dots,q_u, \quad k=0,\dots,N-1 \\
&amp; \quad \phi_i (x_k,u_k) \geq 0, i=1,\dots,q, \quad k=0,\dots,N.
\end{split}
\tag{3.21}
\end{equation}\]</span></p>
<p>Let us apply direct collocation to the same double integrator Example <a href="approximatedp.html#exm:multiple-shooting-double-integrator">3.6</a>.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:collocation-double-integrator" class="example"><strong>Example 3.7  (Direct Collocation for Minimum-Time Double Integrator) </strong></span>Consider the same minimum-time optimal control problem in Example <a href="approximatedp.html#exm:multiple-shooting-double-integrator">3.6</a>.</p>
<p>To apply direct collocation, we have our NLP variable
<span class="math display">\[
v = (T,x_0,\dots,x_N,u_0,\dots,u_N).
\]</span>
Similarly we can enforce the control constraints and the initial / terminal constraints.</p>
<p>The following script shows how to enforce the collocation constraint.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb4-1"><a href="approximatedp.html#cb4-1" tabindex="-1"></a><span class="kw">function</span> [<span class="va">c</span><span class="op">,</span><span class="va">ceq</span>] <span class="op">=</span> <span class="va">collocation</span>(<span class="va">v</span><span class="op">,</span><span class="va">N</span><span class="op">,</span><span class="va">initial_state</span>)</span>
<span id="cb4-2"><a href="approximatedp.html#cb4-2" tabindex="-1"></a><span class="va">T</span> <span class="op">=</span> <span class="va">v</span>(<span class="fl">1</span>)<span class="op">;</span></span>
<span id="cb4-3"><a href="approximatedp.html#cb4-3" tabindex="-1"></a><span class="va">h</span> <span class="op">=</span> <span class="va">T</span><span class="op">/</span>(<span class="va">N</span><span class="op">-</span><span class="fl">1</span>)<span class="op">;</span></span>
<span id="cb4-4"><a href="approximatedp.html#cb4-4" tabindex="-1"></a><span class="va">x</span> <span class="op">=</span> <span class="va">reshape</span>(<span class="va">v</span>(<span class="fl">2</span><span class="op">:</span><span class="fl">2</span><span class="op">*</span><span class="va">N</span><span class="op">+</span><span class="fl">1</span>)<span class="op">,</span><span class="fl">2</span><span class="op">,</span><span class="va">N</span>)<span class="op">;</span></span>
<span id="cb4-5"><a href="approximatedp.html#cb4-5" tabindex="-1"></a><span class="va">u</span> <span class="op">=</span> <span class="va">v</span>(<span class="fl">2</span><span class="op">*</span><span class="va">N</span><span class="op">+</span><span class="fl">2</span><span class="op">:</span><span class="kw">end</span>)<span class="op">;</span></span>
<span id="cb4-6"><a href="approximatedp.html#cb4-6" tabindex="-1"></a><span class="va">c</span> <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb4-7"><a href="approximatedp.html#cb4-7" tabindex="-1"></a><span class="va">ceq</span> <span class="op">=</span> []<span class="op">;</span></span>
<span id="cb4-8"><a href="approximatedp.html#cb4-8" tabindex="-1"></a><span class="kw">for</span> <span class="va">k</span><span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">-</span><span class="fl">1</span></span>
<span id="cb4-9"><a href="approximatedp.html#cb4-9" tabindex="-1"></a>    <span class="va">uk</span> <span class="op">=</span> <span class="va">u</span>(<span class="va">k</span>)<span class="op">;</span></span>
<span id="cb4-10"><a href="approximatedp.html#cb4-10" tabindex="-1"></a>    <span class="va">ukp1</span> <span class="op">=</span> <span class="va">u</span>(<span class="va">k</span><span class="op">+</span><span class="fl">1</span>)<span class="op">;</span></span>
<span id="cb4-11"><a href="approximatedp.html#cb4-11" tabindex="-1"></a>    <span class="va">xk</span> <span class="op">=</span> <span class="va">x</span>(<span class="op">:,</span><span class="va">k</span>)<span class="op">;</span></span>
<span id="cb4-12"><a href="approximatedp.html#cb4-12" tabindex="-1"></a>    <span class="va">xkp1</span> <span class="op">=</span> <span class="va">x</span>(<span class="op">:,</span><span class="va">k</span><span class="op">+</span><span class="fl">1</span>)<span class="op">;</span></span>
<span id="cb4-13"><a href="approximatedp.html#cb4-13" tabindex="-1"></a>    <span class="va">fk</span> <span class="op">=</span> <span class="va">double_integrator</span>(<span class="va">xk</span><span class="op">,</span><span class="va">uk</span>)<span class="op">;</span></span>
<span id="cb4-14"><a href="approximatedp.html#cb4-14" tabindex="-1"></a>    <span class="va">fkp1</span> <span class="op">=</span> <span class="va">double_integrator</span>(<span class="va">xkp1</span><span class="op">,</span><span class="va">ukp1</span>)<span class="op">;</span></span>
<span id="cb4-15"><a href="approximatedp.html#cb4-15" tabindex="-1"></a>    <span class="co">% collocation points</span></span>
<span id="cb4-16"><a href="approximatedp.html#cb4-16" tabindex="-1"></a>    <span class="va">xkc</span> <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>(<span class="va">xk</span><span class="op">+</span><span class="va">xkp1</span>) <span class="op">+</span> <span class="va">h</span><span class="op">/</span><span class="fl">8</span> <span class="op">*</span> (<span class="va">fk</span> <span class="op">-</span> <span class="va">fkp1</span>)<span class="op">;</span></span>
<span id="cb4-17"><a href="approximatedp.html#cb4-17" tabindex="-1"></a>    <span class="va">ukc</span> <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>(<span class="va">uk</span> <span class="op">+</span> <span class="va">ukp1</span>)<span class="op">;</span></span>
<span id="cb4-18"><a href="approximatedp.html#cb4-18" tabindex="-1"></a>    <span class="va">dxkc</span> <span class="op">=</span> <span class="op">-</span><span class="fl">3</span><span class="op">/</span>(<span class="fl">2</span><span class="op">*</span><span class="va">h</span>) <span class="op">*</span> (<span class="va">xk</span><span class="op">-</span><span class="va">xkp1</span>) <span class="op">-</span> <span class="fl">0.25</span><span class="op">*</span>(<span class="va">fk</span> <span class="op">+</span> <span class="va">fkp1</span>)<span class="op">;</span></span>
<span id="cb4-19"><a href="approximatedp.html#cb4-19" tabindex="-1"></a>    <span class="co">% collocation constraint</span></span>
<span id="cb4-20"><a href="approximatedp.html#cb4-20" tabindex="-1"></a>    <span class="va">ceq</span> <span class="op">=</span> [<span class="va">ceq</span><span class="op">;</span></span>
<span id="cb4-21"><a href="approximatedp.html#cb4-21" tabindex="-1"></a>           <span class="va">dxkc</span> <span class="op">-</span> <span class="va">double_integrator</span>(<span class="va">xkc</span><span class="op">,</span><span class="va">ukc</span>)]<span class="op">;</span></span>
<span id="cb4-22"><a href="approximatedp.html#cb4-22" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb4-23"><a href="approximatedp.html#cb4-23" tabindex="-1"></a><span class="va">ceq</span> <span class="op">=</span> [<span class="va">ceq</span><span class="op">;</span></span>
<span id="cb4-24"><a href="approximatedp.html#cb4-24" tabindex="-1"></a>       <span class="va">x</span>(<span class="op">:,</span><span class="fl">1</span>) <span class="op">-</span> <span class="va">initial_state</span><span class="op">;</span> <span class="co">% initial condition</span></span>
<span id="cb4-25"><a href="approximatedp.html#cb4-25" tabindex="-1"></a>       <span class="va">x</span>(<span class="op">:,</span><span class="kw">end</span>)]<span class="op">;</span> <span class="co">% land at zero</span></span>
<span id="cb4-26"><a href="approximatedp.html#cb4-26" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>As you can see, we do not need any numerical integrator such as <code>ode45</code>. The only thing we need is the continuous-time double integrator dynamics.</p>
<p>Running the <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/double_integrator_collocation.m">complete code</a> with <span class="math inline">\(x_0 = (-10,0)\)</span> and <span class="math inline">\(N=51\)</span>, we get the control signal in Fig. <a href="approximatedp.html#fig:min-time-di-u-collocation">3.9</a> that is bang-bang.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:min-time-di-u-collocation"></span>
<img src="images/min_time_di_u_collocation.png" alt="Optimal control signal by direct collocation." width="70%" />
<p class="caption">
Figure 3.9: Optimal control signal by direct collocation.
</p>
</div>
<p>Using <code>ode45</code> to integrate the double integrator dynamics from <span class="math inline">\(x_0\)</span> with the controller in Fig. <a href="approximatedp.html#fig:min-time-di-u-collocation">3.9</a>, we obtain the following state trajectory. Comparing Fig. <a href="approximatedp.html#fig:min-time-di-x-collocation">3.10</a> with Fig. <a href="approximatedp.html#fig:min-time-di-x-ms">3.7</a>, we can observe that the terminal state of the trajectory obtained from direct collocation is more accurate than that obtained from direct multiple shooting.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:min-time-di-x-collocation"></span>
<img src="images/min_time_di_x_collocation.png" alt="ODE45 integration with the optimal control signal found by direct collocation." width="70%" />
<p class="caption">
Figure 3.10: ODE45 integration with the optimal control signal found by direct collocation.
</p>
</div>
<p>Not only is direct collocation more accurate for this example, it is also faster. This is evident because in direct multiple shooting, evaluating the nonlinear constraints requires runing <code>ode45</code>, while in direct collocation, evaluating the nonlinear constraints simply requires calling the original continuous-time dynamics.</p>
<p>We can easily optimize with a larger <span class="math inline">\(N=101\)</span> and get the following results.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:min-time-di-u-x-collocation-N-100"></span>
<img src="images/min_time_di_u_collocation_N_100.png" alt="Direct collocation with N=101." width="45%" /><img src="images/min_time_di_x_collocation_N_100.png" alt="Direct collocation with N=101." width="45%" />
<p class="caption">
Figure 3.11: Direct collocation with N=101.
</p>
</div>
</div>
</div>
<p>If you are interested in direct collocation, there is a nice tutorial with Matlab examples in <span class="citation">(<a href="#ref-kelly17siam-trajopt">Kelly 2017</a>)</span>.</p>
</div>
<div id="direct-orthogonal-collocation" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Direct Orthogonal Collocation<a href="approximatedp.html#direct-orthogonal-collocation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="failure-of-open-loop-control" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Failure of Open-Loop Control<a href="approximatedp.html#failure-of-open-loop-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Trajectory optimization produces very satisfying trajectories when the nonlinear programming algorithms work as expected, as shown by the previous Examples <a href="approximatedp.html#exm:collocation-double-integrator">3.7</a> and <a href="approximatedp.html#exm:multiple-shooting-double-integrator">3.6</a> on the double integrator system.</p>
<p>However, does this imply that if we simply run the planned controls on a real system, i.e., doing <em>open-loop control</em>, the trajectory will behave as planned?</p>
<p>Unfortunately the answer is No, for two major reasons.</p>
<ol style="list-style-type: decimal">
<li><p>Trajectory optimization only produces an <em>approximate</em> solution to the optimal control problem (OCP) <a href="approximatedp.html#eq:constrained-optimal-control">(3.9)</a>, regardless of what transcription method is used (such as multiple shooting and direct collocation). This means every <span class="math inline">\(u_k\)</span> we obtained is an approximation to the true optimal controller <span class="math inline">\(u(t_k)\)</span>, and the approximation errors will accumulate as the dynamical system is evolving.</p></li>
<li><p>Even the OCP <a href="approximatedp.html#eq:constrained-optimal-control">(3.9)</a> is an imperfect approximation of the true control task. This is due to we assumed we have perfect knowledge of the system dynamics
<span class="math display">\[
\dot{x} = f(x(t),u(t)),
\]</span>
which rarely holds in practice. For example, in the double integrator example, there is always friction between the mass and the ground. A more realistic assumption is that the system dynamics is
<span class="math display">\[
\dot{x} = f(x(t),u(t)) + w_t,
\]</span>
where <span class="math inline">\(w_t\)</span> is some unknown disturbance, or modeling error.</p></li>
</ol>
<p>Let us observe the failure of open-loop control on our favorite pendulum example.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:pendulum-collocation-failure" class="example"><strong>Example 3.8  (Failure of Open-Loop Control on A Simple Pendulum) </strong></span>Consider an “ideal” pendulum dynamics model
<span class="math display">\[
x = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}, \quad \dot{x} = f(x,u) = \begin{bmatrix} \dot{\theta} \\ - \frac{1}{ml^2} (b \dot{\theta} + mgl \sin \theta) + \frac{1}{ml^2} u \end{bmatrix},
\]</span>
with <span class="math inline">\(m=1,l=1,g=9.8,b=0.1\)</span>. We enforce control saturation
<span class="math display">\[
u \in [-u_{\max}, u_{\max}]
\]</span>
with <span class="math inline">\(u_{\max} = 4.9\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-model-failure"></span>
<img src="images/pendulum-drawing.png" alt="Simple pendulum." width="40%" />
<p class="caption">
Figure 3.12: Simple pendulum.
</p>
</div>
<p>Consider the optimal control problem with <span class="math inline">\(T=10\)</span>
<span class="math display" id="eq:pendulum-swingup-ocp">\[\begin{equation}
\begin{split} \min_{u(t), t \in [0,T]} &amp; \quad \int_{t=0}^T (\cos \theta(t) + 1)^2 + \sin^2\theta(t) + \dot{\theta}^2(t) + u^2 dt \\
\text{subject to} &amp; \quad \dot{x} = f(x(t),u(t)) \\
&amp; \quad x(0) = [0,0]^T, \quad x(T) = [\pi,0]^T, \\
&amp; \quad u(t) \in [-u_{\max},u_{\max}],
\end{split}
\tag{3.22}
\end{equation}\]</span>
where we start from the initial state <span class="math inline">\([0,0]^T\)</span>, i.e., the bottomright position, and we want to swing up the pendulum to the final state <span class="math inline">\([\pi,0]^T\)</span>, i.e., the upright position. The cost function in <a href="approximatedp.html#eq:pendulum-swingup-ocp">(3.22)</a> penalizes the deviation of the state trajectory from the target state (the target state has <span class="math inline">\(\cos \theta = -1\)</span>, <span class="math inline">\(\sin \theta = 0\)</span> and <span class="math inline">\(\dot{\theta} = 0\)</span>), together with the magnitude of control.</p>
<p><strong>Trajectory optimization with direct collocation</strong>. We perform trajectory optimization for the OCP <a href="approximatedp.html#eq:pendulum-swingup-ocp">(3.22)</a> with direct collocation. We choose <span class="math inline">\(N=101\)</span> break points with <span class="math inline">\(h=0.1\)</span> equal interval to discretize the time.</p>
<p>The result of trajectory optimization is shown in Fig. <a href="approximatedp.html#fig:pendulum-swingup-trajopt-collocation">3.13</a>. We can see that the pendulum is perfectly swung up to <span class="math inline">\([\pi,0]^T\)</span> and stabilized there.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-swingup-trajopt-collocation"></span>
<img src="images/pendulum_trajopt_result.png" alt="Pendulum swing-up with direct collocation" width="80%" />
<p class="caption">
Figure 3.13: Pendulum swing-up with direct collocation
</p>
</div>
<p><strong>Deploy the optimized plan</strong>. We then deploy the optimized controls in Fig. <a href="approximatedp.html#fig:pendulum-swingup-trajopt-collocation">3.13</a> on the “ideal” pendulum. We deploy the control every <span class="math inline">\(0.1\)</span> seconds with zero-order hold, and use Matlab <code>ode89</code> to integrate the pendulum dynamics.</p>
<p>Fig. <a href="approximatedp.html#fig:pendulum-swingup-trajopt-collocation-deploy">3.14</a> shows the true trajectory of the pendulum. Unfortunately, the pendulum swing-up and stabilization is unsuccessful.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-swingup-trajopt-collocation-deploy"></span>
<img src="images/pendulum_trajopt_deploy.png" alt="Deploy optimized controls" width="80%" />
<p class="caption">
Figure 3.14: Deploy optimized controls
</p>
</div>
<p><strong>Adding noise</strong>. What if the true pendulum dynamics is
<span class="math display">\[
x = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}, \quad \dot{x} = f(x,u) = \begin{bmatrix} \dot{\theta} \\ - \frac{1}{ml^2} (b \dot{\theta} + mgl \sin \theta) + \frac{1}{ml^2} u \end{bmatrix} + \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
\]</span>
That is, there is a constant unmodelled angular acceleration of <span class="math inline">\(1\)</span>, which maybe come from some unknown external force.</p>
<p>Fig. <a href="approximatedp.html#fig:pendulum-swingup-trajopt-collocation-deploy-noise">3.15</a> shows the trajectory of the pendulum with the optimized controlls. We can clearly see the unmodelled dynamics makes the performance much worse.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-swingup-trajopt-collocation-deploy-noise"></span>
<img src="images/pendulum_trajopt_deploy_noise.png" alt="Deploy optimized controls on a system with disturbance" width="80%" />
<p class="caption">
Figure 3.15: Deploy optimized controls on a system with disturbance
</p>
</div>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/pendulum_mpc/pendulum_open_loop.m">here</a>.</p>
</div>
</div>
<p>The failure of open-loop control motivates feedback control.</p>
</div>
<div id="lqr-trajectory-tracking" class="section level3 hasAnchor" number="3.2.6">
<h3><span class="header-section-number">3.2.6</span> LQR Trajectory Tracking<a href="approximatedp.html#lqr-trajectory-tracking" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="model-predictive-control" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Model Predictive Control<a href="approximatedp.html#model-predictive-control" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The model predictive control framework leverages the idea of <em>receding horizon control</em> (RHC) to turn an open-loop controller into a closed-loop controller, i.e., a feedback controller.</p>
<p>For example, suppose we have an optimal control problem with horizon <span class="math inline">\(T\)</span>:
<span class="math display" id="eq:optimal-control-problem-mpc">\[\begin{equation}
\begin{split}
\min_{u(t), t \in [0,T]} &amp; \quad g_T(x(T)) + \int_{t=0}^T g(x(t),u(t)) dt \\
\text{subject to} &amp; \quad \dot{x} = f(x(t),u(t)), \quad x(0) = x_0 \\
&amp; (x(t),u(t)) \in \mathcal{X} \times \mathcal{U}, \quad x(T) \in \mathcal{X}_T
\end{split}
\tag{3.23}
\end{equation}\]</span>
where <span class="math inline">\(\dot{x} = f(x(t),u(t))\)</span> is the best “ideal” model we have about our system.</p>
<p>The idea of RHC is as follows. At time <span class="math inline">\(t\)</span>, we obtain a measurement of the current state of the system, denoted as <span class="math inline">\(x_t\)</span>, and we solve the following OCP with horizon <span class="math inline">\(H &lt; T\)</span>:
<span class="math display" id="eq:optimal-control-problem-mpc-open-loop">\[\begin{equation}
\begin{split}
\min_{u(\tau), \tau \in [0,H]} &amp; \quad g_t(x(H)) + \int_{\tau = 0}^H g(x(\tau),u(\tau)) d\tau \\
\text{subject to} &amp; \quad  \dot{x} = f(x(\tau),u(\tau)), \quad x(0) = x_t \\
&amp; \quad (x(t),u(t)) \in \mathcal{X} \times \mathcal{U}, \quad x(H) \in \mathcal{X}_{t}
\end{split}
\tag{3.24}
\end{equation}\]</span>
where notice that I used a different notation for the terminal cost <span class="math inline">\(g_t\)</span> (as supposed to <span class="math inline">\(g_T\)</span> in <a href="approximatedp.html#eq:optimal-control-problem-mpc">(3.23)</a>), and the terminal constraint <span class="math inline">\(\mathcal{X}_t\)</span> (as supposed to <span class="math inline">\(\mathcal{X}_T\)</span> in <a href="approximatedp.html#eq:optimal-control-problem-mpc-open-loop">(3.24)</a>). Using a different terminal cost and terminal constraint is meant to make the problem <a href="approximatedp.html#eq:optimal-control-problem-mpc-open-loop">(3.24)</a> always feasible.</p>
<p>Suppose we solve the open-loop control problem <a href="approximatedp.html#eq:optimal-control-problem-mpc-open-loop">(3.24)</a> using trajectory optimization with <span class="math inline">\(N\)</span> time intervals and obtained the following solution
<span class="math display">\[
u_{t,0},u_{t,1},\dots,u_{t,N}.
\]</span>
Then RHS will only take the first control <span class="math inline">\(u_{t,0}\)</span> and apply it to the system. Therefore, the closed-loop dynamics is effectively
<span class="math display" id="eq:receding-horizon-control">\[\begin{equation}
\dot{x} = f(x(t),u_{t,0}),
\tag{3.25}
\end{equation}\]</span>
where <span class="math inline">\(u_{t,0}\)</span> is the first control solution to the OCP <a href="approximatedp.html#eq:optimal-control-problem-mpc-open-loop">(3.24)</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:receding-horizon-control"></span>
<img src="images/receding_horizon_control.png" alt="Receding horizon control" width="80%" />
<p class="caption">
Figure 3.16: Receding horizon control
</p>
</div>
<p>It is worth noting that RHS effectively introduces feedback control, because the controller <span class="math inline">\(u_{t,0}\)</span> in <a href="approximatedp.html#eq:receding-horizon-control">(3.25)</a> depends on <span class="math inline">\(x(t)\)</span> via the open-loop optimal control problem <a href="approximatedp.html#eq:optimal-control-problem-mpc-open-loop">(3.24)</a>.</p>
<p>Trajectory optimization and model predictive control are the workhorse for control of highly dynamic robots. For example, here is a <a href="https://youtu.be/EGABAx52GKI?feature=shared">talk by Scott Kuindersma</a> on how trajectory optimization and model predictive control enable a diverse set of behaviors of Boston Dynamics’s humanoid robot.</p>
<div id="turn-trajectory-optimization-into-feedback-control" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Turn Trajectory Optimization into Feedback Control<a href="approximatedp.html#turn-trajectory-optimization-into-feedback-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us turn our trajectory optimization algorithm in Example <a href="approximatedp.html#exm:pendulum-collocation-failure">3.8</a> into a feedback controller through receding horizon control.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:mpc-pendulum-swingup" class="example"><strong>Example 3.9  (Pendulum Swingup with Model Predictive Control) </strong></span>Consider again the pendulum swingup task in Example <a href="approximatedp.html#exm:pendulum-collocation-failure">3.8</a>.</p>
<p>This time, we will apply MPC to this task.</p>
<p>Again, we discretize the time windown <span class="math inline">\([0,T]\)</span> into <span class="math inline">\(N-1 = 100\)</span> equal intervals with <span class="math inline">\(h = 0.1\)</span>. At each timestep <span class="math inline">\(t_k = kh\)</span>, <span class="math inline">\(k=0,\dots,N-1\)</span>, we first measure the current state of the pendulum as <span class="math inline">\(x_k\)</span>, then solve the following OCP with planning horizon <span class="math inline">\(H=5\)</span>
<span class="math display" id="eq:pendulum-swingup-mpc-subroutine">\[\begin{equation}
\begin{split} \min_{u(t), t \in [0,H]} &amp; \quad \int_{t=0}^H (\cos \theta(t) + 1)^2 + \sin^2\theta(t) + \dot{\theta}^2(t) + u^2 dt \\
\text{subject to} &amp; \quad \dot{x} = f(x(t),u(t)) \\
&amp; \quad x(0) = x_k, \quad x(H) = [\pi,0]^T, \\
&amp; \quad u(t) \in [-u_{\max},u_{\max}].
\end{split}
\tag{3.26}
\end{equation}\]</span>
We solve the OCP <a href="approximatedp.html#eq:pendulum-swingup-mpc-subroutine">(3.26)</a> using trajectory optimization with direct collocation (using <span class="math inline">\(M\)</span> break points), which gives us a sequence of controls
<span class="math display">\[
u_{k,1},\dots,u_{k,M},
\]</span>
we deploy the first control <span class="math inline">\(u_{k,1}\)</span> to the pendulum system.</p>
<p><strong>MPC results</strong>. Fig. <a href="approximatedp.html#fig:pendulum-swingup-mpc-noise-free">3.17</a> shows the control and state trajectories when using MPC to swing up the pendulum. We can see that it works very well.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-swingup-mpc-noise-free"></span>
<img src="images/pendulum_mpc_noise_free.png" alt="MPC for pendulum swing-up" width="80%" />
<p class="caption">
Figure 3.17: MPC for pendulum swing-up
</p>
</div>
<p><strong>Robustness to noise</strong>. MPC is naturally robust to modelling errors and noises. When the true system dynamics is
<span class="math display">\[
x = \begin{bmatrix} \theta \\ \dot{\theta} \end{bmatrix}, \quad \dot{x} = f(x,u) = \begin{bmatrix} \dot{\theta} \\ - \frac{1}{ml^2} (b \dot{\theta} + mgl \sin \theta) + \frac{1}{ml^2} u \end{bmatrix} + \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
\]</span>
The MPC controller still works very well, as shown in Fig. <a href="approximatedp.html#fig:pendulum-swingup-mpc-noise">3.18</a>!</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pendulum-swingup-mpc-noise"></span>
<img src="images/pendulum_mpc_noise.png" alt="MPC for pendulum swing-up with noise" width="80%" />
<p class="caption">
Figure 3.18: MPC for pendulum swing-up with noise
</p>
</div>
<p>Feel free to play with the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/pendulum_mpc/pendulum_mpc.m">here</a>.</p>
</div>
</div>
<p><strong>Software tools</strong>. Matlab implements a nice package for nonlinear MPC, you can refer to the <a href="https://www.mathworks.com/help/mpc/ug/nonlinear-mpc.html">documentation and examples</a>. In Python, the <a href="https://www.do-mpc.com/en/latest/">do-mpc package</a> is quite popular.</p>
<p>However, it is important to recognize that trajectory optimization for the nonconvex open-loop optimal control problen <a href="approximatedp.html#eq:optimal-control-problem-mpc-open-loop">(3.24)</a> (a) only guarantees a locally optimal solution and hence can be brittle, (b) can be time-consuming when the problem is high-dimensional.</p>
<p>In fact, MPC was originally developed in the context of chemical plant control <span class="citation">(<a href="#ref-borrelli17book-mpc">Borrelli, Bemporad, and Morari 2017</a>)</span>, which has linear system dyanmics, as we will introduce soon. Before we introduce the MPC formulation and understand its theoretical properties, we need to take a detour and introduce various important notions of sets.</p>
</div>
<div id="controllability-reachability-and-invariance" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Controllability, Reachability, and Invariance<a href="approximatedp.html#controllability-reachability-and-invariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the autonomous system
<span class="math display" id="eq:mpc-autonomous-system">\[\begin{equation}
x_{t+1} = f_a(x_t)
\tag{3.27}
\end{equation}\]</span>
and the controlled system
<span class="math display" id="eq:mpc-controlled-system">\[\begin{equation}
x_{t+1} = f(x_t, u_t)
\tag{3.28}
\end{equation}\]</span>
for <span class="math inline">\(t \geq 0\)</span>. Both systems are subject to state and input constraints
<span class="math display" id="eq:mpc-state-control-constraint">\[\begin{equation}
x_t \in \mathcal{X}, \quad u_t \in \mathcal{U}, \forall t \geq 0,
\tag{3.29}
\end{equation}\]</span>
with <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{U}\)</span> both being polyhedral sets (see Definition <a href="appconvex.html#def:polyhedra">B.9</a>)
<span class="math display" id="eq:mpc-polyhedral-definition">\[\begin{equation}
\begin{split}
x_t \in \mathcal{X} = \left\{ x \in \mathbb{R}^n \mid C x \leq c \right\}, \quad C \in \mathbb{R}^{l_x \times n}, c \in \mathbb{R}^{l_x}, \\
u_t \in \mathcal{U} = \left\{ u \in \mathbb{R}^m \mid D u \leq d \right\}, \quad D \in \mathbb{R}^{l_u \times m}, d \in \mathbb{R}^{l_u}.
\end{split}
\tag{3.30}
\end{equation}\]</span></p>
<p>We are going to make a few definitions.</p>
<div id="controllable-and-reachable-sets" class="section level4 hasAnchor" number="3.3.2.1">
<h4><span class="header-section-number">3.3.2.1</span> Controllable and Reachable Sets<a href="approximatedp.html#controllable-and-reachable-sets" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="definitionbox">
<div class="definition">
<p><span id="def:precursor-set" class="definition"><strong>Definition 3.1  (Precursor Set) </strong></span>For the autonomous system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a>, we define the precursor set to a set <span class="math inline">\(\mathcal{S}\)</span> as
<span class="math display">\[
\text{Pre}(\mathcal{S}) = \left\{ x \in \mathbb{R}^n \mid f_a(x) \in \mathcal{S} \right\}.
\]</span>
In words, <span class="math inline">\(\text{Pre}(\mathcal{S})\)</span> is the set of states which evolve into the target set <span class="math inline">\(\mathcal{S}\)</span> in one time step.</p>
<p>For the controlled system <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a>, we define the precursor set to the set <span class="math inline">\(\mathcal{S}\)</span> as
<span class="math display">\[
\text{Pre}(\mathcal{S}) = \left\{ x \in \mathbb{R}^n : \exists u \in \mathcal{U} \text{  s.t. } f(x,u) \in \mathcal{S} \right\}.
\]</span>
In words, <span class="math inline">\(\text{Pre}(\mathcal{S})\)</span> is the set of states that can be driven into the target set <span class="math inline">\(\mathcal{S}\)</span> while satisfying control and state constraints.</p>
</div>
</div>
<p>A related concept is the successor set.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:successor-set" class="definition"><strong>Definition 3.2  (Successor Set) </strong></span>For the autonomous system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a>, we define the successor set to a set <span class="math inline">\(\mathcal{S}\)</span> as
<span class="math display">\[
\text{Suc}(\mathcal{S}) = \left\{ x \in \mathbb{R}^n \mid \exists x&#39; \in \mathcal{S} \text{ s.t. } x = f_a(x&#39;)\right\}.
\]</span>
In words, the states in <span class="math inline">\(\mathcal{S}\)</span> get mapped into the states in <span class="math inline">\(\text{Suc}(\mathcal{S})\)</span> after one time step.</p>
<p>For the controlled system <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a>, we define the successor set to the set <span class="math inline">\(\mathcal{S}\)</span> as
<span class="math display">\[
\text{Suc}(\mathcal{S}) = \left\{ x \in \mathbb{R}^n \mid \exists x&#39; \in \mathcal{S}, \exists u&#39; \in \mathcal{U} \text{ s.t. } x = f(x&#39;,u&#39;). \right\}
\]</span>
In words, the states in <span class="math inline">\(\mathcal{S}\)</span> get mapped into the states in <span class="math inline">\(\text{Suc}(\mathcal{S})\)</span> after one time step while satisfying the control constraints.</p>
</div>
</div>
<p>The <span class="math inline">\(N\)</span>-step controllable set is defined by iterating <span class="math inline">\(\text{Pre}(\cdot)\)</span> computations.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:controllable-set" class="definition"><strong>Definition 3.3  (N-Step Controllable Set) </strong></span>Let <span class="math inline">\(\mathcal{S} \subseteq \mathcal{X}\)</span> be a target set. The <span class="math inline">\(N\)</span>-step controllable set <span class="math inline">\(\mathcal{K}_{N}(\mathcal{S})\)</span> of the system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a> or <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a> subject to the constraints <a href="approximatedp.html#eq:mpc-state-control-constraint">(3.29)</a> is defined recursively as:
<span class="math display">\[
\mathcal{K}_0 (\mathcal{S}) = \mathcal{S}, \quad \mathcal{K}_{i}(\mathcal{S}) = \text{Pre}(\mathcal{K}_{i-1}(\mathcal{S})) \cap \mathcal{X}, i=1,\dots,N.
\]</span></p>
<p>According to this definition, given any <span class="math inline">\(x_0 \in \mathcal{K}_N(\mathcal{S})\)</span>:</p>
<ul>
<li><p>For the autonomous system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a>, its state will evolve into the target set <span class="math inline">\(\mathcal{S}\)</span> in <span class="math inline">\(N\)</span> steps while satisfying state constraints</p></li>
<li><p>For the controlled system <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a>, there exists a sequence of admissible controls (i.e., satisfying control constraints) such that its state will evolve into the target set <span class="math inline">\(\mathcal{S}\)</span> in <span class="math inline">\(N\)</span> steps while satisfying state constraints.</p></li>
</ul>
</div>
</div>
<p>Similarly, the <span class="math inline">\(N\)</span>-step reachable set is defined by iterating <span class="math inline">\(\text{Suc}(\cdot)\)</span> computations.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:reachable-set" class="definition"><strong>Definition 3.4  (N-Step Reachable Set) </strong></span>Let <span class="math inline">\(\mathcal{X}_0 \subseteq \mathcal{X}\)</span> be an initial set. The <span class="math inline">\(N\)</span>-step reachable set <span class="math inline">\(\mathcal{R}_N (\mathcal{X}_0)\)</span> of the system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a> or <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a> subject to the constraints <a href="approximatedp.html#eq:mpc-state-control-constraint">(3.29)</a> is defined recursively as:
<span class="math display">\[
\mathcal{R}_0(\mathcal{X}_0) = \mathcal{X}_0, \quad \mathcal{R}_{i}(\mathcal{X}_0) = \text{Suc}(\mathcal{R}_{i-1}(\mathcal{X}_0)) \cap \mathcal{X}, i=1,\dots,N.
\]</span></p>
<p>According to this definition, given any <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span>:</p>
<ul>
<li><p>For the autonomous system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a>, its state will evolve into <span class="math inline">\(\mathcal{R}_N(\mathcal{X}_0)\)</span> in <span class="math inline">\(N\)</span> steps while satisfying state constraints</p></li>
<li><p>For the controlled system <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a>, there exists a sequence of admissible controls (i.e., satisfying control constraints) such that its state will evolve into <span class="math inline">\(\mathcal{R}_N(\mathcal{X}_0)\)</span> in <span class="math inline">\(N\)</span> steps while satisfying state constraints.</p></li>
</ul>
</div>
</div>
<p>In the literature, the controllable set is often referred to as the <em>backwards reachable set</em>.</p>
</div>
<div id="computation-of-controllable-and-reachable-sets" class="section level4 hasAnchor" number="3.3.2.2">
<h4><span class="header-section-number">3.3.2.2</span> Computation of Controllable and Reachable Sets<a href="approximatedp.html#computation-of-controllable-and-reachable-sets" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a linear time-invariant system, when the state constraint set <span class="math inline">\(\mathcal{X}\)</span>, control constraint set <span class="math inline">\(\mathcal{U}\)</span>, the target set <span class="math inline">\(\mathcal{S}\)</span>, and the initial set <span class="math inline">\(\mathcal{X}_0\)</span> are all polytopes, then the <span class="math inline">\(N\)</span>-step controllable set <span class="math inline">\(\mathcal{K}_{N}(\mathcal{S})\)</span> and the <span class="math inline">\(N\)</span>-step reachable set <span class="math inline">\(\mathcal{R}_{N}(\mathcal{X}_0)\)</span> are both polytopes and can be computed exactly and efficiently.</p>
<p>We will not describe the underlying algorithm for computing <span class="math inline">\(\mathcal{K}_{N}(\mathcal{S})\)</span> and <span class="math inline">\(\mathcal{R}_{N}(\mathcal{X}_0)\)</span> (these details can be found in <span class="citation">(<a href="#ref-borrelli17book-mpc">Borrelli, Bemporad, and Morari 2017</a>)</span> and they are not very difficult to understand, so I suggest you to read them), but we now show you how to use the <a href="https://www.mpt3.org/">Multi-Parametric Toolbox (MPT)</a> to compute them.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:compute-controllable-reachable-sets" class="example"><strong>Example 3.10  (Compute Controllable and Reachable Sets) </strong></span>Consider a linear system
<span class="math display">\[
x_{t+1} = \begin{bmatrix} 1.5 &amp; 0 \\ 1 &amp; -1.5 \end{bmatrix} x_t + \begin{bmatrix} 1 \\ 0 \end{bmatrix} u_t
\]</span>
with state and control constraints
<span class="math display">\[
\mathcal{X} = [-10,10]^2, \quad \mathcal{U} = [-5,5].
\]</span>
Given <span class="math inline">\(\mathcal{S} = \mathcal{X}_0 = [-1,1]^2\)</span>, I want to compute <span class="math inline">\(\mathcal{K}_i(\mathcal{S})\)</span> and <span class="math inline">\(\mathcal{R}_i(\mathcal{X}_0)\)</span>, for <span class="math inline">\(i=0,1,2,3,4\)</span>.</p>
<p><strong>Dynamical system</strong>. We first define the linear time-invariant system as follows.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb5-1"><a href="approximatedp.html#cb5-1" tabindex="-1"></a><span class="va">A</span> <span class="op">=</span> [<span class="fl">1.5</span><span class="op">,</span> <span class="fl">0</span><span class="op">;</span> <span class="fl">1</span><span class="op">,</span> <span class="op">-</span><span class="fl">1.5</span>]<span class="op">;</span> <span class="va">B</span> <span class="op">=</span> [<span class="fl">1</span><span class="op">;</span> <span class="fl">0</span>]<span class="op">;</span></span>
<span id="cb5-2"><a href="approximatedp.html#cb5-2" tabindex="-1"></a><span class="va">sys</span> <span class="op">=</span> <span class="va">LTISystem</span>(<span class="ss">&#39;A&#39;</span><span class="op">,</span><span class="va">A</span><span class="op">,</span><span class="ss">&#39;B&#39;</span><span class="op">,</span><span class="va">B</span>)<span class="op">;</span></span></code></pre></div>
<p>We then define the state and control constraints.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb6-1"><a href="approximatedp.html#cb6-1" tabindex="-1"></a><span class="va">calX</span> <span class="op">=</span> <span class="va">Polyhedron</span>(<span class="ss">&#39;A&#39;</span><span class="op">,...</span></span>
<span id="cb6-2"><a href="approximatedp.html#cb6-2" tabindex="-1"></a>    [<span class="fl">1</span><span class="op">,</span><span class="fl">0</span><span class="op">;</span><span class="fl">0</span><span class="op">,</span><span class="fl">1</span><span class="op">;-</span><span class="fl">1</span><span class="op">,</span><span class="fl">0</span><span class="op">;</span><span class="fl">0</span><span class="op">,-</span><span class="fl">1</span>]<span class="op">,</span> <span class="op">...</span></span>
<span id="cb6-3"><a href="approximatedp.html#cb6-3" tabindex="-1"></a>    <span class="ss">&#39;b&#39;</span><span class="op">,</span>[<span class="fl">10</span><span class="op">;</span><span class="fl">10</span><span class="op">;</span><span class="fl">10</span><span class="op">;</span><span class="fl">10</span>])<span class="op">;</span></span>
<span id="cb6-4"><a href="approximatedp.html#cb6-4" tabindex="-1"></a><span class="va">calU</span> <span class="op">=</span> <span class="va">Polyhedron</span>(<span class="ss">&#39;A&#39;</span><span class="op">,</span>[<span class="fl">1</span><span class="op">;-</span><span class="fl">1</span>]<span class="op">,</span><span class="ss">&#39;b&#39;</span><span class="op">,</span>[<span class="fl">5</span><span class="op">;</span><span class="fl">5</span>])<span class="op">;</span></span></code></pre></div>
<p>Note that in the MPT toolbox, to define a polytope <span class="math inline">\(P = \{ x \in \mathbb{R}^n \mid A x \leq b \}\)</span>, we just need to call the <code>Polyhedron</code> function with inputs <code>A</code> and <code>b</code>.</p>
<p><strong>Controllable sets</strong>. We have from Definition <a href="approximatedp.html#def:controllable-set">3.3</a> the recursion
<span class="math display">\[
\mathcal{K}_0(\mathcal{S}) = \mathcal{S}, \quad \mathcal{K}_i(\mathcal{S}) = \text{Pre}(\mathcal{K}_{i-1}(\mathcal{S})) \cap \mathcal{X}.
\]</span>
To implement the above set operation, we need two functions, one for the “<span class="math inline">\(\cap\)</span>” intersection operation, and the other for the “<span class="math inline">\(\text{Pre}(\cdot)\)</span>” precursor set operation. These two functions are both available through MPT.</p>
<ul>
<li><p><code>intersect(P,S)</code> computes the intersection of two sets <code>P</code> and <code>S</code>, both defined as polytopes.</p></li>
<li><p><code>system.reachableSet('X', X, 'U', U, 'N', 1, 'direction', 'backwards')</code> computes the one-step backwards reachable set (which is just the precursor set) of the <code>system</code> with target set <code>X</code> and control constraints <code>U</code>.</p></li>
</ul>
<p>Using these two functions, the following code snippet computes recursively the controllable sets for <span class="math inline">\(N\)</span> steps.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb7-1"><a href="approximatedp.html#cb7-1" tabindex="-1"></a><span class="co">% target set</span></span>
<span id="cb7-2"><a href="approximatedp.html#cb7-2" tabindex="-1"></a><span class="va">S</span> <span class="op">=</span> <span class="va">Polyhedron</span>(<span class="ss">&#39;A&#39;</span><span class="op">,</span>[<span class="fl">1</span><span class="op">,</span><span class="fl">0</span><span class="op">;</span><span class="fl">0</span><span class="op">,</span><span class="fl">1</span><span class="op">;-</span><span class="fl">1</span><span class="op">,</span><span class="fl">0</span><span class="op">;</span><span class="fl">0</span><span class="op">,-</span><span class="fl">1</span>]<span class="op">,</span><span class="ss">&#39;b&#39;</span><span class="op">,</span>[<span class="fl">1</span><span class="op">;</span><span class="fl">1</span><span class="op">;</span><span class="fl">1</span><span class="op">;</span><span class="fl">1</span>])<span class="op">;</span></span>
<span id="cb7-3"><a href="approximatedp.html#cb7-3" tabindex="-1"></a><span class="va">K</span> <span class="op">=</span> [<span class="va">S</span>]<span class="op">;</span></span>
<span id="cb7-4"><a href="approximatedp.html#cb7-4" tabindex="-1"></a><span class="va">N</span> <span class="op">=</span> <span class="fl">4</span><span class="op">;</span></span>
<span id="cb7-5"><a href="approximatedp.html#cb7-5" tabindex="-1"></a><span class="kw">for</span> <span class="va">i</span> <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span></span>
<span id="cb7-6"><a href="approximatedp.html#cb7-6" tabindex="-1"></a>    <span class="va">Si</span> <span class="op">=</span> <span class="va">sys</span>.<span class="va">reachableSet</span>(<span class="ss">&#39;X&#39;</span><span class="op">,</span><span class="va">K</span>(<span class="va">i</span>)<span class="op">,</span><span class="ss">&#39;U&#39;</span><span class="op">,</span><span class="va">calU</span><span class="op">,</span><span class="ss">&#39;N&#39;</span><span class="op">,</span><span class="fl">1</span><span class="op">,</span><span class="ss">&#39;direction&#39;</span><span class="op">,</span><span class="ss">&#39;backwards&#39;</span>)<span class="op">;</span></span>
<span id="cb7-7"><a href="approximatedp.html#cb7-7" tabindex="-1"></a>    <span class="va">K</span> <span class="op">=</span> [<span class="va">K</span><span class="op">;</span> <span class="va">intersect</span>(<span class="va">Si</span><span class="op">,</span><span class="va">calX</span>)]<span class="op">;</span></span>
<span id="cb7-8"><a href="approximatedp.html#cb7-8" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
Fig. <a href="approximatedp.html#fig:controllable-set-example">3.19</a> plots the controllable sets computed by running the above code.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:controllable-set-example"></span>
<img src="images/controllable_set_overlap.png" alt="Computation of controllable sets using MPT. The bottom plot shifts the sets horizontally for better visualization." width="60%" /><img src="images/controllable_set_shift.png" alt="Computation of controllable sets using MPT. The bottom plot shifts the sets horizontally for better visualization." width="60%" />
<p class="caption">
Figure 3.19: Computation of controllable sets using MPT. The bottom plot shifts the sets horizontally for better visualization.
</p>
</div>
<p><strong>Reachable sets</strong>. We have Definition <a href="approximatedp.html#def:reachable-set">3.4</a> the recursion
<span class="math display">\[
\mathcal{R}_0(\mathcal{X}_0) = \mathcal{X}_0, \quad \mathcal{R}_i(\mathcal{X}_0) = \text{Suc}(\mathcal{R}_{i-1}(\mathcal{X}_0)) \cap \mathcal{X}.
\]</span>
To implement the recursion above, we need “<span class="math inline">\(\cap\)</span>” set intersection, which is available via <code>intersect(P,S)</code>, and the “<span class="math inline">\(\text{Suc}(\cdot)\)</span>” successor set operation, which is available as</p>
<ul>
<li><code>system.reachableSet('X', X, 'U', U, 'N', 1, 'direction', 'forward')</code> computes the one-step forward reachable set (which is just the successor set) of the <code>system</code> with initial set <code>X</code> and control constraints <code>U</code>.</li>
</ul>
<p>Therefore, we can compute the reachable sets recursively using the following code snippet</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb8-1"><a href="approximatedp.html#cb8-1" tabindex="-1"></a><span class="co">% initial set</span></span>
<span id="cb8-2"><a href="approximatedp.html#cb8-2" tabindex="-1"></a><span class="va">X0</span> <span class="op">=</span> <span class="va">Polyhedron</span>(<span class="ss">&#39;A&#39;</span><span class="op">,</span>[<span class="fl">1</span><span class="op">,</span><span class="fl">0</span><span class="op">;</span><span class="fl">0</span><span class="op">,</span><span class="fl">1</span><span class="op">;-</span><span class="fl">1</span><span class="op">,</span><span class="fl">0</span><span class="op">;</span><span class="fl">0</span><span class="op">,-</span><span class="fl">1</span>]<span class="op">,</span><span class="ss">&#39;b&#39;</span><span class="op">,</span>[<span class="fl">1</span><span class="op">;</span><span class="fl">1</span><span class="op">;</span><span class="fl">1</span><span class="op">;</span><span class="fl">1</span>])<span class="op">;</span></span>
<span id="cb8-3"><a href="approximatedp.html#cb8-3" tabindex="-1"></a><span class="va">R</span> <span class="op">=</span> [<span class="va">X0</span>]<span class="op">;</span></span>
<span id="cb8-4"><a href="approximatedp.html#cb8-4" tabindex="-1"></a><span class="va">N</span> <span class="op">=</span> <span class="fl">4</span><span class="op">;</span></span>
<span id="cb8-5"><a href="approximatedp.html#cb8-5" tabindex="-1"></a><span class="kw">for</span> <span class="va">i</span> <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span></span>
<span id="cb8-6"><a href="approximatedp.html#cb8-6" tabindex="-1"></a>    <span class="va">Ri</span> <span class="op">=</span> <span class="va">sys</span>.<span class="va">reachableSet</span>(<span class="ss">&#39;X&#39;</span><span class="op">,</span><span class="va">R</span>(<span class="va">i</span>)<span class="op">,</span><span class="ss">&#39;U&#39;</span><span class="op">,</span><span class="va">calU</span><span class="op">,</span><span class="ss">&#39;N&#39;</span><span class="op">,</span><span class="fl">1</span><span class="op">,</span><span class="ss">&#39;direction&#39;</span><span class="op">,</span><span class="ss">&#39;forward&#39;</span>)<span class="op">;</span></span>
<span id="cb8-7"><a href="approximatedp.html#cb8-7" tabindex="-1"></a>    <span class="va">R</span> <span class="op">=</span> [<span class="va">R</span><span class="op">;</span> <span class="va">intersect</span>(<span class="va">Ri</span><span class="op">,</span><span class="va">calX</span>)]<span class="op">;</span></span>
<span id="cb8-8"><a href="approximatedp.html#cb8-8" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>Fig. <a href="approximatedp.html#fig:reachable-set-example">3.20</a> plots the reachable sets computed by running the above code.</p>
<p>Feel free to play with the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/mpt_examples/controllable_reachable_sets.m">here</a>. Note that you need to <a href="https://www.mpt3.org/Main/Installation">install the MPT toolbox</a> before running the code.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:reachable-set-example"></span>
<img src="images/reachable_set_overlap.png" alt="Computation of reachable sets using MPT. The bottom plot shifts the sets horizontally for better visualization." width="60%" /><img src="images/reachable_set_shift.png" alt="Computation of reachable sets using MPT. The bottom plot shifts the sets horizontally for better visualization." width="60%" />
<p class="caption">
Figure 3.20: Computation of reachable sets using MPT. The bottom plot shifts the sets horizontally for better visualization.
</p>
</div>
</div>
</div>
</div>
<div id="invariant-sets" class="section level4 hasAnchor" number="3.3.2.3">
<h4><span class="header-section-number">3.3.2.3</span> Invariant Sets<a href="approximatedp.html#invariant-sets" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="definitionbox">
<div class="definition">
<p><span id="def:positive-invariant-set" class="definition"><strong>Definition 3.5  (Positive Invariant Set) </strong></span>A set <span class="math inline">\(\mathcal{O} \subseteq \mathcal{X}\)</span> is said to be a positive invariant set for the autonomous system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a> subject to the constraints in <a href="approximatedp.html#eq:mpc-state-control-constraint">(3.29)</a> if
<span class="math display">\[
x_0 \in \mathcal{O} \Longrightarrow x_t \in \mathcal{O}, \forall t \geq 0.
\]</span>
That is, if the system starts in <span class="math inline">\(\mathcal{O}\)</span>, it stays in <span class="math inline">\(\mathcal{O}\)</span> for all future timesteps.</p>
</div>
</div>
<p>The maximal positive invariant set is the largest positive invariant set.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:maximal-positive-invariant-set" class="definition"><strong>Definition 3.6  (Maximal Positive Invariant Set) </strong></span>A set <span class="math inline">\(\mathcal{O}_{\infty} \subseteq \mathcal{X}\)</span> is said to be the maximal positive invariant set for the autonomous system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a> subject to the constraints in <a href="approximatedp.html#eq:mpc-state-control-constraint">(3.29)</a> if (a) <span class="math inline">\(\mathcal{O}_{\infty}\)</span> is positive invariant and (b) <span class="math inline">\(\mathcal{O}_{\infty}\)</span> contains all the invariant sets contained in <span class="math inline">\(\mathcal{X}\)</span>.</p>
</div>
</div>
<p>Essentially, the (maximal) positive invariant set is the set of initial conditions under which the system does not blow up.</p>
<p>For the controlled system <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a>, we have the similar notion of a control invariant set.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:control-invariant-set" class="definition"><strong>Definition 3.7  (Control Invariant Set) </strong></span>A set <span class="math inline">\(\mathcal{C} \subseteq \mathcal{X}\)</span> is said to be a control invariant set for the controlled system <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a> subject to the constraints in <a href="approximatedp.html#eq:mpc-state-control-constraint">(3.29)</a> if
<span class="math display">\[
x_0 \in \mathcal{C} \Longrightarrow \exists u_t \in \mathcal{U} \text{ s.t. } f(x_t,u_t) \in \mathcal{C}, \forall t \geq 0
\]</span>
That is, if the system starts in <span class="math inline">\(\mathcal{C}\)</span>, it can be controlled to stay in <span class="math inline">\(\mathcal{C}\)</span> for all future time steps.</p>
</div>
</div>
<p>The maximal control invariant set is the largest control invariant set.</p>
<div class="definitionbox">
<div class="definition">
<p><span id="def:maximal-control-invariant-set" class="definition"><strong>Definition 3.8  (Maximal Control Invariant Set) </strong></span>A set <span class="math inline">\(\mathcal{C}_{\infty} \subseteq \mathcal{X}\)</span> is said to be the maximal control invariant set for the controlled system <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a> subject to the constraints in <a href="approximatedp.html#eq:mpc-state-control-constraint">(3.29)</a> if (a) <span class="math inline">\(\mathcal{C}_{\infty}\)</span> is control invariant, and (b) <span class="math inline">\(\mathcal{C}_{\infty}\)</span> contains all control invariant sets contained in <span class="math inline">\(\mathcal{X}\)</span>.</p>
</div>
</div>
<p>Essentially, the (maximal) control invariant set is the set of initial conditions under which the system can be controlled to not blow up.</p>
<p>We now state a necessary and sufficient condition for a set to be positive invariant and control invariant.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:geometric-condition-for-invariance" class="theorem"><strong>Theorem 3.1  (Geometric Condition for Invariance) </strong></span>For the autonomous system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a> subject to the constraint <a href="approximatedp.html#eq:mpc-state-control-constraint">(3.29)</a>, a set <span class="math inline">\(\mathcal{O} \subseteq \mathcal{X}\)</span> is positive invariant if and only if
<span class="math display" id="eq:positive-invariant-geometric">\[\begin{equation}
\mathcal{O} \subseteq \text{Pre}(\mathcal{O}).
\tag{3.31}
\end{equation}\]</span></p>
<p>Similarly, for the controlled system <a href="approximatedp.html#eq:mpc-controlled-system">(3.28)</a> subject to the constraint <a href="approximatedp.html#eq:mpc-state-control-constraint">(3.29)</a>, a set <span class="math inline">\(\mathcal{C}\)</span> is control invariant if and only if
<span class="math display" id="eq:control-invariant-geometric">\[\begin{equation}
\mathcal{C} \subseteq \text{Pre}(\mathcal{C}).
\tag{3.32}
\end{equation}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>We only prove <a href="approximatedp.html#eq:positive-invariant-geometric">(3.31)</a> since <a href="approximatedp.html#eq:control-invariant-geometric">(3.32)</a> can be proved using similar arguments.</p>
<ul>
<li><p>“<span class="math inline">\(\Leftarrow\)</span>”: we want to show <span class="math inline">\(\mathcal{O}\)</span> is positive invariant if <a href="approximatedp.html#eq:positive-invariant-geometric">(3.31)</a> holds. We prove this by contradiction. If <span class="math inline">\(\mathcal{O}\)</span> is not positive invariant, the <span class="math inline">\(\exists \hat{x} \in \mathcal{O}\)</span> such that <span class="math inline">\(f_a(\hat{x}) \not\in \mathcal{O}\)</span>. This implies we have found <span class="math inline">\(\hat{x} \in \mathcal{O}\)</span> but <span class="math inline">\(\hat{x} \not\in \text{Pre}(\mathcal{O})\)</span>, contradicting <a href="approximatedp.html#eq:positive-invariant-geometric">(3.31)</a>.</p></li>
<li><p>“<span class="math inline">\(\Rightarrow\)</span>”: we want to show <a href="approximatedp.html#eq:positive-invariant-geometric">(3.31)</a> holds if <span class="math inline">\(\mathcal{O}\)</span> is positive invariant. We prove this by contradiction. Suppose <a href="approximatedp.html#eq:positive-invariant-geometric">(3.31)</a> does not hold, then <span class="math inline">\(\exists \hat{x}\)</span> such that <span class="math inline">\(\hat{x} \in \mathcal{O}\)</span> but <span class="math inline">\(\hat{x} \not\in \text{Pre}(\mathcal{O})\)</span>. This implies we have found <span class="math inline">\(\hat{x} \in \mathcal{O}\)</span> that does not remain in <span class="math inline">\(\mathcal{O}\)</span> in the next step, contradicting <span class="math inline">\(\mathcal{O}\)</span> being positive invariant.</p></li>
</ul>
<p>This shows that <a href="approximatedp.html#eq:positive-invariant-geometric">(3.31)</a> is a sufficient and necessary condition.</p>
</div>
</div>
<p>Theorem <a href="approximatedp.html#thm:geometric-condition-for-invariance">3.1</a> immediately suggests an algoithm for computing (control) invariant sets, as we will describe in the next section.</p>
</div>
<div id="mpc-compute-control-invariant-set" class="section level4 hasAnchor" number="3.3.2.4">
<h4><span class="header-section-number">3.3.2.4</span> Computation of Invariant Sets<a href="approximatedp.html#mpc-compute-control-invariant-set" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Observe that the geometric conditions <a href="approximatedp.html#eq:positive-invariant-geometric">(3.31)</a> and <a href="approximatedp.html#eq:control-invariant-geometric">(3.32)</a> are equivalent to the following conditions
<span class="math display">\[
\mathcal{O} = \text{Pre}(\mathcal{O}) \cap \mathcal{O}, \quad \mathcal{C} = \text{Pre}(\mathcal{C}) \cap \mathcal{C}.
\]</span></p>
<p>Based on the equation above, we can design an algoithm that iteratively evaluates <span class="math inline">\(\text{Pre}(\mathcal{\Omega}) \cap \mathcal{\Omega}\)</span> until it converges.</p>
<hr />
<p><strong>Algorithm</strong>: Computation of <span class="math inline">\(\mathcal{O}_{\infty}\)</span></p>
<hr />
<p><strong>Input:</strong> <span class="math inline">\(f_a\)</span>, <span class="math inline">\(\mathcal{X}\)</span></p>
<p><strong>Output:</strong> <span class="math inline">\(\mathcal{O}_{\infty}\)</span></p>
<p>   <span class="math inline">\(\Omega_0 \leftarrow \mathcal{X}\)</span>, <span class="math inline">\(k=0\)</span></p>
<p>   <strong>Repeat</strong></p>
<p>     <span class="math inline">\(\Omega_{k+1} \leftarrow \text{Pre}(\Omega_k) \cap \Omega_k\)</span></p>
<p>     <span class="math inline">\(k \leftarrow k+1\)</span></p>
<p>   <strong>Until</strong> <span class="math inline">\(\Omega_{k+1} = \Omega_k\)</span>.</p>
<p>   <span class="math inline">\(\mathcal{O}_{\infty} \leftarrow \Omega_k\)</span></p>
<hr />
<p>The algorithm above generates a sequence of sets <span class="math inline">\(\{ \Omega_k \}\)</span> satisfying <span class="math inline">\(\Omega_{k+1} \subseteq \Omega_k\)</span> for any <span class="math inline">\(k\)</span>, and it terminates when <span class="math inline">\(\Omega_{k+1} = \Omega_k\)</span>. If it terminates, then <span class="math inline">\(\Omega_k\)</span> is the maximal positive invariant set <span class="math inline">\(\mathcal{O}_{\infty}\)</span>. If <span class="math inline">\(\Omega_k = \emptyset\)</span> for some intege <span class="math inline">\(k\)</span> then <span class="math inline">\(\mathcal{O}_{\infty} = \emptyset\)</span>.</p>
<p>In general, the algoithm above may never terminate. If the algoithm does not terminate in a finite number of iterations, then it can be proven that <span class="citation">(<a href="#ref-kolmanovsky98mpe-theory">Kolmanovsky, Gilbert, et al. 1998</a>)</span>
<span class="math display">\[
\mathcal{O}_{\infty} = \lim_{k \rightarrow \infty} \Omega_k.
\]</span></p>
<p>Conditions for finite time termination of the algoithm can be found in <span class="citation">(<a href="#ref-gilbert91tac-linear">Gilbert and Tan 1991</a>)</span>. A simple sufficient condition requires the dynamics <span class="math inline">\(f_a\)</span> to be linear and stable, and the constraint set <span class="math inline">\(\mathcal{X}\)</span> to be bounded and contain the origin.</p>
<p>The same algoithm can be used to compute the maximal control invariant set <span class="math inline">\(\mathcal{C}_{\infty}\)</span>.</p>
<hr />
<p><strong>Algorithm</strong>: Computation of <span class="math inline">\(\mathcal{C}_{\infty}\)</span></p>
<hr />
<p><strong>Input:</strong> <span class="math inline">\(f\)</span>, <span class="math inline">\(\mathcal{X}\)</span>, <span class="math inline">\(\mathcal{U}\)</span></p>
<p><strong>Output:</strong> <span class="math inline">\(\mathcal{C}_{\infty}\)</span></p>
<p>   <span class="math inline">\(\Omega_0 \leftarrow \mathcal{X}\)</span>, <span class="math inline">\(k=0\)</span></p>
<p>   <strong>Repeat</strong></p>
<p>     <span class="math inline">\(\Omega_{k+1} \leftarrow \text{Pre}(\Omega_k) \cap \Omega_k\)</span></p>
<p>     <span class="math inline">\(k \leftarrow k+1\)</span></p>
<p>   <strong>Until</strong> <span class="math inline">\(\Omega_{k+1} = \Omega_k\)</span>.</p>
<p>   <span class="math inline">\(\mathcal{C}_{\infty} \leftarrow \Omega_k\)</span></p>
<hr />
<p>Similarly, the above algoithm generates <span class="math inline">\(\{\Omega_k \}\)</span> such that <span class="math inline">\(\Omega_{k+1} \subseteq \Omega_k\)</span> for any <span class="math inline">\(k\)</span>. If the algoithm terminates, then <span class="math inline">\(\Omega_k = \mathcal{C}_{\infty}\)</span>.</p>
<p>In general, the algoithm may never terminate. If the algoithm does not terminate, then in general convergence is not guaranteed
<span class="math display">\[
\mathcal{C}_{\infty} \neq \lim_{k \rightarrow \infty} \Omega_k.
\]</span>
The work in <span class="citation">(<a href="#ref-bertsekas72tac-infinite">Bertsekas 1972</a>)</span> reports examples of nonlinear systems for which the above equation can be observed. A sufficient condition for the convergence of <span class="math inline">\(\Omega_k\)</span> to <span class="math inline">\(\mathcal{C}_{\infty}\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span> requires the polyhedral sets <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{U}\)</span> to be bounded and the system <span class="math inline">\(f(x,u)\)</span> to be continuous <span class="citation">(<a href="#ref-bertsekas72tac-infinite">Bertsekas 1972</a>)</span>.</p>
<p>Let us apply the algoithm to compute the maximal control invariant set for the linear system in Example <a href="approximatedp.html#exm:compute-controllable-reachable-sets">3.10</a>.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:compute-control-invariant-set" class="example"><strong>Example 3.11  (Computation of the Maximal Control Invariant Set) </strong></span>Consider the linear system in Example <a href="approximatedp.html#exm:compute-controllable-reachable-sets">3.10</a> with same state constraint and control constraint.</p>
<p>The following code snippet shows how to apply the iterative algorithm introduced above to compute the maximal control invariant set.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb9-1"><a href="approximatedp.html#cb9-1" tabindex="-1"></a><span class="va">Omega</span> <span class="op">=</span> [<span class="va">calX</span>]<span class="op">;</span></span>
<span id="cb9-2"><a href="approximatedp.html#cb9-2" tabindex="-1"></a><span class="kw">while</span> <span class="va">true</span></span>
<span id="cb9-3"><a href="approximatedp.html#cb9-3" tabindex="-1"></a>    <span class="va">last_Omega</span> <span class="op">=</span> <span class="va">Omega</span>(<span class="kw">end</span>)<span class="op">;</span></span>
<span id="cb9-4"><a href="approximatedp.html#cb9-4" tabindex="-1"></a>    <span class="va">pre_omega</span> <span class="op">=</span> <span class="va">sys</span>.<span class="va">reachableSet</span>(<span class="ss">&#39;X&#39;</span><span class="op">,</span><span class="va">last_Omega</span><span class="op">,</span><span class="ss">&#39;U&#39;</span><span class="op">,</span><span class="va">calU</span><span class="op">,</span><span class="ss">&#39;N&#39;</span><span class="op">,</span><span class="fl">1</span><span class="op">,...</span></span>
<span id="cb9-5"><a href="approximatedp.html#cb9-5" tabindex="-1"></a>        <span class="ss">&#39;direction&#39;</span><span class="op">,</span><span class="ss">&#39;backwards&#39;</span>)<span class="op">;</span></span>
<span id="cb9-6"><a href="approximatedp.html#cb9-6" tabindex="-1"></a>    <span class="va">new_Omega</span> <span class="op">=</span> <span class="va">intersect</span>(<span class="va">pre_omega</span><span class="op">,</span><span class="va">last_Omega</span>)<span class="op">;</span></span>
<span id="cb9-7"><a href="approximatedp.html#cb9-7" tabindex="-1"></a>    <span class="va">Omega</span> <span class="op">=</span> [<span class="va">Omega</span><span class="op">;</span><span class="va">new_Omega</span>]<span class="op">;</span></span>
<span id="cb9-8"><a href="approximatedp.html#cb9-8" tabindex="-1"></a>    <span class="kw">if</span> <span class="va">new_Omega</span> <span class="op">==</span> <span class="va">last_Omega</span></span>
<span id="cb9-9"><a href="approximatedp.html#cb9-9" tabindex="-1"></a>        <span class="va">fprintf</span>(<span class="st">&quot;Converged to maximal control invariant set.\n&quot;</span>)<span class="op">;</span></span>
<span id="cb9-10"><a href="approximatedp.html#cb9-10" tabindex="-1"></a>        <span class="kw">break</span><span class="op">;</span></span>
<span id="cb9-11"><a href="approximatedp.html#cb9-11" tabindex="-1"></a>    <span class="kw">end</span></span>
<span id="cb9-12"><a href="approximatedp.html#cb9-12" tabindex="-1"></a><span class="kw">end</span></span></code></pre></div>
<p>The algoithm converges in 37 iterations and Fig. <a href="approximatedp.html#fig:max-control-invariant-set">3.21</a> plots the sequence of sets generated by the algorithm.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:max-control-invariant-set"></span>
<img src="images/maximal_control_invariant_set.png" alt="Maximal control invariant set." width="60%" />
<p class="caption">
Figure 3.21: Maximal control invariant set.
</p>
</div>
<p>The MPT toolbox actually implements this algoithm for us to use directly. If we use</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span id="cb10-1"><a href="approximatedp.html#cb10-1" tabindex="-1"></a><span class="va">C</span> <span class="op">=</span> <span class="va">sys</span>.<span class="va">invariantSet</span>(<span class="ss">&#39;X&#39;</span><span class="op">,</span> <span class="va">calX</span><span class="op">,</span> <span class="ss">&#39;U&#39;</span><span class="op">,</span> <span class="va">calU</span>)<span class="op">;</span></span></code></pre></div>
<p>we get the same result as before.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/mpt_examples/maximal_control_invariant_set.m">here</a>.</p>
</div>
</div>
</div>
</div>
<div id="basic-formulation-for-linear-systems" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Basic Formulation for Linear Systems<a href="approximatedp.html#basic-formulation-for-linear-systems" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We are now ready to introduce the basic formulation of MPC for linear systems and study its theoretical properties.</p>
<p>Consider the problem of regulating the following discrete-time linear system to the origin
<span class="math display" id="eq:mpc-linear-system">\[\begin{equation}
x_{t+1} = A x_t + B u_t,
\tag{3.33}
\end{equation}\]</span>
where the state <span class="math inline">\(x_t\)</span> and control <span class="math inline">\(u_t\)</span> are constrained to lie in polyhedral sets <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{U}\)</span>, respectively.
We assume <span class="math inline">\(\mathcal{X}\)</span> contains the origin <span class="math inline">\(0\)</span>.</p>
<p>We can formulate the following optimal control problem to regulate the system to the origin
<span class="math display" id="eq:infinite-horizon-lqr-constraints">\[\begin{equation}
\begin{split}
J^\star(x_0) = \min_{u_t,t=0,\dots} &amp; \quad \sum_{t=0}^{\infty} x_t^T Q x_t + u_t^T R u_t \\
\text{subject to} &amp; \quad x_{t+1} = A x_t + B u_t, \\
&amp; \quad (u_t,x_t) \in \mathcal{U} \times \mathcal{X}, \forall t = 0,\dots
\end{split}
\tag{3.34}
\end{equation}\]</span>
with <span class="math inline">\(Q \succeq 0, R \succ 0\)</span>.
Had we not included the constraints <span class="math inline">\((u_t,x_t) \in \mathcal{U} \times \mathcal{X}\)</span>, then problem <a href="approximatedp.html#eq:infinite-horizon-lqr-constraints">(3.34)</a> is exactly the infinite-horizon LQR problem in Section <a href="exactdp.html#infinite-horizon-lqr">2.1.1</a>, for which we know the optimal controller is <span class="math inline">\(u_t = - K x_t\)</span> with <span class="math inline">\(K\)</span> computed in closed-form as <a href="exactdp.html#eq:infinite-horizon-lqr-control">(2.12)</a>.</p>
<p>However, in the presence of constraints, problem <a href="approximatedp.html#eq:infinite-horizon-lqr-constraints">(3.34)</a> does not admit a simple closed-form solution. In fact, problem <a href="approximatedp.html#eq:infinite-horizon-lqr-constraints">(3.34)</a> is commonly referred to as the <em>contrained LQR</em> (CLQR) problem and it is known that the optimal controller is <em>piece-wise affine</em>, see Theorem 11.4 in <span class="citation">(<a href="#ref-borrelli17book-mpc">Borrelli, Bemporad, and Morari 2017</a>)</span>. We have asked you to numerically play with a toy example of CLQR in Exercise <a href="psets.html#exr:lqrconstraints">5.2</a>.</p>
<p><strong>Receding horizon control</strong>. Leveraging the receding horizon control framework, we can approach problem <a href="approximatedp.html#eq:infinite-horizon-lqr-constraints">(3.34)</a> by online solving convex optimization problems.</p>
<p>At time <span class="math inline">\(t\)</span>, suppose we can measure the current state of the system <span class="math inline">\(x_t\)</span>, then we solve the following optimal control problem with a finite horizon <span class="math inline">\(N\)</span>
<span class="math display" id="eq:linear-mpc-subroutine">\[\begin{equation}
\begin{split}
J_t^\star(x_t) = \min_{u(0),\dots,u(N-1)} &amp; \quad p(x(N)) + \sum_{k=0}^{N-1} q(x(k),u(k)) \\
\text{subject to} &amp; \quad x(k+1) = A x(k) + B u(k), k =0, \dots, N-1, \quad x(0) = x_t \\
&amp; \quad (x(k),u(k)) \in \mathcal{X} \times \mathcal{U}, k=0,\dots,N-1 \\
&amp; \quad x(N) \in \mathcal{X}_f,
\end{split}
\tag{3.35}
\end{equation}\]</span>
and <span class="math inline">\(p(x)\)</span>, <span class="math inline">\(q(x,u)\)</span> convex functions. For example, a simple choice is <span class="math inline">\(p(x) = x^T P x\)</span> and <span class="math inline">\(q = x^T Q x + u^T R u\)</span> with <span class="math inline">\(P,Q\succeq 0\)</span> and <span class="math inline">\(R \succ 0\)</span>.
I hope you could pay attention to the notation in <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a>. I used <span class="math inline">\(x_t, u_t\)</span> to denote the state and control for the original linear system <a href="approximatedp.html#eq:mpc-linear-system">(3.33)</a> at time <span class="math inline">\(t\)</span>, as well as in the CLQR problem <a href="approximatedp.html#eq:infinite-horizon-lqr-constraints">(3.34)</a>. However, in every subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> of RHS at time <span class="math inline">\(t\)</span>, I used <span class="math inline">\(x(k),u(k)\)</span>, with <span class="math inline">\(k\)</span> as the time step, to denote the state and control in the finite-horizon optimal control problem starting at <span class="math inline">\(x_t\)</span>, with <span class="math inline">\(x_t = x(0)\)</span> (<span class="math inline">\(k\)</span> is the shifted time horizon in RHS that always starts at zero). In addition to the difference in notation between <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> and <a href="approximatedp.html#eq:infinite-horizon-lqr-constraints">(3.34)</a>, problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is also different in the following two ways:</p>
<ol style="list-style-type: decimal">
<li><p>Terminal cost <span class="math inline">\(p(x)\)</span>: in the objective of problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a>, there is an additional terminal cost <span class="math inline">\(p(x(N))\)</span>.</p></li>
<li><p>Terminal constraint set <span class="math inline">\(\mathcal{X}_f\)</span>: problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> has an additional constraint that the final state <span class="math inline">\(x(N)\)</span> must belong to the set <span class="math inline">\(\mathcal{X}_f\)</span>. We assume <span class="math inline">\(\mathcal{X}_f\)</span> is also polyheral (and convex).</p></li>
</ol>
<p><strong>Feasible sets</strong>. We denote as <span class="math inline">\(\mathcal{X}_0 \subseteq \mathcal{X}\)</span> the set of initial states <span class="math inline">\(x_t\)</span> such that the RHC subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is feasible, i.e.,
<span class="math display" id="eq:definition-calX-0">\[\begin{equation}
\hspace{-14mm}
\begin{split}
\mathcal{X}_0 = \{ x(0) \in \mathbb{R}^n \mid \exists (u(0),\dots,u(N-1)) \text{ such that } x(k) \in \mathcal{X},u(k) \in \mathcal{U},k=0,\dots,N-1, \\ x(N) \in \mathcal{X}_f \text{ with } x(k+1) = A x(k) + B u(k), k=0,\dots,N-1 \}.
\end{split}
\tag{3.36}
\end{equation}\]</span>
Similarly, we denote as <span class="math inline">\(\mathcal{X}_i\)</span> the set of states such that the RHC subproblem is feasible from step <span class="math inline">\(k=i\)</span>:
<span class="math display" id="eq:definition-calX-i">\[\begin{equation}
\hspace{-14mm}
\begin{split}
\mathcal{X}_i = \{ x(i) \in \mathbb{R}^n \mid \exists (u(i),\dots,u(N-1)) \text{ such that } x(k) \in \mathcal{X}, u(k) \in \mathcal{U}, k=i,\dots,N-1, \\ x(N) \in \mathcal{X}_f \text{ with } x(k+1) = A x(k) + B u(k), k=i,\dots,N-1 \}.
\end{split}
\tag{3.37}
\end{equation}\]</span>
Clearly, by definition we have
<span class="math display">\[
\mathcal{X}_N = \mathcal{X}_f,
\]</span>
and
<span class="math display">\[
\mathcal{X}_i = \{ x \in \mathcal{X} \mid \exists u \in \mathcal{U} \text{ such that } A x + Bu \in \mathcal{X}_{i+1} \},i=0,\dots,N-1,
\]</span>
or written in a compact way as
<span class="math display" id="eq:mpc-feasible-set-recursion">\[\begin{equation}
\mathcal{X}_i = \text{Pre}(\mathcal{X}_{i+1}) \cap \mathcal{X}.
\tag{3.38}
\end{equation}\]</span>
Note that from equation <a href="approximatedp.html#eq:mpc-feasible-set-recursion">(3.38)</a> we have that, if we pick <span class="math inline">\(x_i \in \mathcal{X}_i\)</span> and let <span class="math inline">\(U(x_i)\)</span> be the set of feasible controls at <span class="math inline">\(x_i\)</span>, then pick any <span class="math inline">\((u(0),\dots,u(N-1)) \in U(x_i)\)</span> and apply <span class="math inline">\(u(0)\)</span> to the system to get <span class="math inline">\(x_{i+1} = A x_i + B u(0)\)</span>, we have <span class="math inline">\(x_{i+1} \in \mathcal{X}_{i+1}\)</span>.</p>
<p>Since the definitions <a href="approximatedp.html#eq:definition-calX-0">(3.36)</a> and <a href="approximatedp.html#eq:definition-calX-i">(3.37)</a> are rather abstract, let us visualize them using the double integrator example.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:compute-calXi-double-integrator" class="example"><strong>Example 3.12  (Double Integrator RHC Feasible Sets) </strong></span>Consider the discrete-time double integrator dynamics
<span class="math display">\[
x_{t+1} = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix} x_t + \begin{bmatrix} 0 \\ 1 \end{bmatrix} u_t,
\]</span>
subject to control constraint
<span class="math display">\[
u \in \mathcal{U} = [-0.5,0.5],
\]</span>
and state constraint
<span class="math display">\[
x \in \mathcal{X} = [-5, 5] \times [-5, 5].
\]</span>
We use <span class="math inline">\(N=3\)</span> and visualize the feasible sets <a href="approximatedp.html#eq:definition-calX-0">(3.36)</a> and <a href="approximatedp.html#eq:definition-calX-i">(3.37)</a> for two choices of <span class="math inline">\(\mathcal{X}_f\)</span>.</p>
<p><strong>Choice 1</strong>. <span class="math inline">\(\mathcal{X}_f = \mathcal{X}\)</span> is the full state space. We use the recursion <a href="approximatedp.html#eq:mpc-feasible-set-recursion">(3.38)</a> to compute the feasible sets <span class="math inline">\(\mathcal{X}_i\)</span> for <span class="math inline">\(i=0,1,2,3\)</span>. Fig. <a href="approximatedp.html#fig:double-integrator-feasible-set-1">3.22</a> plots the feasible sets. Observe that in this case <span class="math inline">\(\mathcal{X}_0 \subset \mathcal{X}_1 \subset \mathcal{X}_2 \subset \mathcal{X}_3\)</span>. This creates a concern: suppose the RHC starts at <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span> that is feasible, in the next iteration we have <span class="math inline">\(x_1 \in \mathcal{X}_1\)</span>. However, since <span class="math inline">\(\mathcal{X}_1\)</span> is larger than <span class="math inline">\(\mathcal{X}_0\)</span>, <span class="math inline">\(x_1\)</span> is not guaranteed to be feasible.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:double-integrator-feasible-set-1"></span>
<img src="images/double_integrator_feaset_1.png" alt="Feasible sets of the double integrator receding horizon controller without terminal constraint." width="60%" />
<p class="caption">
Figure 3.22: Feasible sets of the double integrator receding horizon controller without terminal constraint.
</p>
</div>
<p><strong>Choice 2</strong>. <span class="math inline">\(\mathcal{X}_f = \{(0,0)\}\)</span> is the origin. Fig. <a href="approximatedp.html#fig:double-integrator-feasible-set-2">3.23</a> plots the feasible sets. Observe that in this case <span class="math inline">\(\mathcal{X}_3 \subset \mathcal{X}_2 \subset \mathcal{X}_1 \subset \mathcal{X}_0\)</span>. This is a nice case, because if RHC starts at <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span> that is feasible, in the next iteration we have <span class="math inline">\(x_1 \in \mathcal{X}_1 \subset \mathcal{X}_0\)</span>, which implies that <span class="math inline">\(x_1\)</span> is guaranteed to remain feasible!</p>
<p>In fact, as we will soon show, in the first choice the RHC does suffer from infeasibility.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:double-integrator-feasible-set-2"></span>
<img src="images/double_integrator_feaset_2.png" alt="Feasible sets of the double integrator receding horizon controller with terminal constraint." width="60%" />
<p class="caption">
Figure 3.23: Feasible sets of the double integrator receding horizon controller with terminal constraint.
</p>
</div>
<p>The code for this example can be found <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/mpt_examples/double_integrator_feasible_sets.m">here</a>.</p>
</div>
</div>
<p><strong>Algorithm</strong>. Above all, problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is a convex optimization problem that we know how to solve efficiently (you have solved such convex optimization problems in Exercise <a href="psets.html#exr:lqrconstraints">5.2</a>).</p>
<p>Let <span class="math inline">\(u^\star(0),\dots,u^\star(N-1)\)</span> be the optimal solution of problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> when it is feasible, the RHS framework will only apply the first control <span class="math inline">\(u^\star(0)\)</span> to the system, and hence the closed-loop system is
<span class="math display" id="eq:mpc-closed-loop">\[\begin{equation}
x_{t+1} = A x_t + u^\star_{x_t}(0) = f_{\mathrm{cl}}(x_t),
\tag{3.39}
\end{equation}\]</span>
where I have used <span class="math inline">\(u^\star_{x_t}(0)\)</span> to make it explicit that the control is the first optimal control of solving <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> with an initial state <span class="math inline">\(x_t\)</span>. The following algorithm summarizes the receding horizon control algorithm.</p>
<hr />
<p><strong>Algorithm</strong>: Online Receding Horizon Control</p>
<hr />
<p><strong>Input:</strong> State <span class="math inline">\(x_t\)</span> at time <span class="math inline">\(t\)</span></p>
<p><strong>Output:</strong> Control <span class="math inline">\(u^\star_{x_t}(0)\)</span></p>
<ol style="list-style-type: decimal">
<li>Solve problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> to get the optimal controls <span class="math inline">\(u^\star(0), \dots, u^\star(N-1)\)</span></li>
<li><strong>if</strong> the problem is infeasible, <strong>then</strong> stop</li>
<li><strong>else</strong> <strong>return</strong> <span class="math inline">\(u^\star_{x_t}(0) = u^\star(0)\)</span></li>
</ol>
<hr />
<p><strong>RHC main questions</strong>. Two main questions arise regarding the RHC controller.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Persistent feasibility</strong>. If the RHC algorithm starts at a state <span class="math inline">\(x_0\)</span> for which the convex optimization <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is feasible, i.e., <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span>, will the convex optimization <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> remain feasible for all future time steps?</p></li>
<li><p><strong>Stability</strong>. Assuming the convex optimization is always feasible. Will the closed-loop system <a href="approximatedp.html#eq:mpc-closed-loop">(3.39)</a> (induced by the RHC controller) converge to the desired origin <span class="math inline">\(x=0\)</span>?</p></li>
</ol>
<p>Let us use a couple of examples to illustrate that, in general, the answers to the above two questions are both NO.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:double-integrator-mpc" class="example"><strong>Example 3.13  (Receding Horizon Control for Double Integrator) </strong></span>Consider the discrete-time double integrator dynamics
<span class="math display">\[
x_{t+1} = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix} x_t + \begin{bmatrix} 0 \\ 1 \end{bmatrix} u_t,
\]</span>
subject to control constraint
<span class="math display">\[
u \in \mathcal{U} = [-0.5,0.5],
\]</span>
and state constraint
<span class="math display">\[
x \in \mathcal{X} = [-5, 5] \times [-5, 5].
\]</span>
In the RHS subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a>, we use <span class="math inline">\(N = 3\)</span>, <span class="math inline">\(p(x) = x^T P x\)</span>, <span class="math inline">\(q(x,u) = x^T Q x + u^T R u\)</span> with <span class="math inline">\(P = Q = I\)</span>, <span class="math inline">\(R=10\)</span>, and <span class="math inline">\(\mathcal{X}_f = \mathbb{R}^2\)</span> (i.e., there is not terminal constraint). The subproblem is implemented using CVX in Matlab.</p>
<p>Fig. <a href="approximatedp.html#fig:double-integrator-mpc-two-initial-states">3.24</a> shows the state trajectory of executing RHC starting at <span class="math inline">\(x_0 = [-4.5;2]\)</span> and <span class="math inline">\(x_0 = [-4.5;3]\)</span>, respectively.</p>
<p>We can see that when the initial state is <span class="math inline">\(x_0 = [-4.5;2]\)</span>, the trajectory successfully converges to the origin (the blue line). However, when the initial state is <span class="math inline">\(x_0 = [-4.5;3]\)</span>, RHC fails in the third iteration because the subproblem becomes infeasible (the red line).</p>
<p>You can find code for this example <a href="https://github.com/ComputationalRobotics/OptimalControlEstimation-Examples/blob/main/double_integrator_mpc/double_integrator_mpc.m">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:double-integrator-mpc-two-initial-states"></span>
<img src="images/double_integrator_mpc_two_initial_states.png" alt="Receding horizon control for the double integrator with two initial states." width="70%" />
<p class="caption">
Figure 3.24: Receding horizon control for the double integrator with two initial states.
</p>
</div>
</div>
</div>
<p>We now show another example, adapted from <span class="citation">(<a href="#ref-borrelli17book-mpc">Borrelli, Bemporad, and Morari 2017</a>)</span> where the design of <span class="math inline">\(N\)</span>, <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x,u)\)</span> affects the closed-loop performance.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:mpc-linear-design-choice" class="example"><strong>Example 3.14  (RHC Performance Affected by Parameters) </strong></span>Consider the system
<span class="math display">\[
x_{t+1} = \begin{bmatrix}
2 &amp; 1 \\ 0 &amp; 0.5
\end{bmatrix} x_t + \begin{bmatrix} 1 \\ 0 \end{bmatrix} u_t,
\]</span>
with control constraint
<span class="math display">\[
u \in \mathcal{U} = [-1,1],
\]</span>
and state constraint
<span class="math display">\[
x \in \mathcal{X} = [-10,10] \times [-10,10].
\]</span></p>
<p>In the RHC problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a>, we choose <span class="math inline">\(p(x) = x^T P x\)</span>, <span class="math inline">\(q(x,u) = x^T Q x + u^T R u\)</span>. We fix <span class="math inline">\(P=0\)</span>, <span class="math inline">\(Q = I\)</span>, and <span class="math inline">\(\mathcal{X}_f = \mathbb{R}^2\)</span>, but vary <span class="math inline">\(N\)</span> and <span class="math inline">\(R\)</span>:</p>
<ul>
<li><p>Setting 1: <span class="math inline">\(N=2\)</span>, <span class="math inline">\(R = 10\)</span>;</p></li>
<li><p>Setting 2: <span class="math inline">\(N=3\)</span>, <span class="math inline">\(R = 2\)</span>;</p></li>
<li><p>Setting 3: <span class="math inline">\(N=4\)</span>, <span class="math inline">\(R=1\)</span>.</p></li>
</ul>
<p>Fig. <a href="approximatedp.html#fig:mpc-linear-effect-parameters">3.25</a> shows the closed-loop trajectories of three different settings. As we can see, the closed-loop performance depends on the parameters in a very complicated manner.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mpc-linear-effect-parameters"></span>
<img src="images/mpc_linear_effect_parameter.png" alt="Closed-loop trajectories for different settings of horizon N and weight R. Boxes (circles) are initial points leading to feasible (infeasible) closed-loop trajectories." width="80%" />
<p class="caption">
Figure 3.25: Closed-loop trajectories for different settings of horizon N and weight R. Boxes (circles) are initial points leading to feasible (infeasible) closed-loop trajectories.
</p>
</div>
</div>
</div>
</div>
<div id="persistent-feasibility" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Persistent Feasibility<a href="approximatedp.html#persistent-feasibility" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Under what conditions can we guarantee the RHC subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is always feasible?</p>
<p>Intuitively, we will show that, by designing the terminal constraint set <span class="math inline">\(\mathcal{X}_f\)</span>, we can guarantee the configuration of the feasible sets will look like choice 2 in Example <a href="approximatedp.html#exm:compute-calXi-double-integrator">3.12</a>.</p>
<p>There are various sets here of interest for answering this question.</p>
<ul>
<li><p><span class="math inline">\(\mathcal{C}_{\infty}\)</span>: The maximal control invariant set <span class="math inline">\(\mathcal{C}_{\infty}\)</span> is only affected by the system dynamics <a href="approximatedp.html#eq:mpc-linear-system">(3.33)</a> and the constraint sets <span class="math inline">\(\mathcal{X} \times \mathcal{U}\)</span>. It is the largest set over which we can expect <em>any</em> controller to work, because otherwise the system trajectory will blow up.</p></li>
<li><p><span class="math inline">\(\mathcal{X}_0\)</span>: The set of states at which the RHC subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is feasible. The set <span class="math inline">\(\mathcal{X}_0\)</span> depends on the system dynamics, the constraint sets <span class="math inline">\(\mathcal{X} \times \mathcal{U}\)</span>, as well as the RHC horizon <span class="math inline">\(N\)</span> and the terminal constraint set <span class="math inline">\(\mathcal{X}_f\)</span>. It is worth noting that it does not depend on the objective function in <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> (i.e., <span class="math inline">\(P,Q,R\)</span>).</p></li>
<li><p><span class="math inline">\(\mathcal{O}_{\infty}\)</span>: The maximal positive invariant set for the closed-loop system <a href="approximatedp.html#eq:mpc-closed-loop">(3.39)</a> induced by the RHC control law. This depends on the RHC controller and hence it depends on the system dynamics, the constraint set <span class="math inline">\(\mathcal{X} \times \mathcal{U}\)</span>, <span class="math inline">\(N\)</span>, <span class="math inline">\(\mathcal{X}_f\)</span> and the objective function of <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> <span class="math inline">\(P,Q,R\)</span>.</p></li>
</ul>
<p>A few relationships between these sets are easy to observe.</p>
<ul>
<li><p><span class="math inline">\(\mathcal{O}_{\infty} \subseteq \mathcal{X}_0\)</span>. Any state <span class="math inline">\(x \in \mathcal{O}_{\infty}\)</span> needs to stay in <span class="math inline">\(\mathcal{O}_{\infty}\)</span> for all future time steps. Thus, the subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> must be feasible for any <span class="math inline">\(x \in \mathcal{O}_{\infty}\)</span>, otherwise the closed-loop system <a href="approximatedp.html#eq:mpc-closed-loop">(3.39)</a> is not even well defined.</p></li>
<li><p><span class="math inline">\(\mathcal{O}_{\infty} \subseteq \mathcal{C}_{\infty}\)</span>. The set <span class="math inline">\(\mathcal{O}_{\infty}\)</span> is control invariant because there exists a controller (specifically, the RHC controller) that makes it positive invariant. Therefore, <span class="math inline">\(\mathcal{O}_{\infty}\)</span> must belong to the maximal control invariant set <span class="math inline">\(\mathcal{C}_{\infty}\)</span>.</p></li>
</ul>
<p>We can now state necessary and sufficient conditions guaranteeing persistent feasibility.</p>
<div class="theorembox">
<div class="lemma">
<p><span id="lem:sufficient-necessary-feasibility" class="lemma"><strong>Lemma 3.1  (Sufficient and Necessary Condition for Persistent Feasibility) </strong></span>The RHC subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is persistently feasible if and only if <span class="math inline">\(\mathcal{X}_0 = \mathcal{O}_{\infty}\)</span>.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>We have already argued that <span class="math inline">\(\mathcal{O}_{\infty} \subseteq \mathcal{X}_0\)</span>. It remains to show <span class="math inline">\(\mathcal{X}_0 \subseteq \mathcal{O}_{\infty}\)</span>. By definition, <span class="math inline">\(\mathcal{X}_0\)</span> is persistently feasible implies that
<span class="math display">\[
x \in \mathcal{X}_0 \Longrightarrow f_{\mathrm{cl}}^t(x) \in \mathcal{X}_0, \forall t
\]</span>
where <span class="math inline">\(f_{\mathrm{cl}}^t\)</span> means applying the closed-loop dynamics <span class="math inline">\(f_{\mathrm{cl}}\)</span> in <a href="approximatedp.html#eq:mpc-closed-loop">(3.39)</a> <span class="math inline">\(t\)</span> times. This shows that <span class="math inline">\(\mathcal{X}_0\)</span> is a positive invariant set for the closed-loop system, and hence <span class="math inline">\(\mathcal{X}_0 \subseteq \mathcal{O}_{\infty}\)</span>.</p>
</div>
</div>
<p>We argued that <span class="math inline">\(\mathcal{X}_0\)</span> does not depend on the RHC parameters <span class="math inline">\(P,Q,R\)</span> but <span class="math inline">\(\mathcal{O}_{\infty}\)</span> does. Therefore, in general only some <span class="math inline">\(P,Q,R\)</span> are allowed for persistent feasibility to hold. Due to the complicated relationship between <span class="math inline">\(P,Q,R\)</span> and <span class="math inline">\(\mathcal{O}_{\infty}\)</span>, it is generally difficult to design <span class="math inline">\(P,Q,R\)</span> such that RHC has persistent feasibility.</p>
<p>We now state a sufficient condition for persistent feasibility to hold.</p>
<div class="theorembox">
<div class="lemma">
<p><span id="lem:sufficient-feasibility" class="lemma"><strong>Lemma 3.2  (Sufficient Condition for Persistent Feasibility) </strong></span>Consider the RHC subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> with <span class="math inline">\(N \geq 1\)</span>. If <span class="math inline">\(\mathcal{X}_1\)</span> is a control invariant set for the linear system <a href="approximatedp.html#eq:mpc-linear-system">(3.33)</a>, then the RHC controller is persistently feasible.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-9" class="proof"><em>Proof</em>. </span>If <span class="math inline">\(\mathcal{X}_1\)</span> is control invariant, then by definition <span class="math inline">\(\mathcal{X}_1 \subseteq \text{Pre}(\mathcal{X}_1)\)</span>. Since <span class="math inline">\(\mathcal{X}_1 \subseteq \mathcal{X}\)</span>, we have
<span class="math display">\[
\mathcal{X}_1 \subseteq \text{Pre}(\mathcal{X}_1) \cap \mathcal{X} = \mathcal{X}_0.
\]</span>
where the last equality in the equation above is due to <a href="approximatedp.html#eq:mpc-feasible-set-recursion">(3.38)</a>.
Now pick any <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span>, the set of controls that make <span class="math inline">\(x_0\)</span> feasible is denoted as
<span class="math display">\[
U = \{ u(0),\dots,u(N-1) \mid x(0)=x_0, x(k) \in \mathcal{X},k=0,\dots,N-1,x(N) \in \mathcal{X}_f \}.
\]</span>
The RHC will pick some control sequence from <span class="math inline">\(U\)</span>, say <span class="math inline">\(\hat{u}(0),\dots,\hat{u}(N-1)\)</span> and apply the first control <span class="math inline">\(\hat{u}(0)\)</span> to the system, which will bring the system to a new state
<span class="math display">\[
x_1 = A x_0 + B \hat{u}(0).
\]</span>
Observe that <span class="math inline">\(x_1 \in \mathcal{X}_1\)</span> by definition. Since <span class="math inline">\(\mathcal{X}_1 \subseteq \mathcal{X}_0\)</span>, we have <span class="math inline">\(x_1 \in \mathcal{X}_0\)</span>. This proves that the RHC is persistently feasible, i.e., starting with any <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span>, the RHC subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is always feasible.</p>
</div>
</div>
<p>Lemma <a href="approximatedp.html#lem:sufficient-feasibility">3.2</a> states that if <span class="math inline">\(\mathcal{X}_1\)</span> is control invariant, then the RHC subproblem is persistently feasible. An immediate result of this Lemma is that when <span class="math inline">\(N=1\)</span>, then <span class="math inline">\(\mathcal{X}_1 = \mathcal{X}_f\)</span>. Therefore, if we choose the terminal constraint set <span class="math inline">\(\mathcal{X}_f\)</span> to be control invariant, then RHC is has persistent feasibility.</p>
<p>The next theorem states that when <span class="math inline">\(N \geq 1\)</span>, as long as we choose the terminal constraint set <span class="math inline">\(\mathcal{X}_f\)</span> to be control invariant, then persistent feasibility also holds.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:sufficient-feasibility-terminal" class="theorem"><strong>Theorem 3.2  (Control Invariant Terminal Constraint Set Guarantees Persistent Feasibility) </strong></span>Consider the RHC subproblem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> with <span class="math inline">\(N \geq 1\)</span>. If <span class="math inline">\(\mathcal{X}_f\)</span> is a control invariant set for the linear system <a href="approximatedp.html#eq:mpc-linear-system">(3.33)</a>, then the RHC controller is persistently feasible.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span>We will prove that <span class="math inline">\(\mathcal{X}_f\)</span> being control invariant implies
<span class="math display">\[
\mathcal{X}_{N-1}, \mathcal{X}_{N-2},\dots,\mathcal{X}_1
\]</span>
are all control invariant, and then by Lemma <a href="approximatedp.html#lem:sufficient-feasibility">3.2</a>, we can guarantee persistent feasibility. Fig. <a href="approximatedp.html#fig:mpc-nested-control-invariant-set">3.26</a> shows the nested control invariant sets when <span class="math inline">\(\mathcal{X}_f\)</span> is control invariant.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mpc-nested-control-invariant-set"></span>
<img src="images/mpc_nested_control_invariant_set.png" alt="Nested control invariant sets." width="80%" />
<p class="caption">
Figure 3.26: Nested control invariant sets.
</p>
</div>
<p>It suffices to show <span class="math inline">\(\mathcal{X}_{i+1}\)</span> being control invariant leads to <span class="math inline">\(\mathcal{X}_i\)</span> being control invariant. First, by <span class="math inline">\(\mathcal{X}_{i+1}\)</span> control invariant, we have <span class="math inline">\(\mathcal{X}_{i+1} \subseteq \text{Pre}(\mathcal{X}_i) \cap \mathcal{X} = \mathcal{X}_i\)</span>. Now pick any <span class="math inline">\(x_{i} \in \mathcal{X}_{i}\)</span>, for any feasible <span class="math inline">\(\hat{u}(0),\dots,\hat{u}(N-1)\)</span>, applying the first control <span class="math inline">\(\hat{u}(0)\)</span> brings the system to a new state
<span class="math display">\[
x_{i+1} = A x_i + B \hat{u}(0) \in \mathcal{X}_{i+1} \subseteq \mathcal{X}_i.
\]</span>
This shows <span class="math inline">\(\mathcal{X}_i\)</span> is control invariant.</p>
</div>
</div>
<p>Persistent feasibility does not guarantee the closed-loop system will converge to the origin. In fact, from Theorem <a href="approximatedp.html#thm:sufficient-feasibility-terminal">3.2</a>, it is clear that if the system starts at <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span>, then we can only guarantee <span class="math inline">\(x_t \in \mathcal{X}_1\)</span> for all <span class="math inline">\(t \geq 0\)</span>.</p>
</div>
<div id="mpc-stability" class="section level3 hasAnchor" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> Stability<a href="approximatedp.html#mpc-stability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now answer the question of how can we guarantee the RHC controller will drive the system to the desired origin.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:mpc-stability" class="theorem"><strong>Theorem 3.3  (Sufficient Condition for Stability) </strong></span>Consider the linear system <a href="approximatedp.html#eq:mpc-linear-system">(3.33)</a>, and the RHC algorithm <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a>. Assume that</p>
<ol style="list-style-type: decimal">
<li><p>The stage cost <span class="math inline">\(q(x,u)\)</span> and terminal cost <span class="math inline">\(p(x)\)</span> are continuous and positive definite functions.</p></li>
<li><p>The sets <span class="math inline">\(\mathcal{X}\)</span>, <span class="math inline">\(\mathcal{X}_f\)</span> and <span class="math inline">\(\mathcal{U}\)</span> contain the origin in their interior and are closed.</p></li>
<li><p><span class="math inline">\(\mathcal{X}_f \subseteq \mathcal{X}\)</span> is control invariant.</p></li>
<li><p>For any <span class="math inline">\(x \in \mathcal{X}_f\)</span>, the following inequality holds
<span class="math display" id="eq:mpc-stability-lyapunov">\[\begin{equation}
\min_{u \in \mathcal{U}, A x + Bu \in \mathcal{X}_f} \left( - p(x) + q(x,u) + p(Ax + Bu) \right) \leq 0.
\tag{3.40}
\end{equation}\]</span></p></li>
</ol>
<p>Then, the origin of the closed-loop system <a href="approximatedp.html#eq:mpc-closed-loop">(3.39)</a> is asymptotically stable with domain of attraction <span class="math inline">\(\mathcal{X}_0\)</span>. In words, for any <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span>, if the closed-loop system <a href="approximatedp.html#eq:mpc-closed-loop">(3.39)</a> starts at <span class="math inline">\(x_0\)</span>, then the system trajectory converges to the origin as <span class="math inline">\(t\)</span> tends to infinity.</p>
</div>
</div>
<p>Let us interpret Theorem <a href="approximatedp.html#thm:mpc-stability">3.3</a> before proving it. It should be clear that the first three assumptions are easy to satisfy. For example, if we choose <span class="math inline">\(p(x) = x^T P x\)</span> and <span class="math inline">\(q(x,u) = x^T Q x + u^T R u\)</span> with <span class="math inline">\(P, Q, R \succ 0\)</span>, then assumption 1 is satisfied. Usually <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{U}\)</span> are both polyhedral sets containing the origin in the interior, so assumption 2 also holds naturally. Finding a control invariant set <span class="math inline">\(\mathcal{X}_f\)</span> is not a trivial task but there exists numerical algorithms for this task (e.g., using the algorithms introduced in Section <a href="approximatedp.html#mpc-compute-control-invariant-set">3.3.2.4</a>). After we find a control invariant <span class="math inline">\(\mathcal{X}_f\)</span>, by Theorem <a href="approximatedp.html#thm:sufficient-feasibility-terminal">3.2</a>, we know persistent feasibility will hold.</p>
<p>We now prove the theorem by showing that Assumption 4 guarantees stability. Before we show the proof, we need the concept of a <em>Lyapunov function</em>. Below we introduce Lyapunov function for a discrete-time dynamical system, but we will study more details of Lyapunov function for continuous-time dynamical systems in Chapter <a href="stability.html#stability">4</a>.</p>
<div class="theorembox">
<div class="lemma">
<p><span id="lem:lyapunov-stability-discrete-time" class="lemma"><strong>Lemma 3.3  (Discrete-time Lyapunov Function) </strong></span>Consider the discrete-time autonomous system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a>, restated below for convenience:
<span class="math display">\[
x_{t+1} = f_a(x_t),
\]</span>
and assume <span class="math inline">\(x=0\)</span> is an equilibrium point of the system, i.e., <span class="math inline">\(0 = f_a (0)\)</span> (if the system starts at the origin, it stays at the origin). Let <span class="math inline">\(\Omega \subset \mathbb{R}^n\)</span> be a closed and bounded set containing the origin.
If there exists a function <span class="math inline">\(V: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> that is (a) continuous at the origin, (b) finite for every <span class="math inline">\(x \in \Omega\)</span>, and (c) satisfies</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(V\)</span> is positive definite: <span class="math inline">\(V(0) = 0\)</span> and <span class="math inline">\(V(x) &gt; 0,\forall x \in \Omega \backslash \{ 0\}\)</span>,</p></li>
<li><p><span class="math inline">\(V(x_{t+1}) - V(x_t) \leq - \alpha(x_t) &lt; 0\)</span> for any <span class="math inline">\(x_t \in \Omega \backslash \{ 0\}\)</span>,</p></li>
</ol>
<p>where <span class="math inline">\(\alpha: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is a continuous positive definite function. Then <span class="math inline">\(x=0\)</span> is asymptotically stable in <span class="math inline">\(\Omega\)</span>, i.e, if the system starts within <span class="math inline">\(\Omega\)</span>, then its trajectory tends to <span class="math inline">\(0\)</span> as <span class="math inline">\(t \rightarrow \infty\)</span>.</p>
<p>A function <span class="math inline">\(V\)</span> satisfying the conditions above is called a Lyapunov function for the system.</p>
</div>
</div>
<p>One can think of the Lyapunov function <span class="math inline">\(V\)</span> as an energy function for the system <a href="approximatedp.html#eq:mpc-autonomous-system">(3.27)</a> that maps a system state to a single scalar. Condition 1 in Lemma <a href="approximatedp.html#lem:lyapunov-stability-discrete-time">3.3</a> states that <span class="math inline">\(V\)</span> (the energy function) is strictly positive except at the origin. Condition 2 in Lemma <a href="approximatedp.html#lem:lyapunov-stability-discrete-time">3.3</a> states that, the system energy <span class="math inline">\(V\)</span> is strictly decreasing along any system trajectory. Therefore, we conclude that <span class="math inline">\(V\)</span> must converge to <span class="math inline">\(V=0\)</span>, and hence any state trajectory must converge to the origin.</p>
<p>We will now use Lemma <a href="approximatedp.html#lem:lyapunov-stability-discrete-time">3.3</a> to prove RHC stability under Theorem <a href="approximatedp.html#thm:mpc-stability">3.3</a>.</p>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span>Our goal is to prove that the optimal cost of the RHC problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a>, <span class="math inline">\(J_t^\star(x_t)\)</span>, is a Lyapunov function for the closed-loop system <a href="approximatedp.html#eq:mpc-closed-loop">(3.39)</a> on the domain <span class="math inline">\(\mathcal{X}_0\)</span>. Then using Lemma <a href="approximatedp.html#lem:lyapunov-stability-discrete-time">3.3</a> we can conclude that the closed-loop system <a href="approximatedp.html#eq:mpc-closed-loop">(3.39)</a> will converge to the origin.</p>
<p><strong>Positive definite</strong>. Clearly, we have <span class="math inline">\(J_t^\star(0) = 0\)</span> by the positive definiteness of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> and the fact that <span class="math inline">\(x=0\)</span> is an equilibrium point of the linear system <a href="approximatedp.html#eq:mpc-linear-system">(3.33)</a>. For <span class="math inline">\(x_t \neq 0\)</span>, it is also clear that <span class="math inline">\(J_t^\star(x) &gt; 0\)</span>. Therefore, <span class="math inline">\(J_t^\star(x)\)</span> is positive definite on <span class="math inline">\(\mathcal{X}_0\)</span>.</p>
<p><strong>Strictly decrease</strong>. It suffices to show <span class="math inline">\(J_1^\star(x_1) - J_0^\star(x_0) &lt; 0\)</span> because the constraints of the RHC problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> is time-invariant. Pick any <span class="math inline">\(x_0 \in \mathcal{X}_0\)</span>, and let
<span class="math display" id="eq:mpc-stability-proof-control-1">\[\begin{equation}
\left( u^\star(0),u^\star(1),\dots,u^\star(N-1) \right)
\tag{3.41}
\end{equation}\]</span>
be an optimal control trajectory to problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a>. Denote
<span class="math display" id="eq:mpc-stability-proof-state-1">\[\begin{equation}
\left( x_0,x(1),\dots,x(N) \right)
\tag{3.42}
\end{equation}\]</span>
as the associated optimal state trajectory for problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a>. The RHC controller will apply <span class="math inline">\(u^\star(0)\)</span> to the system, leading to the next state
<span class="math display">\[
x_1 = x(1) = A x_0 + B u^\star(0).
\]</span>
Then we will solve problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> to get <span class="math inline">\(J^\star_1(x_1)\)</span>, and we want to show <span class="math inline">\(J^\star_1(x_1) &lt; J^\star_0(x_0)\)</span>. Towards this, we will construct a feasible solution to <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> starting at <span class="math inline">\(x_1\)</span>, and hence an upper bound on <span class="math inline">\(J^\star_1(x_1)\)</span>. Consider the control sequence
<span class="math display" id="eq:mpc-stability-proof-control-2">\[\begin{equation}
\left( u^\star(1),\dots,u^\star(N-1),v \right)
\tag{3.43}
\end{equation}\]</span>
which is different from the control sequence in <a href="approximatedp.html#eq:mpc-stability-proof-control-1">(3.41)</a> by removing <span class="math inline">\(u^\star(0)\)</span> and appending <span class="math inline">\(v\)</span>. The corresponding state trajectory to <a href="approximatedp.html#eq:mpc-stability-proof-control-2">(3.43)</a> is
<span class="math display" id="eq:mpc-stability-proof-state-2">\[\begin{equation}
\left( x_1 = x(1), x(2),\dots,x(N), Ax(N) + Bv \right)
\tag{3.44}
\end{equation}\]</span>
which is different from the state trajectory in <a href="approximatedp.html#eq:mpc-stability-proof-state-1">(3.42)</a> by removing <span class="math inline">\(x_0\)</span> and appending <span class="math inline">\(A x(N) + Bv\)</span>. Since <span class="math inline">\(x(N) \in \mathcal{X}_f\)</span> and <span class="math inline">\(\mathcal{X}_f\)</span> is control invariant, there exists <span class="math inline">\(v\)</span> such that <span class="math inline">\(Ax(N) + Bv \in \mathcal{X}_f\)</span> and the corresponding control trajectory <a href="approximatedp.html#eq:mpc-stability-proof-control-2">(3.43)</a> is feasible. Applying the control trajectory <a href="approximatedp.html#eq:mpc-stability-proof-control-2">(3.43)</a> to the optimization problem <a href="approximatedp.html#eq:linear-mpc-subroutine">(3.35)</a> will lead to the total cost
<span class="math display">\[
J_1(x_1) = J_0^\star(x_0) - q(x_0,u^\star(0)) \underbrace{- p(x(N)) + q(x(N),v) + p(Ax(N) + Bv)}_{s(x(N),v)}.
\]</span>
Now by assumption 4 in Theorem <a href="approximatedp.html#thm:mpc-stability">3.3</a>, we can choose <span class="math inline">\(v\)</span> such that the sum <span class="math inline">\(s(x(N),v) \leq 0\)</span>. Consequently, we have
<span class="math display">\[
J_1^\star(x_1) \leq J_1(x_1) \leq J_0^\star(x_0) - q(x_0,u^\star(0)),
\]</span>
which leads to
<span class="math display">\[
J_1^\star(x_1) - J_0^\star(x_0) \leq - q(x_0,u^\star(0)).
\]</span>
Because the choice of <span class="math inline">\(x_0\)</span> was arbitrary, we conclude that <span class="math inline">\(J_t^\star(x_t)\)</span> strictly decreases along any system trajectory that starts within <span class="math inline">\(\mathcal{X}_0\)</span>.</p>
<p><strong>Continuity at the origin</strong>. We will show that <span class="math inline">\(J_0^\star(x) \leq p(x)\)</span> for any <span class="math inline">\(x \in \mathcal{X}_f\)</span>. With this argument, since <span class="math inline">\(p(x)\)</span> is positive definite and continuous, then <span class="math inline">\(J_0^\star(x)\)</span> must be continuous at the origin. We now prove <span class="math inline">\(J_0^\star(x) \leq p(x)\)</span> for any <span class="math inline">\(x \in \mathcal{X}_f\)</span>. Since <span class="math inline">\(\mathcal{X}_f\)</span> is control invariant, pick any <span class="math inline">\(x \in \mathcal{X}_f\)</span>, there exists a sequence of controls <span class="math inline">\((u(0),u(1),\dots,u(N-1))\)</span> such that the state trajectory <span class="math inline">\((x(0)=x,x(1),\dots,x(N))\)</span> stays in <span class="math inline">\(\mathcal{X}_f\)</span>. Such a control sequence lead to an upper bound on <span class="math inline">\(J^\star_0(x)\)</span>:
<span class="math display">\[
J^\star_0(x(0)) \leq p(x(N)) + \sum_{i=0}^{N-1} q(x(i),u(i)) = p(x(0)) + \sum_{i=0}^{N-1} \left( q(x(i),u(i)) + p(x(i+1)) - p(x(i)) \right),
\]</span>
since each <span class="math inline">\(x(i) \in \mathcal{X}_f\)</span>, according to Assumption 4, we can choose <span class="math inline">\(u(i)\)</span> such that
<span class="math display">\[
J^\star_0(x(0)) \leq p(x(0))
\]</span>
for any <span class="math inline">\(x(0) \in \mathcal{X}_f\)</span>.</p>
<p>In conclusion, we have shown that <span class="math inline">\(J^\star_t(x_t)\)</span> is a Lyapunov function, and by Lemma <a href="approximatedp.html#lem:lyapunov-stability-discrete-time">3.3</a>, the closed-loop system is asymptotically stable.</p>
</div>
</div>
<p>A function <span class="math inline">\(p(x)\)</span> that satisfies Assumption 4 in Theorem <a href="approximatedp.html#thm:mpc-stability">3.3</a> is typicalled known as a <em>control Lyapunov function</em>.</p>
<p>Now two natural problems arise:</p>
<ol style="list-style-type: decimal">
<li><p>Given <span class="math inline">\(p(x)\)</span>, how to verify if assumption 3 holds in Theorem <a href="approximatedp.html#thm:mpc-stability">3.3</a>?</p></li>
<li><p>How to synthesize a <span class="math inline">\(p(x)\)</span> that satisfies assumption 3 in Theorem <a href="approximatedp.html#thm:mpc-stability">3.3</a>?</p></li>
</ol>
<p>Unfortunately, both problems are hard. To see this, suppose we are given a candidate function <span class="math inline">\(p(x) = x^T P x\)</span> with <span class="math inline">\(P \succ 0\)</span> that is clearly positive definite. Assume <span class="math inline">\(q(x,u) = x^T Q x + u^T R u\)</span> with <span class="math inline">\(Q, R \succ 0\)</span> and <span class="math inline">\(\mathcal{U}, \mathcal{X}_f\)</span> are given polyhedral sets. Then verifying if <span class="math inline">\(p(x)\)</span> satisifies Assumption 4 boils down to checking if
<span class="math display">\[
\min_{u \in \mathcal{U}, Ax + Bu \in \mathcal{X}_f} x^T Q x + u^T R u + (Ax + Bu)^T P (Ax + Bu) - x^T P x
\]</span>
is non-positive for any possible <span class="math inline">\(x \in \mathcal{X}_f\)</span>. Although for each possible <span class="math inline">\(x\)</span>, the above problem is a convex optimization problem, there are an infinite number of points in the set <span class="math inline">\(\mathcal{X}_f\)</span> and enumerating over all points is a daunting task. We will see in Chapter <a href="stability.html#stability">4</a> that convex relaxations, in particular <a href="https://hankyang.seas.harvard.edu/Semidefinite/">semidefinite relaxations</a>, can help us partially solve these hard problems.</p>
<p><strong>A simple control lyapunov function</strong>. One can solve the infinite-horizon unconstrained LQR problem
<span class="math display">\[
\min_{u_t} \sum_{t=0}^{\infty}x_t ^T Q x_t + u_t^T R u_t,
\]</span>
for which the optimal cost-to-go is
<span class="math display">\[
J_{\infty}(x) = x^T S x,
\]</span>
with <span class="math inline">\(S\)</span> the solution to the algebraic Riccati equation <a href="exactdp.html#eq:algebraic-riccati">(2.13)</a>. Denote
<span class="math display">\[
u_t = \Pi_{\mathcal{U}} (- K x_t)
\]</span>
as the optimal controller (<span class="math inline">\(\Pi_{\mathcal{U}}\)</span> is the projection of the controller to the feasible set <span class="math inline">\(\mathcal{U}\)</span>), and use <span class="math inline">\(\mathcal{X}_f\)</span> as the maximal positive invariant set of the closed-loop system
<span class="math display">\[
x_{t+1} = A x_t + B u_t.
\]</span>
Then <span class="math inline">\(J_{\infty}(x) = x^T S x\)</span> is a control Lyapunov function over the set <span class="math inline">\(\mathcal{X}_f\)</span>.</p>
</div>
<div id="explicit-mpc" class="section level3 hasAnchor" number="3.3.6">
<h3><span class="header-section-number">3.3.6</span> Explicit MPC<a href="approximatedp.html#explicit-mpc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>See the original paper <span class="citation">(<a href="#ref-bemporad02automatica-explicit">Bemporad et al. 2002</a>)</span>, and check out Matlab’s <a href="https://www.mathworks.com/help/mpc/explicit-mpc-design.html">explict MPC design</a>.</p>
<!-- ## Approximation in value space

Let us first recap the iteration process of the generic form of DP as mentioned in theorem \@ref(thm:dynamicprogramming). We can obtain the cost-to-go function $J_k$, which means the cost-to-go value at time $k$, thereby defining corresponding control $u_k$ or policy $\mu_k$. 

\begin{equation}
J_k(x_k) = \min_{u_k \in \mathbb{U}} \displaystyle \mathbb{E} \displaystyle \left\{ g_k(x_k,u_k) + J_{k+1}(f_k(x_k,u_k,w_k) ) \right\}, \ k=N-1,\dots,1,0.
\end{equation}

By using the _approximation in value space_ methods, we could replace the optimal cost-to-go function $J_k$ with some other functions $\tilde J_k$. In other words, the suboptimal policy $\tilde{\mu}_k(x_k)$ (and the corresponding control) is obtained from the one-step lookahead minimization

\begin{align}
\tilde{J}_k(x_k) &= \min_{u_k \in \mathbb{U}_k (x_k)} \displaystyle \mathbb{E} \displaystyle \left\{g_k(x_k,u_k,w_k) + \tilde J_{k+1} (f_k(x_k,u_k,w_k) ) \right\} \\
\tilde{\mu}_k(x_k) &= \arg \min_{u_k \in \mathbb{U}_k (x_k)} \displaystyle \mathbb{E} \displaystyle \left\{g_k(x_k,u_k,w_k) + \tilde J_{k+1} (f_k(x_k,u_k,w_k) ) \right\} (\#eq:apprinv-dp)
\end{align}

The major issue in value space approximation is how to compute the approximate cost-to-go functions $\tilde J_{k+1}$ in \@ref(eq:apprinv-dp). We will consider three types of methods: 

1. _Problem approximation_

2. _Parametric cost approximation_

3. _Online approximate optimization_

In approximation in value space, we may also distinguish between _online_ and _offline_ methods. 

1. _Offline_ methods, where the entire function $\tilde J_{k+1}$ in \@ref(eq:apprinv-dp) is computed for every $k$ before the control process begins. The advantage of this is that most of the computation is done offline. Once the control process starts, the only thing we have to do is one-step lookahead minimization. These methods are well-suited for settings where there are strict time constraints for the online computation of the control, and where there is no need for online replanning. 

2. _Online_ methods, where most of the computation is performed just after the current state $x_k$ becomes known, the values $\tilde J_{k+1}(x_{k+1})$ are computed only at the relevant next states $x_{k+1}$ and are used to compute $u_k$ via \@ref(eq:apprinv-dp). These methods require the computation of control only for the $N$ states actually encountered in the control process. These methods are well-suited for online replanning. 

### Problem Approximation

The functions $\tilde J_{k+1}$ are obtained (by exact DP, or other methods) as the optimal or nearly optimal cost functions of a simplified version of the original problem. The problem is how to simplify the problem, which is more convenient for computation. There are three widely-used approaches to simplify the initial problem: 

1. _Simplifying the structure of the problem through enforced decomposition_.

2. _Simplifying the probabilistic structure of the problem_, such as replacing the stochastic problem with a deterministic one by _certainty equivalence_. To be more specific, the original stochastic system contains the disturbance term $w_k(x_k,u_k)$. To simplify the probabilistic structure of the problem, we could fix the disturbances at some "typical" values and transform the stochastic problem into a deterministic one. 

3. _Aggregation_, where the original problem is approximated with a new problem with fewer states, makes it easier to obtain the cost-to-go function. The state in this new problem is the "combination" of the states in the initial problem. It is worth noting that the discretization of continuous state space and action space could be viewed as a kind of aggregation. 

### Parametric cost approximation

For discrete problems, it is natural to consider using the tabular method to represent the $\tilde J_k$ functions. However, if the number of the state space is large this method's memory cost will be overwhelming. On the other hand, for tabular representation, it is not convenient to optimize the function, while we can only update the value of _one_ state at a time, but in many circumstances, there is a cluster of states that have similar attributes, which means their corresponding $\tilde J_k$ are also similar. 
It is inconvenient to update them one by one. In this part, we will discuss an alternative approach to represent $\tilde J_k$ function, whereby $\tilde J_k$ are chosen to be members of a parametric class of functions, with the parameters "optimized" or "trained" by using some algorithms. 

To be more specific, the $\tilde J_k$ functions could be described as $\tilde J_k (x_k,r_k)$ that for each $k$, depend on the current state $x_k$ and a vector $r_k=(r_{1,k}, ..., r_{m,k})$ of $m$ "tunable" scalar parameters, also called _weights_. By adjusting the weights, one can change the "shape" of $\tilde J_k$ so that it is a reasonably close approximation to the true cost-to-go function $J_k$. 
In order to train those weights, we can use some cost functions to measure the accuracy of the approximation. The most common cost function is _least squares_. Training the parameters $r_k$ using least squares as the cost function is sometimes referred to as _fitted value iteration_. 
Value iteration could be viewed as a special form of dynamic programming, where the parameter vectors $r_k$ are determined sequentially, starting from the end of the horizon and proceeding backward. The algorithm samples the state space for each stage $k$ and generates a large number of states $x_k^s$, $s=1,...,q$. It then determines sequentially the parameter vectors $r_k$ to obtain a good "least square fit" to the DP algorithm. 

\begin{equation}
\beta_k^s=\min_{u \in \mathbb U_k(x_k^s)} E \displaystyle \left\{g(x_k^s,u,w_k) + \tilde J_{k+1} (f_k(x_k^s,u,w_k),r_{k+1}) \right\} (\#eq:apprinv-fvi-1)
\end{equation}
\begin{equation}
r_k = \arg \min_r \sum_{s=1}^q (\tilde J_k(x_k^s,r) - \beta_k^s)^2 (\#eq:apprinv-fvi-2)
\end{equation}

The next question is how to choose the most suitable class of functions, which is called _approximation architecture_. It is obvious that approximation architecture can greatly affect the performance of the approximation and the difficulty of training. The most popular architecture is _neural networks_, which are widely used in _reinforcement learning_, but the optimization process is difficult and the optimality is not guaranteed. We will start with a simpler linear feature-based approximation architecture. 

#### Linear feature-based architecture -->
<!-- ### Online approximate optimization

Different from previous sections, in this section, we will discuss _online_ approaches for computing the one-step lookahead control $u_k$ just after the current state $x_k$ becomes known. Here, to compute $u_k$, the values $\tilde J_{k+1} (x_{k+1})$ need only be computed at the relevant next states $x_{k+1}$ (the ones that can occur following application of $u_k$). 

A particularly effective online approach is _rollout_. In rollout algorithm, $\tilde J_{k+1}(x_{k+1})$ is calculated by a _suboptimal policy_, or _base policy_. $\tilde J_{k+1}$ could be calculated either analytically or by Monte Carlo simulation. This part is interconnected with _model predictive control (MPC)_, which we will also discuss at the end of this section. 

#### Rollout algorithm

The essence of the rollout is policy improvement, which generates a better policy on top of the base policy. In the rollout algorithm, $\tilde J_{k+1}$ is the cost-to-go of some known suboptimal policy $\pi = \{\mu_0,...,\mu_{N-1}\}$, referred to as _base policy_.
The policy $\bar \pi=\{\bar\mu_0,...,\bar\mu_{N-1}\}$ thus obtained is called the _rollout policy_ based on $\pi$. In short, _the rollout_ policy is the one-step lookahead policy, with the optimal cost-to-go approximated by the cost-to-go of the base policy_. 

**Definition 3.1 (One-step Rollout Algorithm)** We can get an improved policy from the base policy $\pi$

\begin{equation}
\bar\mu_k(x_k) = \arg\min_{u_k\in\mathbb U_k(x_k)} E\left\{g_k(x_k,u_k,w_k)+\tilde J_{k+1}(f_k(x_k,u_k,w_k))\right\} (\#eq:apprinv-rollout)
\end{equation}

where $\tilde J_{k+1}$ is the corresponding cost-to-go function of the base policy $\pi$. If we use $H_{k+1}$ to represent the cost-to-go function of the base policy $\pi$, the rollout algorithm will be: 

\begin{equation}
\bar\mu_k(x_k) = \arg\min_{u_k\in\mathbb U_k(x_k)} E\left\{g_k(x_k,u_k,w_k)+H_{k+1}(f_k(x_k,u_k,w_k))\right\} (\#eq:apprinv-rollout-2)
\end{equation}

In the control system, after the current state $x_k$ is revealed, we calculate the cost-to-go function $H_{k+1}$ of the known base policy $\pi$ and conduct one-step lookahead minimization to find $\bar\mu_k(x_k)$ and feed it into the system immediately. 

Note that it is also possible to define the rollout policy that makes use of multistep lookahead. While such multistep lookahead involves much more online computation, it will likely yield better performance than its one-step counterpart. In what follows, we concentrate on rollout policy with one-step lookahead. 

::: {.theorembox}
::: {.theorem #cost-improvement name="Cost improvement property of rollout algorithm"}
It is possible to show that the rollout policy's performance is no worse than the one of the base policy, while some special conditions must hold to guarantee this cost improvement property. Here we introduce the _sequential improvement_ condition. We say that the base policy has sequential improvement property if, for all $x_k$ and $k$, we have

\begin{equation}
\min_{u_k \in \mathbb U_k(x_k)} \left\{g_k(x_k,u_k)+H_{k+1}(f_k(x_k,u_k))\right\} \leq H_k(x_k)
\end{equation}

where $H_k(x_k)$ denotes the cost of the base policy starting from $x_k$. Here we use deterministic problems to make our proof concise. Sometimes people also use the _Q factor_ mentioned below: 

\begin{equation}
\tilde Q_k(x_k,u_k) = g_k(x_k,u_k)+H_{k+1}(f_k(x_k,u_k))
\end{equation}

so now the sequential improvement property could also be written as: 

\begin{equation}
\min_{u_k \in \mathbb U_k(x_k)} \tilde Q_k(x_k,u_k) \leq H_k(x_k)
\end{equation}

We will now show that the rollout algorithm obtained with a base policy with sequential improvement property yields no worse cost than the base policy. In particular, consider the rollout policy $\tilde \pi = \{\tilde \mu_0, ..., \tilde \mu_{N-1}\}$, and let $J_{k, \tilde \pi} (x_k)$ denote the cost obtained with $\tilde \pi$ starting from $x_k$. We claim that 

\begin{equation}
J_{k,\tilde \pi} (x_k) \leq H_k(x_k), for \ all \ x_k \ and \ k (\#eq:apprinv-cost-improvement)
\end{equation}
:::
:::
::: {.proofbox}
::: {.proof}
We prove this inequality by induction. Clearly it holds for $k=N$, since $J_{N,\tilde \pi} = H_N = g_N$. Assume it holds for index $k+1$. We have: 

\begin{align}
\tilde J_{k, \tilde \pi}(x_k) &= g_k(x_k, \tilde \mu_k(x_k)) + J_{k+1,\tilde \pi}(f_k(x_k, \tilde \mu_k(x_k))) \\
&\leq g_k(x_k, \tilde \mu_k(x_k)) + H_{k+1}(f_k(x_k, \tilde \mu_k(x_k))) \\ 
&= \min_{u_k \in \mathbb U_k(x_k)} \left[g_k(x_k,u_k) + H_{k+1} (f_k(x_k,u_k))\right] \\ 
&= \min_{u_k \in \mathbb U_k(x_k)} \tilde Q_k(x_k,u_k) \\
&\leq H_k(x_k)
\end{align}

where: 

a. The first equality is the DP equation for the rollout policy $\tilde \pi$. 

b. The first inequality holds by the induction hypothesis. 

c. The second equality holds by the definition of the rollout algorithm. 

d. The second inequality holds by the sequential improvement property. 

This completes the induction proof of the cost improvement property \@ref(eq:apprinv-cost-improvement). 

:::
:::

**Computational issues in rollout algorithms**. In the rollout algorithm, the cost-to-go function $H_{k+1}$ of the base policy is required to be computed online at all possible next states $f_k(x_k,u_k,w_k)$. However, the real-time constraint will be a critical problem, for the corresponding cost-to-go function of a given base policy is not easy to calculate in real-time. 
In most cases, we will use the approximate version of the cost-to-go $\tilde H_{k+1}$ to simplify the calculation. So the rollout algorithm will be: 

\begin{equation}
\bar\mu_k(x_k) = \arg\min_{u_k\in\mathbb U_k(x_k)} E\left\{g_k(x_k,u_k,w_k)+\tilde H_{k+1}(f_k(x_k,u_k,w_k))\right\} (\#eq:apprinv-rollout-3)
\end{equation}

There are two variants to handle the computational difficulties, deterministic case, and stochastic case. 

1. _Deterministic case_. If the problem is deterministic, the calculation is greatly simplified. 

2. _Stochastic case_. In these cases, the $\tilde H_{k+1}$ are evaluated online by Monte Carlo simulation for all $u_k \in \mathbb U_k(x_k)$. 

**Truncated rollout algorithm with multistep lookahead and terminal cost approximation**. We may incorporate multistep lookahead into the rollout framework. Let us start with a two-step lookahead for deterministic problems. Suppose that after $k$ steps we have reached state $x_k$. We then consider the set of all two-step-ahead states $x_{k+2}$, run the base policy starting from each of them, and compute the two-stage cost to get from $x_k$ to $x_{k+2}$, plus the cost of the base policy from $x_{k+2}$. We select the state, say $\tilde x_{k+2}$, that is associated with minimum cost, compute the controls $\tilde u_k$ and $\tilde u_{k+1}$ that lead from $x_k$ to $\tilde x_{k+2}$, and choose $\tilde u_k$ as the next rollout control and $x_{k+1}=f_k(x_k,\tilde u_k)$ as the next state. 

<div class="figure" style="text-align: center">
<img src="images/drawme.png" alt="Illustration of truncated rollout with two-step lookahead" width="60%" />
<p class="caption">(\#fig:multi-step-rollout)Illustration of truncated rollout with two-step lookahead</p>
</div>

The extension of the algorithm to lookahead of more than two steps is straightforward: instead of the two-step-ahead states $x_{k+2}$ we run the base policy starting from all the possible $l$-step ahead states $x_{k+l}$, etc. 

An important variation for problems with a large number of stages is _truncated rollout with terminal cost approximation_. Here the rollout trajectories are obtained by running the base policy from the leaf nodes of the lookahead tree, and they are truncated after a given number of steps, while a terminal cost approximation is added to the policy cost to compensate for the resulting error. One possibility that works well for many problems is to simply set the terminal cost approximation to zero. Alternatively, the terminal cost function approximation may be obtained by problem approximation or by using some sophisticated offline training process that may involve an approximation architecture such as a neural network. 

#### Model predictive control (MPC)

In this section, we will discuss a popular control algorithm called _model predictive control (MPC)_. We will start by considering the case where the objective is to keep the state close to the origin (or more generally some point of interest, called the _set point_, or _fixed point_); this is called the _regulation problem_. Similar approaches have been developed for the problem of maintaining the state of a non-stationary system along a given state trajectory, and also, with appropriate modifications, to control problems involving disturbances. In particular, in some cases, the trajectory is treated like a sequence of set points, and the subsequently described algorithm is applied repeatedly. 

We will consider a deterministic system

\begin{equation}
x_{k+1} = f_k(x_k,u_k)
\end{equation}

whose state $x_k$ and control $u_k$ are vectors that consist of a finite number of scalar components. The cost per stage is assumed nonnegative

\begin{equation}
g_k(x_k,u_k) \geq 0, for \ all \ (x_k,u_k)
\end{equation}

(e.g., a quadratic cost). We impose state and control constraints

\begin{equation}
x_k \in \mathbb X_k, u_k \in \mathbb U_k(x_k), k = 0,1,...
\end{equation}

We also assume that the system can be kept at the origin at zero cost, i.e., 

\begin{equation}
f_k(0,\bar u_k)=0,g_k(0,\bar u_k)=0 
\end{equation}

for some control $\bar u_k \in \mathbb U_k(0)$. This is a characteristic that all fixed points possess. 

For a given initial state $x_0 \in \mathbb X_0$, we want to obtain a sequence $\{u_0,u_1,...\}$ such that the states and controls of the system satisfy the state and control constraints with a small total cost. 

**The MPC algorithm**. Let us describe the MPC algorithm for the deterministic problem just described. At the current state $x_k$: 

1. MPC solves an $l$-step lookahead version of the problem, which requires that $x_{k+l}=0$. 

2. If $\{\tilde u_k, ..., \tilde u_{k+l-1}\}$ is the optimal control sequence of this problem, MPC applies $\tilde u_k$ and discards the other controls $\tilde u_{k+1}, ..., \tilde u_{k+l-1}$. 

3. At the next stage, MPC repeats this process, once the next state $x_{k+1}$ is revealed. 

In some literature, this MPC algorithm is also called _Receding Horizon Control_ algorithm, or RHC for short. One obvious drawback of this method is the online computation time limit. The MPC algorithm needs to solve an optimization problem online, which is time-consuming and does not guarantee a solution. 

To make the connection between MPC and rollout, we first recap the case of the truncated rollout algorithm. In a truncated rollout algorithm with multistep lookahead and terminal cost approximation, 

<!-- \begin{equation}
\min_{u_k \in \mathbb U_k(x_k), ..., u_{k+l} \in \mathbb U_{k+l}(x_{k+l})} \left\{\sum_{i=k}^{k+l} g_i(x_i, u_i) + \tilde H_{k+l+1} (x_{k+l+1}) \right\}
\end{equation} -->
<!-- \begin{equation}
\min_{u_k \in \mathbb U_k(x_k), ..., u_{k+l} \in \mathbb U_{k+l}(x_{k+l})} \left\{\sum_{i=k}^{k+l} g_i(x_i, u_i) + \sum_{i=k+l+1}^{k+l+m} g_i(x_i,\mu_i(x_i)) + \tilde J (x_{k+l+m+1}) \right\}
\end{equation}

such that

\begin{equation}
x_{i+1} = f_i(x_i,u_i)
\end{equation} -->
<!-- The control $u_k$ will be used as the control at step $k$ (online current step). All the $x_i$ are admissible states. Here $\tilde J$ means the terminal cost approximation, which can be obtained through offline computation or sometimes be set to zero. $l$ means the number of lookahead steps, and $m$ means the number of steps that the base policy runs to evaluate the cost-to-go function $H_{k+l+1}$. Let us discuss a special case, where the $\tilde J$ is set to zero while $m$ is also zero. So now the rollout algorithm becomes: 

\begin{equation}
\min_{u_k \in \mathbb U_k(x_k), ..., u_{k+l} \in \mathbb U_{k+l}(x_{k+l})} \sum_{i=k}^{k+l} g_i(x_i, u_i)
\end{equation} -->
<!-- while $u_k$ still be used as the current online control, and all other optimized controls are discarded. We can see that now it is _almost_ the case of model predictive control, without the terminal state constraint (in this case the terminal state constraint is $x_{k+l+1}=0$). This constraint is also called _recursive feasibility_, for it guarantees the optimization will not suddenly encounter a situation where the solver returns "infeasible". -->
</div>
</div>
<div id="policy-gradient" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Policy Gradient<a href="approximatedp.html#policy-gradient" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- ## Approximation in policy space

A major alternative to approximation in value space is _approximation in policy space_, whereby we select the policy from a suitably restricted class of policies, usually a parametric class of some form. In particular, we can introduce a parametric family of policies

\begin{equation}
\mu_k(x_k,r_k), k=0,...,N-1
\end{equation}

where $r_k$ is a parameter, such as a family represented by a neural network, and then estimate the parameters $r_k$ using some type of optimization. 

An important advantage of approximation in policy space is that the computation of controls during the online operation of the system is often much easier compared with the lookahead minimization \@ref(eq:apprinv-dp). In this section, we will present two distinct approaches for computing $r$: _training by cost optimization_ and _training by using an expert_. 

### Training by using an expert

This approach is pretty similar to _supervised learning_ in machine learning. We $r_k$ by "training" on a large number of sample state-control pairs $(x_k^s, u_k^s), s=1, ... ,q$, such that for each $s$, $u_k^s$ is a "good" control at state $x_k^s$. This can be done for example by solving for each $k$ the least squares problem

\begin{equation}
\min_{r_k} \sum_{s=1}^q \left\Vert{u_k^s - \tilde \mu_k(x_k^s,r_k)}\right\Vert^2
\end{equation}

(possibly with added regularization). In particular, we may determine $u_k^s$ by a human or a software "expert" that can choose "near-optimal" controls at the given states $x_k^s$, so $\tilde{\mu}_k$ is trained to match the behavior of the expert. Of course, in the expert training approach, we cannot expect to obtain a controller that performs better than the expert with which it is trained. 

The "near-optimal" controls of sampled states $x_k^s, s = 1, ...,q$ could also be calculated from one-step lookahead minimization with a suitable approximation $\tilde{J}_{k+1}$. 

\begin{equation}
u_k^s = \arg \min_{u_k \in \mathbb{U}_k (x_k)} \displaystyle \mathbb{E} \displaystyle \left\{g_k(x_k^s,u_k,w_k) + \tilde J_{k+1} (f_k(x_k^s,u_k,w_k) ) \right\}
\end{equation}

### Training by cost optimization

POLICY GRADIENT

## Extension

It is possible for a suboptimal control scheme to employ both types of approximation: in policy space and in value space, with a distinct architecture for each case. This is known as the simultaneous use of a "policy network" (or "actor network") and a "value network" (or "critic network"), each with its own set of parameters. Simultaneous approximation in policy space and value space through the use of deep neural networks are central in AlphaGo and AlphaZero, DeepMind's Go and chess playing programs.  -->

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bemporad02automatica-explicit" class="csl-entry">
Bemporad, Alberto, Manfred Morari, Vivek Dua, and Efstratios N Pistikopoulos. 2002. <span>“The Explicit Linear Quadratic Regulator for Constrained Systems.”</span> <em>Automatica</em> 38 (1): 3–20.
</div>
<div id="ref-bertsekas72tac-infinite" class="csl-entry">
Bertsekas, Dimitri. 1972. <span>“Infinite Time Reachability of State-Space Regions by Using Feedback Control.”</span> <em>IEEE Transactions on Automatic Control</em> 17 (5): 604–13.
</div>
<div id="ref-borrelli17book-mpc" class="csl-entry">
Borrelli, Francesco, Alberto Bemporad, and Manfred Morari. 2017. <em>Predictive Control for Linear and Hybrid Systems</em>. Cambridge University Press.
</div>
<div id="ref-gilbert91tac-linear" class="csl-entry">
Gilbert, Elmer G, and K Tin Tan. 1991. <span>“Linear Systems with State and Control Constraints: The Theory and Application of Maximal Output Admissible Sets.”</span> <em>IEEE Transactions on Automatic Control</em> 36 (9): 1008–20.
</div>
<div id="ref-kelly17siam-trajopt" class="csl-entry">
Kelly, Matthew. 2017. <span>“An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation.”</span> <em>SIAM Review</em> 59 (4): 849–904.
</div>
<div id="ref-kolmanovsky98mpe-theory" class="csl-entry">
Kolmanovsky, Ilya, Elmer G Gilbert, et al. 1998. <span>“Theory and Computation of Disturbance Invariant Sets for Discrete-Time Linear Systems.”</span> <em>Mathematical Problems in Engineering</em> 4: 317–67.
</div>
<div id="ref-levine17neurips-shallow" class="csl-entry">
Levine, Nir, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. 2017. <span>“Shallow Updates for Deep Reinforcement Learning.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-mnih15nature-dqn" class="csl-entry">
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. <span>“Human-Level Control Through Deep Reinforcement Learning.”</span> <em>Nature</em> 518 (7540): 529–33.
</div>
<div id="ref-nocedal99book-numerical" class="csl-entry">
Nocedal, Jorge, and Stephen J Wright. 1999. <em>Numerical Optimization</em>. Springer.
</div>
<div id="ref-yang23arxiv-value" class="csl-entry">
Yang, Alan, and Stephen Boyd. 2023. <span>“Value-Gradient Iteration with Quadratic Approximate Value Functions.”</span> <em>arXiv Preprint arXiv:2307.07086</em>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exactdp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="stability.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/03-approximate-dp.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
