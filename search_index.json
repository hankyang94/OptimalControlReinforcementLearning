[["index.html", "Optimal Control and Estimation Preface", " Optimal Control and Estimation Heng Yang 2025-03-08 Preface This is the textbook for Harvard ES/AM 158: Introduction to Optimal Control and Estimation. Information about the offerings of the class is listed below. 2023 Fall Time: Mon/Wed 2:15 - 3:30pm Location: Science and Engineering Complex, 1.413 Instructor: Heng Yang Teaching Fellow: Weiyu Li Syllabus "],["formulation.html", "Chapter 1 The Optimal Control Formulation 1.1 The Basic Problem 1.2 Dynamic Programming and Principle of Optimality 1.3 Infinite-horizon Formulation", " Chapter 1 The Optimal Control Formulation 1.1 The Basic Problem Consider a discrete-time dynamical system \\[\\begin{equation} x_{k+1} = f_k (x_k, u_k, w_k), \\quad k =0,1,\\dots,N-1 \\tag{1.1} \\end{equation}\\] where \\(x_k \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) is the state of the system, \\(u_k \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) is the control we wish to design, \\(w_k \\in \\mathbb{W} \\subseteq \\mathbb{R}^p\\) a random disturbance or noise (e.g., due to unmodelled dynamics) which is described by a probability distribution \\(P_k(\\cdot \\mid x_k, u_k)\\) that may depend on \\(x_k\\) and \\(u_k\\) but not on prior disturbances \\(w_0,\\dots,w_{k-1}\\), \\(k\\) indexes the discrete time, \\(N\\) denotes the horizon, \\(f_k\\) models the transition function of the system (typically \\(f_k \\equiv f\\) is time-invariant, especially for robotics systems; we use \\(f_k\\) here to keep full generality). Remark (Deterministic v.s. Stochastic). When \\(w_k \\equiv 0\\) for all \\(k\\), we say the system (1.1) is deterministic; otherwise we say the system is stochastic. In the following we will deal with the stochastic case, but most of the methodology should carry over to the deterministic setup. We consider the class of controllers (also called policies) that consist of a sequence of functions \\[ \\pi = \\{ \\mu_0,\\dots,\\mu_{N-1} \\}, \\] where \\(\\mu_k (x_k) \\in \\mathbb{U}\\) for all \\(x_k\\), i.e., \\(\\mu_k\\) is a feedback controller that maps the state to an admissible control. Given an initial state \\(x_0\\) and an admissible policy \\(\\pi\\), the state trajectory of the system is a sequence of random variables that evolve according to \\[\\begin{equation} x_{k+1} = f_k(x_k,\\mu_k(x_k),w_k), \\quad k=0,\\dots,N-1 \\tag{1.2} \\end{equation}\\] where the randomness comes from the disturbance \\(w_k\\). We assume the state-control trajectory \\(\\{u_k\\}_{k=0}^{N-1}\\) and \\(\\{x_k \\}_{k=0}^{N}\\) induce an additive cost \\[\\begin{equation} g_N(x_N) + \\sum_{k=0}^{N-1} g_k(x_k,u_k) \\tag{1.3} \\end{equation}\\] where \\(g_k,k=0,\\dots,N\\) are some user-designed functions. With (1.2) and (1.3), for any admissible policy \\(\\pi\\), we denote its induced expected cost with initial state \\(x_0\\) as \\[\\begin{equation} J_\\pi (x_0) = \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=0}^{N-1} g_k (x_k, \\mu_k(x_k)) \\right\\}, \\tag{1.4} \\end{equation}\\] where the expectation is taken over the randomness of \\(w_k\\). Definition 1.1 (Discrete-time, Finite-horizon Optimal Control) Find the best admissible controller that minimizes the expected cost in (1.4) \\[\\begin{equation} \\pi^\\star \\in \\arg\\min_{\\pi \\in \\Pi} J_\\pi(x_0), \\end{equation}\\] where \\(\\Pi\\) is the set of all admissible controllers. The cost attained by the optimal controller, i.e., \\(J^\\star = J_{\\pi^\\star}(x_0)\\) is called the optimal cost-to-go, or the optimal value function. Remark (Open-loop v.s. Closed-loop). An important feature of the basic problem in Definition 1.1 is that the problem seeks feedback policies, instead of numerical values of the controls, i.e., \\(u_k = \\mu_k(x_k)\\) is in general a function of the state \\(x_k\\). In other words, the controls are executed sequentially, one at a time after observing the state at each time. This is called closed-loop control, and is in general better than open-loop control \\[ \\min_{u_0,\\dots,u_{N-1}} \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=0}^{N-1} g_k (x_k, u_k) \\right\\} \\] where all the controls are planned at \\(k=0\\). Intuitively, a closed-loop policy is able to utilize the extra information received at each timestep (i.e., it observes \\(x_{k+1}\\) and hence also observes the disturbance \\(w_k\\)) to obtain a lower cost than an open-loop controller. Example 1.2.1 in (Bertsekas 2012) gives a concrete application where a closed-loop policy attains a lower cost than an open-loop policy. In deterministic control (i.e., when \\(w_k \\equiv 0,\\forall k\\)), however, a closed-loop policy has no advantage over an open-loop controller. This is obvious because at \\(k=0\\), even the open-loop controller predicts perfectly the consequences of all its actions and there is no extra information to be observed at later time steps. In fact, even in stochastic problems, a closed-loop policy may not be advantageous, see Exercise 1.27 in (Bertsekas 2012). 1.2 Dynamic Programming and Principle of Optimality We now introduce a general and powerful algorithm, namely dynamic programming (DP), for solving the optimal control problem 1.1. The DP algorithm builds upon a quite simple intuition called the Bellman principle of optimality. Theorem 1.1 (Bellman Principle of Optimality) Let \\(\\pi^\\star = \\{ \\mu_0^\\star,\\mu_1^\\star,\\dots,\\mu_{N-1}^\\star \\}\\) be an optimal policy for the optimal control problem 1.1. Assume that when using \\(\\pi^\\star\\), a given state \\(x_i\\) occurs at timestep \\(i\\) with positive probability (i.e., \\(x_i\\) is reachable at time \\(i\\)). Now consider the following subproblem where we are at \\(x_i\\) at time \\(i\\) and wish to minimize the cost-to-go from time \\(i\\) to time \\(N\\) \\[ \\min_{\\mu_i,\\dots,\\mu_{N-1}} \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=i}^{N-1} g_k (x_k, \\mu_k(x_k)) \\right\\}. \\] Then the truncated policy \\(\\{\\mu^\\star_i,\\mu^\\star_{i+1},\\dots, \\mu^\\star_{N-1}\\}\\) must be optimal for the subproblem. Theorem 1.1 can be proved intuitively by contradiction: if the truncated policy \\(\\{\\mu^\\star_i,\\mu^\\star_{i+1},\\dots, \\mu^\\star_{N-1}\\}\\) is not optimal for the subproblem, say there exists a different policy \\(\\{\\mu_i&#39;,\\mu_{i+1}&#39;,\\dots, \\mu_{N-1}&#39;\\}\\) that attains a lower cost for the subproblem starting at \\(x_i\\) at time \\(i\\). Then the combined policy \\(\\{\\mu_0^\\star,\\dots,\\mu^\\star_{i-1},\\mu_i&#39;,\\dots,\\mu_{N-1}&#39;\\}\\) must attain a lower cost for the original optimal control problem 1.1 due to the additive cost structure, contradicting the optimality of \\(\\pi^\\star\\). The Bellman principle of optimality is more than just a principle, it is also an algorithm. It suggests that, to build an optimal policy, one can start by solving the last-stage subproblem to obtain \\(\\{\\mu^\\star_{N-1} \\}\\), and then proceed to solve the subproblem containing the last two stages to obtain \\(\\{ \\mu^\\star_{N-2},\\mu^\\star_{N-1} \\}\\). The recursion continues until optimal policies at all stages are computed. The following theorem formalizes this concept. Theorem 1.2 (Dynamic Programming) The optimal value function \\(J^\\star(x_0)\\) of the optimal control problem 1.1 (starting from any given initial condition \\(x_0\\)) is equal to \\(J_0(x_0)\\), which can be computed backwards and recursively as \\[\\begin{align} J_N(x_N) &amp;= g_N(x_N) \\\\ J_k(x_k) &amp;= \\min_{u_k \\in \\mathbb{U}} \\displaystyle \\mathbb{E}_{w_k \\sim P_k(\\cdot \\mid x_k, u_k)} \\displaystyle \\left\\{ g_k(x_k,u_k) + J_{k+1}(f_k(x_k,u_k,w_k) ) \\right\\}, \\ k=N-1,\\dots,1,0. \\tag{1.5} \\end{align}\\] Moreover, if \\(u_k^\\star = \\mu_k^\\star(x_k)\\) is a minimizer of (1.5) for every \\(x_k\\), then the policy \\(\\pi^\\star = \\{\\mu_0^\\star,\\dots,\\mu_{N-1}^\\star \\}\\) is optimal. Proof. For any admissible policy \\(\\pi = \\{ \\mu_0,\\dots,\\mu_{N-1} \\}\\), denote \\(\\pi^k = \\{ \\mu_k,\\dots,\\mu_{N-1} \\}\\) the last-\\((N-k)\\)-stage truncated policy. Consider the subproblem consisting of the last \\(N-k\\) stages starting from \\(x_k\\), and let \\(J^\\star_k(x_k)\\) be its optimal cost-to-go. Mathematically, this is \\[\\begin{equation} J^\\star_{k}(x_k) = \\min_{\\pi^k} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k}^{N-1} g_i (x_i,\\mu_i(x_i)) \\right\\}, \\quad k=0,1,\\dots,N-1. \\tag{1.6} \\end{equation}\\] We define \\(J^\\star_N(x_N) = g(x_N)\\) for \\(k=N\\). Our goal is to prove the \\(J_k(x_k)\\) computed by dynamic programming from (1.5) is equal to \\(J^\\star_k (x_k)\\) for all \\(k=0,\\dots,N\\). We will prove this by induction. Firstly, we already have \\(J^\\star_N(x_N) = J_N(x_N) = g(x_N)\\), so \\(k=N\\) holds automatically. Now we assume \\(J^\\star_{k+1}(x_{k+1}) = J_{k+1}(x_{k+1})\\) for all \\(x_{k+1}\\), and we wish to induce \\(J^\\star_{k}(x_{k}) = J_{k}(x_{k})\\). To show this, we write \\[\\begin{align} \\hspace{-16mm} J^\\star_{k}(x_k) &amp;= \\min_{\\pi^k} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k}^{N-1} g_i (x_i,\\mu_i(x_i)) \\right\\} \\tag{1.7}\\\\ &amp;= \\min_{\\mu_k,\\pi^{k+1}} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_k(x_k,\\mu_k(x_k)) + g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\} \\tag{1.8}\\\\ &amp;= \\min_{\\mu_k} \\left[ \\min_{\\pi^{k+1}} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_k(x_k,\\mu_k(x_k)) + g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\}\\right] \\tag{1.9}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + \\min_{\\pi^{k+1}} \\left[ \\mathbb{E}_{w_{k+1},\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\} \\right] \\right\\} \\tag{1.10}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J^\\star_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.11}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.12}\\\\ &amp;= \\min_{u_k \\in \\mathbb{U}} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.13}\\\\ &amp;= J_k(x_k), \\tag{1.14} \\end{align}\\] where (1.7) follows from definition (1.6); (1.8) expands \\(\\pi^k = \\{ \\mu_k, \\pi^{k+1}\\}\\) and \\(\\sum_{i=k}^{N-1} g_i = g_k + \\sum_{i=k+1}^{N-1}\\); (1.9) writes the joint minimization over \\(\\mu_k\\) and \\(\\pi^{k+1}\\) as equivalently first minimizing over \\(\\pi^{k+1}\\) and then minimizing over \\(\\mu_k\\); (1.10) is the key step and holds because \\(g_k\\) and \\(w_k\\) depend only on \\(\\mu_k\\) but not on \\(\\pi^{k+1}\\); (1.11) follows again from definition (1.6) with \\(k\\) replaced by \\(k+1\\); (1.12) results from the induction assumption; (1.13) clearly holds because any \\(\\mu_k(x_k)\\) belongs to \\(\\mathbb{U}\\) and any element in \\(\\mathbb{U}\\) can be chosen by a feedback controller \\(\\mu_k\\); and lastly (1.14) follows from the dynamic programming algorithm (1.5). By induction, this shows that \\(J^\\star_k(x_k) = J_k(x_k)\\) for all \\(k=0,\\dots,N\\). The careful reader, especially from a robotics background, may soon become disappointed when seeing the DP algorithm (1.5) because it is rather conceptual than practical. To see this, we only need to run DP for \\(k=N-1\\): \\[\\begin{equation} J_{N-1}(x_{N-1}) = \\min_{u_{N-1} \\in \\mathbb{U}} \\mathbb{E}_{w_{N-1}} \\left\\{ g_{N-1}(x_{N-1},u_{N-1}) + J_N(f_{N-1}(x_{N-1},u_{N-1},w_{N-1})) \\right\\}. \\tag{1.15} \\end{equation}\\] Two challenges immediately show up: How to perform the minimization over \\(u_{N-1}\\) when \\(\\mathbb{U}\\) is a continuous constraint set? Even if we assume \\(g_{N-1}\\) is convex1 in \\(u_{N-1}\\), \\(J_N\\) is convex in \\(x_{N}\\), and the dynamics \\(f_{N-1}\\) is also convex in \\(u_{N-1}\\) (so that the optimization (1.15) is convex), we may be able to solve the minimization numerically for each \\(x_{N-1}\\) using a convex optimization solver, but rarely will we be able to find an analytical policy \\(\\mu_{N-1}^\\star\\) such that \\(u_{N-1}^\\star = \\mu_{N-1}^\\star (x_{N-1})\\) for every \\(x_{N-1}\\) (i.e., the optimal policy \\(\\mu_{N-1}^\\star\\) is implict but not explict). Suppose we can find an anlytical optimal policy \\(\\mu_{N-1}^\\star\\), say \\(\\mu_{N-1}^\\star = K x_{N-1}\\) a linear policy, how will plugging \\(\\mu_{N-1}^\\star\\) into (1.15) affect the complexity of \\(J_{N-1}(x_{N-1})\\)? One can see that even if \\(\\mu_{N-1}^\\star\\) is linear in \\(x_{N-1}\\), \\(J_{N-1}\\) may be highly nonlinear in \\(x_{N-1}\\) due to the composition with \\(g_{N-1}\\), \\(f_{N-1}\\) and \\(J_N\\). If \\(J_{N-1}(x_{N-1})\\) becomes too complex, then clearly it becomes more challenging to perform (1.15) for the next step \\(k=N-2\\). Due to these challenges, only in a very limited amount of cases will we be able to perform exact dynamic programming. For example, when the state space \\(\\mathbb{X}\\) and control space \\(\\mathbb{U}\\) are discrete, we can design efficient algorithms for exact DP. For another example, when the dynamics \\(f_k\\) is linear and the cost \\(g_k\\) is quadratic, we will also be able to compute \\(J_k(x_k)\\) in closed form (though this sounds a bit surprising!). We will study these problems in more details in Chapter 2. For general optimal control problems with continuous state space and control space (and most problems we care about in robotics), unfortunately, we will have to resort to approximate dynamic programming, basically variations of the DP algorithm (1.5) where approximate value functions \\(J_k(x_k)\\) and/or control policies \\(\\mu_k(x_k)\\) are used (e.g., with neural networks and machine learning).2 We will introduce several popular approximation schemes in Chapter 3. We will see that, although exact DP is not possible anymore, the Bellman principle of optimality still remains one of the most important guidelines for designing approximation algorithms. Efficient algorithms for approximate dynamic programming, preferrably with performance guarantees, still remain an active area of research. 1.3 Infinite-horizon Formulation So far we are focusing on problems with a finite horizon \\(N\\), what if the horizon \\(N\\) tends to infinity? In particular, consider the controller \\(\\pi\\) now contains an infinite sequence of functions \\[ \\pi = \\{ \\mu_0,\\dots \\} \\] and let us try to find the best policy that minimizes the cost-to-go starting from \\(x_0\\) subject to the same dynamics as in (1.1) (with \\(N\\) tends to infinity and \\(f_k \\equiv f\\)) \\[\\begin{equation} J_{\\pi}(x_0) = \\mathbb{E} \\left\\{ \\sum_{k=0}^{\\infty} g(x_k, \\mu_k(x_k)) \\right\\} \\tag{1.16}, \\end{equation}\\] where the expectation is taken over the (infinite number of) disturbances \\(\\{w_0,\\dots \\}\\). We can write (1.16) equivalently as \\[ J_{\\pi}(x_0) = \\lim_{N \\rightarrow \\infty} J_\\pi^N(x_0), \\] where, with a slight abuse of notation, \\(J_\\pi^N(x_0)\\) is (1.4) with \\(g_N(x_N)\\) set to zero. Now we invoke the dynamic programming algorithm in Theorem 1.2. We will first set \\(J_N(x_N) = g_N(x_N)=0\\), and then compute backwards in time \\[ J_k(x_k) = \\min_{u_k \\in \\mathbb{U}} \\mathbb{E}_{w_k} \\left\\{ g(x_k,u_k) + J_{k+1}(f(x_k,u_k,w_k)) \\right\\}, \\quad k=N-1,\\dots,0. \\] To make our presentation easier later, the above DP iterations are equivalent to \\[\\begin{align} J_0(x_0) &amp;= 0 \\\\ J_{k+1}(x_{k+1}) &amp;= \\min_{u_k \\in \\mathbb{U}} \\mathbb{E}_{w_k} \\left\\{ g(x_k,u_k) + J_k(f(x_k,u_k,w_k)) \\right\\}, \\quad k=0,\\dots,N, \\tag{1.17} \\end{align}\\] where I have done nothing but reversed the time indexing. Observe that when \\(N \\rightarrow \\infty\\), (1.17) performs the recursion an infinite number of times. We may want to conjecture three natural consequences of the infinite-horizon solution: The optimal infinite-horizon cost is the limit of the corresponding \\(N\\)-stage optimal cost as \\(N \\rightarrow \\infty\\), i.e., \\[ J^\\star(x) = \\lim_{N \\rightarrow \\infty} J_N(x_N), \\] where \\(J_N(x_N)\\) is computed from DP (1.17). Bacause \\(J^\\star\\) is the result of DP (1.17) when \\(N\\) tends to infinity, if the DP algorithm converges to \\(J^\\star\\), then \\(J^\\star\\) should satisfy \\[\\begin{equation} J^\\star(x) = \\min_{u \\in \\mathbb{U}} \\mathbb{E}_w \\left\\{ g(x,u) + J^\\star(f(x,u,w)) \\right\\}, \\quad \\forall x \\tag{1.18} \\end{equation}\\] Note that (1.18) is an equation that \\(J^\\star(x)\\) should satisfy for all \\(x\\). In fact, this is called the Bellman Optimality Equation. If \\(\\mu(x)\\) satisfies the Bellman equation (1.18), i.e., \\(u = \\mu(x)\\) minimizes the right-hand side of (1.18) for any \\(x\\), then the policy \\(\\pi = \\{\\mu,\\mu,\\dots \\}\\) should be optimal. This is saying, the optimal policy is time-invariant. In fact, all of our conjectures above are true, for most infinite-horizon problems. For example, in Chapter 2.2, we will investigate the Markov Decision Process (MDP) formulation, under which the above conjectures all hold. However, one should know that there also exist many infinite-horizon problems where our conjectures will fail, and there are many mathematical subtleties in rigorously proving the conjectures. The reader should see why it can be more convenient to study the infinite-horizon formulation: (i) the optimal cost-to-go is only a function of the state \\(x\\), but not a function of timestep \\(k\\); (ii) the optimal policy is time-invariant and easier to implement. Value Iteration. The Bellman optimality equation (1.18) also suggests a natural algorithm for computing \\(J^\\star(x)\\). We start with \\(J(x)\\) being all zero, and then iteratively update \\(J(x)\\) by performing the right-hand side of (1.18). This is the famous value iteration algorithm. We will study it in Chapter 2.2. As practitioners, we may simply execute the dynamic programming (value iteration) algoithm without carefully checking if our problem satisfies the assumptions. If the algorithm converges, oftentimes the problem indeed satisfies the assumptions. Otherwise, the algorithm may fail to converge, as we will see in Example 2.3. References ———. 2012. Dynamic Programming and Optimal Control: Volume i. Vol. 1. Athena scientific. You may want to read Appendix B if this is your first time seeing “convex” things.↩︎ Another possible solution is to discretize continuous states and controls. However, when the dimension of state and control is high, discretization becomes too expensive in terms of memory and computational complexity.↩︎ "],["exactdp.html", "Chapter 2 Exact Dynamic Programming 2.1 Linear Quadratic Regulator 2.2 Markov Decision Process", " Chapter 2 Exact Dynamic Programming In Chapter 1, we introduced the basic formulation of the finite-horizon and discrete-time optimal control problem, presented the Bellman principle of optimality, and derived the dynamic programming (DP) algorithm. We mentioned that, despite being a general-purpose algorithm, it can be difficult to implement DP exactly in practical applications. In this Chapter, we will introduce two problem setups where DP can in fact be implemented exactly. 2.1 Linear Quadratic Regulator Consider a linear discrete-time dynamical system \\[\\begin{equation} x_{k+1} = A_k x_k + B_k u_k + w_k, \\quad k=0,1,\\dots,N-1, \\tag{2.1} \\end{equation}\\] where \\(x_k \\in \\mathbb{R}^n\\) the state, \\(u_k \\in \\mathbb{R}^m\\) the control, \\(w_k \\in \\mathbb{R}^n\\) the independent, zero-mean disturbance with given probability distribution that does not depend on \\(x_k,u_k\\), and \\(A_k \\in \\mathbb{R}^{n \\times n}, B_k \\in \\mathbb{R}^{n \\times m}\\) are known matrices determining the transition dynamics. We want to solve the following optimal control problem \\[\\begin{equation} \\min_{\\mu_0,\\dots,\\mu_{N-1}} \\mathbb{E} \\left\\{ x_N^T Q_N x_N + \\sum_{k=0}^{N-1} \\left( x_k^T Q_k x_k + u_k^T R_k u_k \\right) \\right\\}, \\tag{2.2} \\end{equation}\\] where the expectation is taken over the randomness in \\(w_0,\\dots,w_{N-1}\\). In (2.2), \\(\\{Q_k \\}_{k=0}^N\\) are positive semidefinite matrices, and \\(\\{ R_k \\}_{k=0}^{N-1}\\) are positive definite matrices. The formulation (2.2) is typically known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to “regulate” the system around the origin \\(x=0\\). We will now show that the DP algorithm in Theorem 1.2 can be exactly implemented for LQR. The DP algorithm computes the optimal cost-to-go backwards in time. The terminal cost is \\[ J_N(x_N) = x_N^T Q_N x_N \\] by definition. The optimal cost-to-go at time \\(N-1\\) is equal to \\[\\begin{equation} \\begin{split} J_{N-1}(x_{N-1}) = \\min_{u_{N-1}} \\mathbb{E}_{w_{N-1}} \\{ x_{N-1}^T Q_{N-1} x_{N-1} + u_{N-1}^T R_{N-1} u_{N-1} + \\\\ \\Vert \\underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \\Vert^2_{Q_N} \\} \\end{split} \\tag{2.3} \\end{equation}\\] where \\(\\Vert v \\Vert_Q^2 = v^T Q v\\) for \\(Q \\succeq 0\\). Now observe that the objective in (2.3) is \\[\\begin{equation} \\begin{split} x_{N-1}^T Q_{N-1} x_{N-1} + u_{N-1}^T R_{N-1} u_{N-1} + \\Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \\Vert_{Q_N}^2 + \\\\ \\mathbb{E}_{w_{N-1}} \\left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^T Q_{N-1} w_{N-1} \\right] + \\\\ \\mathbb{E}_{w_{N-1}} \\left[ w_{N-1}^T Q_N w_{N-1} \\right] \\end{split} \\end{equation}\\] where the second line is zero due to \\(\\mathbb{E}(w_{N-1}) = 0\\) and the third line is a constant with respect to \\(u_{N-1}\\). Consequently, the optimal control \\(u_{N-1}^\\star\\) can be computed by setting the derivative of the objective with respect to \\(u_{N-1}\\) equal to zero \\[\\begin{equation} u_{N-1}^\\star = - \\left[ \\left( R_{N-1} + B_{N-1}^T Q_N B_{N-1} \\right)^{-1} B_{N-1}^T Q_N A_{N-1} \\right] x_{N-1}. \\tag{2.4} \\end{equation}\\] Plugging the optimal controller \\(u^\\star_{N-1}\\) back to the objective of (2.3) leads to \\[\\begin{equation} J_{N-1}(x_{N-1}) = x_{N-1}^T S_{N-1} x_{N-1} + \\mathbb{E} \\left[ w_{N-1}^T Q_N w_{N-1} \\right], \\tag{2.5} \\end{equation}\\] with \\[ S_{N-1} = Q_{N-1} + A_{N-1}^T \\left[ Q_N - Q_N B_{N-1} \\left( R_{N-1} + B_{N-1}^T Q_N B_{N-1} \\right)^{-1} B_{N-1}^T Q_N \\right] A_{N-1}. \\] We note that \\(S_{N-1}\\) is positive semidefinite (this is an exercise for you to convince yourself). Now we realize that something surprising and nice has happened. The optimal controller \\(u^{\\star}_{N-1}\\) in (2.4) is a linear feedback policy of the state \\(x_{N-1}\\), and The optimal cost-to-go \\(J_{N-1}(x_{N-1})\\) in (2.5) is quadratic in \\(x_{N-1}\\), just the same as \\(J_{N}(x_N)\\). This implies that, if we continue to compute the optimal cost-to-go at time \\(N-2\\), we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is, The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time. We summarize the solution for the LQR problem (2.2) as follows. Proposition 2.1 (Solution of Discrete-Time Finite-Horizon LQR) The optimal controller for the LQR problem (2.2) is a linear state-feedback policy \\[\\begin{equation} \\mu_k^\\star(x_k) = - K_k x_k, \\quad k=0,\\dots,N-1. \\tag{2.6} \\end{equation}\\] The gain matrix \\(K_k\\) can be computed as \\[ K_k = \\left( R_k + B_k^T S_{k+1} B_k \\right)^{-1} B_k^T S_{k+1} A_k, \\] where the matrix \\(S_k\\) satisfies the following backwards recursion \\[\\begin{equation} \\hspace{-6mm} \\begin{split} S_N &amp;= Q_N \\\\ S_k &amp;= Q_k + A_k^T \\left[ S_{k+1} - S_{k+1}B_k \\left( R_k + B_k^T S_{k+1} B_k \\right)^{-1} B_k^T S_{k+1} \\right] A_k, k=N-1,\\dots,0. \\end{split} \\tag{2.7} \\end{equation}\\] The optimal cost-to-go is given by \\[ J_0(x_0) = x_0^T S_0 x_0 + \\sum_{k=0}^{N-1} \\mathbb{E} \\left[ w_k^T S_{k+1} w_k\\right]. \\] The recursion (2.7) is called the discrete-time Riccati equation. Proposition 2.1 states that, to evaluate the optimal policy (2.6), one can first run the backwards Riccati equation (2.7) to compute all the positive definite matrices \\(S_k\\), and then compute the gain matrices \\(K_k\\). For systems of reasonable dimensions, evalutating the matrix inversion in (2.7) should be fairly efficient. 2.1.1 Infinite-Horizon LQR In many robotics applications, it is often more useful to study the infinite-horizon LQR problem \\[\\begin{align} \\min_{u_k} &amp; \\quad \\sum_{k=0}^{\\infty} \\left( x_k^T Q x_k + u_k^T R u_k \\right) \\tag{2.8} \\\\ \\text{subject to} &amp; \\quad x_{k+1} = A x_k + B u_k, \\quad k=0,\\dots,\\infty, \\tag{2.9} \\end{align}\\] where \\(Q \\succeq 0\\), \\(R \\succ 0\\), and \\(A,B\\) are constant matrices. The reason for studying the formulation (2.8) is twofold. First, for nonlinear systems, we often linearize the nonlinear dynamics around an (equilibrium) point we care about, leading to constant \\(A\\) and \\(B\\) matrices. Second, we care more about the asymptotic effect of our controller than its behavior in a fixed number of steps. We will soon see an example of this formulation for balancing a simple pendulum. The infinite-horizon formulation is essentially the finite-horizon formulation (2.2) with \\(N \\rightarrow \\infty\\). Based on our intuition in deriving the finite-horizon LQR solution, we may want to hypothesize that the optimal cost-to-go is a quadratic function \\[\\begin{equation} J_{k}(x_{k}) = x_{k}^T S x_{k}, k=0,\\dots,\\infty \\tag{2.10} \\end{equation}\\] for some positive definite matrix \\(S\\), and proceed to invoke the DP algorithm. Notice that we hypothesize the matrix \\(S\\) is in fact stationary, i.e., it does not change with respect to time. This hypothesis makes sense because the \\(A,B,Q,R\\) matrices are stationary in the formulation (2.8). Invoking the DP algorithm we have \\[\\begin{equation} x_k^T S x_k = J_k(x_k) = \\min_{u_k} \\left\\{ x_k^T Q x_k + u_k^T R u_k + \\Vert \\underbrace{A x_k + B u_k}_{x_{k+1}} \\Vert_S^2 \\right\\}. \\tag{2.11} \\end{equation}\\] The minimization over \\(u_k\\) in (2.11) can again be solved in closed-form by setting the gradient of the objective with respect to \\(u_k\\) to be zero \\[\\begin{equation} u_k^\\star = - \\underbrace{\\left[ \\left( R + B^T S B \\right)^{-1} B^T S A \\right]}_{K} x_k. \\tag{2.12} \\end{equation}\\] Plugging the optimal \\(u_k^\\star\\) back into (2.11), we see that the matrix \\(S\\) has to satisfy the following equation \\[\\begin{equation} S = Q + A^T \\left[ S - SB \\left( R + B^T S B \\right)^{-1} B^T S \\right] A. \\tag{2.13} \\end{equation}\\] Equation (2.13) is the famous algebraic Riccati equation. Let’s zoom out to see what we have done. We started with a hypothetical optimal cost-to-go (2.10) that is stationary, and invoked the DP algorithm in (2.11), which led us to the algebraic Riccati equation (2.13). Therefore, if there actually exists a solution to the algebraic Riccati equation (2.13), then the linear controller (2.12) is indeed optimal (by the optimality of DP)! So the question boils down to if the algebraic Riccati equation has a solution \\(S\\) that is positive definite? The following proposition gives an answer. Proposition 2.2 (Solution of Discrete-Time Infinite-Horizon LQR) Consider a linear system \\[ x_{k+1} = A x_k + B u_k, \\] with \\((A,B)\\) controllable (see Appendix C.2). Let \\(Q \\succeq 0\\) in (2.8) be such that \\(Q\\) can be written as \\(Q = C^T C\\) with \\((A,C)\\) observable. Then the optimal controller for the infinite-horizon LQR problem (2.8) is a stationary linear policy \\[ \\mu^\\star (x) = - K x, \\] with \\[ K = \\left( R + B^T S B \\right)^{-1} B^T S A. \\] The matrix \\(S\\) is the unique positive definite matrix that satisfies the algebraic Riccati equation \\[ S = Q + A^T \\left[ S - SB \\left( R + B^T S B \\right)^{-1} B^T S \\right] A. \\] Moreover, the closed-loop system \\[ x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k \\] is stable, i.e., the eigenvalues of the matrix \\(A - BK\\) are strictly within the unit circle (see Appendix C.1.2). A rigorous proof of Proposition 2.2 is available in Proposition 3.1.1 of (Bertsekas 2012). The proof basically studies the limit of the discrete-time Riccati equation (2.7) when \\(N \\rightarrow \\infty\\). Indeed, the algebraic Riccati equation (2.13) is the limit of the discrete-time Riccati equation (2.7) when \\(N \\rightarrow \\infty\\). The assumptions of \\((A,B)\\) being controllable and \\((A,C)\\) being observable can be relaxted to \\((A,B)\\) being stabilizable and \\((A,C)\\) being detectable (for definitions of stabilizability and detectability, see Appendix C). We have not discussed how to solve the algebraic Riccati equation (2.7). It is clear that (2.7) is not a linear system of equations in \\(S\\). In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see (Arnold and Laub 1984). Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab dlqr function computes the \\(K\\) and \\(S\\) matrices from \\(A,B,Q,R\\). Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum. Example 2.1 (Pendulum Stabilization by LQR) Consider the simple pendulum in Fig. 2.1 with dynamics \\[\\begin{equation} x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ -\\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix} \\tag{2.14} \\end{equation}\\] where \\(m\\) is the mass of the pendulum, \\(l\\) is the length of the pole, \\(g\\) is the gravitational constant, \\(b\\) is the damping ratio, and \\(u\\) is the torque applied to the pendulum. We are interested in applying the LQR controller to balance the pendulum in the upright position \\(x_d = [\\pi,0]^T\\) with a zero velocity. Figure 2.1: A Simple Pendulum. Let us first shift the dynamics so that “\\(0\\)” is the upright position. This can be done by defining a new variable \\(z = x - x_d = [\\theta - \\pi, \\dot{\\theta}]^T\\), which leads to \\[\\begin{equation} \\dot{z} = \\dot{x} = f(x,u) = f(z + x_d,u) = \\begin{bmatrix} z_2 \\\\ \\frac{1}{ml^2} \\left( u - b z_2 + mgl \\sin z_1 \\right) \\end{bmatrix} = f&#39;(z,u). \\tag{2.15} \\end{equation}\\] We then linearize the nonlinear dynamics \\(\\dot{z} = f&#39;(z,u)\\) at the point \\(z^\\star = 0, u^\\star = 0\\): \\[\\begin{align} \\dot{z} &amp; \\approx f&#39;(z^\\star,u^\\star) + \\left( \\frac{\\partial f&#39;}{\\partial z} \\right)_{z^\\star,u^\\star} (z - z^\\star) + \\left( \\frac{\\partial f&#39;}{\\partial u} \\right)_{z^\\star,u^\\star} (u - u^\\star) \\\\ &amp; = \\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} \\cos z_1 &amp; - \\frac{b}{ml^2} \\end{bmatrix}_{z^\\star, u^\\star} z + \\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix} u \\\\ &amp; = \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} &amp; - \\frac{b}{ml^2} \\end{bmatrix}}_{A_c} z + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix}}_{B_c} u. \\end{align}\\] Finally, we convert the continuous-time dynamics to discrete time with a fixed discretization \\(h\\) \\[ z_{k+1} = \\dot{z}_k \\cdot h + z_k = \\underbrace{(h \\cdot A_c + I )}_{A} z_k + \\underbrace{(h \\cdot B_c)}_{B} u_k. \\] We are now ready to implement the LQR controller. In the formulation (2.8), we choose \\(Q = I\\), \\(R = I\\), and solve the gain matrix \\(K\\) using the Matlab dlqr function. Fig. 2.2 shows the simulation result for \\(m=1,l=1,b=0.1\\), \\(g = 9.8\\), and \\(h = 0.01\\), with an initial condition \\(z^0 = [0.1,0.1]^T\\). We can see that the LQR controller successfully stabilizes the pendulum at \\(z^\\star\\), the upright position. You can play with the Matlab code here. Figure 2.2: LQR stabilization of a simple pendulum. 2.1.2 LQR with Constraints Let’s explore LQR with constraints in Exercise 9.2 2.2 Markov Decision Process In Section 2.1, we see that linear dynamics and quadratic costs leads to exact dynamic programming. We now introduce another setup where the number of states and controls is finite (as opposed to the LQR case where \\(x_k\\) and \\(u_k\\) live in continuous spaces). We will see that we can execute DP exactly in this setup as well. Optimal control in the case of finite states and controls is typically introduced in the framework of a Markov Decision Process (MDP, which is common in Reinforcement Learning). There are many variations of a MDP, and here we only focus on the discounted infinite-horizon MDP. For a more complete treatment of MDPs, I suggest checking out this course at Harvard. Formally, a discounted infinite-horizon MDP \\(\\mathcal{M} = (\\mathbb{X},\\mathbb{U},P,g,\\gamma,\\sigma)\\) is specified by a state space \\(\\mathbb{X}\\) that is finite with size \\(|\\mathbb{X}|\\) a control space \\(\\mathbb{U}\\) that is finite with size \\(|\\mathbb{U}|\\) a transition function \\(P: \\mathbb{X} \\times \\mathbb{U} \\rightarrow \\Delta(\\mathbb{X})\\), where \\(\\Delta(\\mathbb{X})\\) is the space of probability distributions over \\(\\mathbb{X}\\); specifically, \\(P(x&#39; \\mid x, u)\\) is the probability of transitioning into state \\(x&#39;\\) from state \\(x\\) using control \\(u\\). If the system is deterministic, then \\(P(x&#39; \\mid x, u)\\) is nonzero only for a single next state \\(x&#39;\\) a cost function \\(g: \\mathbb{X} \\times \\mathbb{U} \\rightarrow [0,1]\\); \\(g(x,u)\\) is the cost of taking the control \\(u\\) at state \\(x\\) a discount factor \\(\\gamma \\in [0,1)\\) an initial state distribution \\(\\sigma \\in \\Delta(\\mathbb{X})\\) that specifies how the initial state \\(x_0\\) is generated; in many cases we will assume \\(x_0\\) is fixed and \\(\\sigma\\) is a distribution supported only on \\(x_0\\). In an MDP, the system starts at some state \\(x_0 \\sim \\sigma\\). At each step \\(k = 0,1,2,\\dots\\), the system decides a control \\(u_k \\in \\mathbb{U}\\) and incurs a cost \\(g(s_k,u_k)\\). The control \\(u_k\\) brings the system into a new state \\(x_{k+1} \\sim P(\\cdot \\mid x_k, u_k)\\), at which the controller decides a new control \\(u_{k+1}\\). This process continues forever. Controller (policy). In general, a time-varying controller \\(\\pi = (\\pi_0,\\dots,\\pi_k,\\dots)\\) is a mapping from all previous states and controls to a distribution over current controls. The mapping \\(\\pi_k\\) at timestep \\(k\\) is \\[ \\pi_k: (x_0,u_0,x_1,u_1,\\dots,x_k) \\mapsto u_k \\sim q_k \\in \\Delta(\\mathbb{U}). \\] Note that \\(u_k\\) can be randomized and it is drawn from a distribution \\(q_k\\) supported on the set of controls \\(\\mathbb{U}\\). A stationary controller (policy) \\(\\pi: \\mathbb{X} \\rightarrow \\Delta(\\mathbb{U})\\) specifies a decison-making strategy that is purely based on the current state \\(x_k\\). A deterministic and stationary controller \\(\\pi: \\mathbb{X} \\rightarrow \\mathbb{U}\\) excutes a deterministic control \\(u_k\\) at each step. Cost-to-go and \\(Q\\)-value. Given a controller \\(\\pi\\) and an initial state \\(x_0\\), we associate with it the following discounted infinite-horizon cost \\[\\begin{equation} J_\\pi(x_0) = \\mathbb{E} \\left\\{ \\sum_{k=0}^{\\infty} \\gamma^k g(x_k, u_k^{\\pi}) \\right\\}, \\tag{2.16} \\end{equation}\\] where the expectation is taken over the randomness of the transition \\(P\\) and the controller \\(\\pi\\). Note that we have used \\(u^{\\pi}_k\\) to denote the control at step \\(k\\) by following the controller \\(\\pi\\). Similarly, we define the \\(Q\\)-value function as \\[\\begin{equation} Q_\\pi(x_0,u_0) = \\mathbb{E} \\left\\{ g(x_0,u_0) + \\sum_{k=1}^{\\infty} \\gamma^k g(x_k,u_k^{\\pi}) \\right\\}. \\tag{2.17} \\end{equation}\\] The difference between \\(Q_\\pi(x_0,u_0)\\) and \\(J_\\pi(x_0)\\) is that at step zero, \\(J_\\pi(x_0)\\) follows the controller \\(\\pi\\) while \\(Q_\\pi(x_0,u_0)\\) assumes the control \\(u_0\\) is given. By the assumption that \\(g(x_k,u_k) \\in [0,1]\\), we have \\[ 0 \\leq J_\\pi(x_0), Q_\\pi(x_0,u_0) \\leq \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}, \\quad \\forall \\pi. \\] Our goal is to find the best controller that minimizes the cost function \\[\\begin{equation} \\pi^\\star \\in \\arg\\min_{\\pi \\in \\Pi} J_\\pi(x_0) \\tag{2.18} \\end{equation}\\] for a given initial state \\(x_0\\), where \\(\\Pi\\) is the space of all non-stationary and randomized controllers. A remarkable property of MDPs is that there exists an optimal controller that is stationary and deterministic. Theorem 2.1 (Deterministic and Stationary Optimal Policy) Let \\(\\Pi\\) be the space of all non-stationary and randomized policies. Define \\[ J^\\star_\\pi(x) = \\min_{\\pi \\in \\Pi} J_\\pi(x), \\quad Q^\\star_\\pi(x,u) = \\min_{\\pi \\in \\Pi } Q_\\pi(x,u). \\] There exists a deterministic and stationary policy \\(\\pi^\\star\\) such that for all \\(x \\in \\mathbb{X}\\) and \\(u \\in \\mathbb{U}\\), \\[ J_{\\pi^\\star}(x) = J^\\star(x), \\quad Q_{\\pi^\\star}(x,u) = Q^\\star(x,u). \\] We call such a policy \\(\\pi\\) an optimal policy. Proof. See Theorem 1.7 in (Agarwal et al. 2022). This Theorem shows that we can restrict ourselves to stationary and deterministic policies without losing performance. In the next, we show how to characterize the optimal policy and value function. 2.2.1 Bellman Optimality Equations We now restrict ourselves to stationary policies. We first introduce the Bellman Consistency Equations for stationary policies. Lemma 2.1 (Bellman Consistency Equations) Let \\(\\pi\\) be a stationary policy. Then \\(J_\\pi\\) and \\(Q_\\pi\\) satisfy the following Bellman consistency equations \\[\\begin{align} J_\\pi(x) &amp;= \\mathbb{E}_{u \\sim \\pi(\\cdot \\mid x)} Q_\\pi(x,u), \\tag{2.19} \\\\ Q_\\pi(x,u) &amp;=g(x,u) + \\gamma \\mathbb{E}_{x&#39; \\sim P(\\cdot \\mid x,u)} J_\\pi(x&#39;). \\tag{2.20} \\end{align}\\] Proof. By the definition of the cost-to-go function in (2.16), we have \\[ J_\\pi(x_0) = \\mathbb{E}\\left\\{ \\sum_{k=0}^{\\infty} \\gamma^k g(x_k, \\pi(x_k)) \\right\\} = \\mathbb{E}_{u_0 \\sim \\pi(\\cdot \\mid x_0)} \\underbrace{\\mathbb{E} \\left\\{ g(x_0,u_0) + \\sum_{k=1}^{\\infty} \\gamma^k g(x_k,\\pi(x_k)) \\right\\}}_{Q_\\pi(x_0,u_0)}. \\] The above equation holds for any \\(x_0\\), proving (2.19). To show (2.20), we recall the definition of the \\(Q\\)-value function (2.17) \\[\\begin{align} Q_\\pi(x_0,u_0) &amp; = \\mathbb{E} \\left\\{ g(x_0,u_0) + \\sum_{k=1}^{\\infty} \\gamma^k g(x_k,\\pi(x_k)) \\right\\} \\\\ &amp;= g(x_0,u_0) + \\gamma \\mathbb{E} \\left\\{ \\sum_{k=1}^{\\infty} \\gamma^{k-1} g(x_k,\\pi(x_k)) \\right\\} \\tag{2.21} \\end{align}\\] Now observe that the expectation of the second term in (2.21) is taken over both the randomness of \\(x_1\\) and the randomness of the policy after \\(x_1\\) is reached. Therefore, \\[ \\mathbb{E} \\left\\{ \\sum_{k=1}^{\\infty} \\gamma^{k-1} g(x_k,\\pi(x_k)) \\right\\} = \\mathbb{E}_{x_1 \\sim P(\\cdot \\mid x_0,u_0)} \\underbrace{\\left\\{ \\mathbb{E} \\left\\{ \\sum_{k=1}^{\\infty} \\gamma^{k-1} g(x_1,\\pi(x_1)) \\right\\} \\right\\}}_{J_\\pi(x_1)}. \\] Plugging the above equation back to (2.21), we obtain the desired result in (2.20). Matrix Representation. It is useful to think of \\(P,g,J_\\pi,Q_\\pi\\) as matrices. In particular, the transition function \\(P\\) can be considered as a matrix of dimension \\(|\\mathbb{X}||\\mathbb{U}| \\times \\mathbb{X}\\), where \\[ P_{(x,u),x&#39;} = P(x&#39; \\mid x,u) \\] is the entry of \\(P\\) at the row \\((x,u)\\) (there are \\(|\\mathbb{X}||\\mathbb{U}|\\) such rows) and column \\(x&#39;\\) (there are \\(|\\mathbb{X}|\\) such columns). The running cost \\(g\\) is vector of \\(|\\mathbb{X}||\\mathbb{U}|\\) entries. The cost-to-go \\(J_\\pi(x)\\) is a vector of \\(|\\mathbb{X}|\\) entries. The \\(Q\\)-value function \\(Q_\\pi(x,u)\\) is a vector of \\(|\\mathbb{X}||\\mathbb{U}|\\) entries. We also introduce \\(P^{\\pi}\\) with dimension \\(|\\mathbb{X}||\\mathbb{U}| \\times |\\mathbb{X}||\\mathbb{U}|\\) as the transition matrix induced by a stationary policy \\(\\pi\\). In particular, \\[ P^\\pi_{(x,u),(x&#39;,u&#39;)} = P(x&#39; \\mid x,u) \\pi(u&#39;\\mid x&#39;). \\] In words, \\(P^\\pi_{(x,u),(x&#39;,u&#39;)}\\) is the probability that \\((x&#39;,u&#39;)\\) follows \\((x,u)\\). With the matrix representation, we can compactly write the Bellman consistency equation (2.20) as \\[\\begin{equation} Q_\\pi = g + \\gamma P J_\\pi. \\tag{2.22} \\end{equation}\\] We can also combine (2.19) and (2.20) together and write \\[ Q_\\pi(x,u) = g(x,u) + \\gamma \\mathbb{E}_{x&#39; \\sim P(\\cdot \\mid x,u)} \\left\\{ \\mathbb{E}_{u&#39; \\sim \\pi(\\cdot \\mid x&#39;)} Q_\\pi(x&#39;,u&#39;) \\right\\} = g(x,u) + \\gamma \\mathbb{E}_{(x&#39;,u&#39;) \\sim P^\\pi(\\cdot \\mid (x,u))} Q_\\pi(x&#39;,u&#39;), \\] which, using matrix representation, becomes \\[\\begin{equation} Q_\\pi = g + \\gamma P^\\pi Q_\\pi. \\tag{2.23} \\end{equation}\\] Equation (2.23) immediately yields \\[\\begin{equation} Q_\\pi = (I - \\gamma P^\\pi)^{-1} g, \\tag{2.24} \\end{equation}\\] that is, the \\(Q\\)-value function associated with a stationary policy \\(\\pi\\) can be simply computed from solving a linear system as in (2.24).3 Lemma 2.1, together with the equivalent matrix equations (2.22) and (2.23), provide the conditions that \\(J_\\pi\\) and \\(Q_\\pi\\), induced by any stationary policy \\(\\pi\\), need to satisfy. In the next, we describe the conditions that characterize the optimal policy. Theorem 2.2 (Bellman Optimality Equations) A vector \\(Q \\in \\mathbb{R}^{|\\mathbb{X}||\\mathbb{U}|}\\) is said to satisfy the Bellman optimality equation if \\[\\begin{equation} Q(x,u) = g(x,u) + \\gamma \\mathbb{E}_{x&#39; \\sim P(\\cdot \\mid x,u)} \\left\\{ \\min_{u&#39; \\in \\mathbb{U}} Q(x&#39;,u&#39;) \\right\\}, \\quad \\forall (x,u) \\in \\mathbb{X} \\times \\mathbb{U}. \\tag{2.25} \\end{equation}\\] A vector \\(Q^\\star\\) is the optimal \\(Q\\)-value function if and only if it satisfies (2.25). Moreover, the deterministic policy defined by \\[ \\pi^\\star(x) \\in \\arg\\min_{u \\in \\mathbb{U}} Q^\\star(x,u) \\] with ties broken arbitrarily is an optimal policy. Proof. See Theorem 1.8 in (Agarwal et al. 2022). We now make a few definitions to interpret Theorem 2.2. For any vector \\(Q \\in \\mathbb{R}^{|\\mathbb{X}||\\mathbb{U}|}\\), define the greedy policy as \\[\\begin{equation} \\pi_{Q}(x) \\in \\arg\\min_{u \\in \\mathbb{U}} Q(x,u) \\tag{2.26} \\end{equation}\\] with ties broken arbitrarily. With this notation, by Theorem 2.2, the optimal policy is \\[ \\pi^\\star = \\pi_{Q^\\star}, \\] where \\(Q^\\star\\) is the optimal \\(Q\\)-value function. Similarly, let us define \\[ J_Q(x) = \\min_{u \\in \\mathbb{U}} Q(x,u). \\] Note that \\(J_Q\\) has dimension \\(|\\mathbb{X}|\\). With these notations, the Bellman optimality operator is defined as \\[\\begin{equation} \\mathcal{T}Q = g + \\gamma P J_Q, \\tag{2.27} \\end{equation}\\] which is nothing but a matrix representation of the right-hand side of (2.25). This allows us to concisely write the Bellman optimality equation (2.25) as \\[\\begin{equation} Q = \\mathcal{T}Q. \\tag{2.28} \\end{equation}\\] Therefore, an equivalent way to interpret Theorem 2.2 is that \\(Q = Q^\\star\\) if and only if \\(Q\\) is a fixed point to the Bellman optimality operator \\(\\mathcal{T}\\). 2.2.2 Value Iteration Intepreting the optimal \\(Q\\)-value function as the fixed point to the Bellman optimality operator (2.28) leads us to a natural algorithm for solving the optimal control problem. We start with \\(Q^{(0)}\\) being an all-zero vector and then at iteration \\(t\\), we perform \\[ Q^{(t+1)} \\leftarrow \\mathcal{T} Q^{(t)}, \\] with \\(\\mathcal{T}\\) defined in (2.27). Let us observe the simplicity of this algorithm: at each iteration, one only needs to perform \\(\\min_{u \\in \\mathbb{U}} Q^{(t)}(x,u)\\), which is very efficient when \\(|\\mathbb{U}|\\) is not too large. The next theorem states this simple algorithm converges to the optimal value function. Theorem 2.3 (Value Iteration) Set \\(Q^{(0)} = 0\\). For \\(t=0,\\dots\\), perform \\[ Q^{(t+1)} \\leftarrow \\mathcal{T} Q^{(t)}. \\] Let \\(\\pi^{(k)} = \\pi_{Q^{(k)}}\\) (see the definition in (2.26)). For \\(t \\geq \\frac{\\log \\frac{2}{(1-\\gamma)^2 \\epsilon}}{1-\\gamma}\\), we have \\[ J^{\\pi^{(t)}} \\leq J^\\star + \\epsilon \\mathbb{1}, \\] where \\(\\mathbb{1}\\) is the all-ones vector. Essentially, the value function obtained from value iteration converges to the optimal cost-to-go. Let us use an example to appreciate this algorithm. Example 2.2 (Shortest Path in Grid World) Consider the following \\(10 \\times 10\\) grid world, where the top-right cell is the goal location, and the dark blue colored cells are obstacles. Figure 2.3: Grid World with Obstacles. We want to find the shortest path from a given cell to the target cell, while not hitting obstacles. To do so, we set the state space of the system as \\[ \\mathbb{X} = \\left\\{ \\begin{bmatrix} r \\\\ c \\end{bmatrix} \\middle\\vert r,c \\in \\{ 1,\\dots,10 \\} \\right\\} \\] where \\(r\\) is the row index (from top to bottom) and \\(c\\) is the column index (from left to right). The control space is moving left, right, up, down, or do nothing: \\[ \\mathbb{U} = \\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\right\\}. \\] The system dynamics is deterministic \\[ x&#39; = \\begin{cases} x + u &amp; \\text{if } x + u \\text{ is inside the grid} \\\\ x &amp; \\text{otherwise} \\end{cases}. \\] We then design the following running cost function \\(g\\) \\[ g(x,u) = \\begin{cases} 0 &amp; \\text{if } x = [1,10]^T \\text{ is the target} \\\\ 20 &amp; \\text{if } x \\text{ is an obstacle} \\\\ 1 &amp; \\text{otherwise} \\end{cases}. \\] Note that \\(g(x,u)\\) defined above does not even satisfy \\(g \\in [0,1]\\). We then use value iteration to solve the optimal control problem with \\(\\gamma = 1\\) \\[ J(x_0) = \\min_{\\pi} \\sum_{k=0}^{\\infty} g(x_k,\\pi(x_k)). \\] The Matlab script of value iteration converges in \\(27\\) iterations, and we obtain the optimal cost-to-go in Fig. 2.4. Starting from the cell \\([8,5]^T\\), the red line in Fig. 2.4 plots the optimal trajectory that clearly avoids the obstacles. Feel free to play with the size of the grid and the number of obstacles. Figure 2.4: Optimal cost-to-go and an optimal trajectory. Example 2.2 shows the simplicity and power of value iteration. However, the states and controls in the grid world are naturally discrete and finite. Is it possible to apply value iteration to optimal control problems where the states and controls live in continuous spaces? 2.2.3 Value Iteration with Barycentric Interpolation Let us consider the discrete-time dynamics \\[ x_{k+1} = f(x_k, u_k) \\] where both \\(x_k\\) and \\(u_k\\) live in a continuous space, say \\(\\mathbb{R}^{n}\\) and \\(\\mathbb{R}^m\\), respectively. A natural idea to apply value iteration is to discretize the state space and control space. For example, suppose \\(x \\in \\mathbb{R}^2\\) and we have discretized \\(\\mathbb{R}^2\\) using \\(N\\) points \\[ \\mathcal{S} = \\{s_1,\\dots,s_N\\} \\] that lie on a 2D grid, as shown in Fig. 2.5. Assume \\(x_k \\in \\mathcal{S}\\) lies on the mesh grid, the next state \\(x_{k+1} = f(x_k,u_k)\\) will, however, most likely not lie exactly on one of the grid points. Figure 2.5: Barycentric Interpolation. Nevertheless, \\(x_{k+1}\\) will lie inside a triangle with vertices \\(s_p, s_q, s_m\\). We will now try to write \\(x_{k+1}\\) using the vertices, that is, to find three numbers \\(\\lambda_p, \\lambda_q, \\lambda_m\\) such that \\[ \\lambda_p, \\lambda_q, \\lambda_m \\geq 0, \\quad \\lambda_p + \\lambda_q + \\lambda_m = 1, \\quad x_{k+1} = \\lambda_p s_p + \\lambda_q s_q + \\lambda_m s_m. \\] \\(\\lambda_p,\\lambda_q,\\lambda_m\\) are called the barycentric coordinates of \\(x_{k+1}\\) in the triangle formed by \\(s_p,s_q,s_m\\). With the barycentric coordinates, we will assign the transition matrix \\[ P(x_{k+1}=s_p \\mid x_k,u_k) = \\lambda_p, \\quad P(x_{k+1}=s_q \\mid x_k,u_k) = \\lambda_q, \\quad P(x_{k+1}=s_m \\mid x_k,u_k) = \\lambda_m. \\] Let us apply value iteration with barycentric interpolation to the simple pendulum. Example 2.3 (Value Iteration with Barycentric Interpolation on A Simple Pendulum) Consider the continuous-time pendulum dynamics in (2.15) that is already shifted such that \\(z=0\\) corresponds to the upright position. With time discretization \\(h\\), we can write the discrete-time dynamics as \\[ z_{k+1} = \\dot{z}_k \\cdot h + z_k = f&#39;(z_k,u_k) \\cdot h + z_k. \\] We are interested in solving the optimal control problem \\[ J(z_0) = \\min_{u_k} \\left\\{ \\sum_{k=0}^{\\infty} \\gamma^k g(x_k,u_k) \\right\\}, \\] where the running cost is simply \\[ g(x_k,u_k) = x_k^T x_k + u_k^2. \\] We will use the parameters \\(m=1,g=9.8,l=1,b=0.1\\), and assume the control is bounded in \\([-4.9,4.9]\\). We want to compute the optimal cost-to-go in the range \\(z_1 \\in [-\\pi,\\pi]\\) and \\(z_2 \\in [-\\pi,\\pi]\\). We discretize both \\(z_1\\) and \\(z_2\\) using \\(N\\) points, leading to \\(N^2\\) points in the state space. We also discretize \\(u\\) using \\(N\\) points. Applying value iteration with \\(\\gamma=0.9\\) and \\(N=50\\), we obtain the optimal cost-to-go in Fig. 2.6. The value iteration converges in \\(277\\) iterations. Figure 2.6: Optimal cost-to-go with discount factor 0.9. Applying value iteration with \\(\\gamma=0.99\\) and \\(N=50\\), we obtain the optimal cost-to-go in Fig. 2.7. The value iteration converges in \\(2910\\) iterations. Figure 2.7: Optimal cost-to-go with discount factor 0.99. Applying value iteration with \\(\\gamma=0.999\\) and \\(N=50\\), we obtain the optimal cost-to-go in Fig. 2.8. The value iteration converges in \\(28850\\) iterations. Figure 2.8: Optimal cost-to-go with discount factor 0.999. You can find the Matlab code here. References Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. “Reinforcement Learning: Theory and Algorithms.” CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 32. Arnold, William F, and Alan J Laub. 1984. “Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.” Proceedings of the IEEE 72 (12): 1746–54. ———. 2012. Dynamic Programming and Optimal Control: Volume i. Vol. 1. Athena scientific. One can show that the matrix \\(I - \\gamma P^\\pi\\) is indeed invertible, see Corollary 1.5 in (Agarwal et al. 2022).↩︎ "],["approximatedp.html", "Chapter 3 Approximate Optimal Control 3.1 Fitted Value Iteration 3.2 Trajectory Optimization 3.3 Model Predictive Control 3.4 Policy Gradient", " Chapter 3 Approximate Optimal Control Thanks to Jiarui Li for contributing to this Chapter. In Chapter 2, we have studied two cases where dynamic programming (DP) can be executed exactly. Both cases are interesting yet limiting. In the linear quadratic regulator (LQR) case, the optimal controller is a linear controller and it does not require discretization of the state and control space. However, LQR only handles systems with linear dynamics. In the Markov Decision Process (MDP) case, value iteration can obtain the optimal cost-to-go (or the optimal \\(Q\\)-value function) usually in a finite number of iterations. With barycentric interpolation, value iteration also leads to a practical controller for swinging up the pendulum (see Example 2.3), at least starting from some initial states. However, value iteration suffers from the curse of dimensionality, i.e., the amount of memory and computation needed to compute the optimal cost-to-go grows exponentially with the number of grids used to discretize the state space and control space. In this Chapter, we will study approximate dynamic programming, which aims to not find the optimal controller (simply because it is too demanding), but only a suboptimal controller that is perhaps good enough. It is worth noting that there exists a large amount of algorithms and literature for approximate DP, most of which are closely related to reinforcement learning, and studying all of them is beyond the scope of this course. In the following we will only go through several algorithmic frameworks that are representative. 3.1 Fitted Value Iteration Let us consider the infinite-horizon optimal control problem introduced in Chapter 1.3 \\[ \\min_{\\pi} \\mathbb{E} \\left\\{ \\sum_{k=0}^\\infty g(x_k, \\pi(x_k)) \\right\\}. \\] Under some technical conditions, we know that the optimal policy is a deterministic and stationary policy and the optimal cost-to-go satisfies the following Bellman Optimality Equation \\[\\begin{equation} J^\\star(x) = \\min_{u \\in \\mathbb{U}} \\mathbb{E}_w \\left\\{ g(x,u) + J^\\star(f(x,u,w)) \\right\\}, \\quad \\forall x \\in \\mathbb{X}. \\tag{3.1} \\end{equation}\\] From the plots in Example 2.3, we observe that even for a “simple” problem like pendulum swing-up, the optimal cost-to-go \\(J^\\star(x)\\) does not look simple at all. So here comes a very natural idea. What if we parametrize a cost-to-go function \\(\\tilde{J}(x,r)\\) by a vector of unknown coefficients \\(r \\in \\mathbb{R}^{d}\\) and ask \\(\\tilde{J}(x,r)\\) to satisfy (3.1) as closely as possible? This indeed leads to a valid algoithm called fitted value iteration (FVI). In FVI, we initialize the parameters \\(r\\) as \\(r_0\\) in the first step. Then, at the \\(k\\)-th iteration, we perform two subroutines. Value update. Firstly, we sample a large amount of points in the state space \\(\\{x_k^s \\}_{s=1}^q\\), and for each \\(x_k^s\\), we solve the minimization problem on the right-hand side of (3.1) using \\(J(x,r^{(k)})\\) as the cost-to-go. Formally, this is \\[\\begin{equation} \\beta_k^s \\leftarrow \\min_{u \\in \\mathbb{U}} \\mathbb{E}_w \\left\\{ g(x_k^s, u) + \\tilde{J}(f(x_k^s,u,w),r^{(k)}) \\right\\}, \\quad \\forall x_k^s, s= 1,\\dots,q. \\tag{3.2} \\end{equation}\\] This step gives us a list of scalars \\(\\{ \\beta_k^s \\}_{s=1}^q\\). If \\(\\tilde{J}(x,r)\\) indeed satisfies the Bellman optimality equation, then we should have \\[ \\tilde{J}(x_k^s,r^{(k)}) = \\beta_k^s, \\quad \\forall s = 1,\\dots,q. \\] This is certainly not true when the parameter \\(r\\) is imperfect. Parameter update. Therefore, we will see the best parameter that minimizes the violation of the above equation \\[\\begin{equation} r^{(k+1)} \\leftarrow \\arg\\min_{r \\in \\mathbb{R}^d} \\sum_{s=1}^q \\left(\\tilde{J}(x_k^s,r) - \\beta_k^s\\right)^2. \\tag{3.3} \\end{equation}\\] FVI essentially carries out (3.2) and (3.3) until some convergence metric is met. Two challenges immediately show up: How to perform the minimization over \\(u\\) in the first step (3.2)? How to find the best parameter update in the second step (3.3)? To solve the first challenge, we will assume that \\(\\mathbb{U}\\) is a finite set so that minimization over \\(u\\) can be solved exactly. Note that this assumption is not strictly necessary. In fact, as long as \\(\\mathbb{U}\\) is a convex set and the objective in (3.2) is also convex, then the minimization can also be solved exactly (A. Yang and Boyd 2023). However, we assume \\(\\mathbb{U}\\) is finite to simplify our presentation. To solve the second challenge, we will assume \\(\\tilde{J}(x,r)\\) is linear in the parameters \\(r\\), as we will study further in the following. 3.1.1 Linear Features We parameterize \\(\\tilde{J}(x,r)\\) as follows: \\[\\begin{equation} \\tilde{J}(x,r) = \\sum_{i=1}^d r_i \\cdot \\phi_i(x) = \\begin{bmatrix} \\phi_1(x) &amp; \\cdots &amp; \\phi_d(x) \\end{bmatrix} r = \\phi(x)^T r, \\end{equation}\\] where \\(\\phi_1(x),\\dots,\\phi_d(x)\\) are a set of known basis functions, or features. Common examples of features include polynomials and radial basis functions. With this parametrization, the optimization in (3.3) becomes \\[\\begin{equation} \\min_{r \\in \\mathbb{R}^d} \\sum_{s=1}^q \\left( \\phi(x_k^s)^T r - \\beta_k^s \\right)^2, \\tag{3.4} \\end{equation}\\] which is a least-squares problem that can be solved in closed form. In particular, we can compute the gradient of its objective with respect to \\(r\\) and set it to zero, leading to \\[\\begin{align} 0 = \\sum_{s=1}^q 2(\\phi(x_k^s)^T r - \\beta_k^s ) \\phi(x_k^s) \\Longrightarrow \\\\ r = \\left( \\sum_{s=1}^q \\phi(x_k^s) \\phi(x_k^s)^T \\right)^{-1} \\left( \\sum_{s=1}^q \\beta_k^s \\phi(x_k^s) \\right). \\end{align}\\] Let us apply FVI with linear features to a linear system for which we know the optimal cost-to-go. Example 3.1 (Fitted Value Iteration for Double Integrator) Consider the double integrator \\[ \\ddot{q} = u. \\] Take \\(x = [q;\\dot{q}]\\), we can write the dynamics in state-space form \\[\\begin{equation} \\dot{x} = \\begin{bmatrix} \\dot{q} \\\\ u \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} x + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u \\tag{3.5} \\end{equation}\\] We use a constant time \\(h=0.01\\) differentiation to convert the continuous-time dynamics into discrete-time: \\[ x_{k+1} = h \\cdot \\dot{x}_k + x_k = \\begin{bmatrix} 1 &amp; h \\\\ 0 &amp; 1 \\end{bmatrix} x_k + \\begin{bmatrix} 0 \\\\ h \\end{bmatrix} u_k. \\] The goal is to regulate the system at \\((0,0)\\). To do so, we use a quadratic cost \\[ J(x) = \\min \\sum_{k=0}^{\\infty} x_k^T Q x_k + u_k^T R u_k, \\quad x_0 = x. \\] with \\(Q = 0.1I_2\\) and \\(R = 1\\). In Chapter 2, we learned how to use LQR to precisely calculate the cost-to-go function of these systems using the Algebraic Recatti Equation. Solving the ARE, we obtain \\[ S_{\\mathrm{LQR}} = \\begin{bmatrix} 27.1640 &amp; 31.7584 \\\\ 31.7584 &amp; 85.9510 \\end{bmatrix}. \\] Now we want to investigate if FVI can find the same matrix \\(S\\). To do so, we parametrize \\[ \\tilde{J}(x) = x^T S x = x^T \\begin{bmatrix} S_1 &amp; S_2 \\\\ S_2 &amp; S_3 \\end{bmatrix} x = \\begin{bmatrix} x_1^2 &amp; 2 x_1 x_2 &amp; x_2^2 \\end{bmatrix} \\begin{bmatrix} S_1 \\\\ S_2 \\\\ S_3 \\end{bmatrix}, \\] where \\(S\\) only has three independent variables due to being symmetric. Do note that \\(\\tilde{J}(x)\\) is linear in \\(S\\), so the parameter update step can be solved in closed form. We now show that the value update step can also be solved in closed form. Suppose we choose \\(x_k\\) as a sample, then the right-hand side of (3.2) reads: \\[ \\min_{u} x_k^T Q x_k + u^2 + (A x_k + B u)^T S^{(k)} (A x_k + B u), \\] where \\(S^{(k)}\\) is the value of \\(S\\) at the \\(k\\)-th iteration. Clearly, the optimization problem above is a convex quadratic optimization and can be solved in closed form: \\[ u = - (1 + B^T S^{(k)} B) ^{-1} B^T S^{(k)} A x_k. \\] Applying fitted value iteration on randomly sampled points mentioned above, we obtain the fitted \\(S\\) \\[ S_{\\mathrm{FVI}} = \\begin{bmatrix} 27.1640 &amp; 31.7583 \\\\ 31.7584 &amp; 85.9509 \\end{bmatrix}. \\] We can see that \\(S_{\\mathrm{FVI}}\\) almost exactly matches the groundtruth LQR solution \\(S_{\\mathrm{LQR}}\\). You can play with the code here. 3.1.2 Neural Network Features We have seen that simple quadratic features work for the LQR problem. For more complicated problems, we will need more powerful function approximators like neural networks. When we parameterize \\(\\tilde{J}(x,r)\\) as a neural network, \\(r\\) will be the weights of the neural network. We may still be able to solve the value update step if \\(u\\) lives in a finite space. However, the parameter update step usually cannot be solved exactly and will need to rely on numerical algoithms such as gradient descent. Let us try neural FVI on the same double integrator problem. Example 3.2 (Neural Fitted Value Iteration for the Double Integrator) In this example, we will use neural network as the approximation of the cost-to-go function and conduct neural FVI on a double integrator. The dynamics of the double integrator has been introduced in example 3.1. We use a positive definite network to model the cost-to-go function \\[\\begin{equation} J(x) = N(x)^T N(x), \\end{equation}\\] where \\(N(x)\\) is a 3-layer Multi-Layer-Perceptron with ReLU activation. Using mini-batch learning plus Adam optimizer, we obtain the cost-to-go in Fig. 3.1 after 300 epochs of training. Figure 3.1: Comparison between the result of NN-based FVI and ground truth The figure shows that the approximation performance of NN is pretty good. Simulation experiments also shows that the corresponding controller could successfully regulate the system at \\((0,0)\\). You can see the code here. 3.1.3 Fitted Q-value Iteration From the MDP Chapter we know there is an equivalent representation of the Bellman Optimality Equation by replacing the \\(J\\) value function in (3.1) with the \\(Q\\)-value function \\(Q(x,u)\\). In particular, with \\[ J^\\star(x) = \\min_{u \\in \\mathbb{U}} Q^\\star(x,u) \\] substituted into (3.1), we obtain the Bellman Optimality Equation in \\(Q^\\star(x,u)\\): \\[\\begin{equation} Q^\\star(x,u) = g(x,u) + \\mathbb{E}_w \\left\\{ \\min_{u&#39; \\in \\mathbb{U}} Q^\\star(f(x,u,w),u&#39;) \\right\\}. \\tag{3.6} \\end{equation}\\] We can then use the same idea to approximate \\(Q^\\star(x,u)\\) as \\[ \\tilde{Q}(x,u,r) \\] with \\(r \\in \\mathbb{R}^d\\) a parameter vector. By iteratively evaluating the right-hand side of (3.6), we obtain the algoithm known as fitted \\(Q\\)-value iteration (FQI). For example, we can similarly adopt the linear feature parameterization and set \\[ \\tilde{Q}(x,u,r) = \\phi(x,u)^T r, \\] where \\(\\phi(x,u)\\) is a known pre-selected feature vector. Then at the \\(k\\)-th iteration of FQI, we perform two subroutines. Value update. We sample a number of state-control pairs \\(\\{(x_k^s,u_k^s) \\}_{s=1}^q\\) and evaluate the right-hand side of the Bellman optimality equation (3.6): \\[\\begin{equation} \\beta_k^s \\leftarrow g(x_k^s,u_k^s) + \\mathbb{E}_w \\left\\{ \\min_{u&#39; \\in \\mathbb{U}} \\tilde{Q}(f(x_k^s,u_k^s,w),u&#39;,r^{(k)}) \\right\\}, \\quad \\forall s = 1,\\dots,q. \\tag{3.7} \\end{equation}\\] Again, if \\(u\\) lives in a finite space, or the minimization is convex, the above value update can be solved exactly. Parameter update. We update the parameter vector using the updated values \\(\\{ \\beta_k^s \\}_{s=1}^q\\): \\[\\begin{equation} r^{(k+1)} \\leftarrow \\arg\\min_{r \\in \\mathbb{R}^d} \\sum_{s=1}^q \\left( \\tilde{Q}(x_k^s,u_k^s,r) - \\beta_k^s \\right)^2, \\tag{3.8} \\end{equation}\\] which is a least squares problem that can be solved in closed form. Let us apply FQI to the same double integrator example. Example 3.3 (Fitted Q-value Iteration for Double Integrator) Consider the same double integrator dynamics in Example 3.1. From the LQR solution we know the optimal \\(Q\\)-value function is \\[\\begin{align} Q^\\star(x,u) &amp;= x^T Q x + u^T R u + (Ax + Bu)^T S (Ax + Bu) \\\\ &amp;= \\begin{bmatrix} x \\\\ u \\end{bmatrix}^T \\underbrace{\\begin{bmatrix} A^T S A + Q &amp; A^T S B \\\\ B^T S A &amp; B^T S B + R \\end{bmatrix}}_{M_{\\mathrm{LQR}}} \\begin{bmatrix} x \\\\ u \\end{bmatrix}, \\end{align}\\] where \\(M_{\\mathrm{LQR}}\\) is \\[ M_{\\mathrm{LQR}} = \\begin{bmatrix} 27.2640 &amp; 32.0300 &amp; 0.3176 \\\\ 32.0300 &amp; 86.6889 &amp; 0.8627 \\\\ 0.3176 &amp; 0.8627 &amp; 1.0086 \\end{bmatrix}. \\] Let us apply FQI to see if we get the same solution. We parametrize \\[\\begin{align} \\tilde{Q}(x,u) &amp;= \\begin{bmatrix} x \\\\ u \\end{bmatrix}^T \\begin{bmatrix} M_1 &amp; M_2 &amp; M_3 \\\\ M_2 &amp; M_4 &amp; M_5 \\\\ M_3 &amp; M_5 &amp; M_6 \\end{bmatrix} \\begin{bmatrix} x \\\\ u \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} x_1^2 &amp; 2 x_1 x_2 &amp; 2x_1 u &amp; x_2^2 &amp; 2x_2 u &amp; u^2 \\end{bmatrix} \\begin{bmatrix} M_1 \\\\ M_2 \\\\ M_3 \\\\ M_4 \\\\ M_5 \\\\ M_6 \\end{bmatrix}. \\end{align}\\] At the \\(k\\)-th FQI iteration, we are given \\(M^{(k)}\\). Suppose we sample \\((x_k, u_k)\\), then the value update step needs to solve (3.7), which reads: \\[ \\beta_k = g(x_k, u_k) + \\min_{u&#39;} \\underbrace{\\begin{bmatrix} A x_k + B u_k \\\\ u&#39; \\end{bmatrix}^T M^{(k)} \\begin{bmatrix} A x_k + B u_k \\\\ u&#39; \\end{bmatrix}}_{\\psi(u&#39;)}. \\] The objective function \\(\\psi(u&#39;)\\) can be shown to be quadratic: \\[ \\psi(u&#39;) = (Lu&#39; + a_k)^T M^{(k)} (Lu&#39; + a_k), \\quad L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, a_k = \\begin{bmatrix} A x_k + B u_k \\\\ 0 \\end{bmatrix}. \\] Therefore, we can solve \\(u&#39;\\) in closed-form \\[ u&#39; = - (L^T M^{(k)} L)^{-1} L^T M^{(k)} a_k. \\] Applying FQI with the closed-form update above, we get \\[ M_{\\mathrm{FQI}} = \\begin{bmatrix} 27.2640 &amp; 32.0300 &amp; 0.3176 \\\\ 32.0300 &amp; 86.6889 &amp; 0.8627 \\\\ 0.3176 &amp; 0.8627 &amp; 1.0086 \\end{bmatrix}, \\] which is exactly the same as the solution obtained from LQR. You can play with the code here. 3.1.4 Deep Q Network Although we have only tested FVI and FQI on the simple double integrator linear system, these algoithms are really not so different from the state-of-the-art reinforcement learning algoithms. For example, the core of the seminal work Deep \\(Q\\)-Network (DQN) (Mnih et al. 2015) is to use a deep neural network to parameterize the \\(Q\\)-value function. Of course the DQN work has used other clever ideas to make it work in practice, such as experience replay, but the essence is fitted \\(Q\\)-value iteration. You can find a good explanation of DQN in this tutorial, and a practical step-by-step Pytorch implementation that applies DQN on the cart-pole system. 3.1.5 Deep + Shallow Combine the rich features of DQN with the stable learning of FQI (Levine et al. 2017). 3.2 Trajectory Optimization Consider a continuous-time optimal control problem (OCP) in full generality \\[\\begin{equation} \\begin{split} \\min_{u(t), t \\in [0,T]} &amp; \\quad g_T(x(T)) + \\int_{t = 0}^T g(x(t),u(t)) dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(t),u(t)) \\\\ &amp; \\quad x(0) = x_0 \\\\ &amp; \\quad (x(t),u(t)) \\in \\mathcal{X} \\times \\mathcal{U} \\\\ &amp; \\quad \\phi_i (x(t),u(t)) \\geq 0, i=1,\\dots,q. \\end{split} \\tag{3.9} \\end{equation}\\] where \\(g_T\\) the terminal cost, \\(g\\) the running cost, \\(x_0\\) the initial condition, \\(\\mathcal{X}\\) the state constraint set, \\(\\mathcal{U}\\) the control constraint set, and \\(\\{ \\phi_i \\}_{i=1}^q\\) are general state-control constraints. We assume that the state constraint set \\(\\mathcal{X}\\) and control constraint set \\(\\mathcal{U}\\) can be described by a finite set of inequalities, i.e., \\[ \\mathcal{X} = \\{x \\in \\mathbb{R}^n \\mid c^x_i(x) \\geq 0, i =1,\\dots,q_x\\}, \\quad \\mathcal{U} = \\{u \\in \\mathbb{R}^m \\mid c^u_i(u) \\geq 0, i = 1,\\dots,q_u \\}. \\] Assume that all functions are differentiable. Our goal is to “transcribe” the infinite-dimensional continuous-time optimization (3.9) into a nonlinear programming problem (NLP) of the following form \\[\\begin{equation} \\begin{split} \\min_{v \\in \\mathbb{R}^{n_v}} &amp; \\quad c(v) \\\\ \\text{subject to} &amp; \\quad c_{\\text{eq},i}(v) = 0, i =1,\\dots, n_{\\text{eq}} \\\\ &amp; \\quad c_{\\text{ineq},i}(v) \\geq 0, i=1,\\dots,n_{\\text{ineq}}, \\end{split} \\tag{3.10} \\end{equation}\\] where \\(v\\) is a finite-dimensional vector variable to be optimized, \\(c, c_{\\text{eq},i}, c_{\\text{ineq},i}\\) are (continuously differentiable) objective and constraint functions. Once we have done the transcription from (3.9) to (3.10), then we have a good number of numerical optimization algorithms at our disposal. For example, the Matlab fmincon provides a nice interface to many such algorithms, e.g., interior-point method and sequential quadratic programming. You have already played with a simple example of fmincon in Exercise 9.1. Other well-implemented NLP algorithms include IPOPT and SNOPT. However, usually the Matlab fmincon gives a decent start point for trying out different algorithms before moving to commercial solvers such as SNOPT. For a more comprehensive understanding about how NLP algoithms work under the hood, I suggest reading (Nocedal and Wright 1999). Before we talk about how to transcribe the OCP into an NLP, let us use a simple example to get a taste of fmincon. Example 3.4 (Fire a Canon with fmincon) On a 2D plane, suppose we have a canon at the origin \\((0,0)\\), and we want to fire the canon, with control over the initial velocity \\((v_1, v_2)\\), so that the canon ball hits the target location \\((10,10)\\). From our basic physics knowledge, we know the trajectory of the canon ball is described by \\[ \\begin{cases} x_1(t) = v_1 t \\\\ x_2(t) = v_2 t - \\frac{1}{2}g t^2, \\end{cases} \\] where \\(g\\) is the gravity constant. With the trajectory of the canon ball written above, we can formulate our nonlinear programming problem (NLP) as: \\[\\begin{equation} \\begin{split} \\min_{v_1, v_2, T} &amp; \\quad \\frac{1}{2} (v_1^2 + v_2^2) \\\\ \\text{subject to} &amp; \\quad v_1, v_2, T \\geq 0 \\\\ &amp; \\quad x_1(T) = v_1 T = 10 \\\\ &amp; \\quad x_2(T) = v_2 T - \\frac{1}{2}g T^2 = 10 \\end{split} \\tag{3.11} \\end{equation}\\] where our unknown variables are the initial velocities and the time \\(T\\) at which the canon ball hits the target. In the NLP (3.11), the first constraint asks all our varaibles to be nonnegative, the second and third constraints enforce the canon ball to hit the target. The following script formulates the NLP (3.11) in matlab. clc; clear; close all; g = 9.8; x0 = [0;0;1]; % initial guess of the solution % define objective function obj = @(x) objective(x); % define nonlinear constraints nonlincon = @(x) nonlinear_con(x,g); % define options for fmincon options = optimoptions(&#39;fmincon&#39;,&#39;Algorithm&#39;,&#39;interior-point&#39;,... &#39;SpecifyObjectiveGradient&#39;,true,&#39;SpecifyConstraintGradient&#39;,true,... &#39;checkGradients&#39;,false); % call fmincon [xopt,fopt,~,out] = fmincon(obj,x0,... % objective and initial guess -eye(3),zeros(3,1),... % linear inequality constraints [],[],... % no linear equality constraints [],[],... % no upper and lower bounds nonlincon,... % nonlinear constraints options); fprintf(&quot;Maximum constraint violation: %3.2f.\\n&quot;,out.constrviolation); fprintf(&quot;Objective: %3.2f.\\n&quot;,fopt); % plot the solution T = xopt(3); t = 0:0.01:T; xt = xopt(1)*t; yt = xopt(2)*t - 0.5*g*(t.^2); figure; plot(xt,yt,&#39;LineWidth&#39;,2); hold on scatter(10,10,100,&quot;red&quot;,&#39;filled&#39;,&#39;diamond&#39;); axis equal; grid on; xlabel(&#39;$x_1(t)$&#39;,&#39;FontSize&#39;,24,&#39;Interpreter&#39;,&#39;latex&#39;); ylabel(&#39;$x_2(t)$&#39;,&#39;FontSize&#39;,24,&#39;Interpreter&#39;,&#39;latex&#39;); ax = gca; ax.FontSize = 20; %% helper functions function [f,df] = objective(x) % x is our decision variable: [v1, v2, T] % objective function f = 0.5 * (x(1)^2 + x(2)^2); % gradient of the objective function (optional) df = [x(1); x(2); 0]; % column vector end function [c,ceq,dc,dceq] = nonlinear_con(x,g) % no inequality constraints c = []; dc = []; % two equality constraints ceq = [x(1)*x(3)-10; x(2)*x(3)-0.5*g*x(3)^2-10]; % explicit gradient of ceq dceq = [x(3), 0; 0, x(3); x(1), x(2)-g*x(3)]; end Running the code above, we get the following trajectory Figure 3.2: Trajectory of the canon ball obtained from fmincon. What if you change the initial guess to fmincon, or add more constraints? Feel free to play with the code here. It turns out there are multiple different ways to transcribe the optimal control problem (3.9) as the nonlinear programming problem (3.10). In the following, we will introduce three different formulations, direct single shooting, direct multiple shooting, and direct collocation. 3.2.1 Direct Single Shooting In direct single shooting, we transcribe the OCP to the NLP by only optimizing a sequence of control values, with the intuition being that the state values are functions of the controls by invoking the system dynamics. In particular, we follow the procedure below. Time discretization. We first discretize the total time window \\([0,T]\\) into a set of \\(N\\) intervals: \\[ 0 = t_0 \\leq t_1 \\leq \\dots \\leq t_k \\leq t_{k+1} \\leq \\dots \\leq t_N = T. \\] We denote \\[ h_k = t_{k+1} - t_k \\] as the length of the \\(k\\)-th time interval. Piece-wise constant control. We will approximate the continuous-time control signal \\(u(t)\\) as a piece-wise constant function, i.e., \\[ u(t) = u_k, \\forall t \\in [t_k, t_{k+1}). \\] This is shown in Fig. 3.3. Let us collect all the constant control values as our decision variable to be optimized \\[\\begin{equation} v = \\begin{bmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{N-1} \\end{bmatrix} \\in \\mathbb{R}^{N m}. \\tag{3.12} \\end{equation}\\] This \\(v\\) will be our variable in the NLP (3.10). Note that \\(v\\) has dimension \\(N m\\). Figure 3.3: Direct single shooting. Dynamics integration. It is clear that once \\(v\\) in (3.12), i.e., the set of controls, is determined, then the state trajectory \\(x(t)\\) is also uniquely determined by the initial condition \\(x(0) = x_0\\) and the dynamics \\[ \\dot{x}(t) = f(x(t),u(t)). \\] In order to enforce the state constraint \\(x(t) \\in \\mathcal{X}\\), we will enforce the values of \\(x\\) at \\(t_0,t_1,\\dots,t_N\\), i.e., \\(x_k = x(t_k)\\), to lie in the constraint set \\(\\mathcal{X}\\). To do so, we need to integrate the dynamics. When the dynamics is linear, this integration can be done in closed form: \\[ x_{k+1} = x_k + \\int_{\\tau=t_k}^{t_k + h_k} A x(\\tau) + B u(\\tau) d\\tau = e^{Ah_k} x_k + A^{-1}(e^{Ah_k} - I) B u_k. \\] By running the above equation from \\(k=0\\) to \\(k=N-1\\), we obtain the values of \\(x_k,k=1,\\dots,N\\) as functions of the control vectors \\(v\\), shown in Fig. 3.3. When the dynamics is nonlinear, we will need to perform the dynamics integration using numerical integration. One of the most well known family of integrators is called “Runge-Kutta methods”. Among the family of methods, RK45 is perhaps the most popular one. We now briefly describe how a fourth-order RK integrator, i.e., RK4, works. Suppose we have already computed \\(x_k\\), and we want to integrate the dynamics to obtain \\(x_{k+1}\\). The RK4 integrator first computes the time derivatives at a sequence of four points: \\[\\begin{align} \\alpha_1 &amp;= f(x_k, u_k) \\\\ \\alpha_2 &amp;= f\\left( x_k + \\frac{1}{2} h_k \\alpha_1, u_k \\right) \\\\ \\alpha_3 &amp;= f\\left( x_k + \\frac{1}{2} h_k \\alpha_2, u_k \\right) \\\\ \\alpha_4 &amp;= f\\left( x_k + h_k \\alpha_3, u_k \\right). \\end{align}\\] Then we can obtain \\(x_{k+1}\\) via a weighted sum of the \\(\\alpha_i\\)’s \\[\\begin{equation} x_{k+1} = x_k + \\frac{1}{6} h_k \\left( \\alpha_1 + 2 \\alpha_2 + 2 \\alpha_3 + \\alpha_4 \\right) = \\text{RK4}(x_k, u_k). \\tag{3.13} \\end{equation}\\] Notice that a naive integrator would just perform \\(x_{k+1} = x_k + h_k \\alpha_1\\) without querying the gradients at three other points. The nice property of RK4 is that it leads to better accuracy than the naive integration. Writing the RK4 integrator recursively, we obtain \\[ x_{k+1} = \\text{RK4}(x_k, u_k) = \\text{RK4}(\\text{RK4}(x_{k-1},u_{k-1}),u_k) = \\text{RK4}(\\text{RK4}(...(x_0,u_0))...u_k), \\] where RK4 is invoked for \\(k+1\\) times. Clearly, each \\(x_k\\) is a complicated function of the sequence of controls \\(v\\) and the initial state \\(x_0\\). We will write \\(x_k(v)\\) to make this explict, as shown in Fig. 3.3. Objective approximation. We can approximate the objective of the OCP (3.9) using trapezoidal integration: \\[ g_T(x(T)) + \\int_{t=0}^T g(x(t),u(t)) dt \\approx g_T(x_N(v)) + \\sum_{k=0}^{N-1} \\frac{h_k}{2}(g(x_k(v),u_k) + g(x_{k+1}(v),u_{k+1})). \\] Summary. In summary, the transcribed NLP for the optimal control problem using direct single shooting would be \\[\\begin{equation} \\begin{split} \\min_{v = (u_0,\\dots,u_{N-1})} &amp; \\quad g_T(x_N(v)) + \\sum_{k=0}^{N-1} \\frac{g(x_k(v),u_k) + g(x_{k+1}(v), u_{k+1})}{2} h_k \\\\ \\text{subject to} &amp; \\quad c^u_i(u_k) \\geq 0, i=1,\\dots,q_u, \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad c^x_i(x_k(v)) \\geq 0, i=1,\\dots,q_x, \\quad k=1,\\dots,N \\\\ &amp; \\quad \\phi_i(x_k(v),u_k) \\geq 0, i=1,\\dots, q, \\quad k=0,\\dots,N. \\end{split} \\tag{3.14} \\end{equation}\\] Pros and Cons. The advantage of using direct single shooting to transcribe the OCP is clear: it leads to fewer variables because the only decision variables in (3.14) are the sequence of controls. In other transcription methods, the decision variable of the NLP typically involves the states as well. The disadvantage of direct single shooting is the complication of the function \\(x_k(v)\\) caused by numerical integrators such as RK4. Even though the original continuous-time dynamics \\(f(x,u)\\) may be simple, the RK4 function can be highly complicated due to evaluating \\(\\dot{x}\\) at multiple other locations. Moreover, the recursion of the RK4 operator makes this complication even worse. Let us try the RK4 simulator for a simple example. Example 3.5 (Propagation of Nonlinearities) Consider the following dynamics \\[ x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\dot{x} = \\begin{bmatrix} 10(x_2 - x_1) \\\\ x_1(u - x_3) - x_2 \\\\ x_1 x_2 - 3 x_3 \\end{bmatrix} \\] where \\(u\\) is a single scalar control. We are interested in running the RK4 simulator for \\(100\\) seconds (with \\(h_k = 0.01\\) for all \\(k\\)) at \\(x_0 = [1,0,0]^T\\) with \\(u\\) varying from \\(0\\) to \\(100\\), and see how the nonlinearities propagate. Fig. 3.4 plots \\(x\\) as a function of \\(u\\). We can clearly see the nonlinearities getting worse due to RK4. You can play with the code here. Figure 3.4: RK4 simulation. 3.2.2 Direct Multiple Shooting Figure 3.5: Direct multiple shooting. Propagation of the nonlinearities through numerical integrators motivates using direct multiple shooting to transcribe the OCP problem into a nonlinear programming problem. Instead of only optimizing the controls, direct multiple shooting optimizes both the controls and states. The decision variable in the NLP becomes \\[ v = \\begin{bmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{N-1} \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} \\in \\mathbb{R}^{N(n+m)}. \\] With the state variables also introduced in the optimization, we no longer need to recursively run the RK4 integrators. Instead, we just need to run it once and enforce \\[ x_{k+1} = \\text{RK4}(x_k, u_k), \\] i.e., the current state and the next state satisfy the dynamics constraint. This is shown in Fig. 3.5. In summary, the transcribed NLP for the OCP problem using direct multiple shooting becomes \\[\\begin{equation} \\begin{split} \\min_{v = (u_0,\\dots,u_{N-1},x_1,\\dots,x_N)} &amp; \\quad g_T(x_N) + \\sum_{k=0}^{N-1} \\frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k \\\\ \\text{subject to} &amp; \\quad x_{k+1} = \\text{RK4}(x_k, u_k), \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad c^u_i (u_k) \\geq 0, i=1,\\dots,q_u, \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad c^x_i(x_k) \\geq 0, i=1,\\dots,q_x, \\quad k=1,\\dots,N \\\\ &amp; \\quad \\phi_i(x_k, u_k) \\geq 0, i=1,\\dots,q, \\quad k=0,\\dots,N. \\end{split} \\tag{3.15} \\end{equation}\\] Direct multiple shooting avoids the recursion of numerical integrators such as RK4, but at the expense of introducing additional state variables in the NLP. Let us try direct multiple shooting on the double integrator. Example 3.6 (Direct Multiple Shooting for Minimum-Time Double Integrator) The double integrator has continuous-time dynamics (which you have already seen in Exercise 9.2): \\[ \\ddot{q} = u. \\] In standard state-space form, the dynamics is linear \\[ x = \\begin{bmatrix} q \\\\ \\dot{q} \\end{bmatrix}, \\quad \\dot{x} = \\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} x + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u. \\] Let us consider the minimum-time optimal control problem \\[\\begin{equation} \\begin{split} \\min_{u(t),t \\in [0,T]} &amp; \\quad T \\\\ \\text{subject to} &amp; \\quad \\dot{x} = A x + Bu, \\quad x(0) = x_0 \\\\ &amp; \\quad x(T) = 0 \\\\ &amp; \\quad u(t) \\in [-1,1], \\forall t \\in [0,T]. \\end{split} \\end{equation}\\] which seeks to get from the initial condition \\(x_0\\) to the origin as fast as possible. You should know from our physics intuition that the optimal controller is bang-bang, i.e., the optimal control first accelerates (deaccelerates) using the maximum control and then reverses using the opposite maximum control. Let’s see if we can obtain the optimal controller using direct multiple shooting. In direct multiple shooting, we will optimize the control trajectory, the state trajectory, and the final time \\(T\\). We fix the number of intervals \\(N\\), and discretize \\([0,T]\\) evenly into \\(N\\) intervals, which leads to variables for the NLP as \\[ v=(T,x_0,\\dots,x_{N},u_0,\\dots,u_N). \\] We can easily enforce the control constraints: \\[ u_k \\in [-1,1],k=0,\\dots,N, \\] and the initial / terminal constraints \\[ x_0 = x_0, \\quad x_N = 0. \\] The only nontrivial constraint is the dynamics constraint: \\[ x_{k+1} = \\text{RK4}(x_k,u_k),k=0,\\dots,N-1. \\] The following script shows how to use the Matlab integrator ode45 to enforce the dynamics constraint. function dx = double_integrator(t,states,v) % return xdot at the selected times t and states, using information from v % assume the controls in v define piece-wise constant control signal T = v(1); % final time N = (length(v) - 1) / 3; % number of knot points u_grid = v(2+2*N:3*N+1); % N controls t_grid = linspace(0,1,N)*T; u_t = interp1(t_grid,u_grid,t,&#39;previous&#39;); % piece-wise constant A = [0 1; 0 0]; B = [0; 1]; dx = A * states + B * u_t; end function [c,ceq] = double_integrator_nonlincon(v,initial_state) % enforce x_k+1 = RK45(x_k, u_k); integration done using ode45 T = v(1); % final time N = (length(v) - 1) / 3; % number of knot points t_grid = linspace(0,1,N)*T; x1 = v(2:N+1); % position x2 = v(2+N:2*N+1); % velocity % u = v(2+2*N:3*N+1); % controls % no inequality constraints c = []; % equality constraints ceq = []; for i = 1:(N-1) ti = t_grid(i); tip1 = t_grid(i+1); xi = [x1(i);x2(i)]; xip1 = [x1(i+1);x2(i+1)]; % integrate system dynamics starting from xi in [ti,tip1] tt = ti:(tip1-ti)/20:tip1; % fine-grained time discretization [~,sol_int] = ode45(@(t,y) double_integrator(t,y,v),tt,xi); xip1_int = sol_int(end,:); % enforce them to be the same ceq = [ceq; xip1_int(1) - xip1(1); xip1_int(2) - xip1(2)]; end % add initial state constraint ceq = [ceq; x1(1) - initial_state(1); x2(1) - initial_state(2)]; % add terminal state constraint: land at origin ceq = [ceq; x1(end); x2(end)]; end I first define a function double_integrator that returns the continuous-time dynaimcs, and then in the nonlinear constraints function double_integrator_nonlincon I use ode45 to simulate the dynamics starting at \\(x_k\\) from \\(t_k\\) to \\(t_{k+1}\\): [~,sol_int] = ode45(@(t,y) double_integrator(t,y,v),tt,xi); The solution I got from ode45 should be equal to my decision state variable. Running the complete code with \\(x_0 = [-10;0]\\) and \\(N=51\\), I obtain the control signal in Fig. 3.6, which is bang-bang. Figure 3.6: Optimal control signal by direct multiple shooting. Using ode45 to integrate the double integrator dynamics from \\(x_0\\) with the controller in Fig. 3.6, we obtain the following state trajectory. Notice that the final state does not exactly land at the origin. This is expected due to our time discretization and imperfect dynamics integration using ode45. Make sure you play with the code, e.g., by changing the initial state \\(x_0\\) and see what happens. Figure 3.7: ODE45 integration with the optimal control signal found by direct multiple shooting. 3.2.3 Direct Collocation In direct multiple shooting, we still need to rely on the numerical integrator RK4, which complicates the original nonlinear dynamics. In direct collocation, we will remove our dependency of RK4. The key idea of direct collocation is to approximate the state trajectory \\(x(t)\\) and the control trajectory \\(u(t)\\) as piece-wise polynomial functions. In the following, we will describe the Hermite-Simpson collocation method. Figure 3.8: Direct collocation. Time discretization. We first discretize the total time window \\([0,T]\\) into a set of \\(N\\) intervals: \\[ 0 = t_0 \\leq t_1 \\leq \\dots \\leq t_k \\leq t_{k+1} \\leq \\dots \\leq t_N = T. \\] We denote \\[ h_k = t_{k+1} - t_k \\] as the length of the \\(k\\)-th time interval. As we will see, the length of the time interval does not need to be fixed, and instead they can themselves be unknown variables to be optimized (in which case the final time \\(T\\) also becomes flexible). Knot variables. At each of the timestamps \\(t_0,\\dots,t_N\\), we assign knot variables, which are unknown state and control varaibles that need to be optimized. In particular, we have state knot variables \\[ x_k = x(t_k), k=1,\\dots,N, \\] and control knot variables \\[ u_k = u(t_k),k=0,\\dots,N-1. \\] As a result, the entire set of knot varaibles to be optimized is \\[\\begin{equation} v = \\begin{bmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{N-1} \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{N} \\end{bmatrix}. \\tag{3.16} \\end{equation}\\] If the time-discretization is also optimized, then \\(v\\) includes the time intervals as well. Transcribe dynamics. The most important step is to transcribe the nonlinear dynamics \\(\\dot{x}=f(x(t),u(t))\\) as constraints on the knot varaibles. In direct collocation, the way this is done is to enforce the dynamics equation at the set of collocation points that are the mid-points between each consecutive pair of knot variables \\((x_k, x_{k+1})\\). Specifically, in each subinterval \\([t_k, t_{k+1}]\\), we approximate the state trajectory as a cubic polynomial \\[\\begin{equation} x(t) = p_{k,0} + p_{k,1}(t-t_k) + p_{k,2}(t-t_k)^2 + p_{k,3}(t - t_k)^3, \\quad t \\in [t_k, t_{k+1}], \\tag{3.17} \\end{equation}\\] where \\(p_{k,0},p_{k,1},p_{k,2},p_{k,3} \\in \\mathbb{R}^n\\) are the coefficients of the polynomial. You would think that we would need to optimize the coefficients as well, but actually we won’t need to, as will be shown soon. With this parameterization, we can obtain the time derivative of \\(x(t)\\) as \\[\\begin{equation} \\dot{x}(t) = p_{k,1} + 2 p_{k,2}(t-t_k) + 3p_{k,3}(t - t_k)^2, \\quad t \\in [t_k, t_{k+1}]. \\tag{3.18} \\end{equation}\\] Now the key step is to write the coefficients \\(p_{k,0},p_{k,1},p_{k,2},p_{k,3}\\) using our knot variables (3.16). To do so, we can invoke (3.17) and (3.18) to obtain \\[ \\begin{bmatrix} x_k \\\\ \\dot{x}_k = f(x_k,u_k) \\\\ x_{k+1} \\\\ \\dot{x}_{k+1} = f(x_{k+1}, u_{k+1}) \\end{bmatrix} = \\begin{bmatrix} I &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I &amp; 0 &amp; 0 \\\\ I &amp; h_k I &amp; h_k^2 I &amp; h_k^3 I \\\\ 0 &amp; I &amp; 2 h_k I &amp; 3 h_k^2 I \\end{bmatrix} \\begin{bmatrix} p_{k,0}\\\\ p_{k,1}\\\\ p_{k,2}\\\\ p_{k,3} \\end{bmatrix}. \\] Solving the above equation, we get \\[\\begin{equation} \\begin{bmatrix} p_{k,0}\\\\ p_{k,1}\\\\ p_{k,2}\\\\ p_{k,3} \\end{bmatrix} = \\begin{bmatrix} I &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I &amp; 0 &amp; 0 \\\\ - \\frac{3}{h_k^2} I &amp; - \\frac{2}{h_k} I &amp; \\frac{3}{h_k^2} I &amp; -\\frac{1}{h_k} I \\\\ \\frac{2}{h_k^3} I &amp; \\frac{1}{h_k^2} I &amp; -\\frac{2}{h_k^3} I &amp; \\frac{1}{h_k^2} I \\end{bmatrix} \\begin{bmatrix} x_k \\\\ f(x_k,u_k) \\\\ x_{k+1} \\\\ f(x_{k+1}, u_{k+1}) \\end{bmatrix}. \\tag{3.19} \\end{equation}\\] Equation (3.19) implies, using the knot variables \\(v\\) in (3.16), we can query the value of \\(x(t)\\) and \\(\\dot{x}(t)\\) at any time \\(t \\in [0,T]\\). In particular, we will query the values of \\(x(t)\\) and \\(\\dot{x}(t)\\) at the midpoints to obtain \\[ x_k^c = x\\left( t_k + \\frac{h_k}{2} \\right) = \\frac{1}{2} (x_k + x_{k+1}) + \\frac{h_k}{8} (f(x_k,u_k) - f(x_{k+1},u_{k+1})), \\] and \\[ \\dot{x}_k^c = \\dot{x} \\left( t_k + \\frac{h_k}{2} \\right) = - \\frac{3}{2h_k} (x_k - x_{k+1}) - \\frac{1}{4} \\left( f(x_k,u_k) + f(x_{k+1},u_{k+1}) \\right). \\] At the midpoint, we assume the control is \\[ u_k^c = \\frac{1}{2}(u_k + u_{k+1}). \\] Therefore, we can enforce the dynamics constraint at the midpoint as \\[\\begin{equation} \\dot{x}^c_k = f(x_k^c, u_k^c). \\tag{3.20} \\end{equation}\\] Transcribe other constraints. The other constraints in the continuous-time formulation (3.9) can be transcribed to the knot variables in a straightforward way: \\[\\begin{align} x_k \\in \\mathcal{X} \\Rightarrow c_i^x(x_k)\\geq 0, i=1,\\dots,q_x, \\quad k=1,\\dots,N \\\\ u_k \\in \\mathcal{X} \\Rightarrow c^u_i(u_k) \\geq 0, i=1,\\dots,q_u, \\quad k=0,\\dots,N-1\\\\ \\phi_i(x_k, u_k) \\geq 0, i=1,\\dots,q, \\quad k=0,\\dots,N. \\end{align}\\] Transcribe the objective. We can write the objective as \\[ g_T(x_N) + \\sum_{k=0}^{N-1} \\frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k. \\] Summary. In summary, the final optimization problem becomes \\[\\begin{equation} \\begin{split} \\min_{u_0,\\dots,u_{N-1},x_1,\\dots,x_N} &amp; \\quad g_T(x_N) + \\sum_{k=0}^{N-1} \\frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k \\\\ \\text{subject to} &amp; \\quad \\dot{x}_k^c = f(x_k^c,u_k^c), \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad c_i^x(x_k) \\geq 0,i=1,\\dots,q_x, \\quad k=1,\\dots,N \\\\ &amp; \\quad c_i^u(u_k) \\geq 0,i=1,\\dots,q_u, \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad \\phi_i (x_k,u_k) \\geq 0, i=1,\\dots,q, \\quad k=0,\\dots,N. \\end{split} \\tag{3.21} \\end{equation}\\] Let us apply direct collocation to the same double integrator Example 3.6. Example 3.7 (Direct Collocation for Minimum-Time Double Integrator) Consider the same minimum-time optimal control problem in Example 3.6. To apply direct collocation, we have our NLP variable \\[ v = (T,x_0,\\dots,x_N,u_0,\\dots,u_N). \\] Similarly we can enforce the control constraints and the initial / terminal constraints. The following script shows how to enforce the collocation constraint. function [c,ceq] = collocation(v,N,initial_state) T = v(1); h = T/(N-1); x = reshape(v(2:2*N+1),2,N); u = v(2*N+2:end); c = []; ceq = []; for k=1:N-1 uk = u(k); ukp1 = u(k+1); xk = x(:,k); xkp1 = x(:,k+1); fk = double_integrator(xk,uk); fkp1 = double_integrator(xkp1,ukp1); % collocation points xkc = 0.5*(xk+xkp1) + h/8 * (fk - fkp1); ukc = 0.5*(uk + ukp1); dxkc = -3/(2*h) * (xk-xkp1) - 0.25*(fk + fkp1); % collocation constraint ceq = [ceq; dxkc - double_integrator(xkc,ukc)]; end ceq = [ceq; x(:,1) - initial_state; % initial condition x(:,end)]; % land at zero end As you can see, we do not need any numerical integrator such as ode45. The only thing we need is the continuous-time double integrator dynamics. Running the complete code with \\(x_0 = (-10,0)\\) and \\(N=51\\), we get the control signal in Fig. 3.9 that is bang-bang. Figure 3.9: Optimal control signal by direct collocation. Using ode45 to integrate the double integrator dynamics from \\(x_0\\) with the controller in Fig. 3.9, we obtain the following state trajectory. Comparing Fig. 3.10 with Fig. 3.7, we can observe that the terminal state of the trajectory obtained from direct collocation is more accurate than that obtained from direct multiple shooting. Figure 3.10: ODE45 integration with the optimal control signal found by direct collocation. Not only is direct collocation more accurate for this example, it is also faster. This is evident because in direct multiple shooting, evaluating the nonlinear constraints requires runing ode45, while in direct collocation, evaluating the nonlinear constraints simply requires calling the original continuous-time dynamics. We can easily optimize with a larger \\(N=101\\) and get the following results. Figure 3.11: Direct collocation with N=101. If you are interested in direct collocation, there is a nice tutorial with Matlab examples in (Kelly 2017). 3.2.4 Direct Orthogonal Collocation 3.2.5 Failure of Open-Loop Control Trajectory optimization produces very satisfying trajectories when the nonlinear programming algorithms work as expected, as shown by the previous Examples 3.7 and 3.6 on the double integrator system. However, does this imply that if we simply run the planned controls on a real system, i.e., doing open-loop control, the trajectory will behave as planned? Unfortunately the answer is No, for two major reasons. Trajectory optimization only produces an approximate solution to the optimal control problem (OCP) (3.9), regardless of what transcription method is used (such as multiple shooting and direct collocation). This means every \\(u_k\\) we obtained is an approximation to the true optimal controller \\(u(t_k)\\), and the approximation errors will accumulate as the dynamical system is evolving. Even the OCP (3.9) is an imperfect approximation of the true control task. This is due to we assumed we have perfect knowledge of the system dynamics \\[ \\dot{x} = f(x(t),u(t)), \\] which rarely holds in practice. For example, in the double integrator example, there is always friction between the mass and the ground. A more realistic assumption is that the system dynamics is \\[ \\dot{x} = f(x(t),u(t)) + w_t, \\] where \\(w_t\\) is some unknown disturbance, or modeling error. Let us observe the failure of open-loop control on our favorite pendulum example. Example 3.8 (Failure of Open-Loop Control on A Simple Pendulum) Consider an “ideal” pendulum dynamics model \\[ x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix}, \\] with \\(m=1,l=1,g=9.8,b=0.1\\). We enforce control saturation \\[ u \\in [-u_{\\max}, u_{\\max}] \\] with \\(u_{\\max} = 4.9\\). Figure 3.12: Simple pendulum. Consider the optimal control problem with \\(T=10\\) \\[\\begin{equation} \\begin{split} \\min_{u(t), t \\in [0,T]} &amp; \\quad \\int_{t=0}^T (\\cos \\theta(t) + 1)^2 + \\sin^2\\theta(t) + \\dot{\\theta}^2(t) + u^2 dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(t),u(t)) \\\\ &amp; \\quad x(0) = [0,0]^T, \\quad x(T) = [\\pi,0]^T, \\\\ &amp; \\quad u(t) \\in [-u_{\\max},u_{\\max}], \\end{split} \\tag{3.22} \\end{equation}\\] where we start from the initial state \\([0,0]^T\\), i.e., the bottomright position, and we want to swing up the pendulum to the final state \\([\\pi,0]^T\\), i.e., the upright position. The cost function in (3.22) penalizes the deviation of the state trajectory from the target state (the target state has \\(\\cos \\theta = -1\\), \\(\\sin \\theta = 0\\) and \\(\\dot{\\theta} = 0\\)), together with the magnitude of control. Trajectory optimization with direct collocation. We perform trajectory optimization for the OCP (3.22) with direct collocation. We choose \\(N=101\\) break points with \\(h=0.1\\) equal interval to discretize the time. The result of trajectory optimization is shown in Fig. 3.13. We can see that the pendulum is perfectly swung up to \\([\\pi,0]^T\\) and stabilized there. Figure 3.13: Pendulum swing-up with direct collocation Deploy the optimized plan. We then deploy the optimized controls in Fig. 3.13 on the “ideal” pendulum. We deploy the control every \\(0.1\\) seconds with zero-order hold, and use Matlab ode89 to integrate the pendulum dynamics. Fig. 3.14 shows the true trajectory of the pendulum. Unfortunately, the pendulum swing-up and stabilization is unsuccessful. Figure 3.14: Deploy optimized controls Adding noise. What if the true pendulum dynamics is \\[ x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. \\] That is, there is a constant unmodelled angular acceleration of \\(1\\), which maybe come from some unknown external force. Fig. 3.15 shows the trajectory of the pendulum with the optimized controlls. We can clearly see the unmodelled dynamics makes the performance much worse. Figure 3.15: Deploy optimized controls on a system with disturbance You can play with the code here. The failure of open-loop control motivates feedback control. 3.2.6 LQR Trajectory Tracking 3.3 Model Predictive Control The model predictive control framework leverages the idea of receding horizon control (RHC) to turn an open-loop controller into a closed-loop controller, i.e., a feedback controller. For example, suppose we have an optimal control problem with horizon \\(T\\): \\[\\begin{equation} \\begin{split} \\min_{u(t), t \\in [0,T]} &amp; \\quad g_T(x(T)) + \\int_{t=0}^T g(x(t),u(t)) dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(t),u(t)), \\quad x(0) = x_0 \\\\ &amp; (x(t),u(t)) \\in \\mathcal{X} \\times \\mathcal{U}, \\quad x(T) \\in \\mathcal{X}_T \\end{split} \\tag{3.23} \\end{equation}\\] where \\(\\dot{x} = f(x(t),u(t))\\) is the best “ideal” model we have about our system. The idea of RHC is as follows. At time \\(t\\), we obtain a measurement of the current state of the system, denoted as \\(x_t\\), and we solve the following OCP with horizon \\(H &lt; T\\): \\[\\begin{equation} \\begin{split} \\min_{u(\\tau), \\tau \\in [0,H]} &amp; \\quad g_t(x(H)) + \\int_{\\tau = 0}^H g(x(\\tau),u(\\tau)) d\\tau \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(\\tau),u(\\tau)), \\quad x(0) = x_t \\\\ &amp; \\quad (x(t),u(t)) \\in \\mathcal{X} \\times \\mathcal{U}, \\quad x(H) \\in \\mathcal{X}_{t} \\end{split} \\tag{3.24} \\end{equation}\\] where notice that I used a different notation for the terminal cost \\(g_t\\) (as supposed to \\(g_T\\) in (3.23)), and the terminal constraint \\(\\mathcal{X}_t\\) (as supposed to \\(\\mathcal{X}_T\\) in (3.24)). Using a different terminal cost and terminal constraint is meant to make the problem (3.24) always feasible. Suppose we solve the open-loop control problem (3.24) using trajectory optimization with \\(N\\) time intervals and obtained the following solution \\[ u_{t,0},u_{t,1},\\dots,u_{t,N}. \\] Then RHS will only take the first control \\(u_{t,0}\\) and apply it to the system. Therefore, the closed-loop dynamics is effectively \\[\\begin{equation} \\dot{x} = f(x(t),u_{t,0}), \\tag{3.25} \\end{equation}\\] where \\(u_{t,0}\\) is the first control solution to the OCP (3.24). Figure 3.16: Receding horizon control It is worth noting that RHS effectively introduces feedback control, because the controller \\(u_{t,0}\\) in (3.25) depends on \\(x(t)\\) via the open-loop optimal control problem (3.24). Trajectory optimization and model predictive control are the workhorse for control of highly dynamic robots. For example, here is a talk by Scott Kuindersma on how trajectory optimization and model predictive control enable a diverse set of behaviors of Boston Dynamics’s humanoid robot. 3.3.1 Turn Trajectory Optimization into Feedback Control Let us turn our trajectory optimization algorithm in Example 3.8 into a feedback controller through receding horizon control. Example 3.9 (Pendulum Swingup with Model Predictive Control) Consider again the pendulum swingup task in Example 3.8. This time, we will apply MPC to this task. Again, we discretize the time windown \\([0,T]\\) into \\(N-1 = 100\\) equal intervals with \\(h = 0.1\\). At each timestep \\(t_k = kh\\), \\(k=0,\\dots,N-1\\), we first measure the current state of the pendulum as \\(x_k\\), then solve the following OCP with planning horizon \\(H=5\\) \\[\\begin{equation} \\begin{split} \\min_{u(t), t \\in [0,H]} &amp; \\quad \\int_{t=0}^H (\\cos \\theta(t) + 1)^2 + \\sin^2\\theta(t) + \\dot{\\theta}^2(t) + u^2 dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(t),u(t)) \\\\ &amp; \\quad x(0) = x_k, \\quad x(H) = [\\pi,0]^T, \\\\ &amp; \\quad u(t) \\in [-u_{\\max},u_{\\max}]. \\end{split} \\tag{3.26} \\end{equation}\\] We solve the OCP (3.26) using trajectory optimization with direct collocation (using \\(M\\) break points), which gives us a sequence of controls \\[ u_{k,1},\\dots,u_{k,M}, \\] we deploy the first control \\(u_{k,1}\\) to the pendulum system. MPC results. Fig. 3.17 shows the control and state trajectories when using MPC to swing up the pendulum. We can see that it works very well. Figure 3.17: MPC for pendulum swing-up Robustness to noise. MPC is naturally robust to modelling errors and noises. When the true system dynamics is \\[ x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. \\] The MPC controller still works very well, as shown in Fig. 3.18! Figure 3.18: MPC for pendulum swing-up with noise Feel free to play with the code here. Software tools. Matlab implements a nice package for nonlinear MPC, you can refer to the documentation and examples. In Python, the do-mpc package is quite popular. However, it is important to recognize that trajectory optimization for the nonconvex open-loop optimal control problen (3.24) (a) only guarantees a locally optimal solution and hence can be brittle, (b) can be time-consuming when the problem is high-dimensional. In fact, MPC was originally developed in the context of chemical plant control (Borrelli, Bemporad, and Morari 2017), which has linear system dyanmics, as we will introduce soon. Before we introduce the MPC formulation and understand its theoretical properties, we need to take a detour and introduce various important notions of sets. 3.3.2 Controllability, Reachability, and Invariance Consider the autonomous system \\[\\begin{equation} x_{t+1} = f_a(x_t) \\tag{3.27} \\end{equation}\\] and the controlled system \\[\\begin{equation} x_{t+1} = f(x_t, u_t) \\tag{3.28} \\end{equation}\\] for \\(t \\geq 0\\). Both systems are subject to state and input constraints \\[\\begin{equation} x_t \\in \\mathcal{X}, \\quad u_t \\in \\mathcal{U}, \\forall t \\geq 0, \\tag{3.29} \\end{equation}\\] with \\(\\mathcal{X}\\) and \\(\\mathcal{U}\\) both being polyhedral sets (see Definition B.9) \\[\\begin{equation} \\begin{split} x_t \\in \\mathcal{X} = \\left\\{ x \\in \\mathbb{R}^n \\mid C x \\leq c \\right\\}, \\quad C \\in \\mathbb{R}^{l_x \\times n}, c \\in \\mathbb{R}^{l_x}, \\\\ u_t \\in \\mathcal{U} = \\left\\{ u \\in \\mathbb{R}^m \\mid D u \\leq d \\right\\}, \\quad D \\in \\mathbb{R}^{l_u \\times m}, d \\in \\mathbb{R}^{l_u}. \\end{split} \\tag{3.30} \\end{equation}\\] We are going to make a few definitions. 3.3.2.1 Controllable and Reachable Sets Definition 3.1 (Precursor Set) For the autonomous system (3.27), we define the precursor set to a set \\(\\mathcal{S}\\) as \\[ \\text{Pre}(\\mathcal{S}) = \\left\\{ x \\in \\mathbb{R}^n \\mid f_a(x) \\in \\mathcal{S} \\right\\}. \\] In words, \\(\\text{Pre}(\\mathcal{S})\\) is the set of states which evolve into the target set \\(\\mathcal{S}\\) in one time step. For the controlled system (3.28), we define the precursor set to the set \\(\\mathcal{S}\\) as \\[ \\text{Pre}(\\mathcal{S}) = \\left\\{ x \\in \\mathbb{R}^n : \\exists u \\in \\mathcal{U} \\text{ s.t. } f(x,u) \\in \\mathcal{S} \\right\\}. \\] In words, \\(\\text{Pre}(\\mathcal{S})\\) is the set of states that can be driven into the target set \\(\\mathcal{S}\\) while satisfying control and state constraints. A related concept is the successor set. Definition 3.2 (Successor Set) For the autonomous system (3.27), we define the successor set to a set \\(\\mathcal{S}\\) as \\[ \\text{Suc}(\\mathcal{S}) = \\left\\{ x \\in \\mathbb{R}^n \\mid \\exists x&#39; \\in \\mathcal{S} \\text{ s.t. } x = f_a(x&#39;)\\right\\}. \\] In words, the states in \\(\\mathcal{S}\\) get mapped into the states in \\(\\text{Suc}(\\mathcal{S})\\) after one time step. For the controlled system (3.28), we define the successor set to the set \\(\\mathcal{S}\\) as \\[ \\text{Suc}(\\mathcal{S}) = \\left\\{ x \\in \\mathbb{R}^n \\mid \\exists x&#39; \\in \\mathcal{S}, \\exists u&#39; \\in \\mathcal{U} \\text{ s.t. } x = f(x&#39;,u&#39;). \\right\\} \\] In words, the states in \\(\\mathcal{S}\\) get mapped into the states in \\(\\text{Suc}(\\mathcal{S})\\) after one time step while satisfying the control constraints. The \\(N\\)-step controllable set is defined by iterating \\(\\text{Pre}(\\cdot)\\) computations. Definition 3.3 (N-Step Controllable Set) Let \\(\\mathcal{S} \\subseteq \\mathcal{X}\\) be a target set. The \\(N\\)-step controllable set \\(\\mathcal{K}_{N}(\\mathcal{S})\\) of the system (3.27) or (3.28) subject to the constraints (3.29) is defined recursively as: \\[ \\mathcal{K}_0 (\\mathcal{S}) = \\mathcal{S}, \\quad \\mathcal{K}_{i}(\\mathcal{S}) = \\text{Pre}(\\mathcal{K}_{i-1}(\\mathcal{S})) \\cap \\mathcal{X}, i=1,\\dots,N. \\] According to this definition, given any \\(x_0 \\in \\mathcal{K}_N(\\mathcal{S})\\): For the autonomous system (3.27), its state will evolve into the target set \\(\\mathcal{S}\\) in \\(N\\) steps while satisfying state constraints For the controlled system (3.28), there exists a sequence of admissible controls (i.e., satisfying control constraints) such that its state will evolve into the target set \\(\\mathcal{S}\\) in \\(N\\) steps while satisfying state constraints. Similarly, the \\(N\\)-step reachable set is defined by iterating \\(\\text{Suc}(\\cdot)\\) computations. Definition 3.4 (N-Step Reachable Set) Let \\(\\mathcal{X}_0 \\subseteq \\mathcal{X}\\) be an initial set. The \\(N\\)-step reachable set \\(\\mathcal{R}_N (\\mathcal{X}_0)\\) of the system (3.27) or (3.28) subject to the constraints (3.29) is defined recursively as: \\[ \\mathcal{R}_0(\\mathcal{X}_0) = \\mathcal{X}_0, \\quad \\mathcal{R}_{i}(\\mathcal{X}_0) = \\text{Suc}(\\mathcal{R}_{i-1}(\\mathcal{X}_0)) \\cap \\mathcal{X}, i=1,\\dots,N. \\] According to this definition, given any \\(x_0 \\in \\mathcal{X}_0\\): For the autonomous system (3.27), its state will evolve into \\(\\mathcal{R}_N(\\mathcal{X}_0)\\) in \\(N\\) steps while satisfying state constraints For the controlled system (3.28), there exists a sequence of admissible controls (i.e., satisfying control constraints) such that its state will evolve into \\(\\mathcal{R}_N(\\mathcal{X}_0)\\) in \\(N\\) steps while satisfying state constraints. In the literature, the controllable set is often referred to as the backwards reachable set. 3.3.2.2 Computation of Controllable and Reachable Sets For a linear time-invariant system, when the state constraint set \\(\\mathcal{X}\\), control constraint set \\(\\mathcal{U}\\), the target set \\(\\mathcal{S}\\), and the initial set \\(\\mathcal{X}_0\\) are all polytopes, then the \\(N\\)-step controllable set \\(\\mathcal{K}_{N}(\\mathcal{S})\\) and the \\(N\\)-step reachable set \\(\\mathcal{R}_{N}(\\mathcal{X}_0)\\) are both polytopes and can be computed exactly and efficiently. We will not describe the underlying algorithm for computing \\(\\mathcal{K}_{N}(\\mathcal{S})\\) and \\(\\mathcal{R}_{N}(\\mathcal{X}_0)\\) (these details can be found in (Borrelli, Bemporad, and Morari 2017) and they are not very difficult to understand, so I suggest you to read them), but we now show you how to use the Multi-Parametric Toolbox (MPT) to compute them. Example 3.10 (Compute Controllable and Reachable Sets) Consider a linear system \\[ x_{t+1} = \\begin{bmatrix} 1.5 &amp; 0 \\\\ 1 &amp; -1.5 \\end{bmatrix} x_t + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} u_t \\] with state and control constraints \\[ \\mathcal{X} = [-10,10]^2, \\quad \\mathcal{U} = [-5,5]. \\] Given \\(\\mathcal{S} = \\mathcal{X}_0 = [-1,1]^2\\), I want to compute \\(\\mathcal{K}_i(\\mathcal{S})\\) and \\(\\mathcal{R}_i(\\mathcal{X}_0)\\), for \\(i=0,1,2,3,4\\). Dynamical system. We first define the linear time-invariant system as follows. A = [1.5, 0; 1, -1.5]; B = [1; 0]; sys = LTISystem(&#39;A&#39;,A,&#39;B&#39;,B); We then define the state and control constraints. calX = Polyhedron(&#39;A&#39;,... [1,0;0,1;-1,0;0,-1], ... &#39;b&#39;,[10;10;10;10]); calU = Polyhedron(&#39;A&#39;,[1;-1],&#39;b&#39;,[5;5]); Note that in the MPT toolbox, to define a polytope \\(P = \\{ x \\in \\mathbb{R}^n \\mid A x \\leq b \\}\\), we just need to call the Polyhedron function with inputs A and b. Controllable sets. We have from Definition 3.3 the recursion \\[ \\mathcal{K}_0(\\mathcal{S}) = \\mathcal{S}, \\quad \\mathcal{K}_i(\\mathcal{S}) = \\text{Pre}(\\mathcal{K}_{i-1}(\\mathcal{S})) \\cap \\mathcal{X}. \\] To implement the above set operation, we need two functions, one for the “\\(\\cap\\)” intersection operation, and the other for the “\\(\\text{Pre}(\\cdot)\\)” precursor set operation. These two functions are both available through MPT. intersect(P,S) computes the intersection of two sets P and S, both defined as polytopes. system.reachableSet('X', X, 'U', U, 'N', 1, 'direction', 'backwards') computes the one-step backwards reachable set (which is just the precursor set) of the system with target set X and control constraints U. Using these two functions, the following code snippet computes recursively the controllable sets for \\(N\\) steps. % target set S = Polyhedron(&#39;A&#39;,[1,0;0,1;-1,0;0,-1],&#39;b&#39;,[1;1;1;1]); K = [S]; N = 4; for i = 1:N Si = sys.reachableSet(&#39;X&#39;,K(i),&#39;U&#39;,calU,&#39;N&#39;,1,&#39;direction&#39;,&#39;backwards&#39;); K = [K; intersect(Si,calX)]; end Fig. 3.19 plots the controllable sets computed by running the above code. Figure 3.19: Computation of controllable sets using MPT. The bottom plot shifts the sets horizontally for better visualization. Reachable sets. We have Definition 3.4 the recursion \\[ \\mathcal{R}_0(\\mathcal{X}_0) = \\mathcal{X}_0, \\quad \\mathcal{R}_i(\\mathcal{X}_0) = \\text{Suc}(\\mathcal{R}_{i-1}(\\mathcal{X}_0)) \\cap \\mathcal{X}. \\] To implement the recursion above, we need “\\(\\cap\\)” set intersection, which is available via intersect(P,S), and the “\\(\\text{Suc}(\\cdot)\\)” successor set operation, which is available as system.reachableSet('X', X, 'U', U, 'N', 1, 'direction', 'forward') computes the one-step forward reachable set (which is just the successor set) of the system with initial set X and control constraints U. Therefore, we can compute the reachable sets recursively using the following code snippet % initial set X0 = Polyhedron(&#39;A&#39;,[1,0;0,1;-1,0;0,-1],&#39;b&#39;,[1;1;1;1]); R = [X0]; N = 4; for i = 1:N Ri = sys.reachableSet(&#39;X&#39;,R(i),&#39;U&#39;,calU,&#39;N&#39;,1,&#39;direction&#39;,&#39;forward&#39;); R = [R; intersect(Ri,calX)]; end Fig. 3.20 plots the reachable sets computed by running the above code. Feel free to play with the code here. Note that you need to install the MPT toolbox before running the code. Figure 3.20: Computation of reachable sets using MPT. The bottom plot shifts the sets horizontally for better visualization. 3.3.2.3 Invariant Sets Definition 3.5 (Positive Invariant Set) A set \\(\\mathcal{O} \\subseteq \\mathcal{X}\\) is said to be a positive invariant set for the autonomous system (3.27) subject to the constraints in (3.29) if \\[ x_0 \\in \\mathcal{O} \\Longrightarrow x_t \\in \\mathcal{O}, \\forall t \\geq 0. \\] That is, if the system starts in \\(\\mathcal{O}\\), it stays in \\(\\mathcal{O}\\) for all future timesteps. The maximal positive invariant set is the largest positive invariant set. Definition 3.6 (Maximal Positive Invariant Set) A set \\(\\mathcal{O}_{\\infty} \\subseteq \\mathcal{X}\\) is said to be the maximal positive invariant set for the autonomous system (3.27) subject to the constraints in (3.29) if (a) \\(\\mathcal{O}_{\\infty}\\) is positive invariant and (b) \\(\\mathcal{O}_{\\infty}\\) contains all the invariant sets contained in \\(\\mathcal{X}\\). Essentially, the (maximal) positive invariant set is the set of initial conditions under which the system does not blow up. For the controlled system (3.28), we have the similar notion of a control invariant set. Definition 3.7 (Control Invariant Set) A set \\(\\mathcal{C} \\subseteq \\mathcal{X}\\) is said to be a control invariant set for the controlled system (3.28) subject to the constraints in (3.29) if \\[ x_0 \\in \\mathcal{C} \\Longrightarrow \\exists u_t \\in \\mathcal{U} \\text{ s.t. } f(x_t,u_t) \\in \\mathcal{C}, \\forall t \\geq 0 \\] That is, if the system starts in \\(\\mathcal{C}\\), it can be controlled to stay in \\(\\mathcal{C}\\) for all future time steps. The maximal control invariant set is the largest control invariant set. Definition 3.8 (Maximal Control Invariant Set) A set \\(\\mathcal{C}_{\\infty} \\subseteq \\mathcal{X}\\) is said to be the maximal control invariant set for the controlled system (3.28) subject to the constraints in (3.29) if (a) \\(\\mathcal{C}_{\\infty}\\) is control invariant, and (b) \\(\\mathcal{C}_{\\infty}\\) contains all control invariant sets contained in \\(\\mathcal{X}\\). Essentially, the (maximal) control invariant set is the set of initial conditions under which the system can be controlled to not blow up. We now state a necessary and sufficient condition for a set to be positive invariant and control invariant. Theorem 3.1 (Geometric Condition for Invariance) For the autonomous system (3.27) subject to the constraint (3.29), a set \\(\\mathcal{O} \\subseteq \\mathcal{X}\\) is positive invariant if and only if \\[\\begin{equation} \\mathcal{O} \\subseteq \\text{Pre}(\\mathcal{O}). \\tag{3.31} \\end{equation}\\] Similarly, for the controlled system (3.28) subject to the constraint (3.29), a set \\(\\mathcal{C}\\) is control invariant if and only if \\[\\begin{equation} \\mathcal{C} \\subseteq \\text{Pre}(\\mathcal{C}). \\tag{3.32} \\end{equation}\\] Proof. We only prove (3.31) since (3.32) can be proved using similar arguments. “\\(\\Leftarrow\\)”: we want to show \\(\\mathcal{O}\\) is positive invariant if (3.31) holds. We prove this by contradiction. If \\(\\mathcal{O}\\) is not positive invariant, the \\(\\exists \\hat{x} \\in \\mathcal{O}\\) such that \\(f_a(\\hat{x}) \\not\\in \\mathcal{O}\\). This implies we have found \\(\\hat{x} \\in \\mathcal{O}\\) but \\(\\hat{x} \\not\\in \\text{Pre}(\\mathcal{O})\\), contradicting (3.31). “\\(\\Rightarrow\\)”: we want to show (3.31) holds if \\(\\mathcal{O}\\) is positive invariant. We prove this by contradiction. Suppose (3.31) does not hold, then \\(\\exists \\hat{x}\\) such that \\(\\hat{x} \\in \\mathcal{O}\\) but \\(\\hat{x} \\not\\in \\text{Pre}(\\mathcal{O})\\). This implies we have found \\(\\hat{x} \\in \\mathcal{O}\\) that does not remain in \\(\\mathcal{O}\\) in the next step, contradicting \\(\\mathcal{O}\\) being positive invariant. This shows that (3.31) is a sufficient and necessary condition. Theorem 3.1 immediately suggests an algoithm for computing (control) invariant sets, as we will describe in the next section. 3.3.2.4 Computation of Invariant Sets Observe that the geometric conditions (3.31) and (3.32) are equivalent to the following conditions \\[ \\mathcal{O} = \\text{Pre}(\\mathcal{O}) \\cap \\mathcal{O}, \\quad \\mathcal{C} = \\text{Pre}(\\mathcal{C}) \\cap \\mathcal{C}. \\] Based on the equation above, we can design an algoithm that iteratively evaluates \\(\\text{Pre}(\\mathcal{\\Omega}) \\cap \\mathcal{\\Omega}\\) until it converges. Algorithm: Computation of \\(\\mathcal{O}_{\\infty}\\) Input: \\(f_a\\), \\(\\mathcal{X}\\) Output: \\(\\mathcal{O}_{\\infty}\\)    \\(\\Omega_0 \\leftarrow \\mathcal{X}\\), \\(k=0\\)    Repeat      \\(\\Omega_{k+1} \\leftarrow \\text{Pre}(\\Omega_k) \\cap \\Omega_k\\)      \\(k \\leftarrow k+1\\)    Until \\(\\Omega_{k+1} = \\Omega_k\\).    \\(\\mathcal{O}_{\\infty} \\leftarrow \\Omega_k\\) The algorithm above generates a sequence of sets \\(\\{ \\Omega_k \\}\\) satisfying \\(\\Omega_{k+1} \\subseteq \\Omega_k\\) for any \\(k\\), and it terminates when \\(\\Omega_{k+1} = \\Omega_k\\). If it terminates, then \\(\\Omega_k\\) is the maximal positive invariant set \\(\\mathcal{O}_{\\infty}\\). If \\(\\Omega_k = \\emptyset\\) for some intege \\(k\\) then \\(\\mathcal{O}_{\\infty} = \\emptyset\\). In general, the algoithm above may never terminate. If the algoithm does not terminate in a finite number of iterations, then it can be proven that (Kolmanovsky, Gilbert, et al. 1998) \\[ \\mathcal{O}_{\\infty} = \\lim_{k \\rightarrow \\infty} \\Omega_k. \\] Conditions for finite time termination of the algoithm can be found in (Gilbert and Tan 1991). A simple sufficient condition requires the dynamics \\(f_a\\) to be linear and stable, and the constraint set \\(\\mathcal{X}\\) to be bounded and contain the origin. The same algoithm can be used to compute the maximal control invariant set \\(\\mathcal{C}_{\\infty}\\). Algorithm: Computation of \\(\\mathcal{C}_{\\infty}\\) Input: \\(f\\), \\(\\mathcal{X}\\), \\(\\mathcal{U}\\) Output: \\(\\mathcal{C}_{\\infty}\\)    \\(\\Omega_0 \\leftarrow \\mathcal{X}\\), \\(k=0\\)    Repeat      \\(\\Omega_{k+1} \\leftarrow \\text{Pre}(\\Omega_k) \\cap \\Omega_k\\)      \\(k \\leftarrow k+1\\)    Until \\(\\Omega_{k+1} = \\Omega_k\\).    \\(\\mathcal{C}_{\\infty} \\leftarrow \\Omega_k\\) Similarly, the above algoithm generates \\(\\{\\Omega_k \\}\\) such that \\(\\Omega_{k+1} \\subseteq \\Omega_k\\) for any \\(k\\). If the algoithm terminates, then \\(\\Omega_k = \\mathcal{C}_{\\infty}\\). In general, the algoithm may never terminate. If the algoithm does not terminate, then in general convergence is not guaranteed \\[ \\mathcal{C}_{\\infty} \\neq \\lim_{k \\rightarrow \\infty} \\Omega_k. \\] The work in (Bertsekas 1972) reports examples of nonlinear systems for which the above equation can be observed. A sufficient condition for the convergence of \\(\\Omega_k\\) to \\(\\mathcal{C}_{\\infty}\\) as \\(k \\rightarrow \\infty\\) requires the polyhedral sets \\(\\mathcal{X}\\) and \\(\\mathcal{U}\\) to be bounded and the system \\(f(x,u)\\) to be continuous (Bertsekas 1972). Let us apply the algoithm to compute the maximal control invariant set for the linear system in Example 3.10. Example 3.11 (Computation of the Maximal Control Invariant Set) Consider the linear system in Example 3.10 with same state constraint and control constraint. The following code snippet shows how to apply the iterative algorithm introduced above to compute the maximal control invariant set. Omega = [calX]; while true last_Omega = Omega(end); pre_omega = sys.reachableSet(&#39;X&#39;,last_Omega,&#39;U&#39;,calU,&#39;N&#39;,1,... &#39;direction&#39;,&#39;backwards&#39;); new_Omega = intersect(pre_omega,last_Omega); Omega = [Omega;new_Omega]; if new_Omega == last_Omega fprintf(&quot;Converged to maximal control invariant set.\\n&quot;); break; end end The algoithm converges in 37 iterations and Fig. 3.21 plots the sequence of sets generated by the algorithm. Figure 3.21: Maximal control invariant set. The MPT toolbox actually implements this algoithm for us to use directly. If we use C = sys.invariantSet(&#39;X&#39;, calX, &#39;U&#39;, calU); we get the same result as before. You can play with the code here. 3.3.3 Basic Formulation for Linear Systems We are now ready to introduce the basic formulation of MPC for linear systems and study its theoretical properties. Consider the problem of regulating the following discrete-time linear system to the origin \\[\\begin{equation} x_{t+1} = A x_t + B u_t, \\tag{3.33} \\end{equation}\\] where the state \\(x_t\\) and control \\(u_t\\) are constrained to lie in polyhedral sets \\(\\mathcal{X}\\) and \\(\\mathcal{U}\\), respectively. We assume \\(\\mathcal{X}\\) contains the origin \\(0\\). We can formulate the following optimal control problem to regulate the system to the origin \\[\\begin{equation} \\begin{split} J^\\star(x_0) = \\min_{u_t,t=0,\\dots} &amp; \\quad \\sum_{t=0}^{\\infty} x_t^T Q x_t + u_t^T R u_t \\\\ \\text{subject to} &amp; \\quad x_{t+1} = A x_t + B u_t, \\\\ &amp; \\quad (u_t,x_t) \\in \\mathcal{U} \\times \\mathcal{X}, \\forall t = 0,\\dots \\end{split} \\tag{3.34} \\end{equation}\\] with \\(Q \\succeq 0, R \\succ 0\\). Had we not included the constraints \\((u_t,x_t) \\in \\mathcal{U} \\times \\mathcal{X}\\), then problem (3.34) is exactly the infinite-horizon LQR problem in Section 2.1.1, for which we know the optimal controller is \\(u_t = - K x_t\\) with \\(K\\) computed in closed-form as (2.12). However, in the presence of constraints, problem (3.34) does not admit a simple closed-form solution. In fact, problem (3.34) is commonly referred to as the contrained LQR (CLQR) problem and it is known that the optimal controller is piece-wise affine, see Theorem 11.4 in (Borrelli, Bemporad, and Morari 2017). We have asked you to numerically play with a toy example of CLQR in Exercise 9.2. Receding horizon control. Leveraging the receding horizon control framework, we can approach problem (3.34) by online solving convex optimization problems. At time \\(t\\), suppose we can measure the current state of the system \\(x_t\\), then we solve the following optimal control problem with a finite horizon \\(N\\) \\[\\begin{equation} \\begin{split} J_t^\\star(x_t) = \\min_{u(0),\\dots,u(N-1)} &amp; \\quad p(x(N)) + \\sum_{k=0}^{N-1} q(x(k),u(k)) \\\\ \\text{subject to} &amp; \\quad x(k+1) = A x(k) + B u(k), k =0, \\dots, N-1, \\quad x(0) = x_t \\\\ &amp; \\quad (x(k),u(k)) \\in \\mathcal{X} \\times \\mathcal{U}, k=0,\\dots,N-1 \\\\ &amp; \\quad x(N) \\in \\mathcal{X}_f, \\end{split} \\tag{3.35} \\end{equation}\\] and \\(p(x)\\), \\(q(x,u)\\) convex functions. For example, a simple choice is \\(p(x) = x^T P x\\) and \\(q = x^T Q x + u^T R u\\) with \\(P,Q\\succeq 0\\) and \\(R \\succ 0\\). I hope you could pay attention to the notation in (3.35). I used \\(x_t, u_t\\) to denote the state and control for the original linear system (3.33) at time \\(t\\), as well as in the CLQR problem (3.34). However, in every subproblem (3.35) of RHS at time \\(t\\), I used \\(x(k),u(k)\\), with \\(k\\) as the time step, to denote the state and control in the finite-horizon optimal control problem starting at \\(x_t\\), with \\(x_t = x(0)\\) (\\(k\\) is the shifted time horizon in RHS that always starts at zero). In addition to the difference in notation between (3.35) and (3.34), problem (3.35) is also different in the following two ways: Terminal cost \\(p(x)\\): in the objective of problem (3.35), there is an additional terminal cost \\(p(x(N))\\). Terminal constraint set \\(\\mathcal{X}_f\\): problem (3.35) has an additional constraint that the final state \\(x(N)\\) must belong to the set \\(\\mathcal{X}_f\\). We assume \\(\\mathcal{X}_f\\) is also polyheral (and convex). Feasible sets. We denote as \\(\\mathcal{X}_0 \\subseteq \\mathcal{X}\\) the set of initial states \\(x_t\\) such that the RHC subproblem (3.35) is feasible, i.e., \\[\\begin{equation} \\hspace{-14mm} \\begin{split} \\mathcal{X}_0 = \\{ x(0) \\in \\mathbb{R}^n \\mid \\exists (u(0),\\dots,u(N-1)) \\text{ such that } x(k) \\in \\mathcal{X},u(k) \\in \\mathcal{U},k=0,\\dots,N-1, \\\\ x(N) \\in \\mathcal{X}_f \\text{ with } x(k+1) = A x(k) + B u(k), k=0,\\dots,N-1 \\}. \\end{split} \\tag{3.36} \\end{equation}\\] Similarly, we denote as \\(\\mathcal{X}_i\\) the set of states such that the RHC subproblem is feasible from step \\(k=i\\): \\[\\begin{equation} \\hspace{-14mm} \\begin{split} \\mathcal{X}_i = \\{ x(i) \\in \\mathbb{R}^n \\mid \\exists (u(i),\\dots,u(N-1)) \\text{ such that } x(k) \\in \\mathcal{X}, u(k) \\in \\mathcal{U}, k=i,\\dots,N-1, \\\\ x(N) \\in \\mathcal{X}_f \\text{ with } x(k+1) = A x(k) + B u(k), k=i,\\dots,N-1 \\}. \\end{split} \\tag{3.37} \\end{equation}\\] Clearly, by definition we have \\[ \\mathcal{X}_N = \\mathcal{X}_f, \\] and \\[ \\mathcal{X}_i = \\{ x \\in \\mathcal{X} \\mid \\exists u \\in \\mathcal{U} \\text{ such that } A x + Bu \\in \\mathcal{X}_{i+1} \\},i=0,\\dots,N-1, \\] or written in a compact way as \\[\\begin{equation} \\mathcal{X}_i = \\text{Pre}(\\mathcal{X}_{i+1}) \\cap \\mathcal{X}. \\tag{3.38} \\end{equation}\\] Note that from equation (3.38) we have that, if we pick \\(x_i \\in \\mathcal{X}_i\\) and let \\(U(x_i)\\) be the set of feasible controls at \\(x_i\\), then pick any \\((u(0),\\dots,u(N-1)) \\in U(x_i)\\) and apply \\(u(0)\\) to the system to get \\(x_{i+1} = A x_i + B u(0)\\), we have \\(x_{i+1} \\in \\mathcal{X}_{i+1}\\). Since the definitions (3.36) and (3.37) are rather abstract, let us visualize them using the double integrator example. Example 3.12 (Double Integrator RHC Feasible Sets) Consider the discrete-time double integrator dynamics \\[ x_{t+1} = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} x_t + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u_t, \\] subject to control constraint \\[ u \\in \\mathcal{U} = [-0.5,0.5], \\] and state constraint \\[ x \\in \\mathcal{X} = [-5, 5] \\times [-5, 5]. \\] We use \\(N=3\\) and visualize the feasible sets (3.36) and (3.37) for two choices of \\(\\mathcal{X}_f\\). Choice 1. \\(\\mathcal{X}_f = \\mathcal{X}\\) is the full state space. We use the recursion (3.38) to compute the feasible sets \\(\\mathcal{X}_i\\) for \\(i=0,1,2,3\\). Fig. 3.22 plots the feasible sets. Observe that in this case \\(\\mathcal{X}_0 \\subset \\mathcal{X}_1 \\subset \\mathcal{X}_2 \\subset \\mathcal{X}_3\\). This creates a concern: suppose the RHC starts at \\(x_0 \\in \\mathcal{X}_0\\) that is feasible, in the next iteration we have \\(x_1 \\in \\mathcal{X}_1\\). However, since \\(\\mathcal{X}_1\\) is larger than \\(\\mathcal{X}_0\\), \\(x_1\\) is not guaranteed to be feasible. Figure 3.22: Feasible sets of the double integrator receding horizon controller without terminal constraint. Choice 2. \\(\\mathcal{X}_f = \\{(0,0)\\}\\) is the origin. Fig. 3.23 plots the feasible sets. Observe that in this case \\(\\mathcal{X}_3 \\subset \\mathcal{X}_2 \\subset \\mathcal{X}_1 \\subset \\mathcal{X}_0\\). This is a nice case, because if RHC starts at \\(x_0 \\in \\mathcal{X}_0\\) that is feasible, in the next iteration we have \\(x_1 \\in \\mathcal{X}_1 \\subset \\mathcal{X}_0\\), which implies that \\(x_1\\) is guaranteed to remain feasible! In fact, as we will soon show, in the first choice the RHC does suffer from infeasibility. Figure 3.23: Feasible sets of the double integrator receding horizon controller with terminal constraint. The code for this example can be found here. Algorithm. Above all, problem (3.35) is a convex optimization problem that we know how to solve efficiently (you have solved such convex optimization problems in Exercise 9.2). Let \\(u^\\star(0),\\dots,u^\\star(N-1)\\) be the optimal solution of problem (3.35) when it is feasible, the RHS framework will only apply the first control \\(u^\\star(0)\\) to the system, and hence the closed-loop system is \\[\\begin{equation} x_{t+1} = A x_t + u^\\star_{x_t}(0) = f_{\\mathrm{cl}}(x_t), \\tag{3.39} \\end{equation}\\] where I have used \\(u^\\star_{x_t}(0)\\) to make it explicit that the control is the first optimal control of solving (3.35) with an initial state \\(x_t\\). The following algorithm summarizes the receding horizon control algorithm. Algorithm: Online Receding Horizon Control Input: State \\(x_t\\) at time \\(t\\) Output: Control \\(u^\\star_{x_t}(0)\\) Solve problem (3.35) to get the optimal controls \\(u^\\star(0), \\dots, u^\\star(N-1)\\) if the problem is infeasible, then stop else return \\(u^\\star_{x_t}(0) = u^\\star(0)\\) RHC main questions. Two main questions arise regarding the RHC controller. Persistent feasibility. If the RHC algorithm starts at a state \\(x_0\\) for which the convex optimization (3.35) is feasible, i.e., \\(x_0 \\in \\mathcal{X}_0\\), will the convex optimization (3.35) remain feasible for all future time steps? Stability. Assuming the convex optimization is always feasible. Will the closed-loop system (3.39) (induced by the RHC controller) converge to the desired origin \\(x=0\\)? Let us use a couple of examples to illustrate that, in general, the answers to the above two questions are both NO. Example 3.13 (Receding Horizon Control for Double Integrator) Consider the discrete-time double integrator dynamics \\[ x_{t+1} = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} x_t + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u_t, \\] subject to control constraint \\[ u \\in \\mathcal{U} = [-0.5,0.5], \\] and state constraint \\[ x \\in \\mathcal{X} = [-5, 5] \\times [-5, 5]. \\] In the RHS subproblem (3.35), we use \\(N = 3\\), \\(p(x) = x^T P x\\), \\(q(x,u) = x^T Q x + u^T R u\\) with \\(P = Q = I\\), \\(R=10\\), and \\(\\mathcal{X}_f = \\mathbb{R}^2\\) (i.e., there is not terminal constraint). The subproblem is implemented using CVX in Matlab. Fig. 3.24 shows the state trajectory of executing RHC starting at \\(x_0 = [-4.5;2]\\) and \\(x_0 = [-4.5;3]\\), respectively. We can see that when the initial state is \\(x_0 = [-4.5;2]\\), the trajectory successfully converges to the origin (the blue line). However, when the initial state is \\(x_0 = [-4.5;3]\\), RHC fails in the third iteration because the subproblem becomes infeasible (the red line). You can find code for this example here. Figure 3.24: Receding horizon control for the double integrator with two initial states. We now show another example, adapted from (Borrelli, Bemporad, and Morari 2017) where the design of \\(N\\), \\(p(x)\\) and \\(q(x,u)\\) affects the closed-loop performance. Example 3.14 (RHC Performance Affected by Parameters) Consider the system \\[ x_{t+1} = \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 0.5 \\end{bmatrix} x_t + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} u_t, \\] with control constraint \\[ u \\in \\mathcal{U} = [-1,1], \\] and state constraint \\[ x \\in \\mathcal{X} = [-10,10] \\times [-10,10]. \\] In the RHC problem (3.35), we choose \\(p(x) = x^T P x\\), \\(q(x,u) = x^T Q x + u^T R u\\). We fix \\(P=0\\), \\(Q = I\\), and \\(\\mathcal{X}_f = \\mathbb{R}^2\\), but vary \\(N\\) and \\(R\\): Setting 1: \\(N=2\\), \\(R = 10\\); Setting 2: \\(N=3\\), \\(R = 2\\); Setting 3: \\(N=4\\), \\(R=1\\). Fig. 3.25 shows the closed-loop trajectories of three different settings. As we can see, the closed-loop performance depends on the parameters in a very complicated manner. Figure 3.25: Closed-loop trajectories for different settings of horizon N and weight R. Boxes (circles) are initial points leading to feasible (infeasible) closed-loop trajectories. 3.3.4 Persistent Feasibility Under what conditions can we guarantee the RHC subproblem (3.35) is always feasible? Intuitively, we will show that, by designing the terminal constraint set \\(\\mathcal{X}_f\\), we can guarantee the configuration of the feasible sets will look like choice 2 in Example 3.12. There are various sets here of interest for answering this question. \\(\\mathcal{C}_{\\infty}\\): The maximal control invariant set \\(\\mathcal{C}_{\\infty}\\) is only affected by the system dynamics (3.33) and the constraint sets \\(\\mathcal{X} \\times \\mathcal{U}\\). It is the largest set over which we can expect any controller to work, because otherwise the system trajectory will blow up. \\(\\mathcal{X}_0\\): The set of states at which the RHC subproblem (3.35) is feasible. The set \\(\\mathcal{X}_0\\) depends on the system dynamics, the constraint sets \\(\\mathcal{X} \\times \\mathcal{U}\\), as well as the RHC horizon \\(N\\) and the terminal constraint set \\(\\mathcal{X}_f\\). It is worth noting that it does not depend on the objective function in (3.35) (i.e., \\(P,Q,R\\)). \\(\\mathcal{O}_{\\infty}\\): The maximal positive invariant set for the closed-loop system (3.39) induced by the RHC control law. This depends on the RHC controller and hence it depends on the system dynamics, the constraint set \\(\\mathcal{X} \\times \\mathcal{U}\\), \\(N\\), \\(\\mathcal{X}_f\\) and the objective function of (3.35) \\(P,Q,R\\). A few relationships between these sets are easy to observe. \\(\\mathcal{O}_{\\infty} \\subseteq \\mathcal{X}_0\\). Any state \\(x \\in \\mathcal{O}_{\\infty}\\) needs to stay in \\(\\mathcal{O}_{\\infty}\\) for all future time steps. Thus, the subproblem (3.35) must be feasible for any \\(x \\in \\mathcal{O}_{\\infty}\\), otherwise the closed-loop system (3.39) is not even well defined. \\(\\mathcal{O}_{\\infty} \\subseteq \\mathcal{C}_{\\infty}\\). The set \\(\\mathcal{O}_{\\infty}\\) is control invariant because there exists a controller (specifically, the RHC controller) that makes it positive invariant. Therefore, \\(\\mathcal{O}_{\\infty}\\) must belong to the maximal control invariant set \\(\\mathcal{C}_{\\infty}\\). We can now state necessary and sufficient conditions guaranteeing persistent feasibility. Lemma 3.1 (Sufficient and Necessary Condition for Persistent Feasibility) The RHC subproblem (3.35) is persistently feasible if and only if \\(\\mathcal{X}_0 = \\mathcal{O}_{\\infty}\\). Proof. We have already argued that \\(\\mathcal{O}_{\\infty} \\subseteq \\mathcal{X}_0\\). It remains to show \\(\\mathcal{X}_0 \\subseteq \\mathcal{O}_{\\infty}\\). By definition, \\(\\mathcal{X}_0\\) is persistently feasible implies that \\[ x \\in \\mathcal{X}_0 \\Longrightarrow f_{\\mathrm{cl}}^t(x) \\in \\mathcal{X}_0, \\forall t \\] where \\(f_{\\mathrm{cl}}^t\\) means applying the closed-loop dynamics \\(f_{\\mathrm{cl}}\\) in (3.39) \\(t\\) times. This shows that \\(\\mathcal{X}_0\\) is a positive invariant set for the closed-loop system, and hence \\(\\mathcal{X}_0 \\subseteq \\mathcal{O}_{\\infty}\\). We argued that \\(\\mathcal{X}_0\\) does not depend on the RHC parameters \\(P,Q,R\\) but \\(\\mathcal{O}_{\\infty}\\) does. Therefore, in general only some \\(P,Q,R\\) are allowed for persistent feasibility to hold. Due to the complicated relationship between \\(P,Q,R\\) and \\(\\mathcal{O}_{\\infty}\\), it is generally difficult to design \\(P,Q,R\\) such that RHC has persistent feasibility. We now state a sufficient condition for persistent feasibility to hold. Lemma 3.2 (Sufficient Condition for Persistent Feasibility) Consider the RHC subproblem (3.35) with \\(N \\geq 1\\). If \\(\\mathcal{X}_1\\) is a control invariant set for the linear system (3.33), then the RHC controller is persistently feasible. Proof. If \\(\\mathcal{X}_1\\) is control invariant, then by definition \\(\\mathcal{X}_1 \\subseteq \\text{Pre}(\\mathcal{X}_1)\\). Since \\(\\mathcal{X}_1 \\subseteq \\mathcal{X}\\), we have \\[ \\mathcal{X}_1 \\subseteq \\text{Pre}(\\mathcal{X}_1) \\cap \\mathcal{X} = \\mathcal{X}_0. \\] where the last equality in the equation above is due to (3.38). Now pick any \\(x_0 \\in \\mathcal{X}_0\\), the set of controls that make \\(x_0\\) feasible is denoted as \\[ U = \\{ u(0),\\dots,u(N-1) \\mid x(0)=x_0, x(k) \\in \\mathcal{X},k=0,\\dots,N-1,x(N) \\in \\mathcal{X}_f \\}. \\] The RHC will pick some control sequence from \\(U\\), say \\(\\hat{u}(0),\\dots,\\hat{u}(N-1)\\) and apply the first control \\(\\hat{u}(0)\\) to the system, which will bring the system to a new state \\[ x_1 = A x_0 + B \\hat{u}(0). \\] Observe that \\(x_1 \\in \\mathcal{X}_1\\) by definition. Since \\(\\mathcal{X}_1 \\subseteq \\mathcal{X}_0\\), we have \\(x_1 \\in \\mathcal{X}_0\\). This proves that the RHC is persistently feasible, i.e., starting with any \\(x_0 \\in \\mathcal{X}_0\\), the RHC subproblem (3.35) is always feasible. Lemma 3.2 states that if \\(\\mathcal{X}_1\\) is control invariant, then the RHC subproblem is persistently feasible. An immediate result of this Lemma is that when \\(N=1\\), then \\(\\mathcal{X}_1 = \\mathcal{X}_f\\). Therefore, if we choose the terminal constraint set \\(\\mathcal{X}_f\\) to be control invariant, then RHC is has persistent feasibility. The next theorem states that when \\(N \\geq 1\\), as long as we choose the terminal constraint set \\(\\mathcal{X}_f\\) to be control invariant, then persistent feasibility also holds. Theorem 3.2 (Control Invariant Terminal Constraint Set Guarantees Persistent Feasibility) Consider the RHC subproblem (3.35) with \\(N \\geq 1\\). If \\(\\mathcal{X}_f\\) is a control invariant set for the linear system (3.33), then the RHC controller is persistently feasible. Proof. We will prove that \\(\\mathcal{X}_f\\) being control invariant implies \\[ \\mathcal{X}_{N-1}, \\mathcal{X}_{N-2},\\dots,\\mathcal{X}_1 \\] are all control invariant, and then by Lemma 3.2, we can guarantee persistent feasibility. Fig. 3.26 shows the nested control invariant sets when \\(\\mathcal{X}_f\\) is control invariant. Figure 3.26: Nested control invariant sets. It suffices to show \\(\\mathcal{X}_{i+1}\\) being control invariant leads to \\(\\mathcal{X}_i\\) being control invariant. First, by \\(\\mathcal{X}_{i+1}\\) control invariant, we have \\(\\mathcal{X}_{i+1} \\subseteq \\text{Pre}(\\mathcal{X}_i) \\cap \\mathcal{X} = \\mathcal{X}_i\\). Now pick any \\(x_{i} \\in \\mathcal{X}_{i}\\), for any feasible \\(\\hat{u}(0),\\dots,\\hat{u}(N-1)\\), applying the first control \\(\\hat{u}(0)\\) brings the system to a new state \\[ x_{i+1} = A x_i + B \\hat{u}(0) \\in \\mathcal{X}_{i+1} \\subseteq \\mathcal{X}_i. \\] This shows \\(\\mathcal{X}_i\\) is control invariant. Persistent feasibility does not guarantee the closed-loop system will converge to the origin. In fact, from Theorem 3.2, it is clear that if the system starts at \\(x_0 \\in \\mathcal{X}_0\\), then we can only guarantee \\(x_t \\in \\mathcal{X}_1\\) for all \\(t \\geq 0\\). 3.3.5 Stability We now answer the question of how can we guarantee the RHC controller will drive the system to the desired origin. Theorem 3.3 (Sufficient Condition for Stability) Consider the linear system (3.33), and the RHC algorithm (3.35). Assume that The stage cost \\(q(x,u)\\) and terminal cost \\(p(x)\\) are continuous and positive definite functions. The sets \\(\\mathcal{X}\\), \\(\\mathcal{X}_f\\) and \\(\\mathcal{U}\\) contain the origin in their interior and are closed. \\(\\mathcal{X}_f \\subseteq \\mathcal{X}\\) is control invariant. For any \\(x \\in \\mathcal{X}_f\\), the following inequality holds \\[\\begin{equation} \\min_{u \\in \\mathcal{U}, A x + Bu \\in \\mathcal{X}_f} \\left( - p(x) + q(x,u) + p(Ax + Bu) \\right) \\leq 0. \\tag{3.40} \\end{equation}\\] Then, the origin of the closed-loop system (3.39) is asymptotically stable with domain of attraction \\(\\mathcal{X}_0\\). In words, for any \\(x_0 \\in \\mathcal{X}_0\\), if the closed-loop system (3.39) starts at \\(x_0\\), then the system trajectory converges to the origin as \\(t\\) tends to infinity. Let us interpret Theorem 3.3 before proving it. It should be clear that the first three assumptions are easy to satisfy. For example, if we choose \\(p(x) = x^T P x\\) and \\(q(x,u) = x^T Q x + u^T R u\\) with \\(P, Q, R \\succ 0\\), then assumption 1 is satisfied. Usually \\(\\mathcal{X}\\) and \\(\\mathcal{U}\\) are both polyhedral sets containing the origin in the interior, so assumption 2 also holds naturally. Finding a control invariant set \\(\\mathcal{X}_f\\) is not a trivial task but there exists numerical algorithms for this task (e.g., using the algorithms introduced in Section 3.3.2.4). After we find a control invariant \\(\\mathcal{X}_f\\), by Theorem 3.2, we know persistent feasibility will hold. We now prove the theorem by showing that Assumption 4 guarantees stability. Before we show the proof, we need the concept of a Lyapunov function. Below we introduce Lyapunov function for a discrete-time dynamical system, but we will study more details of Lyapunov function for continuous-time dynamical systems in Chapter 5. Lemma 3.3 (Discrete-time Lyapunov Function) Consider the discrete-time autonomous system (3.27), restated below for convenience: \\[ x_{t+1} = f_a(x_t), \\] and assume \\(x=0\\) is an equilibrium point of the system, i.e., \\(0 = f_a (0)\\) (if the system starts at the origin, it stays at the origin). Let \\(\\Omega \\subset \\mathbb{R}^n\\) be a closed and bounded set containing the origin. If there exists a function \\(V: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) that is (a) continuous at the origin, (b) finite for every \\(x \\in \\Omega\\), and (c) satisfies \\(V\\) is positive definite: \\(V(0) = 0\\) and \\(V(x) &gt; 0,\\forall x \\in \\Omega \\backslash \\{ 0\\}\\), \\(V(x_{t+1}) - V(x_t) \\leq - \\alpha(x_t) &lt; 0\\) for any \\(x_t \\in \\Omega \\backslash \\{ 0\\}\\), where \\(\\alpha: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is a continuous positive definite function. Then \\(x=0\\) is asymptotically stable in \\(\\Omega\\), i.e, if the system starts within \\(\\Omega\\), then its trajectory tends to \\(0\\) as \\(t \\rightarrow \\infty\\). A function \\(V\\) satisfying the conditions above is called a Lyapunov function for the system. One can think of the Lyapunov function \\(V\\) as an energy function for the system (3.27) that maps a system state to a single scalar. Condition 1 in Lemma 3.3 states that \\(V\\) (the energy function) is strictly positive except at the origin. Condition 2 in Lemma 3.3 states that, the system energy \\(V\\) is strictly decreasing along any system trajectory. Therefore, we conclude that \\(V\\) must converge to \\(V=0\\), and hence any state trajectory must converge to the origin. We will now use Lemma 3.3 to prove RHC stability under Theorem 3.3. Proof. Our goal is to prove that the optimal cost of the RHC problem (3.35), \\(J_t^\\star(x_t)\\), is a Lyapunov function for the closed-loop system (3.39) on the domain \\(\\mathcal{X}_0\\). Then using Lemma 3.3 we can conclude that the closed-loop system (3.39) will converge to the origin. Positive definite. Clearly, we have \\(J_t^\\star(0) = 0\\) by the positive definiteness of \\(p\\) and \\(q\\) and the fact that \\(x=0\\) is an equilibrium point of the linear system (3.33). For \\(x_t \\neq 0\\), it is also clear that \\(J_t^\\star(x) &gt; 0\\). Therefore, \\(J_t^\\star(x)\\) is positive definite on \\(\\mathcal{X}_0\\). Strictly decrease. It suffices to show \\(J_1^\\star(x_1) - J_0^\\star(x_0) &lt; 0\\) because the constraints of the RHC problem (3.35) is time-invariant. Pick any \\(x_0 \\in \\mathcal{X}_0\\), and let \\[\\begin{equation} \\left( u^\\star(0),u^\\star(1),\\dots,u^\\star(N-1) \\right) \\tag{3.41} \\end{equation}\\] be an optimal control trajectory to problem (3.35). Denote \\[\\begin{equation} \\left( x_0,x(1),\\dots,x(N) \\right) \\tag{3.42} \\end{equation}\\] as the associated optimal state trajectory for problem (3.35). The RHC controller will apply \\(u^\\star(0)\\) to the system, leading to the next state \\[ x_1 = x(1) = A x_0 + B u^\\star(0). \\] Then we will solve problem (3.35) to get \\(J^\\star_1(x_1)\\), and we want to show \\(J^\\star_1(x_1) &lt; J^\\star_0(x_0)\\). Towards this, we will construct a feasible solution to (3.35) starting at \\(x_1\\), and hence an upper bound on \\(J^\\star_1(x_1)\\). Consider the control sequence \\[\\begin{equation} \\left( u^\\star(1),\\dots,u^\\star(N-1),v \\right) \\tag{3.43} \\end{equation}\\] which is different from the control sequence in (3.41) by removing \\(u^\\star(0)\\) and appending \\(v\\). The corresponding state trajectory to (3.43) is \\[\\begin{equation} \\left( x_1 = x(1), x(2),\\dots,x(N), Ax(N) + Bv \\right) \\tag{3.44} \\end{equation}\\] which is different from the state trajectory in (3.42) by removing \\(x_0\\) and appending \\(A x(N) + Bv\\). Since \\(x(N) \\in \\mathcal{X}_f\\) and \\(\\mathcal{X}_f\\) is control invariant, there exists \\(v\\) such that \\(Ax(N) + Bv \\in \\mathcal{X}_f\\) and the corresponding control trajectory (3.43) is feasible. Applying the control trajectory (3.43) to the optimization problem (3.35) will lead to the total cost \\[ J_1(x_1) = J_0^\\star(x_0) - q(x_0,u^\\star(0)) \\underbrace{- p(x(N)) + q(x(N),v) + p(Ax(N) + Bv)}_{s(x(N),v)}. \\] Now by assumption 4 in Theorem 3.3, we can choose \\(v\\) such that the sum \\(s(x(N),v) \\leq 0\\). Consequently, we have \\[ J_1^\\star(x_1) \\leq J_1(x_1) \\leq J_0^\\star(x_0) - q(x_0,u^\\star(0)), \\] which leads to \\[ J_1^\\star(x_1) - J_0^\\star(x_0) \\leq - q(x_0,u^\\star(0)). \\] Because the choice of \\(x_0\\) was arbitrary, we conclude that \\(J_t^\\star(x_t)\\) strictly decreases along any system trajectory that starts within \\(\\mathcal{X}_0\\). Continuity at the origin. We will show that \\(J_0^\\star(x) \\leq p(x)\\) for any \\(x \\in \\mathcal{X}_f\\). With this argument, since \\(p(x)\\) is positive definite and continuous, then \\(J_0^\\star(x)\\) must be continuous at the origin. We now prove \\(J_0^\\star(x) \\leq p(x)\\) for any \\(x \\in \\mathcal{X}_f\\). Since \\(\\mathcal{X}_f\\) is control invariant, pick any \\(x \\in \\mathcal{X}_f\\), there exists a sequence of controls \\((u(0),u(1),\\dots,u(N-1))\\) such that the state trajectory \\((x(0)=x,x(1),\\dots,x(N))\\) stays in \\(\\mathcal{X}_f\\). Such a control sequence lead to an upper bound on \\(J^\\star_0(x)\\): \\[ J^\\star_0(x(0)) \\leq p(x(N)) + \\sum_{i=0}^{N-1} q(x(i),u(i)) = p(x(0)) + \\sum_{i=0}^{N-1} \\left( q(x(i),u(i)) + p(x(i+1)) - p(x(i)) \\right), \\] since each \\(x(i) \\in \\mathcal{X}_f\\), according to Assumption 4, we can choose \\(u(i)\\) such that \\[ J^\\star_0(x(0)) \\leq p(x(0)) \\] for any \\(x(0) \\in \\mathcal{X}_f\\). In conclusion, we have shown that \\(J^\\star_t(x_t)\\) is a Lyapunov function, and by Lemma 3.3, the closed-loop system is asymptotically stable. A function \\(p(x)\\) that satisfies Assumption 4 in Theorem 3.3 is typicalled known as a control Lyapunov function. Now two natural problems arise: Given \\(p(x)\\), how to verify if assumption 3 holds in Theorem 3.3? How to synthesize a \\(p(x)\\) that satisfies assumption 3 in Theorem 3.3? Unfortunately, both problems are hard. To see this, suppose we are given a candidate function \\(p(x) = x^T P x\\) with \\(P \\succ 0\\) that is clearly positive definite. Assume \\(q(x,u) = x^T Q x + u^T R u\\) with \\(Q, R \\succ 0\\) and \\(\\mathcal{U}, \\mathcal{X}_f\\) are given polyhedral sets. Then verifying if \\(p(x)\\) satisifies Assumption 4 boils down to checking if \\[ \\min_{u \\in \\mathcal{U}, Ax + Bu \\in \\mathcal{X}_f} x^T Q x + u^T R u + (Ax + Bu)^T P (Ax + Bu) - x^T P x \\] is non-positive for any possible \\(x \\in \\mathcal{X}_f\\). Although for each possible \\(x\\), the above problem is a convex optimization problem, there are an infinite number of points in the set \\(\\mathcal{X}_f\\) and enumerating over all points is a daunting task. We will see in Chapter 5 that convex relaxations, in particular semidefinite relaxations, can help us partially solve these hard problems. A simple control lyapunov function. One can solve the infinite-horizon unconstrained LQR problem \\[ \\min_{u_t} \\sum_{t=0}^{\\infty}x_t ^T Q x_t + u_t^T R u_t, \\] for which the optimal cost-to-go is \\[ J_{\\infty}(x) = x^T S x, \\] with \\(S\\) the solution to the algebraic Riccati equation (2.13). Denote \\[ u_t = \\Pi_{\\mathcal{U}} (- K x_t) \\] as the optimal controller (\\(\\Pi_{\\mathcal{U}}\\) is the projection of the controller to the feasible set \\(\\mathcal{U}\\)), and use \\(\\mathcal{X}_f\\) as the maximal positive invariant set of the closed-loop system \\[ x_{t+1} = A x_t + B u_t. \\] Then \\(J_{\\infty}(x) = x^T S x\\) is a control Lyapunov function over the set \\(\\mathcal{X}_f\\). 3.3.6 Explicit MPC See the original paper (Bemporad et al. 2002), and check out Matlab’s explict MPC design. 3.4 Policy Gradient References Bemporad, Alberto, Manfred Morari, Vivek Dua, and Efstratios N Pistikopoulos. 2002. “The Explicit Linear Quadratic Regulator for Constrained Systems.” Automatica 38 (1): 3–20. Bertsekas, Dimitri. 1972. “Infinite Time Reachability of State-Space Regions by Using Feedback Control.” IEEE Transactions on Automatic Control 17 (5): 604–13. Borrelli, Francesco, Alberto Bemporad, and Manfred Morari. 2017. Predictive Control for Linear and Hybrid Systems. Cambridge University Press. Gilbert, Elmer G, and K Tin Tan. 1991. “Linear Systems with State and Control Constraints: The Theory and Application of Maximal Output Admissible Sets.” IEEE Transactions on Automatic Control 36 (9): 1008–20. Kelly, Matthew. 2017. “An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation.” SIAM Review 59 (4): 849–904. Kolmanovsky, Ilya, Elmer G Gilbert, et al. 1998. “Theory and Computation of Disturbance Invariant Sets for Discrete-Time Linear Systems.” Mathematical Problems in Engineering 4: 317–67. Levine, Nir, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. 2017. “Shallow Updates for Deep Reinforcement Learning.” Advances in Neural Information Processing Systems 30. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. Nocedal, Jorge, and Stephen J Wright. 1999. Numerical Optimization. Springer. Yang, Alan, and Stephen Boyd. 2023. “Value-Gradient Iteration with Quadratic Approximate Value Functions.” arXiv Preprint arXiv:2307.07086. "],["continuous-time-optimal-control.html", "Chapter 4 Continuous-time Optimal Control 4.1 The Basic Problem 4.2 The Hamilton-Jacobi-Bellman Equation 4.3 Linear Quadratic Regulator 4.4 The Pontryagin Minimum Principle 4.5 Infinite-Horizon Problems 4.6 Viscosity Solution", " Chapter 4 Continuous-time Optimal Control So far we have been focusing on stochastic and discrete-time optimal control problems. In this Chapter, we will switch gear to deterministic and continuous-time optimal control (still with continuous state and action space). The goal of a continuous-time introduction is threefold. (1) Real-world systems are natively continuous-time. (2) We will see the continuous-time analog of the Bellman principle of optimality in discrete-time (cf. Theorem 1.1). (3) The continuous-time setup is more natural and popular for stability analysis to be introduced in Chapter 5. 4.1 The Basic Problem Consider a continuous-time dynamical system \\[\\begin{equation} \\dot{x}(t) = f(x(t),u(t)),\\ t \\in [0,T], \\quad x(0) = x_0, \\tag{4.1} \\end{equation}\\] where \\(x(t) \\in \\mathbb{R}^n\\) is the state of the system, \\(u(t) \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) is the control we wish to design, \\(f: \\mathbb{R}^{n} \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\) models the system dynamics, and \\(x_0 \\in \\mathbb{R}^n\\) is the initial state of the system. We assume the admissible control functions \\(\\{u(t) \\mid u(t) \\in \\mathbb{U}, t\\in [0,T] \\}\\), also called control trajectories, must be piecewise continuous.4 For any control trajectory, we assume the system (4.1) has a unique solution \\(\\{x(t)\\mid t \\in [0,T] \\}\\), called the state trajectory. We now state the continuous-time optimal control problem. Definition 4.1 (Continuous-time, Finite-horizon Optimal Control) Find the best admissible control trajectory \\(\\{u(t) \\mid t \\in [0,T] \\}\\) that minimizes the cost function \\[\\begin{equation} J(0,x_0) = \\min_{u(t) \\in \\mathbb{U}} h(x(T)) + \\int_0^T g(x(t),u(t)) dt, \\tag{4.2} \\end{equation}\\] subject to (4.1), where the functions \\(g\\) and \\(h\\) are continuously differentiable with respect to \\(x\\), and \\(g\\) is continuous with respect to \\(u\\). The function \\(J\\) in (4.2) is called the optimal cost-to-go, or the optimal value function. Notice that the optimal cost-to-go is a function of both the state \\(x\\) and the time \\(t\\), just as in the discrete-time case we used \\(J_k\\) with a subscript \\(k\\) to denote the optimal cost-to-go for the tail problem starting at timestep \\(k\\) (cf. Theorem 1.2). Specifically, we should interpret \\[ J(t,x_0) = \\min_{u(t) \\in \\mathbb{U}} h(x(T)) + \\int_t^T g(x(\\tau),u(\\tau)) d\\tau, \\quad x(t) = x_0, \\] as the optimal cost-to-go of the system starting from \\(x_0\\) at time \\(t\\) (i.e., the tail problem). We assume \\(J(0,x_0)\\) is finite when \\(x_0\\) is in some set \\(\\mathcal{X}_0\\). 4.2 The Hamilton-Jacobi-Bellman Equation Recall that in discrete-time, the dynamic programming (DP) algorithm in Theorem 1.2 states that the optimal cost-to-go has to satisfy a recursive equation (1.5), i.e., the optimal cost-to-go at time \\(k\\) can be calculated by choosing the best action that minimizes the stage cost at time \\(k\\) plus the optimal cost-to-go at time \\(k+1\\). In the next, we will show a result of similar flavor to (1.5), but in the form of a partial differential equation (PDE), known as the Hamilton-Jacobi-Bellman (HJB) equation. Let us informally derive the HJB equation by applying the DP algorithm to a discrete-time approximation of the continuous-time optimal control problem. We divide the time horizon \\([0,T]\\) into \\(N\\) pieces of equal length \\(\\delta = T/N\\), and denote \\[ x_k = x(k\\delta), \\quad u_k = u(k \\delta), \\quad k = 0,1,\\dots,N. \\] We then approximate the continuous-time dynamics (4.1) as \\[ x_{k+1} = x_k + \\dot{x}_k \\cdot \\delta = x_k + f(x_k,u_k) \\cdot \\delta, \\] and the cost function in (4.2) as \\[ h(x_N) + \\sum_{k=0}^{N-1} g(x_k, u_k)\\cdot \\delta. \\] This problem now is in the form of a discrete-time, finite-horizon optimal control 1.1, for which we can apply dynamic programming. Let us use \\(\\tilde{J}(t,x)\\) (as opposed to \\(J(t,x)\\)) to denote the optimal cost-to-go at time \\(t\\) and state \\(x\\) for the discrete-time approximation. According to (1.5), the DP backward recursion is \\[\\begin{align} \\tilde{J}(N\\delta,x) = h(x), \\\\ \\tilde{J}(k\\delta,x) = \\min_{u \\in \\mathbb{U}} \\left[ g(x,u)\\cdot \\delta + \\tilde{J}((k+1)\\delta,x + f(x,u)\\cdot \\delta) \\right], \\quad k = N-1,\\dots,0. \\tag{4.3} \\end{align}\\] Suppose \\(\\tilde{J}(t,x)\\) is differentiable, we can perform a Taylor-series expansion of \\(\\tilde{J}((k+1)\\delta,x+f(x,u)\\delta)\\) in (4.3) as follows \\[ \\tilde{J}((k+1)\\delta,x+f(x,u)\\delta) = \\tilde{J}(k\\delta,x) + \\nabla_t \\tilde{J} (k\\delta,x) \\cdot \\delta + \\nabla_x \\tilde{J}(k\\delta,x)^T f(x,u) \\cdot \\delta + o(\\delta), \\] where \\(o(\\delta)\\) includes high-order terms that approach zero when \\(\\delta\\) tends to zero, \\(\\nabla_t \\tilde{J}\\) and \\(\\nabla_x \\tilde{J}\\) (a column vector) denote the partial derivates of \\(\\tilde{J}\\) with respect to \\(t\\) and \\(x\\), respectively. Plugging the first-order Taylor expansion into the DP recursion (4.3), we obtain \\[\\begin{equation} \\tilde{J}(k\\delta,x) = \\min_{u \\in \\mathbb{U}} \\left[ g(x,u) \\cdot \\delta + \\tilde{J}(k \\delta,x) + \\nabla_t \\tilde{J}(k \\delta,x) \\delta + \\nabla_x \\tilde{J}(k\\delta,x)^T f(x,u) \\delta + o(\\delta) \\right]. \\tag{4.4} \\end{equation}\\] Cancelling \\(\\tilde{J}(k \\delta,x)\\) from both sides, dividing both sides by \\(\\delta\\), and assuming \\(\\tilde{J}\\) converges to \\(J\\) uniformly in time and state, i.e., \\[ \\lim_{\\delta \\rightarrow 0, k\\delta = t} \\tilde{J}(k\\delta,x) = J(t,x), \\quad \\forall t,x, \\] we obtain from (4.4) the following partial differential equation \\[\\begin{equation} 0 = \\min_{u \\in \\mathbb{U}} \\left[ g(x,u) + \\nabla_t J(t,x) + \\nabla_x J(t,x)^T f(x,u) \\right], \\quad \\forall t, x, \\tag{4.5} \\end{equation}\\] with the boundary condition \\(J(T,x) = h(x)\\). Equation (4.5) is called the Hamilton-Jacobi-Bellman equation. Our derivation above is informal, let us formally state the HJB equation. Theorem 4.1 (Hamilton-Jacobi-Bellman Equation as A Sufficient Condition for Optimality) Consider the optimal control problem 4.1 for system (4.1). Suppose \\(V(t,x)\\) is a solution to the Hamilton-Jacobi-Bellman equation, i.e., \\(V(t,x)\\) is continuously differentiable and satisfies \\[\\begin{align} \\tag{4.6} 0 = \\min_{u \\in \\mathbb{U}} \\left[ g(x,u) + \\nabla_t V(t,x) + \\nabla_x V(t,x)^T f(x,u)\\right], \\quad \\forall t,x, \\\\ V(T,x) = h(x), \\quad \\forall x. \\end{align}\\] Suppose \\(\\mu^\\star(t,x)\\) attains the minimum in (4.6) for all \\(t\\) and \\(x\\). Let \\(\\{x^\\star(t) \\mid t \\in [0,T] \\}\\) be the state trajectory obtained from the given initial condition \\(x(0)\\) when the control trajectory \\(u^\\star(t) = \\mu^\\star(t,x^\\star(t))\\) is applied, i.e., \\(x^\\star(0) = x(0)\\) and for any \\(t \\in [0,T]\\), \\(\\dot{x}^\\star(t) = f(x^\\star(t), \\mu^\\star(t,x^\\star(t)))\\) and we assume this differential equation has a unique solution starting at any \\((t,x)\\) and that the control trajectory \\(\\{ \\mu^\\star(t,x^\\star(t)) \\mid t \\in [0,T] \\}\\) is piecewise continuous as a function of \\(t\\). Then \\(V(t,x)\\) is equal to the optimal cost-to-go \\(J(t,x)\\) for all \\(t\\) and \\(x\\). Moreover, the control trajectory \\(u^\\star(t)\\) is optimal. Proof. Let \\(\\{\\hat{u}(t) \\mid t \\in [0,T] \\}\\) be any admissible control trajectory and let \\(\\hat{x}(t)\\) be the resulting state trajectory. From the “\\(\\min\\)” in (4.6), we know \\[ 0 \\leq g(\\hat{x},\\hat{u}) + \\nabla_t V(t,\\hat{x}) + \\nabla_x V(t,\\hat{x})^T f(\\hat{x},\\hat{u}) = g(\\hat{x},\\hat{u}) + \\frac{d}{dt} V(t,\\hat{x}). \\] Integrating the above inequality over \\(t \\in [0,T]\\), we obtain \\[ 0 \\leq \\left( \\int_{0}^T g(\\hat{x}(t),\\hat{u}(t))dt \\right) + V(T,\\hat{x}(T)) - V(0,\\hat{x}(0)). \\] Using the terminal constraint \\(V(T,x) = h(x)\\) for any \\(x\\) and the initial condition \\(\\hat{x}(0) = x(0)\\), we have \\[ V(0,x(0)) \\leq h(\\hat{x}(T)) + \\int_{0}^T g(\\hat{x}(t),\\hat{u}(t)) dt. \\] This shows that \\(V(0,x(0))\\) is a lower bound to the optimal cost-to-go, because any admissible control trajectory \\(\\hat{u}(t)\\) leads to a cost no smaller than \\(V(0,x(0))\\). It remains to show that \\(V(0,x(0))\\) is attainable. This is done by plugging the optimal control trajectory \\(u^\\star(t)\\) and state trajectory \\(x^\\star(t)\\) to the derivation above, leading to \\[ V(0,x(0)) = h(x^\\star(T)) + \\int_0^T g(x^\\star(t),u^\\star(t)) dt. \\] This shows that \\(V(0,x(0)) = J(0,x(0))\\). The argument above is generic and holds for any initial time \\(t \\in [0,T]\\) and initial state \\(x\\). Therefore, \\(V(t,x) = J(t,x)\\) is the optimal cost-to-go. Theorem 4.1 effectively turns the optimal control problem (4.2) into finding a solution for the partial differential equation (4.6). Let us illustrate the theorem using a simple example. Example 4.1 (A Scalar System) Consider the following dynamical system \\[ \\dot{x}(t) = u(t), \\quad t \\in [0,T] \\] where \\(x \\in \\mathbb{R}\\) is the state, and \\(u \\in \\mathbb{U} = [-1,1]\\) is the control. We are interested in the following optimal control problem \\[ \\min_{u(t)} \\frac{1}{2} \\left( x(T) \\right)^2, \\] where the goal is to move the initial state as close as possible to the origin \\(0\\) at the terminal time \\(T\\). There is a simple optimal controller for this scalar system. We move the state as quickly as possible to the origin \\(0\\), using maximum control, and then maintain the state at the origin using zero control. Formally, this is \\[ \\mu^\\star(t,x) = - \\text{sgn}(x) = \\begin{cases} 1 &amp; \\text{if } x &lt; 0 \\\\ 0 &amp; \\text{if } x = 0 \\\\ -1 &amp; \\text{if } x &gt; 0 \\end{cases}. \\] With this controller, we know that if the system starts at \\(x\\) at time \\(t\\), the terminal state will satisfy \\[ \\vert x(T) \\vert = \\begin{cases} |x| - (T - t) &amp; \\text{if } T-t &lt; |x| \\\\ 0 &amp; \\text{otherwise} \\end{cases}. \\] As a result, the optimal cost-to-go is \\[\\begin{equation} J(t,x) = \\frac{1}{2} \\left( \\max\\{0, |x| - (T - t)\\} \\right)^2. \\tag{4.7} \\end{equation}\\] Let us verify if this function satisfies the HJB equation. Boundary condition. Clearly, \\[ J(T,x) = \\frac{1}{2}x^2 \\] satisfies the boundary condition. Differentiability. When viewed as a function of \\(t\\), \\(J(t,x)\\) in (4.7) can be plotted as in Fig. 4.1. We can see that \\(J(t,x)\\) is differentiable in \\(t\\) and \\[\\begin{equation} \\nabla_t J(t,x) = \\max\\{ 0, |x| - (T-t) \\}. \\tag{4.8} \\end{equation}\\] Figure 4.1: Optimal cost-to-go as a function of time. When viewed as a function of \\(x\\), \\(J(t,x)\\) can be plotted as in Fig. 4.2. We can see \\(J(t,x)\\) is differentiable in \\(x\\) and \\[\\begin{equation} \\nabla_x J(t,x) = \\text{sgn}(x) \\cdot \\max\\{ 0,|x| - (T-t) \\}. \\tag{4.9} \\end{equation}\\] Figure 4.2: Optimal cost-to-go as a function of state. PDE. Substituting (4.8) and (4.9) into the HJB equation (4.6), we need to verify that the followng equation holds for all \\(t\\) and \\(x\\) \\[\\begin{equation} 0 = \\min_{u \\in \\mathbb{U}} (1 + \\text{sgn}(x) \\cdot u) \\max\\{0, |x| - (T-t) \\}. \\tag{4.10} \\end{equation}\\] This is easy to verify as \\(u = - \\text{sgn}(x)\\) attains the minimum and sets the right-hand side equal to zero for any \\((t,x)\\). However, we can observe that the optimal controller need not be unique. For example, when \\(|x| \\leq T-t\\), we have \\[ \\max\\{0, |x| - (T-t) \\} = 0, \\] and any \\(u \\in \\mathbb{U}\\) would attain the minimum in (4.10) and hence be optimal. For simple systems, the HJB equation can be solved numerically. Example 4.2 (Numerical HJB Solution of A Scalar System) Consider a scalar linear system \\[ \\dot{x} = x + u, \\] and the optimal control problem with quadratic costs and \\(T=0.5\\) \\[ \\min_{u(t),t\\in[0,T]} x(T)^T P x(T) + \\int_{t=0}^T (x^T Q x + u^T R u ) dt \\] with \\(P=Q=R=1\\). Solve the HJB equation on a mesh \\(x \\in [-8,8]\\), we obtain the optimal cost-to-go in Fig. 4.3. Figure 4.3: Numerical solution of the HJB equation. You can find code for this problem here. 4.3 Linear Quadratic Regulator Let us apply the HJB sufficiency Theorem 4.1 to the continuous-time linear quadratic regulator (LQR). Consider the linear system \\[ \\dot{x}(t) = A x(t) + B u(t), \\] with \\(x \\in \\mathbb{R}^n\\), \\(u \\in \\mathbb{R}^m\\), and the optimal control problem \\[ \\min_{u(t),t\\in [0,T]} x(T)^T P x(T) + \\int_{t=0}^T (x(t)^T Q x(t) + u(t)^T R u(t)) dt \\] where \\(P,Q\\succeq 0\\) and \\(R \\succ 0\\). According to Theorem 4.1, the HJB equation for this problem is \\[\\begin{equation} \\begin{split} 0 = \\min_{u \\in \\mathbb{R}^m } \\left[ x^T Q x + u^T R u + \\nabla_t V(t,x) + \\nabla_x V(t,x)^T f(x,u) \\right], \\quad \\forall t,x, \\\\ V(T,x) = x^T P x, \\quad \\forall x. \\end{split} \\tag{4.11} \\end{equation}\\] Let us guess a solution of the following form, \\[ V(t,x) = x^T S(t) x, \\quad S(t) \\in \\mathbb{S}^n, \\] where \\(\\mathbb{S}^n\\) is the set of \\(n \\times n\\) symmetric matrices. The second equation in the HJB (4.11) implies \\[\\begin{equation} S(T) = P. \\tag{4.12} \\end{equation}\\] The first equation in the HJB (4.11) requires \\[\\begin{equation} 0 = \\min_{u \\in \\mathbb{R}^m} \\left[ x^T Q x + u^T R u + x^T \\dot{S}(t) x + 2x^T S(t) (A x + Bu) \\right]. \\tag{4.13} \\end{equation}\\] The objective function to be minimized in (4.13) is a convex quadratic function of \\(u\\), hence the optimal solution satisfies \\[ 2 R u^\\star + 2B^T S(t) x = 0, \\] solving which yields \\[\\begin{equation} u^\\star = - R^{-1} B^T S(t) x. \\tag{4.14} \\end{equation}\\] Now plugging the solution (4.14) back into (4.13), we obtain \\[ 0 = x^T \\left[ \\dot{S}(t) + S(t) A + A^T S(t) - S(t)B R^{-1} B^T S(t) + Q \\right] x, \\quad \\forall (t,x), \\] which implies that \\(\\dot{S}(t)\\) must satisfy \\[\\begin{equation} \\dot{S}(t) = - S(t) A - A^T S(t) + S(t) B R^{-1} B^T S(t) - Q. \\tag{4.15} \\end{equation}\\] This is known as the continuous-time differential Riccati equation, with the terminal condition given in (4.12). You should compare (4.15) with its discrete-time counterpart (2.7) and observable their similarities. 4.3.1 LQR Trajectory Tracking So far we have been talking about LQR in the context of regulating the system to a desired equilibrium point (usually the origin), you can in fact also use LQR for tracking a reference trajectory. Consider again the linear system \\[ \\dot{x}(t) = A x (t) + B u(t), \\quad x \\in \\mathbb{R}^n, u \\in \\mathbb{R}^m. \\] Given a reference state-control trajectory \\((x_r(t),u_r(t))_{t \\in [0,T]}\\), we consider the following optimal control problem to track the reference trajectory \\[ \\min_{u(t),t \\in [0,T]} \\Vert x(T) - x_r(T) \\Vert_P^2 + \\int_{t=0}^T \\left( \\Vert x(t) - x_r(t) \\Vert_Q^2 + \\Vert u(t) - u_r(t) \\Vert_R^2 \\right) dt \\] where \\(\\Vert v \\Vert_A^2 := v^T A v\\), \\(P,Q\\succeq 0\\) and \\(R \\succ 0\\). Let us guess a solution to the HJB of the following form \\[ V(t,x) = x^T S_{xx}(t) x + 2 x^T s_x(t) + s_0(t) = \\begin{bmatrix} x \\\\ 1 \\end{bmatrix}^T \\underbrace{\\begin{bmatrix} S_{xx}(t) &amp; s_x(t) \\\\ s_x^T(t) &amp; s_0(t) \\end{bmatrix}}_{=: S(t)} \\begin{bmatrix} x \\\\ 1 \\end{bmatrix}, \\quad S(t) \\succ 0. \\] Differentiating \\(V(t,x)\\) with respect to \\(t\\) and \\(x\\) we obtain \\[\\begin{equation} \\begin{split} \\nabla_t V(t,x) = x^T \\dot{S}_{xx}(t) x + 2 x^T \\dot{s}_x(t) + \\dot{s}_0(t), \\\\ \\nabla_x V(t,x) = 2 S_{xx}(t) x + 2 s_x(t). \\end{split} \\end{equation}\\] Plugging the partial derivates into the HJB (4.6), we have \\[ 0 = \\min_{u} \\left[ \\Vert x - x_r \\Vert_Q^2 + \\Vert u - u_r \\Vert_R^2 + \\nabla_x V(t,x)^T (A x + Bu) + \\nabla_t V(t,x) \\right]. \\] The objective to be minimized is convex in \\(u\\), leading to a closed-form solution \\[\\begin{equation} u^{\\star}(t) = u_r(t) - R^{-1}B^T \\left[ S_{xx}(t)x + s_x(t) \\right]. \\tag{4.16} \\end{equation}\\] With the solution of \\(u\\) (4.16) plugged back into the HJB, we can find the differential equations that \\(S_{xx}(t)\\), \\(s_x(t)\\) and \\(s_0(t)\\) need to satisfy \\[\\begin{equation} \\begin{split} -\\dot{S}_{xx}(t) = Q - S_{xx}(t) B R^{-1} B^T S_{xx}(t) + S_{xx}(t)A + A^T S_{xx}(t) \\\\ - \\dot{s}_x(t) = -Q x_r(t) + [A^T - S_{xx}(t)B R^{-1} B^T]s_x(t) + S_{xx}(t) B u_r(t) \\\\ - \\dot{s}_0(t) = x_r(t)^T Q x_r(t) - s_x^T(t) B R^{-1} B^T s_x(t) + 2 s_x(t)^T B u_r(t), \\end{split} \\tag{4.17} \\end{equation}\\] with the terminal conditions \\[\\begin{equation} \\begin{split} S_{xx}(T) = P \\\\ s_x(T) = - P x_r(T) \\\\ s_0(T) = x_r^T(T) P x_r(T). \\end{split} \\end{equation}\\] Notice that the differential equation of \\(S_{xx}(t)\\) in (4.17) is the same as the differential equation in (4.15). Also notice the value of \\(s_0(t)\\) does not affect the optimal control (4.16) and hence can often be ignored. 4.3.2 LQR Trajectory Stabilization Recall that in Example 3.8, we have seen the failure of directly applying open-loop control obtained from trajectory optimization. We then used receding horizon control to turn open-loop trajectory optimization into a close-loop controller. Another way to stabilize the open-loop trajectory is to perform local LQR stabilization. Given the nonlinear system \\[ \\dot{x} = f(x,u), \\] suppose we have computed a nominal state-control reference trajectory \\((x_r(t),u_r(t))_{t \\in [0,T]}\\) (e.g., using trajectory optimization), then we can define a local coordinate system relative to the reference trajectory \\[ \\bar{x}(t) = x(t) - x_r(t), \\quad \\bar{u}(t) = u(t) - u_r(t). \\] The \\(\\bar{x}\\) coordinate satisfies the following dynamics \\[\\begin{equation} \\dot{\\bar{x}} = \\dot{x} - \\dot{x}_r = f(x,u) - f(x_r, u_r). \\tag{4.18} \\end{equation}\\] We can perform a Taylor expansion of the dynamics in (4.18) \\[\\begin{equation} \\hspace{-10mm} \\dot{\\bar{x}} \\approx f(x_r, u_r) + \\frac{\\partial f(x_r,u_r)}{\\partial x} (x - x_r) + \\frac{\\partial f(x_r,u_r)}{\\partial u} (u - u_r) - f(x_r, u_r) = A(t) \\bar{x} + B(t) \\bar{u}, \\tag{4.19} \\end{equation}\\] where \\[ A(t) = \\frac{\\partial f(x_r(t),u_r(t))}{\\partial x}, \\quad B(t) = \\frac{\\partial f(x_r(t),u_r(t))}{\\partial u}, \\] are time-varying linear matrices. We then formulate the LQR problem for the approximate linear time-varying system (4.19) \\[\\begin{equation} \\min_{\\bar{u}(t), t \\in [0,T]} \\bar{x}(T)^T P \\bar{x}(T) + \\int_{t=0}^T \\left( \\bar{x}(t)^T Q \\bar{x}(t) + \\bar{u}(t)^T R \\bar{u}(t) \\right) dt \\tag{4.20} \\end{equation}\\] with \\(P \\succeq 0, Q \\succeq 0, R \\succ 0\\). The solution to (4.20), according to (4.14) is \\[ \\bar{u}(t) = - R^{-1} B^T S(t) \\bar{x}, \\] which implies \\[\\begin{equation} u(t) = u_r(t) - R^{-1} B^T S(t) (x(t) - x_r(t)) \\tag{4.21} \\end{equation}\\] in the original coordinates. Note that the controller in (4.21) is a feedback controller that “finetunes” the open-loop control. 4.4 The Pontryagin Minimum Principle The HJB equation in Theorem 4.1 provides a sufficient condition for the optimal cost-to-go. However, since the HJB equation is a sufficient condition, there do exist cases where the optimal cost-to-go does not satisfy the HJB equation but is still optimal (e.g., when the optimal cost-to-go is not continuously differentiable). We now introduce a necessary condition that any optimal control trajectory and state trajectory must satisfy. This condition is the celebrated Pontryagin minimum principle. A rigorous derivation of the Pontryagin minimum principle can be mathematically involving and is beyond the scope of this lecture notes (see Section 7.3.2 in (Bertsekas 2012) for a more rigorous treatment). In the following, we provide an informal derivation of the Pontryagin minimum principle. Recall the HJB equation in Theorem 4.1 states that, if a control trajectory \\(u^\\star(t)\\) and the associated state trajectory \\(x^\\star(t)\\) is optimal, then for all \\(t \\in [0,T]\\), the following condition must hold \\[\\begin{equation} u^\\star(t) = \\arg\\min_{u \\in \\mathbb{U}} \\left[ g(x^\\star(t),u) + \\nabla_x J(t,x^\\star(t))^T f(x^\\star(t),u) \\right] \\tag{4.22}. \\end{equation}\\] The above equation says, in order to compute the optimal control, we do not need to know the value of \\(\\nabla_x J\\) at all possible values of \\(x\\) and \\(t\\) (which is what the HJB equation tries to do), and we only need to know the value of \\(\\nabla_x J\\) along the optimal trajectory, i.e., to know only \\(\\nabla_x J(t,x^\\star(t))\\). The Pontryagin minimum principle builds upon this key observation, and it points out that \\(\\nabla_x J(t,x^\\star(t))\\) (but not \\(\\nabla_x J(t,x)\\) for any \\(x\\)) satisfies a certain differential equation called the adjoint equation. We now provide an informal derivation of the adjoint equation that is based on differentiating the HJB equation. Towards this goal, we first present the following lemma which is itself quite useful. Lemma 4.1 (Differentiating Functions Involving Minimization) Let \\(F(t,x,u)\\) be a continuously differentiable function of \\(t \\in \\mathbb{R}\\), \\(x \\in \\mathbb{R}^n\\), \\(u \\in \\mathbb{R}^m\\), and let \\(\\mathbb{U}\\) be a convex subset of \\(\\mathbb{R}^m\\). Suppose \\(\\mu^\\star(t,x)\\) is a continuously differentiable function such that \\[ \\mu^\\star(t,x) = \\arg\\min_{u \\in \\mathbb{U}} F(t,x,u), \\quad \\forall t,x. \\] Then \\[\\begin{align} \\nabla_t \\left\\{ \\min_{u \\in \\mathbb{U}} F(t,x,u) \\right\\} = \\nabla_t F(t,x,\\mu^\\star(t,x)), \\quad \\forall t,x, \\\\ \\nabla_x \\left\\{ \\min_{u \\in \\mathbb{U}} F(t,x,u) \\right\\} = \\nabla_x F(t,x,\\mu^\\star(t,x)), \\quad \\forall t,x. \\end{align}\\] In words, the partial derivates (with respect to \\(t\\) and \\(x\\)) of “the minimum of \\(F(t,x,u)\\) over \\(u\\)” (commonly known in optimization as the value function \\(\\psi(t,x)\\) of \\(F(t,x,u)\\)) are equal to the partial derivates of \\(F(t,x,u)\\) (with respect to \\(t\\) and \\(x\\)) after plugging in the optimizer \\(\\mu^\\star(t,x)\\). We now start with the HJB equation in (4.6), restated below with \\(V(t,x)\\) replaced by the optimal \\(J(t,x)\\) for the reader’s convenience \\[\\begin{equation} 0 = \\min_{u \\in \\mathbb{U}} \\left[ g(x,u) + \\nabla_t J(t,x) + \\nabla_x J(t,x)^Tf(x,u) \\right]. \\tag{4.23} \\end{equation}\\] Assume that \\(\\mu^\\star(t,x)\\) attains the minimum in the equation above and it is also continuously differentiable. Note that we have made the restrictive assumption that \\(\\mathbb{U}\\) is convex and \\(\\mu^\\star(t,x)\\) is continuously differentiable, which are not necessary in a more rigorous derivation of Pontryagin’s principle (cf. Section 7.3.2 in (Bertsekas 2012)). We differentiate both sides of (4.23) with respect to \\(t\\) and \\(x\\). In particular, let \\[ F(t,x,u) = g(x,u) + \\nabla_t J(t,x) + \\nabla_x J(t,x)^T f(x,u) \\] and invoke Lemma 4.1, we can write \\[\\begin{align} \\tag{4.24} \\hspace{-12mm} 0 = \\nabla_x g(x, \\mu^\\star(t,x)) + \\nabla^2_{xt} J(t,x) + \\nabla^2_{xx} J(t,x)f(x,\\mu^\\star(t,x)) + \\nabla_x f(x,\\mu^\\star(t,x)) \\nabla_x J(t,x).\\\\ \\hspace{-12mm} 0 = \\nabla^2_{tt}J(t,x) + \\nabla_{xt}^2 J(t,x)^T f(x,\\mu^\\star(t,x)) \\tag{4.25} \\end{align}\\] where the first equation results from differentiation of (4.23) with respect to \\(x\\), and the second equation results from differentiation of (4.23) with respect to \\(t\\). In (4.24), \\(\\nabla_x f(x,\\mu^\\star(t,x))\\) is \\[ \\nabla_f = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_n}{x_1} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_1}{\\partial x_n} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{bmatrix} \\] evaluated at \\((x,\\mu^\\star(t,x))\\). Equations (4.24) and (4.25) hold for any \\((t,x)\\) (under the restrictive assumptions we have made). We then evaluate the equations (4.24) and (4.25) only along the optimal control and state trajectory \\((u^\\star(t),x^\\star(t))\\) that satisfies \\[\\begin{equation} \\dot{x}^\\star(t) = f(x^\\star(t),u^\\star(t)), \\quad u^\\star(t) = \\mu^\\star(t,x^\\star(t)), \\quad t \\in [0,T]. \\tag{4.26} \\end{equation}\\] Specifically, along the optimal trajectory, we have \\[ \\frac{d}{dt} \\left( \\nabla_x J(t,x^\\star(t)) \\right) = \\nabla^2_{xt} J(t,x^\\star(t)) + \\nabla^2_{xx} J(t,x^\\star(t))f(x^\\star(t),u^\\star(t)), \\] where the right-hand side contains terms in the right-hand side of (4.24) (when evaluted along the optimal trajectory). Similarly, \\[ \\frac{d}{dt} \\left( \\nabla_t J(t,x^\\star(t)) \\right) = \\nabla^2_{tt} J(t,x^\\star(t)) + \\nabla^2_{xt} J(t,x^\\star(t))^T f(x^\\star(t),u^\\star(t)), \\] where the right-hand side is exactly the right-hand side of (4.25) (when evaluted along the optimal trajectory). As a result, equations (4.24) and (4.25), when evaluated along the optimal trajectory, are equivalent to \\[\\begin{align} \\tag{4.27} 0 = \\nabla_x g(x^\\star(t), u^\\star(t)) + \\frac{d}{dt}\\left( \\nabla_x J(t,x^\\star(t)) \\right) + \\nabla_x f(x^\\star(t),u^\\star(t)) \\nabla_x J(t,x^\\star(t)).\\\\ 0 = \\frac{d}{dt}\\left( \\nabla_t J(t,x^\\star(t)) \\right). \\tag{4.28} \\end{align}\\] Therefore, if we denote \\[ p(t) = \\nabla_x J(t,x^\\star(t)), \\quad p_0(t) = \\nabla_t J(t,x^\\star(t)), \\] then equations (4.27) and (4.28) become \\[\\begin{align} \\tag{4.29} \\dot{p}(t) = - \\nabla_x f(x^\\star(t),u^\\star(t)) p(t) - \\nabla_x g(x^\\star(t),u^\\star(t)), \\\\ \\dot{p}_0(t) = 0. \\tag{4.30} \\end{align}\\] Equation (4.29), which is a system of \\(n\\) first-order differential equations, is known as the adjoint equation and it describes the evolution of \\(p(t)\\), known as the costate, along the optimal trajectory. To obtain a boundary condition for the adjoint equation (4.29), we note that the boundary condition of the HJB equation \\[ J(T,x) = h(x),\\quad \\forall x \\] implies \\[ p(T) = \\nabla h(x^\\star(T)). \\] This is basically the Pontryagin Minimum Principle. The Hamiltonian formulation. It is usually more convenient to state the Pontryagin Principle using the concept of a Hamiltonian. Formally, we define the Hamiltonian function that maps the triplet \\((x,u,p) \\in \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^n\\) to real numbers given by \\[ H(x,u,p) = g(x,u) + p^T f(x,u). \\] Note that the dynamics along the optimal trajectory (4.26) can be conveniently written as \\[ \\dot{x}^\\star(t) = \\nabla_p H(x^\\star(t),u^\\star(t),p(t)), \\] and the adjoint equation (4.29) can be written as \\[ \\dot{p}(t) = - \\nabla_x H(x^\\star(t), u^\\star(t), p(t)). \\] We are now ready to state the Pontryagin Minimum Principle. Theorem 4.2 (Pontryagin Minimum Principle as A Necessary Condition for Optimality) Let \\((u^\\star(t), x^\\star(t)), t \\in [0,T]\\) be a pair of optimal control and state trajectories satisfying \\[ \\dot{x}^\\star(t) = f(x^\\star(t),u^\\star(t)), \\quad x^\\star(0) = x_0 \\text{ given}. \\] Let \\(p(t)\\) be the solution of the adjoint equation \\[ \\dot{p}(t) = - \\nabla_x H(x^\\star(t),u^\\star(t),p(t)), \\] with the boundary condition \\[ p(T) = \\nabla h(x^\\star(T)), \\] where \\(h\\) is the terminal cost function. Then, for all \\(t \\in [0,T]\\), we have \\[ u^\\star(t) = \\arg\\min_{u \\in \\mathbb{U}} H(x^\\star(t),u,p(t)). \\] Moreover, there is a constant \\(C\\) such that \\[ H(x^\\star(t),u^\\star(t),p(t)) = C, \\quad \\forall t \\in [0,T]. \\] To see why \\(H(x^\\star(t),u^\\star(t),p(t))\\) is a constant along the optimal trajectory, we observe that from the HJB equation (4.23), we obtain \\[\\begin{align} g(x^\\star,u^\\star) + \\nabla_t J(t,x^\\star) + \\nabla_x J(t,x^\\star)^T f(x^\\star, u^\\star) = 0 \\\\ \\Longrightarrow \\underbrace{g(x^\\star,u^\\star) + \\nabla_x J(t,x^\\star)^T f(x^\\star, u^\\star)}_{H(x^\\star(t),u^\\star(t),p(t))} = - \\underbrace{\\nabla_t J(t,x^\\star)}_{p_0(t)}. \\end{align}\\] From (4.30), we know \\(p_0(t)\\) is a constant. A necessary condition. It is important to recognize that the Pontryagin Minimum Principle in Theorem 4.2 is a necessary condition for optimality, i.e., all optimal control and state trajectories must satisfy this condition, but not all trajectories satisfying the condition are optimal. Extra arguments are needed to guarantee optimality. One common strategy is to show that an optimal control trajectory exists, and then verify that there is only one control trajectory satisfying the conditions of the Minimum Principle (or that all trajectories verifying the Minimum Principle have equal costs). A setup where the Minimum Principle is both necessary is sufficient is when \\(f(x,u)\\) is linear in \\((x,u)\\), the the constraint set \\(U\\) is convex, and the cost functions \\(h\\) and \\(g\\) are convex. Two-point boundary value problem (TPBVP). The Pontryagin Minimum Principle is particularly useful when \\[\\begin{equation} u^\\star = \\arg\\min_{u \\in \\mathbb{U}} H(x^\\star,u,p) = \\arg\\min_{u \\in \\mathbb{U}} g(x^\\star,u) + p^T f(x^\\star,u) \\tag{4.31} \\end{equation}\\] can be solved analytically so that \\(u^\\star\\) becomes a function of \\(x^\\star\\) and \\(p\\). For example, this is possible when problem (4.31) is a convex problem, for which one can invoke the KKT optimality conditions (cf. Appendix B.1.4). Once \\(u^\\star(t)\\) is expressed as a function of \\(x^\\star(t)\\) and \\(p(t)\\), we can merge the system equation (4.26) and the adjoint equation (4.29) together and arrive at \\[\\begin{equation} \\begin{cases} \\dot{x}^\\star(t) = f(x^\\star(t), u^\\star(t)) \\\\ \\dot{p}(t) = - \\nabla_x f(x^\\star(t),u^\\star(t)) p(t) - \\nabla_x g(x^\\star(t),u^\\star(t)) \\end{cases}, \\tag{4.32} \\end{equation}\\] which is a set of \\(2n\\) first-order differential equations in \\(x^\\star(t)\\) and \\(p(t)\\). The boundary conditions are \\[\\begin{equation} x^\\star(0) = x_0, \\quad p(T) = \\nabla h(x^\\star(T)). \\tag{4.33} \\end{equation}\\] The number of boundary conditions is also \\(2n\\), so generally we expect to be able to solve these differential equations numerically. Example 4.3 (Shortest Curve) In Fig. 4.4, consider any curve that starts at the point \\((0,\\alpha)\\) and lands on the vertical line that passes through \\((T,0)\\). What is the curve with minimum length? We all know the answer is a straight line. Figure 4.4: Shortest curve between a point and a line. Let us use Pontryagin’s minimum principle to prove this. The length of any curve satisfying our condition is \\[ \\int_{0}^{T} \\sqrt{1 + (\\dot{x}(t))^2} dt, \\quad x(0) = \\alpha. \\] To find the curve with minimum length, we can formulate an optimal control problem \\[ \\min_{u(t)} \\int_{t=0}^T \\sqrt{ 1 + u^2(t)} dt, \\quad \\text{subject to} \\quad \\dot{x}(t) = u(t), x(0) = \\alpha. \\] To apply Pontryagin’s minimum principle, we first formulate the Hamiltonian \\[ H(x,u,p) = g(x,u) + p^T f(x,u) = \\sqrt{1 + u^2} + pu. \\] The adjoint equation says \\[ \\dot{p}(t) = - \\nabla_x H(x^\\star(t),u^\\star(t),p), \\] which simplifies, for our problem, to \\[ \\dot{p}(t) = 0. \\] The boundary condition of the adjoint equation is \\[ p(T) = \\nabla h(x^\\star(T)) = 0. \\] We conclude that \\[ p(t) = 0, \\forall t \\in [0,T]. \\] The optimal controller thus becomes \\[ u^\\star(t) = \\arg\\min_u H(x^\\star(t),u,p(t)) = \\arg\\min_u \\sqrt{1 + u^2} = 0, \\quad \\forall t \\in [0,T]. \\] Therefore, we have \\(\\dot{x}^\\star(t) = u^\\star(t) = 0\\) for all \\(t \\in [0,T]\\), and \\(x^\\star(t) = \\alpha\\) for all time, which is a straight line. 4.4.1 Numerical Solution of the TPBVP When the optimal control can be solved analytically, we obtain the two-point boundary value problem (TPBVP) \\[\\begin{equation} \\begin{cases} x^\\star(0) = x_0 \\\\ p(T) = \\nabla h(x^\\star(T)) \\\\ \\dot{x}^\\star(t) = f(x^\\star(t), u^\\star(t)) \\\\ \\dot{p}(t) = - \\nabla_x f(x^\\star(t),u^\\star(t))p(t) - \\nabla_x g(x^\\star(t),u^\\star(t)) \\end{cases}, \\tag{4.34} \\end{equation}\\] where \\(u^\\star(t)\\) is a function of \\(x^\\star(t)\\) and \\(p(t)\\). Calling \\[ z(t) = \\begin{bmatrix} x^\\star(t) \\\\ p(t) \\end{bmatrix}, \\] we can compactly write (4.34) as \\[\\begin{equation} \\begin{cases} b(z(0),z(T),x_0) = 0 \\\\ \\dot{z}(t) - \\phi(z(t)) = 0 \\end{cases}, \\tag{4.35} \\end{equation}\\] which has \\(2n\\) differential equations and \\(2n\\) boundary conditions and usually well defined. 4.4.1.1 Single Shooting The idea of single shooting is straightforward: if \\(p(0)\\) is known in (4.34), then (i) the entire trajectory of \\(z\\) can be simulated forward in time, and (ii) we only need to enforce the terminal constraint of \\(p(T) = \\nabla h(x^\\star(T))\\). Therefore, in single shooting, we denote the terminal residual \\[ r_T = p(T) - \\nabla h(x^\\star(T)) = r_T(p_0), \\] as a function of \\(p(0) = p_0\\) (note that evaluating this function requires simulating \\(\\dot{z}(t) = \\phi(z(t))\\)). Then we use Newton’s method to iteratively update \\(p_0\\) via \\[ p_0^{(k+1)} = p_0^{(k)} - \\eta_k \\left( \\frac{\\partial r_T}{\\partial p_0}(p_0^{(k)}) \\right)^{-1} r_T(p_0^{(k)}), \\] where \\(\\eta_k\\) is a chosen step size. In some cases, the forward simulation of the combined ODE might be an ill-conditioned problem so that single shooting cannot be employed. Even if the forward simulation problem is well-defined, the region of attraction of the Newton iteration can be very small, such that a good guess for \\(p_0\\) is often required. 4.4.1.2 Multiple Shooting The single shooting method also suffers from the nonlinearity and inaccuracy brought by simulating the ODE for a long time duration \\(T\\). In multiple shooting, we break the time windown \\([0,T]\\) into \\(N\\) pieces such that \\[ 0 = t_0 \\leq \\dots \\leq t_k \\leq t_{k+1} \\leq \\dots \\leq t_N = T. \\] We then assign values \\[ z_k = z(t_k), k = 0,\\dots,N, \\] as the values of \\(z(t)\\) at those breakpoints. The values \\(z_k\\) should satisfy the ODE \\[ z_{k+1} = \\Phi(z_k), \\] where \\(\\Phi(\\cdot)\\) integrates the ODE from time \\(t_k\\) to \\(t_{k+1}\\) with initial condition \\(z_k\\). We can then form a system of equations on \\(z=(z_0,\\dots,z_N)\\) as \\[\\begin{equation} \\begin{cases} b(z_0,z_N,x_0) = 0 \\\\ z_{k+1} - \\Phi(s_k) = 0 \\end{cases}. \\end{equation}\\] Denoting the vector of residuals as \\[ R(z,x_0) = \\begin{bmatrix} b(z_0,z_N,x_0) \\\\ z_{k+1} - \\Phi(s_k) \\end{bmatrix}, \\] we can use Newton’s method to iteratively solve \\(z\\): \\[\\begin{equation} z^{(k+1)} = z^{(k)} - \\eta_k \\left( \\frac{\\partial R}{\\partial z}(z^{(k)}) \\right)^{-1} R(z^{(k)}). \\end{equation}\\] 4.4.1.3 Collocation 4.5 Infinite-Horizon Problems 4.5.1 Infinite-Horizon LQR Consider a linear time-invariant system \\[ \\dot{x} = Ax + Bu, \\] and the infinite-horizon optimal control problem \\[ \\min \\int_{t=0}^{\\infty} (x(t)^T Q x(t) + u(t)^T R u(t)) dt, \\quad x(0) = x_0. \\] with \\(Q \\succeq 0, R \\succ 0\\). The optimal controller takes the following linear feedback form \\[ u = - R^{-1}B^T S x, \\] with \\(S\\) the solution to the continuous-time algebraic Riccati equation \\[ 0 = SA + A^T S - SBR^{-1} B^T S + Q. \\] In Matlab, given \\(A,B,Q,R\\), you can use lqr to compute \\(K\\) and \\(S\\). 4.6 Viscosity Solution References ———. 2012. Dynamic Programming and Optimal Control: Volume i. Vol. 1. Athena scientific. Even though we write \\(dx_i(t)/dt\\) in the system (4.1), we allow \\(x(t)\\) to be only directionally differentiable at a finite number of points to account for the possible discontinuity of \\(u(t)\\).↩︎ "],["stability.html", "Chapter 5 Stability Analysis 5.1 Autonomous Systems 5.2 Controlled Systems 5.3 Non-autonomous Systems", " Chapter 5 Stability Analysis Optimal control formulates a control problem via the language of mathematical optimization. However, there are control problems, and sometimes even the very basic control problems, that cannot be easily stated in the optimal control formulation. For example, suppose our goal is to swing up a pendulum to the upright position and stabilize it there. You may want to formalize the problem as \\[\\begin{equation} \\min_{u(t) \\in \\mathbb{U}} \\int_{0}^{\\infty} \\Vert x(t) - x_d \\Vert^2 dt, \\quad \\text{subject to} \\quad \\dot{x} = f(x,u), x(0) = x_0, \\tag{5.1} \\end{equation}\\] where \\(x_d\\) is the desired upright position for the pendulum. However, does the solution of problem (5.1), if exists, guarantee the stabilization of the pendulum at the upright position? The answer is unclear without a rigorous proof. However, after a slight change of perspective, the optimal control problem may be formulated to better match the goal. Suppose there exists a region, \\(\\Omega\\), in the state space such that as long as the pendulum enters \\(\\Omega\\), there always exists a sequence of control to bring the pendulum to the goal state \\(x_d\\), then we can simply formulate a different optimal control problem \\[\\begin{equation} \\min_{u(t) \\in \\mathbb{U}} \\int_{0}^{T} \\Vert u(t) \\Vert^2 dt, \\quad \\text{subject to} \\quad x(0)=x_0, x(T) \\in \\Omega, \\dot{x} = f(x,u), \\tag{5.2} \\end{equation}\\] where now it is very clear, if a solution exists to problem (5.2), then we will definitely achieve our goal. This is because the constraint \\(x(T) \\in \\Omega\\) guarantees that we will be able to stabilize the pendulum, and the cost function of (5.2) simply encourages minimum control effort along the way. This highlights that, sometimes the formulation of a problem may deserve more thoughts than the actual solution. Of course the formulation (5.2) may be much more difficult to solve. In fact, does the set \\(\\Omega\\) exist, and if so, how to describe it? This is the main focus of this chapter: to introduce tools that can help us analyze the stability of uncontrolled and controlled nonlinear systems. Specifically, we will introduce the notion of stability certificates, which are conditions that, if hold, certify the stability of the system (e.g., in the set \\(\\Omega\\)). Interestingly, you will see that the notion of stability certificates is intuitive and easy, but what is really challenging is to find and compute the stability certificates. We will highlight the power and also limitation of computational tools, especially those that are based on convex optimization (see Appendix B for a review of convex optimization). 5.1 Autonomous Systems Let us first focus on autonomous systems, i.e., systems whose dynamics do not depent on time (and control). We introduce different concepts of stability and ways to certify them. 5.1.1 Concepts of Stability Consider the autonomous system \\[\\begin{equation} \\dot{x} = f(x) \\tag{5.3} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) is the state and \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is the (potentially nonlinear) dynamics. Before talking about concepts of stability, we need to define an equilibrium point. Definition 5.1 (Equilibrium Point) A state \\(x^\\star\\) is called an equilibrium point of system (5.3) if \\(f(x^\\star) = 0\\), i.e., once the system reaches \\(x^\\star\\), it stays at \\(x^\\star\\). For example, a linear system \\[ \\dot{x} = A x \\] has a single equilibrium point \\(x^\\star = 0\\) when \\(A\\) is nonsingular, and an infinite number of equilibrium points when \\(A\\) is singular (those equilibrium points lie in the kernel of matrix \\(A\\)). When analyzing the behavior of a dynamical system around the equilibrium point, it is often helpful to “shift” the dynamics equation so that \\(0\\) is the equilibrium point. For example, if we are interested in the behavior of system (5.3) near the equilibrium point \\(x^\\star\\), we can create a new variable \\[ z = x - x^\\star, \\] so that \\[\\begin{equation} \\dot{z} = \\dot{x} = f(x) = f(z + x^\\star). \\tag{5.4} \\end{equation}\\] Clearly, \\(z^\\star = 0\\) is an equilibrium point for the shifted system (5.4). Let us find the equilibrium points of a simple pendulum. Example 5.1 (Equilibrium Points of A Simple Pendulum) Consider the dynamics of an uncontrolled pendulum \\[\\begin{equation} \\begin{cases} \\dot{\\theta} = \\dot{\\theta} \\\\ \\ddot{\\theta} = - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) \\end{cases} \\tag{5.5} \\end{equation}\\] where \\(\\theta\\) is the angle between the pendulum and the vertical line, and \\(x = [\\theta,\\dot{\\theta}]^T\\) is the state of the pendulum (\\(m,g,l,b\\) denote the mass, gravity constant, length, and damping constant, respectively). To find the equilibrium points of the pendulum, we need the right hand sides of (5.5) to be equal to zero: \\[ \\dot{\\theta} = 0, \\quad - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) = 0. \\] The solutions are easy to find \\[ x^\\star = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\quad \\text{or} \\quad \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}, \\] corresponding to the bottomright and upright positions of the pendulum, respectively. The pendulum dynamics has two equilibrium points, but our physics intuition tells us these two equilibrium points are dramatically different. Specifically, the bottomright equilibrium \\(x^\\star = [0,0]^T\\) is such that if you perturb the pendulum around the equilibrium, the pendulum will go back to that equilibrium; the upright equilibrium \\(x^\\star = [\\pi,0]^T\\) is such that if you perturb the pendulum (even just a little bit) around the equilibrium, it will diverge from that equilibrium. This physical intuition is exactly what we want to formalize as the concepts of stability. In the following, we focus on the nonlinear autonomous system (5.3) with \\(f(0) = 0\\), i.e., \\(x^\\star = 0\\) is an equilibrium point. We now formally define the different concepts of stability. Definition 5.2 (Lyapunov Stability) The equilibrium point \\(x=0\\) is said to be stable in the sense of Lyapunov if, for any \\(R &gt; 0\\), there exists \\(r &gt;0\\) such that if \\(\\Vert x(0) \\Vert &lt; r\\), then \\(\\Vert x(t) \\Vert &lt; R\\) for all \\(t \\geq 0\\). Otherwise, the equilibrium point is unstable. For a system that is Lyapunov stable around \\(x=0\\), the definition says that, if we want to constrain the trajectory of the system to be within the ball \\(B_R = \\{ x \\mid \\Vert x \\Vert &lt; R \\}\\), then we can always find a smaller ball \\(B_r = \\{ x \\mid \\Vert x \\Vert &lt; r \\}\\) such that if the system starts within \\(B_r\\), it will remain in the larger ball \\(B_R\\). On the other hand, if the system is not Lyapunov stable at \\(x=0\\), then there exists at least one ball \\(B_R\\), such that no matter how close the system’s initial condition is to the origin, it will eventually exit the ball \\(B_R\\). The following exercise is left for you to verify the instability of the Van der Pol oscillator. Exercise 5.1 (Instability of the Van der Pol oscillator) Show that the Van der Pol oscillator \\[ \\begin{cases} \\dot{x}_1 = x_2 \\\\ \\dot{x}_2 = - x_1 + (1-x_1^2) x_2 \\end{cases} \\] is unstable at the equilibrium point \\(x = 0\\). Lyapunov stability does not guarantee the system trajectory will actually converge to \\(x =0\\). Instead, asymptotic stability will ask the system trajectory to converge to \\(x=0\\). Definition 5.3 (Asymptotic Stability and Domain of Attraction) The equilibrium point \\(x = 0\\) is said to be asymptotically stable if (i) it is Lyapunov stable, and (ii) there exists some \\(r &gt; 0\\) such that \\(x(0) \\in B_r\\) implies \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). The domain of attraction (for the equilibrium \\(x=0\\)) is the largest set of points in the state space such that trajectories initiated at those points will converge to the equilibrium point. That is, \\[ \\Omega(x^\\star) = \\{ x \\in \\mathbb{X} \\mid x(0) = x \\Longrightarrow \\lim_{t \\rightarrow \\infty} x(t) = x^\\star \\}. \\] The ball \\(B_r\\) is a domain of attraction for the equilibrium point \\(x=0\\), but not necessarily the largest domain of attraction. You may immediately realize that in the definition of asymptotic stability, we require Lyapunov stability to hold first. Is this necessary? i.e., does there exist a system where trajectories eventually converge to zero, but is not stable in the sense of Lyapunov? You should work out the following exercise. Exercise 5.2 (Vinograd System) Show that for the Vinograd dynamical system (Vinograd 1957) \\[ \\begin{cases} \\dot{x} = \\frac{x^2(y-x) + y^5}{(x^2+y^2)(1 + (x^2+y^2)^2)} \\\\ \\dot{y} = \\frac{y^2 (y - 2x)}{(x^2+y^2)(1 + (x^2+y^2)^2)} \\end{cases}, \\] all system trajectories converge to the equilibrium point \\((x,y) = 0\\), but the equilibrium point is not stable in the sense of Lyapunov. (Hint: the system trajectories will behave like the following plot.) Figure 5.1: Trajectories of the Vinograd system. Copied from the original article of Vinograd. In many cases, we want the convergence of the system trajectory towards \\(x=0\\) to be fast, thus bringing in the notion of exponential stability. Definition 5.4 (Exponential Stability) An equilibrium point \\(x=0\\) is said to be exponentially stable, if there exists a ball \\(B_r\\) such that as long as as \\(x(0) \\in B_r\\), then \\[ \\Vert x(t) \\Vert \\leq \\alpha \\Vert x(0) \\Vert e^{-\\lambda t}, \\quad \\forall t, \\] for some \\(\\alpha &gt; 0\\) and \\(\\lambda &gt; 0\\) (\\(\\lambda\\) is called the rate of exponential convergence). Exponential stability implies asymptotic stability (and certainly also Lyapunov stability). What is nice about exponential stability is that we can quantify the distance of the system trajectory to the equilibrium point as a function of time (as long as we know the constants \\(\\alpha, \\Vert x(0) \\Vert, \\lambda\\)). In many safety-critical applications, we need such performance guarantees. For example, in Chapter 6.5, we will see the application of exponential stability in observer-feedback control. All the concepts of stability we have mentioned so far only talk about the stability of the system locally around the equilibrium point \\(x=0\\) (via arguments like \\(B_r\\) and \\(B_R\\)). It would be much nicer if we can guarantee stability of the system globally, i.e., no matter where the system starts in the state space \\(\\mathbb{X}\\), its trajectoy will converge to \\(x=0\\). Definition 5.5 (Global Asymptotic and Exponential Stability) The equilibrium point \\(x = 0\\) is said to be globally asymptotically (exponentially) stable if asymptotic (exponential) stability holds for any initial states. That is, \\[ \\forall x \\in \\mathbb{X}, \\quad x(0) = x \\Longrightarrow \\begin{cases} \\lim_{t \\rightarrow \\infty} x(t) = 0 &amp; \\text{global asymptotic stability} \\\\ \\exists \\alpha, \\lambda &gt; 0, \\text{ s.t. } \\Vert x(t) \\Vert \\leq \\alpha \\Vert x(0) \\Vert e^{-\\lambda t} &amp; \\text{global exponential stability} \\end{cases} \\] This concludes our definitions of stability for nonlinear systems (Definition 5.2-5.5). It is worth mentioning that the concepts of stability are complicated (refined) here due to our focus on nonlinear systems. For linear systems, the concepts of stability are simpler. Specifically, all local stability properties of linear systems are also global and asymptotic stability is equal to exponential stability. In fact, for a linear time-invariant system \\(\\dot{x} = Ax\\), it is either asymptotically (exponentially) stable, or marginally stable, or unstable. Moreover, we can fully characterize the stability property by inspecting the eigenvalues of \\(A\\) (you can find a refreshment of this in Appendix C.1). How do we characterize the stability property of a nonlinear system? If someone gave me a nonlinear system (5.3), how can I provide a certificate to her that the system is stable or unstable (I cannot use eigenvalues anymore in this case)? Let us describe some of these certificates below. 5.1.2 Stability by Linearization A natural idea is to linearize, if possible, the nonlinear system (5.3) at a given equilibrium point \\(x^\\star\\) and inspect the stability of the linearized system (for which we can compute eigenvalues). Therefore, the key question here is how does the stability and instability of the linearized system relate to the stability and instability of the original nonlinear system. Theorem 5.1 (Stability by Linearization) Assume \\(x=0\\) is an equilibrium point of system (5.3) and \\(f\\) is continuously differentiable. Let \\[\\begin{equation} \\dot{x} = Ax, \\quad A = \\frac{\\partial f}{\\partial x} \\Big\\vert_{x=0} \\tag{5.6} \\end{equation}\\] be the linearized system at \\(x=0\\). The following statements are true about the stability relationship between (5.3) and (5.6). If the linearized system (5.6) is strictly stable (i.e., all eigenvalues of \\(A\\) have strictly negative real parts), then the original system (5.3) is asymptotically stable at \\(x=0\\). If the linearized system (5.6) is unstable (i.e., at least one eigenvalue of \\(A\\) has strictly positive real part), then the original system (5.3) is unstable at \\(x=0\\). If the linearized system (5.6) is marginally stable (i.e., all eigenvalues of \\(A\\) have nonpositive real parts, and at least one eigenvalue has zero real part), then the stability of the original system (5.3) at \\(x=0\\) is indeterminate. Theorem 5.1 is actually quite useful when we want to quickly examine the local stability of a nonlinear system around a given equilibrium point, as we will show in the next example. Example 5.2 (Stability of A Simple Pendulum by Linearization) Consider the simple pendulum dynamics (5.5) in Example 5.1. Without loss of generality, let \\(m=1,l=1,b=0.1\\). The Jacobian of the nonlinear dynamics reads \\[ A = \\frac{\\partial f}{\\partial x} = \\begin{bmatrix} 0 &amp; 1 \\\\ -\\frac{g}{l} \\cos \\theta &amp; -\\frac{b}{ml^2} \\end{bmatrix}. \\] At the bottomright equilibrium point \\(\\theta =0, \\dot{\\theta} = 0\\), the matrix \\(A\\) has two eigenvalues \\[ -0.0500 \\pm 3.13i, \\] and hence the pendulum is asymptotically stable at the bottomright equilibrium point. At the upright equilibrium point \\(\\theta =\\pi, \\dot{\\theta} = 0\\), the matrix \\(A\\) has two eigenvalues \\[ 3.08, \\quad -3.18, \\] and hence the pendulum is unstable at the upright equilibrium point. The linearization method is easy to carry out. However, it tells us nothing about global stability or exponential stability. Moreover, when the linearized system is marginally stable, the stability of the orignal system is inconclusive. In the next, we will introduce a more general, and perhaps the most popular framework for analyzing the stability of nonliear systems. 5.1.3 Lyapunov Analysis The basic idea of Lyapunov analysis is quite intuitive: if can find an “energy-like” scalar function for a system such that the scalar function is zero at an equilibrium point and positive everywhere else, and the time-derivative of the scalar function is zero at the equilibrium point but negative otherwise, then we know that the energy of the system will eventually converge to zero, and hence the state trajectory will converge to the equilibrium point. Lyapunov analysis was originally inspired by the energy function of a mechanical system: the total energy of a mechanical system (potental energy plus kinetic energy) will settle down to its minimum value if it is constantly dissipated (e.g., due to damping). However, the concept of a Lyapunov function is much broader than the energy function, i.e., it can be an arbitrary abstract function without any physical meaning. Let us now introduce the concept of a Lyapunov function. Definition 5.6 (Positive Definite Function) A scalar function \\(V(x)\\) is said to be locally positive definite in a ball \\(B_R\\) if \\[ V(0) = 0 \\quad \\text{and} \\quad V(x) &gt; 0, \\forall x \\in B_R \\backslash \\{0\\}, \\] and globally positive definite if \\[ V(0) = 0 \\quad \\text{and} \\quad V(x) &gt; 0, \\forall x \\in \\mathbb{X} \\backslash \\{0\\}, \\] where \\(\\mathbb{X}\\) is the entire state space. A function \\(V(x)\\) is said to be negative definite if \\(-V(x)\\) is positive definite. A function \\(V(x)\\) is said to be positive semidefinite if the “\\(&gt;\\)” sign is replaced by the “\\(\\geq\\)” sign in the above equations. A function \\(V(x)\\) is said to be negative semidefinite if \\(-V(x)\\) is positive semidefinite. For example, when \\(\\mathbb{X} = \\mathbb{R}^2\\), the function \\(V(x) = x_1^2 + x_2^2\\) is positive definite, but the function \\(V(x) = x_1^2\\) is only positive semidefinite. Definition 5.7 (Lyapunov Function) In the ball \\(B_R\\), if a function \\(V(x)\\) is positive definite, and its time derivative along any system trajectory \\[ \\dot{V}(x) = \\frac{\\partial V}{\\partial x} f(x) \\] is negative semidefinite (we assume the partial derivative \\(\\frac{\\partial f}{\\partial x}\\) exists and is continuous), then \\(V(x)\\) is said to be a Lyapunov function for system (5.3). Note that \\(\\dot{V}(x^\\star) = 0\\) at any equilibrium point \\(x^\\star\\) by definition. With the introduction of positive definite and Lyapunov functions, we are now ready to use them to certify different concepts of stability. Theorem 5.2 (Lyapunov Local Stability) Consider the nonlinear system (5.3) in a ball \\(B_R\\) with equilibrium point \\(x=0\\), if there exists a scalar function \\(V(x)\\) (with continuous partial derivatives) such that \\(V(x)\\) is positive definite (in \\(B_R\\)) \\(\\dot{V}(x)\\) is negative semidefinite (in \\(B_R\\)) then the equilibrium point \\(x=0\\) is stable in the sense of Lyapunov (cf. Definition 5.2). Moreover, if \\(\\dot{V}(x)\\) is negative definite in \\(B_R\\), then the equilibrium point is asymptotically stable (cf. Definition 5.3). if \\(\\dot{V}(x) \\leq - \\alpha V(x)\\) for any \\(x \\in B_R\\), then the equilibrium point is exponentially stable (cf. Definition 5.4). Let us apply Theorem 5.2 to the simple pendulum. Example 5.3 (Lyapunov Local Stability for A Simple Pendulum) Consider the pendulum dynamics (5.5). The total energy of a pendulum is \\[\\begin{equation} V(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + mgl (1 - \\cos \\theta). \\tag{5.7} \\end{equation}\\] Clearly, \\(V(x)\\) is positive definite on the entire state space, and the only point where \\(V(x) = 0\\) is the equilibrium point \\(\\theta = 0, \\dot{\\theta} = 0\\). Let us compute the time derivative of \\(V(x)\\): \\[ \\dot{V}(x) = ml^2 \\dot{\\theta} \\ddot{\\theta} + mgl \\sin \\theta \\dot{\\theta} = ml^2 \\dot{\\theta} \\left( -\\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\sin\\theta) \\right) + mgl \\sin \\theta \\dot{\\theta} = -b \\dot{\\theta}^2 , \\] which is clearly negative semidefinite. In fact, \\(\\dot{V}(x)\\) is precisely the energy dissipation rate due to damping. By Theorem 5.2 we conclude that the equilibrium point is stable in the sense of Lyapunov. Note that with this choice of \\(V(x)\\) as in (5.7), we actually cannot certify asymptotic local stability of the bottomright equilibrium point. So a natural question is, can we find a better Lyapunov function that indeed certifies asymptotic stability? The answer is yes. Consider a different Lyapunov function \\[\\begin{equation} \\tilde{V}(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + \\frac{1}{2} ml^2 \\left( \\frac{b}{ml^2}\\theta + \\dot{\\theta} \\right)^2 + 2mgl (1 - \\cos \\theta), \\tag{5.8} \\end{equation}\\] which is positive definite and admits a single zero-value point \\(\\theta = 0, \\dot{\\theta} = 0\\) that is also the bottomright equilibrium point. Simplifying \\(\\tilde{V}(x)\\) we can get \\[\\begin{align} \\tilde{V}(x) &amp;= ml^2 \\dot{\\theta}^2 + 2mgl(1-\\cos \\theta) + \\frac{1}{2} ml^2 \\left( \\frac{b^2}{m^2 l^4} \\theta^2 + \\frac{2b}{ml^2} \\theta \\dot{\\theta} \\right) \\\\ &amp;= 2V(x) + \\frac{1}{2} ml^2 \\left( \\frac{b^2}{m^2 l^4} \\theta^2 + \\frac{2b}{ml^2} \\theta \\dot{\\theta} \\right). \\end{align}\\] The time derivative of the new function \\(\\tilde{V}(x)\\) is \\[\\begin{align} \\dot{\\tilde{V}}(x) &amp;= 2 \\dot{V}(x) + \\frac{ml^2}{2} \\left( \\frac{2b^2}{m^2 l^4} \\theta \\dot{\\theta} + \\frac{2b}{ml^2} (\\dot{\\theta}^2 + \\theta \\ddot{\\theta}) \\right) \\\\ &amp; = 2\\dot{V}(x) + b\\dot{\\theta}^2 + \\left( \\frac{b^2}{ml^2} \\theta\\dot{\\theta} + b \\theta \\left( -\\frac{1}{ml^2} (b\\dot{\\theta} + mgl \\sin \\theta) \\right) \\right) \\\\ &amp; = -b \\left( \\dot{\\theta}^2 + \\frac{g}{l} \\theta \\sin \\theta \\right). \\end{align}\\] \\(\\dot{\\tilde{V}}(x)\\) is negative definite locally around the equilibrium point (locally \\(\\sin\\theta \\approx \\theta\\)). Therefore, with the new Lyapunov function \\(\\tilde{V}(x)\\) we can certify asymptotic stability. Interestingly, \\(V(x)\\) is intuitive (the total energy of the pendulum system), but it fails to certify asymptotic local stability (as least by just using Theorem 5.2). \\(\\tilde{V}(x)\\) does not have any physical intuition, but it successfully certifies local asymptotic stability. In Section 5.1.4, we will see that when using \\(V(x)\\) with the invariant set theorem, we can actually still certify the asymptotic stability of the pendulum around the bottomright equilibrium. In many applications, we desire to certify the global stability of an equilibrium point. The following theorem states that if in addition the scalar function \\(V(x)\\) is radially unbounded, then global stability can be certified. Theorem 5.3 (Lyapunov Global Stability) For the autonomous system (5.3), suppose there exists a scalar function \\(V(x)\\) with (continuous partial derivatives) such that \\(V(x)\\) is positive definite; \\(\\dot{V}(x)\\) is negative definite; \\(V(x) \\rightarrow \\infty\\) as \\(\\Vert x \\Vert \\rightarrow \\infty\\), then the equilibrium point \\(x = 0\\) is globally asymptotically stable (cf. Definition 5.5). Moreover, if in addition to the three conditions above \\(\\dot{V}(x) \\leq - \\alpha V(x)\\) for some \\(\\alpha &gt; 0\\), then the equilibrium point is globally exponentially stable. 5.1.4 Invariant Set Theorem Through Theorem 5.2, Theorem 5.3, and Example 5.3, we see that in order to certify asymptotic stability, the time derivative \\(\\dot{V}(x)\\) is required to be positive definite. However, in many cases, with Example 5.3 being a typical one, \\(\\dot{V}(x)\\) is only negative semidefinite, which makes it difficult to certify asymptotic stability. In this section, we will introduce the invariant set theorem that can help us reason about asymptotic stability even when \\(\\dot{V}(x)\\) is only negative semidefinite. Let us first introduce the notion of an invariant set. Definition 5.8 (Invariant Set) A set \\(G\\) is an invariant set for a dynamical system (5.3) if every system trajectory that starts within \\(G\\) remains in \\(G\\) for all future time. Formally, \\[ x(0) \\in G \\Longrightarrow x(t) \\in G,\\forall t. \\] A trivial invariant set is the entire state space \\(\\mathbb{X}\\). Another example of an invariant set is the singleton \\(\\{x^\\star \\}\\) with \\(x^\\star\\) being an equilibrium point. A nontrivial invariant set is the domain of attraction of an equilibrium point (cf. Definition 5.3). We now state the local invariant set theorem. Theorem 5.4 (Local Invariant Set) Consider the autonomous system (5.3), and let \\(V(x)\\) be a scalar function with continuous partial derivatives. Assume that the sublevel set \\(\\Omega_{\\rho} = \\{ x \\in \\mathbb{X} \\mid V(x) &lt; \\rho \\}\\) is bounded for some \\(\\rho &gt; 0\\), and \\(\\dot{V}(x) \\leq 0\\) for all \\(x \\in \\Omega_{\\rho}\\). Let \\(\\mathcal{R}\\) be the set of all points within \\(\\Omega_{\\rho}\\) such that \\(\\dot{V}(x) = 0\\), and \\(\\mathcal{M}\\) be the largest invariant set in \\(\\mathcal{R}\\). Then, every trajectory that starts in \\(\\Omega_{\\rho}\\) will converge to \\(\\mathcal{M}\\) as \\(t \\rightarrow \\infty\\). With this theorem, we can now revisit the pendulum example 5.3. Example 5.4 (Revisiting the Local Stability of A Simple Pendulum) In Example 5.3, using the Lyapunov function \\[ V(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + mgl(1 - \\cos\\theta), \\] with time derivative \\[ \\dot{V}(x) = -b\\dot{\\theta}^2, \\] we were only able to verify the stability of the bottomright equilibrium point in the sense of Lyapunov. Now let us use the invariant set theorem 5.4 to show the asymptotic stability of the bottomright equilibrium point. First it is easy to see that the sublevel set of \\(V(x)\\) is bounded. For example, with \\(\\rho = \\frac{1}{4} mgl\\), \\[\\begin{align} V(x) &lt; \\frac{1}{4} mgl \\Rightarrow \\frac{1}{2} ml^2 \\dot{\\theta}^2 &lt; \\frac{1}{4} mgl \\Rightarrow \\dot{\\theta}^2 &lt; \\frac{1}{2} \\frac{g}{l} \\\\ V(x) &lt; \\frac{1}{4} mgl \\Rightarrow mgl(1-\\cos\\theta) &lt; \\frac{1}{4} mgl \\Rightarrow \\cos\\theta &gt; \\frac{3}{4} \\Rightarrow \\theta \\in (-\\arccos \\frac{3}{4}, \\arccos \\frac{3}{4}). \\end{align}\\] The set \\(\\mathcal{R}\\), including all the points in \\(\\Omega_{\\rho}\\) such that \\(\\dot{V}(x) = 0\\) is \\[ \\mathcal{R} = \\{ x \\in \\Omega_{\\rho} \\mid \\dot{\\theta} = 0 \\}. \\] We now claim that the largest invariant set \\(\\mathcal{M}\\) in \\(\\mathcal{R}\\) is just the single equilibrium point \\(x = [0,0]^T\\). We can prove this by contradiction. Suppose there is a different point \\(x&#39; = [\\theta,0]^T\\) with \\(\\theta \\neq 0\\) also belonging to the invariant set \\(\\mathcal{M}\\), then \\[ \\ddot{\\theta} = -\\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) = - \\frac{g}{l} \\sin\\theta \\neq 0, \\] which means \\(\\dot{\\theta}\\) will immediately become nonzero, and hence the trajectory will exit \\(\\mathcal{R}\\) and also \\(\\mathcal{M}\\). So that point cannot belong to the invariant set. Now by Theorem 5.4, we conclude the bottomright equilibrium point is asymptotically stable. Note that through this analysis we also obtain \\(\\Omega_{\\rho}\\) as a domain of attraction for the bottomright equilibrium point. Similarly, with the addition of the radial unboundedness of \\(V(x)\\), we have a global version of the invariant set theorem. Theorem 5.5 (Global Invariant Set) For the autonomous system (5.3), let \\(V(x)\\) be a scalar function with continuous partial derivatives that satisfies \\(V(x) \\rightarrow \\infty\\) as \\(\\Vert x \\Vert \\rightarrow \\infty\\), and \\(\\dot{V}(x) \\leq 0\\) over the entire state space. Let \\(\\mathcal{R} = \\{ x\\in \\mathbb{X} \\mid \\dot{V}(x)= 0 \\}\\), and \\(\\mathcal{M}\\) be the largest invariant set in \\(\\mathcal{R}\\). Then all system trajectories asymptotically converge to \\(\\mathcal{M}\\) as \\(t \\rightarrow \\infty\\). 5.1.5 Computing Lyapunov Certificates All the Theorems we have stated so far (Theorems 5.2, 5.3, 5.4, and 5.5) are very general and powerful tools for certifying stability of nonlinear systems. However, the key requirement for applying the results is a Lyapunov function \\(V(x)\\) that verifies different types of nonnegativity constraints. How to find these functions? In Example 5.3, we have seen that physical intuition can help us find a good Lyapunov function (5.7). Nevertheless, it did not quite give us what we want in terms of asymptotic stability. Instead, a hand-crafted function (5.8) helped us certify local asymptotic stability. Wouldn’t it be cool that we can design an algorithm to find the Lyapunov certificates for us? A closer look at the Theorems 5.2, 5.3, 5.4, and 5.5 tells us the key property of a Lyapunov certificate is that it needs to satisfy the positivity (or negativity) constraint for all states inside a set. This is a nontrivial and difficult requirement, because even if we were given a function \\(V(x)\\), naively evaluting if \\(V(x)\\) is nonnegative inside a set requires enumeration over all the states in the set, which is impractical given that the set is continuous and has infinite number of states.5 When the dynamics (5.3) is linear, searching for Lyapunov functions is well understood and presented in Appendix C.1. However, when the dynamics is nonlinear, things can get very complicated. In the next, I want to introduce a general framework for searching Lyapunov certificates for nonlinear systems that is based on convex optimization. This framework, although having deep connections with many other disciplines such as algebraic geometry, theoretical computer science, and mathematical optimization, is based on a very simple intution that we all have since high school. Example 5.5 (A Simple Example for Certifying Nonnegativity) Suppose I give you a polynomial of a single variable \\(x \\in \\mathbb{R}\\) \\[ p(x) = x^2 + 2x + 1 \\] and ask you if \\(p(x) \\geq 0\\) for all \\(x\\). You would not hesitate to answer “yes”, because you know \\[ p(x) = (x+1)^2 \\] is the square of \\(x+1\\) and hence must be nonnegative. Let me make it more challenging. Suppose I give you a different polynomial \\[ p(x) = -x^4 + 2 x^2 + x + 1 \\] and ask you if \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\) (instead of any \\(x \\in \\mathbb{R}\\)). At first glance, it seems much harder to answer this question because (i) we have a constraint set \\(x \\in [-1,1]\\), and (ii) the polynomial \\(p(x)\\) has a higher degree and it is not a polynomial that we are very famliar with (compared to \\(p(x) = x^2 + 2x +1\\)). However, if I show you that \\(p(x)\\) can be written as \\[\\begin{equation} p(x) = -x^4 + 2 x^2 + 2x + 1 = (x+1)^2 + x^2 (1 - x^2), \\tag{5.9} \\end{equation}\\] it becomes easy again to certify that \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\). Why? First notice that \\((x+1)^2 \\geq 0\\) for any \\(x \\in \\mathbb{R}\\), Then notice that \\(1 - x^2 \\geq 0\\) for any \\(x \\in [-1,1]\\), and \\(x^2 \\geq 0\\) for any \\(x\\). Therefore, \\(x^2 (1-x^2) \\geq 0\\) for any \\(x \\in [-1,1]\\). Combining the above two reasonings, it becomes clear \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\). What we have learned from this simple example is that Given a polynomial \\(p(x)\\) and a constraint set \\(x \\in \\mathcal{X} \\subseteq \\mathbb{R}^n\\), if we can write \\(p(x)\\) as a sum of a finite number of products \\[ p(x) = \\sum_{i=1}^K \\sigma_i(x) g_i(x) \\] where \\(\\sigma_i(x)\\) is a polynomial that we know is always nonnegative for any \\(x \\in \\mathbb{R}^n\\) (just like \\((x+1)^2\\) and \\(x^2\\) in (5.9)), and \\(g_i(x)\\) is a polynomial that we know is always nonnegative for any \\(x\\) in the constraint set \\(\\mathcal{X}\\) (just like \\(1-x^2\\) for the set \\([-1,1]\\) in (5.9)), then we have a certificate that \\(p(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\). With this simple intuition, let me now formalize the framework of sum of squares (SOS) certificates for proving nonnegativity (also known as positivstellensatz, or in short P-satz). Positivstellensatz, Sum of Squares, and Convex Optimization Basic Semialgebraic Set. Let \\(x = [x_1,\\dots,x_n] \\in \\mathbb{R}^n\\) be a list of variables, we define a basic semialgebraic set as \\[\\begin{equation} \\mathcal{X} = \\{ x \\in \\mathbb{R}^{n} \\mid p_i(x) = 0, i = 1,\\dots,l_{\\mathrm{eq}}; p_i(x) \\geq 0, i=l_{\\mathrm{eq}}+1,\\dots,l_{\\mathrm{eq}} + l_{\\mathrm{ineq}} \\} \\tag{5.10} \\end{equation}\\] where \\(p_i(x),i=1,\\dots,l_{\\mathrm{eq}}+l_{\\mathrm{ineq}}\\) are polynomial functions in \\(x\\). In other words, the set \\(\\mathcal{X}\\) is a subset of \\(\\mathbb{R}^n\\) that is defined by \\(l_{\\mathrm{eq}}\\) equality constraints and \\(l_{\\mathrm{ineq}}\\) inequality constraints. Observe that a basic semialgebraic set can capture a lot of the common constraint sets, such as a unit sphere, a unit ball, and a box (try this for yourself). Positivstellensatz. We are now given the same question as in Example 5.5. Suppose I give you another polynomial function \\(p_0(x)\\), how can you tell me if \\(p_0(x)\\) is nonnegative for any \\(x\\) in the basic semialgebraic set \\(\\mathcal{X}\\)? That is, to verify if \\[ p_0(x) \\geq 0, \\quad \\forall x \\in \\mathcal{X}. \\] Formalizing the intution obtained from Example 5.5, you will say if someone can produce a decomposition of \\(p_0(x)\\) as \\[\\begin{equation} p_0(x) = \\sigma_0(x) + \\sum_{i=1}^{l_{\\mathrm{ineq}}} \\sigma_i(x) p_{i + l_{\\mathrm{eq}}}(x) + \\sum_{i=1}^{l_{\\mathrm{eq}}} \\lambda_i(x) p_{i}(x), \\tag{5.11} \\end{equation}\\] where \\(\\sigma_0,\\sigma_1,\\dots,\\sigma_{l_{\\mathrm{ineq}}}\\) are “some type of” polynomials that we know are always nonnegative (for any \\(x \\in \\mathbb{R}^n\\)), and \\(\\lambda_1,\\dots,\\lambda_{l_{\\mathrm{eq}}}\\) are arbitrary polynomials. Then I have a “certificate” that \\(p_0(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\). Why? The reasoning is exactly the same as before. \\(\\sigma_0(x) \\geq 0\\) for any \\(x\\), \\(\\sigma_i(x) p_{i+l_{\\mathrm{eq}}}(x) \\geq 0, i=1,\\dots,l_{\\mathrm{ineq}}\\) for any \\(x \\in \\mathcal{X}\\), because (a) \\(\\sigma_i(x) \\geq 0\\) for any \\(x\\), and (b) \\(p_{i+l_{\\mathrm{eq}}}(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\) by definition of the basic semialgebraic set (5.10), \\(\\lambda_i(x) p_i (x) = 0, i=1,\\dots,l_{\\mathrm{eq}}\\) for any \\(x \\in \\mathcal{X}\\) by definition of the basic semialgebraic set (5.10). We call \\(\\sigma_i\\)’s “nonnegative polynomial multipliers”, and \\(\\lambda_i\\)’s “polynomial multipliers”. Sum-of-Squares. Now it comes the key question: what type of polynomials should we choose as the nonnegative polynomial multipliers? Ideally, this type of polynomials should be always (trivially) nonnegative, and have a nice representation for its unknown parameters (coefficients). Looking back at our choice of multipliers, i.e., \\((x+1)^2\\) and \\(x^2\\) in Example 5.5, it is natural to come up with the choice of a “sum-of-squares” (SOS) polynomial. Definition 5.9 (Sum-of-Squares Polynomial) A polynomial \\(\\sigma(x)\\) is called an SOS polynomial if \\[ \\sigma(x) = \\sum_{i=1}^k q_i^2(x), \\] i.e., \\(\\sigma(x)\\) can be written as a sum of \\(k\\) squared polynomials. OK, an SOS polynomial is trivially nonnegative (satisfying requirement (a) above), but does it have a nice representation for its parameters? The following Lemma gives us an affirmative answer. Lemma 5.1 (SOS Polynomial and Positive Semidefinite Matrix) A polynomial \\(\\sigma(x)\\) is SOS if and only if \\[ \\sigma(x) = [x]_d^T Q [x]_d \\] for some \\(Q \\succeq 0\\), where \\([x]_d\\) is the vector of monomials in \\(x\\) of degree up to \\(d\\). For example, if \\(x \\in \\mathbb{R}^2\\) and \\(d = 2\\), then \\[ [x]_2 = [1,x_1,x_2,x_1^2,x_1x_2,x_2^2]^T. \\] With the choice of \\(\\sigma(x)\\) as SOS polynomials, we are now ready to explicitly search for a nonnegativity certificate in the form of (5.11): \\[\\begin{equation} \\begin{split} \\text{find} &amp; \\quad \\{\\sigma_i \\}_{i=0}^{l_{\\mathrm{ineq}}}, \\{ \\lambda_i \\}_{i=1}^{l_{\\mathrm{eq}}} \\\\ \\text{subject to} &amp; \\quad p_0(x) = \\sigma_0(x) + \\sum_{i=1}^{l_{\\mathrm{ineq}}} \\sigma_i(x) p_{i + l_{\\mathrm{eq}}}(x) + \\sum_{i=1}^{l_{\\mathrm{eq}}} \\lambda_i(x) p_{i}(x), \\\\ &amp; \\quad \\sigma_i \\text{ is SOS}, i=0,\\dots,l_{\\mathrm{ineq}}, \\\\ &amp; \\quad \\lambda_i \\text{ is polynomial}, i=1,\\dots,l_{\\mathrm{eq}} \\\\ &amp; \\quad \\mathrm{deg}(\\sigma_0) \\leq 2 \\kappa, \\mathrm{deg}(\\sigma_i p_{i+l_{\\mathrm{eq}}}) \\leq 2 \\kappa, i=1,\\dots,l_{\\mathrm{ineq}}, \\\\ &amp; \\quad \\mathrm{deg}(\\lambda_i p_i) \\leq 2 \\kappa, i=1,\\dots,l_{\\mathrm{eq}}. \\end{split} \\tag{5.12} \\end{equation}\\] Bounding the Degree. The careful reader realizes that in (5.12) we have added constraints on the degrees of the polynomial multipliers \\(\\sigma_i\\)’s and \\(\\lambda_i\\)’s.6 Precisely, we choose an integer \\(\\kappa\\), which we call the relaxation order, such that \\[ 2 \\kappa \\geq \\max \\{ \\mathrm{deg}(p_i(x)) \\}_{i=0}^{l_{\\mathrm{eq}} + l_{\\mathrm{ineq}}}, \\] and restrict the products \\(\\sigma_i p_{i+l_{\\mathrm{eq}}}\\)’s and \\(\\lambda_i p_i\\)’s to have degrees at most \\(2\\kappa\\). With this, we are explicitly limiting the degrees of the multipliers \\(\\sigma_i\\)’s and \\(\\lambda_i\\)’s, and hence asking the formulation (5.12) to search for a finite number of parameters (otherwise, if the degree of the multipliers is unbounded, then the number of parameters to be searched is infinite). Convex Optimization. The last crucial (and surprising) observation is that the problem (5.12) is a convex optimization! This is due to the following three reasons The polynomial multipliers \\(\\lambda_i\\)’s can be fully parametrized by their coefficients, and these coefficients can be arbitrary vectors. Precisely, if \\(\\lambda(x)\\) is a polynomial with degree up to \\(d\\), then \\[ \\lambda(x) = c^T [x]_d, \\] where \\([x]_d\\) is the vector of monomials in \\(x\\) of degree up to \\(d\\), and \\(c\\) is the vector of coefficients. The SOS multipliers \\(\\sigma_i\\)’s can be fully parametrized by their coefficients, and these coefficients are positive semidefinite matrices, according to Lemma 5.1. The equality constraint of decomposing \\(p_0(x)\\) as a sum of products in (5.12) therefore becomes a set of affine equality constraints on the parameters of \\(\\lambda_i\\)’s and \\(\\sigma_i\\)’s, by matching coefficients of the monomials on the left-hand size and the right-hand side. Therefore, the problem (5.12) is a convex semidefinite program (SDP). There are multiple software packages, e.g., SOSTOOLS, YALMIP, SumOfSquares.py, that allow us to model our problem in the form of (5.12), convert the formulation into SDPs, and pass them to SDP solvers (such as MOSEK). We will see an example of this soon. Extensions. I want to congratulate, and welcome you to enter the world of SOS relaxations! Like I said before, this is an active area of research and the framework I just introduced is just a tip of the iceberg. Therefore, before I end this tutorial, I want to point out several extensions of the SOS framework. Necessary Condition. We have seen that a decomposition in the form of (5.12) is a sufficient condition to prove the nonnegativity of \\(p_0(x)\\). Is it also a necessary condition? That is, for any \\(p_0(x)\\) that is nonnegative on the set \\(\\mathcal{X}\\), does it admit a decomposition in the form of (5.12)? In general, the answer is no, and there exist nonnegative polynomials that cannot be written in the form of SOS decompositions (e.g., the Motzkin’s polynomial). However, with certain assumptions on the set \\(\\mathcal{X}\\), the decomposition (5.12) is also necessary for nonnegativity! A well-known assumption is called the Archimedean condition (which, roughly speaking, requires the set \\(\\mathcal{X}\\) to be compact)). I suggest you to read (Blekherman, Parrilo, and Thomas 2012) for more details. Global Polynomial Optimization. The SOS framework can be used for global optimization of polynomials in a straightforward way. Consider the polynomial optimization problem (POP) \\[ \\min_{x \\in \\mathcal{X}} p_0(x), \\] where one seeks the global minimum of the polynomial \\(p_0(x)\\) on the set \\(\\mathcal{X}\\). A POP is generally a nonconvex optimization problem, and it is difficult to obtain a globally optimal solution. However, with a slight change of perspective, we can write the problem above equivalently as \\[\\begin{equation} \\begin{split} \\max &amp; \\quad \\gamma \\\\ \\text{subject to} &amp; \\quad p_0(x) - \\gamma \\geq 0, \\quad \\forall x \\in \\mathcal{X}. \\end{split} \\tag{5.13} \\end{equation}\\] Basically I want to push the lower bound \\(\\gamma\\) as high as possible. The constraint in (5.13) asks \\(p_0(x) -\\gamma\\) to be nonnegative on \\(\\mathcal{X}\\). With the SOS framework introduced above, we can naturally relax it to \\[\\begin{equation} \\begin{split} \\max &amp; \\quad \\gamma \\\\ \\text{subject to} &amp; \\quad p_0(x) - \\gamma \\quad \\text{is SOS on} \\quad \\mathcal{X}, \\end{split} \\tag{5.14} \\end{equation}\\] where the “SOS on \\(\\mathcal{X}\\)” constraint is exactly the problem (5.12). Therefore, we have relaxed the nonconvex optimization (5.13) into a convex problem (5.14)! Moreover, by increasing the relaxation order \\(\\kappa\\), we obtain a sequence of lower bounds that asymptotically converge to the true global optimum of the nonconvex problem (5.13). This is called Lasserre’s hierarchy of moment-SOS relaxations, originally proposed by Lasserre in the seminal work (Jean B. Lasserre 2001). As this name suggests, the dual problem to the SOS relaxation (5.14) is called the moment relaxation. Lasserre’s hierarchy has recently gained a lot of attention due to the empirical observation in many engineering disciplines that the convergence to global optimum is finite, i.e., by solving the convex problem (5.14) at a finite relaxation order \\(\\kappa\\), an exact global optimizer of the original nonconvex problem (5.13) can be extracted. For a pragmatic introduction to the moment relaxation, I suggest to read Section 2.2 of (H. Yang and Carlone 2022). For more applications of Lasserre’s hierarchy, please refer to (Jean Bernard Lasserre 2009). Scalability. I have to warn you that there is no free lunch. The fact that so many challenging problems can be relaxed or restated as convex optimization problems should send you an alert. Does this mean that we can use convex optimization to solve all the challenging problems? Well, although we hope this is the case, in practice we are limited by the computational resources. The caveat is that the problem (5.12) and (5.14), despite being convex, grows very large as the dimension \\(n\\) and relaxation order \\(\\kappa\\) increases. Another way of saying this is that, we seek to solve small-to-medium scale nonconvex problems with large-scale convex problems. Unfortunately, today’s SDP solver cannot solve all the problems we formulate, and hence a major research direction in the mathematical optimization community is to develop SDP solvers that are more scalable. You can read (H. Yang et al. 2022) and references therein for more details. Non-SOS Certificates. Nobody is preventing us to use a different choice of nonnegative polynomial multipliers (other than SOS multipliers) in (5.11). For example, one can use a decomposition as the sum of nonnegative circuit polynomials (Wang 2022) or signomials (Murray, Chandrasekaran, and Wierman 2021). However, to the best of my knowledge, non-SOS certificates are far less popular than SOS certificates. There are many other extensions to the SOS framework, and a complete enumeration is beyond the scope of this lecture notes. For the connection between SOS and theoretical computer science, you can see the lecture notes by Boaz Barak and David Steurer. There are also more recent monographs about SOS, for example (Magron and Wang 2023) and (Nie 2023). I plan to introduce these in more details in an upcoming graduate-level class at Harvard. That was a long detour from Lyapunov analysis! The SOS machinery will come back later when we study multiple other topics in optimal control and estimation. But now let us show how to tackle the problem of computing Lyapunov certificates using the SOS machinery. According to Theorem 5.2, given a set \\(\\mathcal{X}\\) that contains an equilibrium point \\(x^\\star\\), if we can find a Lyapunov function \\(V(x)\\) such that \\(V(x)\\) is positive definite on \\(\\mathcal{X}\\) and \\(\\dot{V}(x)\\) is negative definite on \\(\\mathcal{X}\\), then the equilibrium point \\(x^\\star\\) is locally asymptotically stable. With the SOS machinery, we can search for a \\(V(x)\\) that is a polynomial as \\[\\begin{align} \\text{find} &amp; \\quad V(x) \\\\ \\text{subject to} &amp; \\quad V(x) - \\epsilon_1 \\Vert x - x^\\star \\Vert^2 \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ &amp; \\quad - \\epsilon_2 \\Vert x - x^\\star \\Vert^2 - \\frac{\\partial V(x)}{\\partial x} f(x) \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ &amp; \\quad V(x^\\star) = 0, \\end{align}\\] where \\(\\epsilon_1, \\epsilon_2 &gt; 0\\) are (small) positive constants. This is a convex optimization problem, just like (5.12) (try to convince yourself my claim is true). Similarly, we can choose a relaxation order \\(\\kappa\\) and solve the above problem. If a solution exists, then we find a valid Lyapunov certificate. Let us apply it to the simple pendulum to synthesize local stability certificates. Example 5.6 (Computing Lyapunov Local Stability Certificate for the Simple Pendulum with Convex Optimization) The SOS framework works with polynomials, so let us first write the pendulum dynamics in polynomial form via a change of coordinate \\(x = [\\mathfrak{s}, \\mathfrak{c}, \\dot{\\theta}]^T\\) with \\(\\mathfrak{s} = \\sin \\theta\\), \\(\\mathfrak{c} = \\cos\\theta\\): \\[ \\begin{cases} \\dot{\\mathfrak{s}} = \\mathfrak{c} \\dot{\\theta} \\\\ \\dot{\\mathfrak{c}} = -\\mathfrak{s} \\dot{\\theta} \\\\ \\ddot{\\theta} = - \\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\mathfrak{s}) \\end{cases}. \\] We will use \\(m = 1, l = 1, b=0.1\\) for our numerical experiment. We want to find a local Lyapunov certificate in the compact set \\[\\begin{equation} \\theta \\in \\left[-\\arccos \\frac{3}{4}, \\arccos \\frac{3}{4} \\right], \\quad \\dot{\\theta} \\in \\left[- \\frac{\\pi}{2}, \\frac{\\pi}{2} \\right]. \\tag{5.15} \\end{equation}\\] In the new coordinates \\(x\\), this is equivalent to the semialgebraic set \\[ \\mathcal{X} = \\left\\{ x \\in \\mathbb{R}^{3} \\mid \\mathfrak{s}^2 + \\mathfrak{c}^2 = 1, \\dot{\\theta}^2 \\leq \\frac{\\pi^2}{4}, \\mathfrak{c} \\geq \\frac{3}{4} \\right\\}. \\] Denoting the bottomright equilibrium point as \\(x_e = [0,1,0]^T\\), and with \\(\\epsilon_1,\\epsilon_2 &gt; 0\\) two positive constants, we can seek a Lyapunov function \\(V(x)\\) that satisfies the following conditions \\[\\begin{align} V(x) \\geq \\epsilon_1 (x - x_e)^T (x - x_e), \\quad \\forall x \\in \\mathcal{X} \\tag{5.16}\\\\ \\dot{V}(x) = \\frac{\\partial V}{\\partial x} \\dot{x} \\leq - \\epsilon_2 (x - x_e)^T (x - x_e), \\quad \\forall x \\in \\mathcal{X} \\tag{5.17}\\\\ V(x_e) = 0, \\quad \\dot{V}(x_e) = 0 \\tag{5.18} \\end{align}\\] where (5.16) ensures \\(V(x)\\) is positive definite, (5.17) ensures \\(\\dot{V}(x)\\) is negative definite, and (5.18) ensures \\(V(x),\\dot{V}(x)\\) vanish at the equilibrium point. To leverage the power of convex optimization, we can relax the positivity constraints as SOS constraints \\[\\begin{align} V(x) - \\epsilon_1 (x - x_e)^T (x - x_e) \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ - \\epsilon_2 (x - x_e)^T (x - x_e) - \\frac{\\partial V}{\\partial x} \\dot{x}\\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ V(x_e) = 0, \\quad \\dot{V}(x_e) = 0. \\end{align}\\] If we limit the degree of \\(V\\) to \\(2\\), choose the relaxation order \\(\\kappa = 2\\), and \\(\\epsilon_1 = \\epsilon_2 = 0.01\\), we obtain a solution \\[ V(x) = 2.7982 \\mathfrak{s}^2 + 0.086248 \\mathfrak{s} \\dot{\\theta} + 2.4548\\mathfrak{c}^2 + 0.88117 \\dot{\\theta}^2 - 16.6277 \\mathfrak{c} + 14.1728 \\] with the time derivative \\[ \\dot{V}(x) = 0.68675 \\mathfrak{s} \\mathfrak{c} \\dot{\\theta} + 0.086248* \\mathfrak{c} \\dot{\\theta}^2 - 0.84523 \\mathfrak{s}^2 - 0.65191 \\mathfrak{s} \\dot{\\theta} - 0.17623 \\dot{\\theta}^2. \\] Plotting \\(V(x)\\) in the constraint set (5.15) using \\((\\theta, \\dot{\\theta})\\) coordinates, we get Figure 5.2: Lyapunov local stability certificate computed via convex optimization. and verify that \\(V(x)\\) is locally positive definite. Plotting \\(\\dot{V}(x)\\) in the constraint set (5.15) using \\((\\theta, \\dot{\\theta})\\) coordinates, we get Figure 5.3: Derivative of the Lyapunov local stability certificate computed via convex optimization. and verify that \\(\\dot{V}(x)\\) is locally negative definite. You should try the code for this example here. 5.2 Controlled Systems We can generalize the notion of a Lyapunov function for an autonomous system to the notion of a Control Lyapunov Function (CLF) for a controlled system (Sontag 1983). Consider a controlled system \\[\\begin{equation} \\dot{x} = f(x,u) \\tag{5.19} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) the control, and \\(f\\) is continuously differentiable. Let \\(x = 0, u=0\\) be an equilibrium point, i.e., \\(f(0,0) = 0\\). The existence of a CLF guarantees that we can design a controller to stabilize the equilibrium point. Theorem 5.6 (Control Lyapunov Function) Let \\(V(x): \\mathbb{X} \\rightarrow \\mathbb{R}\\) be a continuously differentiable positive definite function, i.e., \\(V(0) = 0\\) and \\(V(x) &gt; 0, \\forall x \\in \\mathbb{X} \\backslash \\{ 0 \\}\\). If there exists \\(\\rho &gt; 0\\) such that \\(\\Omega := \\{ x \\in \\mathbb{X} \\mid V(x) \\leq \\rho \\}\\) is bounded and \\[\\begin{equation} \\min_{u \\in \\mathbb{U}} \\dot{V}(x) = \\frac{\\partial V}{\\partial x} f(x,u) &lt; 0, \\quad \\forall x \\in \\Omega \\backslash \\{ 0 \\}, \\tag{5.20} \\end{equation}\\] then \\(V(x)\\) is called a control Lyapunov function. In this case, if the system starts within the set \\(\\Omega\\), then there exists a controller \\(u(t)\\) that drives the system towards the equilibrium point \\(0\\). In Theorem 5.6, it is clear that \\(Omega\\) is a control invariant set. In fact, \\(Omega\\) is an inner approximation of the region of attraction to the equilibrium point \\(0\\). Deploying a CLF. Now we observe that, when a CLF \\(V(x)\\) is given, it can help us design an asymptotically stablizing controller. Consider a special instance of the controlled system (5.19) that is control-affine \\[\\begin{equation} \\dot{x} = f_1(x) + f_2(x)u. \\tag{5.21} \\end{equation}\\] Clearly, the dynamics (5.21) is affine in the control \\(u\\). Almost all robotics systems are control affine, so (5.21) is quite general. Suppose we are given an arbitrary controller \\(u_d(t)\\), which may or may not stabilize the system towards the equilibrium point (e.g., \\(u_d(t)\\) may be obtained from neural networks). We can use the CLF \\(V(x)\\) to correct \\(u_d(t)\\) so that we always obtain a stabilizing feedback controller \\[\\begin{equation} \\begin{split} u(x(t)) = \\arg\\min_{u \\in \\mathbb{U}} &amp; \\quad \\Vert u - u_d(t) \\Vert^2 \\\\ \\text{subject to} &amp; \\quad \\dot{V}(x) = \\frac{\\partial V}{\\partial x} (f_1(x) + f_2(x) u) &lt; 0. \\end{split} \\tag{5.22} \\end{equation}\\] Note that the optimization (5.22) is actually a convex optimization problem! Since we are minimizing over \\(u\\) (the state \\(x(t)\\) is given), the objective in (5.22) is quadratic in \\(u\\) and the constraint in (5.22) is linear in \\(u\\), implying (5.22) is a quadratic optimization problem. The CLF condition (5.20) ensures that (5.22) is always feasible. Intuitively, optimization (5.22) seeks to find the feedback controller that (a) minimally modifies the given arbitrary controller \\(u_d(t)\\), and (b) guarantees stabilization towards the equilibrium. Verification and Synthesis of a CLF. When a CLF is given, deploying it to compute a stablizing controller is easy. However, verifying if a candidate function is indeed a control Lyapunov function is a highly nontrivial task. In fact, it is an active area of research. The interested reader can refer to (Kang et al. 2023) and (Dai and Permenter 2023). 5.3 Non-autonomous Systems Lemma 5.2 (Barbalat's Lemma) Let \\(f(t)\\) be differentiable, if \\(\\lim_{t \\rightarrow \\infty} f(t)\\) is finite, and \\(\\dot{f}(t)\\) is uniformly continuous,7 then \\[ \\lim_{t \\rightarrow \\infty} \\dot{f}(t) = 0. \\] Theorem 5.7 (Barbalat's Stability Certificate) If a scalar function \\(V(x,t)\\) satisfies \\(V(x,t)\\) is lower bounded, \\(\\dot{V}(x,t)\\) is negative semidefinite \\(\\dot{V}(x,t)\\) is uniformly continuous then \\(\\dot{V}(x,t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). Proof. \\(V(x,t)\\) is lower bounded and \\(\\dot{V}\\) is negative semidefinite implies the limit of \\(V\\) as \\(t \\rightarrow \\infty\\) is finite (note that \\(V(x,t) \\leq V(x(0),0)\\)). Then the theorem clearly follows from Barbalat’s Lemma 5.2. References Blekherman, Grigoriy, Pablo A Parrilo, and Rekha R Thomas. 2012. Semidefinite Optimization and Convex Algebraic Geometry. SIAM. Dai, Hongkai, and Frank Permenter. 2023. “Convex Synthesis and Verification of Control-Lyapunov and Barrier Functions with Input Constraints.” In 2023 American Control Conference (ACC), 4116–23. IEEE. Dawson, Charles, Sicun Gao, and Chuchu Fan. 2023. “Safe Control with Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction Methods for Robotics and Control.” IEEE Transactions on Robotics. Kang, Shucheng, Yuxiao Chen, Heng Yang, and Marco Pavone. 2023. “Verification and Synthesis of Robust Control Barrier Functions: Multilevel Polynomial Optimization and Semidefinite Relaxation.” In 2023 62nd IEEE Conference on Decision and Control (CDC). Lasserre, Jean B. 2001. “Global Optimization with Polynomials and the Problem of Moments.” SIAM Journal on Optimization 11 (3): 796–817. Lasserre, Jean Bernard. 2009. Moments, Positive Polynomials and Their Applications. Vol. 1. World Scientific. Magron, Victor, and Jie Wang. 2023. Sparse Polynomial Optimization: Theory and Practice. World Scientific. Murray, Riley, Venkat Chandrasekaran, and Adam Wierman. 2021. “Signomial and Polynomial Optimization via Relative Entropy and Partial Dualization.” Mathematical Programming Computation 13: 257–95. Nie, Jiawang. 2023. Moment and Polynomial Optimization. SIAM. Sontag, Eduardo D. 1983. “A Lyapunov-Like Characterization of Asymptotic Controllability.” SIAM Journal on Control and Optimization 21 (3): 462–71. Vinograd, Robert Èlyukimovich. 1957. “Inapplicability of the Method of Characteristic Exponents to the Study of Non-Linear Differential Equations.” Matematicheskii Sbornik 83 (4): 431–38. Wang, Jie. 2022. “Nonnegative Polynomials and Circuit Polynomials.” SIAM Journal on Applied Algebra and Geometry 6 (2): 111–33. Yang, Heng, and Luca Carlone. 2022. “Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite Relaxations and Scalable Global Optimization.” IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (3): 2816–34. Yang, Heng, Ling Liang, Luca Carlone, and Kim-Chuan Toh. 2022. “An Inexact Projected Gradient Method with Rounding and Lifting by Nonlinear Programming for Solving Rank-One Semidefinite Relaxation of Polynomial Optimization.” Mathematical Programming, 1–64. In fact, many of the recent works verify “neural” Lyapunov certificates (and other types of certificates) using this idea, see for example (Dawson, Gao, and Fan 2023).↩︎ The degree of a monomial is the sum of its exponents. For example, \\(\\mathrm{deg}(x_1 x_2^4 x_3^2) = 1 + 4 + 2 = 7\\). The degree of a polynomial is the maximum degree of its monomials. For example, the polynomial \\(p(x) = 1 + x_2 + x_1^2 x_2^3\\) has three monomials with degrees \\(0\\), \\(1\\), and \\(5\\), respectively. Therefore, \\(\\mathrm{deg}(p) = 5\\).↩︎ A sufficient condition for this to hold is that \\(\\ddot{f}\\) exists and is bounded.↩︎ "],["output-feedback.html", "Chapter 6 Output Feedback 6.1 Least-Squares Estimation 6.2 Kalman Filter 6.3 Linear Quadratic Gaussian Control 6.4 Nonlinear Filtering 6.5 State Observer 6.6 Observer Feedback", " Chapter 6 Output Feedback We have presented many algorithms for optimal control of dynamical systems. These algorithms, however, have a strict assumption, i.e., the state of the system is directly available. These controllers are called state-feedback controllers. In many applications, the state of the system is not readily available but measured via sensors. Therefore, a more realistic model of the dynamical system is \\[\\begin{equation} \\begin{split} \\dot{x} &amp;= f(x,u) + w \\\\ y &amp;= h(x,u) + v \\end{split} \\tag{6.1} \\end{equation}\\] where \\(x(t) \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) the state of the system, \\(u(t) \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) the control (or input), \\(y(t) \\in \\mathbb{Y} \\subseteq \\mathbb{R}^{d}\\) the output (i.e., measurement) of the state and control, and \\(f,g\\) the evolution and measurement functions (which are sufficiently smooth). \\(w(t)\\) and \\(v(t)\\) denote disturbance and measurement noise, respectively. The discrete-time analog of (6.1) is \\[\\begin{equation} \\begin{split} x_{k+1} &amp;= f(x_k,u_k) + w_k \\\\ y_k &amp;= h(x_k,u_k) + v_k \\end{split}, \\tag{6.2} \\end{equation}\\] with \\(k\\) denoting the discrete timestep. When faced with systems like (6.1) and (6.2), the controller typically needs to use the history of all measurements, also known as output feedback \\[ u(t) = \\pi(\\{y(\\tau)\\}_{\\tau=0}^t), \\quad u_k = \\pi (\\{y_0,\\dots,y_k\\}). \\] 6.1 Least-Squares Estimation Suppose we are given two random variables \\(x \\in \\mathbb{R}^n\\) and \\(y \\in \\mathbb{R}^m\\), where \\(y\\) is a measurement that provides some information about the model \\(x\\). We are interested in finding an estimator, i.e., a function \\(x(y)\\), that minimizes \\[ \\mathbb{E}_{x,y} \\left\\{ \\Vert x - x(y) \\Vert^2 \\right\\}. \\] The optimal estimator, \\(x^\\star(y)\\), is called a least-squares estimator: \\[ x^\\star(y) = \\arg\\min_{x(\\cdot)} \\mathbb{E}_{x,y} \\left\\{ \\Vert x - x(y) \\Vert^2 \\right\\}. \\] Note that \\(x^\\star(y)\\) is a function with input \\(y\\). Since \\[\\begin{equation} \\mathbb{E}_{x,y}\\left\\{ \\Vert x - x(y) \\Vert^2 \\right\\} = \\mathbb{E}_y \\left\\{ \\mathbb{E}_x \\left\\{ \\Vert x - x(y) \\Vert^2 \\mid y\\right\\} \\right\\}, \\tag{6.3} \\end{equation}\\] it is clear that \\(x^\\star(y)\\) is a least-squares estimator if \\(x^\\star(y)\\) minimizes the conditional expectation in (6.3) for every \\(y \\in \\mathbb{R}^m\\), that is, \\[\\begin{equation} x^\\star(y) = \\arg\\min_{z \\in \\mathbb{R}^n} \\mathbb{E}_x \\left\\{ \\Vert x - z \\Vert^2 \\mid y \\right\\}, \\quad \\forall y \\in \\mathbb{R}^m. \\tag{6.4} \\end{equation}\\] With this observation, we have the following proposition. Proposition 6.1 (Least Squares Estimator) The least squares estimator \\(x^\\star(y)\\) is given by \\[\\begin{equation} x^\\star(y) = \\mathbb{E}_x \\left\\{ x \\mid y \\right\\}, \\quad \\forall y \\in \\mathbb{R}^m, \\tag{6.5} \\end{equation}\\] i.e., \\(x^\\star(y)\\) returns the conditional mean of \\(x\\) given \\(y\\). Proof. We expand the objective in (6.4) \\[ \\mathbb{E}_x \\left\\{ \\Vert x - z \\Vert^2 \\mid y \\right\\} = \\mathbb{E}_x \\left\\{ \\Vert x \\Vert^2 \\mid y \\right\\} - 2 z^T \\mathbb{E}_x \\left\\{x \\mid y \\right\\} + \\Vert z \\Vert^2. \\] Observe that problem (6.4) is an unconstrained quadratic optimization, whose optimal solution can be obtained by setting the gradient of objective (w.r.t. \\(z\\)) to zero: \\[ z = \\mathbb{E}_x \\left\\{ x \\mid y \\right\\}, \\] concluding the proof. Depending on the joint distribution of \\((x,y)\\), the least-squares estimator can be very complicated (due to \\(\\mathbb{E}_x\\left\\{ x \\mid y\\right\\}\\) being complicated) and in general does not admit a clean solution. We now turn to the family of linear estimators. 6.1.1 Linear Least-Squares Estimation A linear estimator \\(x(y)\\) takes the following form \\[ x(y) = Ay + b, \\quad A \\in \\mathbb{R}^{n \\times m}, b \\in \\mathbb{R}^n. \\] The optimal linear estimator \\[\\begin{equation} (A^\\star, b^\\star) = \\arg\\min_{A,b} \\mathbb{E}_{x,y} \\left\\{ \\Vert x - Ay - b \\Vert^2 \\right\\} \\tag{6.6} \\end{equation}\\] is called a linear least-squares estimator. In the special case where \\(x\\) and \\(y\\) are jointly Gaussian random variables, the conditional expectation \\(\\mathbb{E}_{x} \\{x \\mid y \\}\\) is indeed linear, and hence a linear least-squares estimator is also the true least-squares estimator. Proposition 6.2 (Linear Least-Squares Estimator) If the random variables \\((x,y)\\) are jointly Gaussian, then the conditional expectation \\(\\mathbb{E}_x \\{x \\mid y \\}\\) is linear in \\(y\\). Therefore, the linear least-squares estimator (6.6) is also the least-squares estimator. Now let us turn to characterize the linear least-squares estimator. Proposition 6.3 (Solution of Linear Least-Squares Estimator) Let \\(x,y\\) be random variables in \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^m\\) drawn from some underlying distribution. The means and covariances of \\(x\\) and \\(y\\) are given by \\[\\begin{equation} \\begin{split} \\mathbb{E}\\{ x\\} = \\bar{x}, \\quad \\mathbb{E} \\{ y \\} = \\bar{y}. \\\\ \\mathbb{E}\\{ (x - \\bar{x})(x - \\bar{x})^T \\} = \\Sigma_{xx}, \\quad \\mathbb{E}\\{ (y - \\bar{y})(y-\\bar{y})^T \\} = \\Sigma_{yy}, \\\\ \\mathbb{E}\\{ (x - \\bar{x})(y - \\bar{y})^T \\} = \\Sigma_{xy}, \\quad \\mathbb{E}\\{ (y - \\bar{y})(x - \\bar{x})^T \\} = \\Sigma^T_{xy}, \\end{split} \\end{equation}\\] and assume \\(\\Sigma_{yy}\\) is invertible (positive definite). Then the linear least-squares estimator \\(x^\\star(y)\\), i.e., solution of (6.6), is given by \\[\\begin{equation} x^\\star(y) = \\bar{x} + \\Sigma_{xy} \\Sigma^{-1}_{yy} (y - \\bar{y}). \\tag{6.7} \\end{equation}\\] The corresponding error covariance matrix is given by \\[\\begin{equation} \\mathbb{E}_{x,y} \\left\\{ (x - x^\\star(y))(x - x^\\star(y))^T \\right\\} = \\Sigma_{xx} - \\Sigma_{xy} \\Sigma^{-1}_{yy} \\Sigma^T_{xy}. \\tag{6.8} \\end{equation}\\] Proof. Problem (6.6) is an unconstrained quadratic optimization in \\((A,b)\\). Therefore, its optimal solution can be computed by taking the derivative of the objective, i.e., \\(f(A,b) = \\mathbb{E}_{x,y} \\{ \\Vert x - Ay - b \\Vert^2 \\}\\) , w.r.t. \\((A,b)\\) and setting them to zero \\[\\begin{equation} \\begin{split} 0 = \\frac{\\partial f}{\\partial A} = 2 \\mathbb{E}_{x,y} \\{ (b + Ay -x) y^T \\}, \\\\ 0 = \\frac{\\partial f}{\\partial b} = 2 \\mathbb{E}_{x,y} \\{ b + Ay - x \\}. \\end{split} \\tag{6.9} \\end{equation}\\] The second equation gives us the solution of \\(b^\\star\\) \\[\\begin{equation} b^\\star = \\bar{x} - A \\bar{y}. \\tag{6.10} \\end{equation}\\] Plug the solution (6.10) back into the first equation in (6.9), we obtain \\[\\begin{equation} \\mathbb{E}_{x,y} \\{ (A(y - \\bar{y}) - (x - \\bar{x})) y^T\\} = 0. \\tag{6.11} \\end{equation}\\] On the other hand, we have \\[\\begin{equation} \\left( \\mathbb{E}_{x,y} \\{ A (y - \\bar{y}) - (x - \\bar{x})\\} \\right) \\bar{y}^T = 0. \\tag{6.12} \\end{equation}\\] Subtracting (6.12) from (6.11), we have \\[ \\mathbb{E}_{x,y} \\left\\{ (A(y-\\bar{y}) - (x - \\bar{x})) (y - \\bar{y})^T \\right\\} = 0, \\] which is exactly \\[ A \\Sigma_{yy} - \\Sigma_{xy} = 0. \\] Therefore, we obtain the optimal \\(A\\) \\[\\begin{equation} A^\\star = \\Sigma_{xy} \\Sigma^{-1}_{yy}. \\tag{6.13} \\end{equation}\\] Combining (6.13) and (6.10), we get the optimal linear least-squares estimator \\[ x^\\star(y) = \\Sigma_{xy} \\Sigma^{-1}_{yy} y + \\left( \\bar{x} - \\Sigma_{xy} \\Sigma^{-1}_{yy} \\bar{y} \\right) = \\bar{x} + \\Sigma_{xy} \\Sigma^{-1}_{yy} (y - \\bar{y}). \\] The error covariance (6.8) follows by direct computation. We next list a few useful properties of the linear least-squares estimator. Corollary 6.1 (Properties of the Linear Least-Squares Estimator) The linear least-squares estimator \\(x^\\star(y)\\) in (6.7) is unbiased, i.e., \\[ \\mathbb{E}_y \\{ x^\\star(y) \\} = \\bar{x}. \\] The estimation error \\(x - x^\\star(y)\\) is uncorrelated with both \\(y\\) and \\(x^\\star(y)\\), i.e., \\[\\begin{equation} \\begin{split} \\mathbb{E}_{x,y} \\{ (x - x^\\star(y))y^T \\} = 0. \\\\ \\mathbb{E}_{x,y} \\{ (x - x^\\star(y)) (x^\\star(y))^T \\} = 0. \\end{split} \\tag{6.14} \\end{equation}\\] Equation (6.14) is known as the orthogonal projection principle. The next corollary considers the linear least-squares estimator of a new variable \\(z\\) that is a linear function of \\(x\\). Corollary 6.2 Consider in addition to \\(x\\) and \\(y\\), the random vector \\(z\\) defined by \\[ z = Cx, \\] where \\(C \\in \\mathbb{R}^{p \\times n}\\) is given. Then the linear least-squares estimator of \\(z\\) given \\(y\\) is \\[ z^\\star(y) = C x^\\star(y), \\] and the corresponding error covariance matrix is \\[ \\mathbb{E}_{z,y}\\left\\{ (z - z^\\star(y))(z - z^\\star(y))^T \\right\\} = C \\mathbb{E}_{x,y} \\left\\{ (x - x^\\star(y))(x - x^\\star(y))^T \\right\\} C^T, \\] where \\(\\mathbb{E}_{x,y} \\left\\{ (x - x^\\star(y))(x - x^\\star(y))^T \\right\\}\\) is given in (6.8). Proof. Since \\(z = Cx\\), we have \\(\\mathbb{E} \\{ z \\} = C \\bar{x} = \\bar{z}\\), and \\[ \\Sigma_{zz} = \\mathbb{E} \\{ (z - \\bar{z})(z - \\bar{z})^T \\} = C \\Sigma_{xx} C^T, \\\\ \\Sigma_{zy} = \\mathbb{E} \\{ (z - \\bar{z})(y - \\bar{y})^T \\} = C \\Sigma_{xy}, \\\\ \\Sigma_{yz} = \\mathbb{E} \\{ (y - \\bar{y}) (z - \\bar{z})^T \\} = \\Sigma^T_{xy} C^T. \\] The result then follows by applying Proposition 6.3 to \\(z\\) and \\(y\\). The next corollary considers the linear least-squares estimator of \\(x\\) given \\(z\\), a random vector that is a linear function of \\(y\\). Corollary 6.3 Consider in addition to \\(x\\) and \\(y\\), an additional random variable \\(z\\) \\[ z = C y + u, \\] where \\(C \\in \\mathbb{R}^{p \\times m}\\), with rank \\(p\\), and \\(u \\in \\mathbb{R}^p\\) are given. Then the linear least-squares estimator of \\(x\\) given \\(z\\) is \\[ x^\\star(z) = \\bar{x} + \\Sigma_{xy} C^T (C \\Sigma_{yy} C^T)^{-1} (z - C \\bar{y} - u), \\] and the corresponding error covariance matrix is \\[ \\mathbb{E}_{x,z}\\left\\{ (x - x^\\star(z))(x - x^\\star(z))^T \\right\\} = \\Sigma_{xx} - \\Sigma_{xy} C^T (C \\Sigma_{yy} C^T)^{-1} C \\Sigma_{xy}^T. \\] Proof. Since \\(z = Cy + u\\), we have \\(\\bar{z} = \\mathbb{E}\\{ z \\} = C \\bar{y} + u\\), and \\[ \\Sigma_{zz} = \\mathbb{E}\\{ (z - \\bar{z})(z - \\bar{z})^T \\} = C \\Sigma_{yy} C^T, \\\\ \\Sigma_{zx} = \\mathbb{E}\\{ (z - \\bar{z})(x - \\bar{x})^T \\} = C \\Sigma_{xy}^T, \\\\ \\Sigma_{xz} = \\mathbb{E}\\{ (x - \\bar{x})(z - \\bar{z})^T \\} = \\Sigma_{xy}C^T. \\] The result then follows by applying Proposition 6.3. Frequently, we want to estimate a vector of parameters \\(x \\in \\mathbb{R}^n\\) given a measurement vector \\(z \\in \\mathbb{R}^m\\) of the form \\(z = C x + v\\) with \\(C \\in \\mathbb{R}^{m \\times n}\\) a given matrix and \\(v \\in \\mathbb{R}^m\\) a noise vector. The following corollary gives the linear least squares estimator \\(x^\\star(z)\\) and its error covariance. Corollary 6.4 (Linear Least-Squares Estimator with Noise) Let \\[ z = C x + v, \\] where \\(C \\in \\mathbb{R}^{m \\times n}\\) is a given matrix, and the random variable \\(x\\) and \\(v\\) are uncorrelated. Denote \\[ \\mathbb{E}\\{x \\} = \\bar{x}, \\quad \\mathbb{E}\\{ (x - \\bar{x})(x - \\bar{x})^T \\} = \\Sigma_{xx}, \\\\ \\mathbb{E}\\{ v\\} = \\bar{v}, \\quad \\mathbb{E}\\{ (v - \\bar{v})(v - \\bar{v})^T \\} = \\Sigma_{vv}, \\] and assume \\(\\Sigma_{vv}\\) is positive definite. Then the linear least-squares estimator of \\(x\\) given \\(z\\) is \\[ x^\\star(z) = \\bar{x} + \\Sigma_{xx} C^T (C \\Sigma_{xx} C^T + \\Sigma_{vv})^{-1} (z - C \\bar{x} - \\bar{v}), \\] and the corresponding error covariance is \\[ \\mathbb{E}_{x,v} \\{ (x - x^\\star(z))(x - x^\\star(z))^T \\} = \\Sigma_{xx} - \\Sigma_{xx}C^T (C \\Sigma_{xx} C^T + \\Sigma_{vv})^{-1} C \\Sigma_{xx}. \\] Proof. Denote \\[ y = \\begin{bmatrix} x \\\\ v \\end{bmatrix}, \\quad \\bar{y} = \\begin{bmatrix} \\bar{x} \\\\ \\bar{v} \\end{bmatrix}, \\] then clearly we have \\[ x = \\underbrace{ \\begin{bmatrix} I &amp; 0 \\end{bmatrix}}_{C_x} y, \\] and \\[ z = \\underbrace{\\begin{bmatrix} C &amp; I \\end{bmatrix}}_{\\tilde{C}} y. \\] Using Corollary 6.2, we have \\[ x^\\star(z) = C_x y^\\star(z) \\\\ \\mathbb{E}\\{ (x - x^\\star(z))(x - x^\\star(z))^T \\} = C_x \\mathbb{E}\\{(y - y^\\star(z))(y - y^\\star(z))^T \\} C_x^T, \\] where \\(y^\\star(z)\\) is the linear least-sqaures estimator of \\(y\\) given \\(z\\). To obtain \\(y^\\star(z)\\), we can apply Corollary 6.3 with \\(u=0\\) and \\(x=y\\), leading to \\[ y^\\star(z) = \\bar{y} + \\Sigma_{yy} \\tilde{C}^T (\\tilde{C} \\Sigma_{yy} \\tilde{C}^T)^{-1} (z - \\tilde{C}\\bar{y}), \\] and the error covariance \\[ \\mathbb{E}\\{(y - y^\\star(z))(y - y^\\star(z))^T \\} = \\Sigma_{yy} - \\Sigma_{yy} \\tilde{C}^T (\\tilde{C}\\Sigma_{yy}\\tilde{C}^T)^{-1} \\tilde{C} \\Sigma_{yy}. \\] The result then follows by noting \\[ \\Sigma_{yy} = \\begin{bmatrix} \\Sigma_{xx} &amp; 0 \\\\ 0 &amp; \\Sigma_{vv} \\end{bmatrix} \\] because \\(x\\) and \\(v\\) are uncorrelated. The next two corollaries deal with least-squares estimators involving multiple measurements arriving sequentially. In particular, the corollaries show how to modify an existing least-squares estimate \\(x^\\star(y)\\) to obtain \\(x^\\star(y,z)\\) once an additional measurement \\(z\\) becomes available. This is a central operation in Kalman filtering. Corollary 6.5 (Linear Least-Squares Estimator of Uncorrelated Measurements) Consider in addition to \\(x\\) and \\(y\\), an additional random vector \\(z \\in \\mathbb{R}^p\\) that is uncorrelated with \\(y\\). Then the linear least-squares estimator \\(x^\\star(y,z)\\) of \\(x\\) given both \\(y\\) and \\(z\\) has the form \\[ x^\\star(y,z) = x^\\star(y) + x^\\star(z) - \\bar{x}, \\] where \\(x^\\star(y)\\) and \\(x^\\star(z)\\) are the linear least-squares estimates of \\(x\\) given \\(y\\) and given \\(z\\), respectively. Furthermore, the error covariance matrix is \\[ \\mathbb{E}_{x,y,z} \\{ (x - x^\\star(y,z))(x - x^\\star(y,z))^T \\} = \\Sigma_{xx} - \\Sigma_{xy} \\Sigma^{-1}_{yy} \\Sigma^T_{xy} - \\Sigma_{xz} \\Sigma^{-1}_{zz} \\Sigma_{xz}^T, \\] where \\[ \\Sigma_{xz} = \\mathbb{E}_{x,z}\\{ (x - \\bar{x})(z - \\bar{z})^T \\}, \\quad \\Sigma_{zz} = \\mathbb{E}_{z} \\{ (z - \\bar{z})(z - \\bar{z})^T \\}, \\quad \\bar{z} = \\mathbb{E}_z \\{ z \\}, \\] and it is assumed that \\(\\Sigma_{zz}\\) and \\(\\Sigma_{yy}\\) are both invertible. Proof. Let \\[ w = \\begin{bmatrix} y \\\\ z \\end{bmatrix}, \\quad \\bar{w} = \\begin{bmatrix} \\bar{y} \\\\ \\bar{z} \\end{bmatrix}. \\] Then we can apply Proposition 6.3 to compute the linear least-squares estimator of \\(x\\) given \\(w\\) \\[ x^\\star(w) = \\bar{x} + \\Sigma_{xw} \\Sigma^{-1}_{ww} (w - \\bar{w}). \\] On the other hand, we have \\[ \\Sigma_{xw} = \\mathbb{E}\\{ (x - \\bar{x}) \\begin{bmatrix} (y - \\bar{y})^T &amp; ( z - \\bar{z} )^T \\end{bmatrix} \\} = \\begin{bmatrix} \\Sigma_{xy} &amp; \\Sigma_{xz} \\end{bmatrix}, \\] and since \\(y\\) and \\(z\\) are uncorrelated, we have \\[ \\Sigma_{ww} = \\begin{bmatrix} \\Sigma_{yy} &amp; 0 \\\\ 0 &amp; \\Sigma_{zz} \\end{bmatrix}. \\] The result then follows by direct computation. The next corollary generalizes the previous result to the case where \\(y\\) and \\(z\\) are correlated. Corollary 6.6 (Linear Least-Squares Estimator of Correlated Measurements) Consider the situation in Corollary 6.5, but with \\(z\\) and \\(y\\) being correlated \\[ \\Sigma_{yz} = \\mathbb{E}_{y,z} \\{ (y - \\bar{y})(z - \\bar{z})^T \\} \\neq 0. \\] Then \\(x^\\star(y,z)\\) has the following form \\[ x^\\star(y,z) = x^\\star(y) + x^\\star(z - z^\\star(y)) - \\bar{x}, \\] where \\(x^\\star(z - z^\\star(y))\\) denotes the linear least-squares estimate of \\(x\\) given the random vector \\(z - z^\\star(y)\\) and \\(z^\\star(y)\\) is the linear least-squares estimate of \\(z\\) given \\(y\\). Furthermore, the error covariance matrix is \\[ \\mathbb{E}_{x,y,z} \\{ (x - x^\\star(y,z))(x - x^\\star(y,z))^T \\} = \\Sigma_{xx} - \\Sigma_{xy} \\Sigma^{-1}_{yy} \\Sigma^T_{xy} - \\hat{\\Sigma}_{xz} \\hat{\\Sigma}^{-1}_{zz} \\hat{\\Sigma}^T_{xz}, \\] where \\[ \\hat{\\Sigma}_{xz} = \\mathbb{E}_{x,y,z} \\{ (x - \\bar{x})(z - z^\\star(y))^T \\}, \\quad \\hat{\\Sigma}_{zz} = \\mathbb{E}_{y,z} \\{ (z - z^\\star(y))(z - z^\\star(y))^T \\}. \\] Proof. By Proposition 6.3, we know \\(z^\\star(y)\\) is linear in \\(y\\) as \\[ z^\\star(y) = \\bar{z} + \\Sigma_{zy} \\Sigma^{-1}_{yy} (y - \\bar{y}) := B y + c, \\] where \\(B = \\Sigma_{zy} \\Sigma^{-1}_{yy}\\) and \\(c = \\bar{z} - \\Sigma_{zy} \\Sigma^{-1}_{yy} \\bar{y}\\). The linear least-squares estimate of \\(x\\) given \\((y,z)\\) has the form \\[\\begin{equation} x = \\begin{bmatrix} A_y^\\star &amp; A_z^\\star \\end{bmatrix} \\begin{bmatrix} y \\\\ z \\end{bmatrix} + b^\\star, \\tag{6.15} \\end{equation}\\] where \\((A_y^\\star, A_z^\\star, b^\\star)\\) is solution to \\[\\begin{equation} (A_y^\\star, A_z^\\star, b^\\star) = \\arg\\min_{A_y,A_z,b} \\mathbb{E}_{x,y,z} \\left\\{ \\Vert x - A_y y - A_z z - b \\Vert^2 \\right\\}. \\tag{6.16} \\end{equation}\\] The linear least-squares estimate of \\(x\\) given \\((y, z - z^\\star(y))\\) has the form \\[\\begin{equation} x = \\begin{bmatrix} G_y^\\star &amp; G_z^\\star \\end{bmatrix} \\begin{bmatrix} y \\\\ z - By - c \\end{bmatrix} + h^\\star, \\tag{6.17} \\end{equation}\\] where \\((G_y^\\star,G_z^\\star,h^\\star)\\) is solution to \\[\\begin{equation} (G_y^\\star,G_z^\\star,h^\\star) = \\arg\\min_{G_y,G_z,h} \\mathbb{E}_{x,y,z} \\left\\{ \\Vert x - G_y y - G_z (z - By - c) - h \\Vert^2 \\right\\}. \\tag{6.18} \\end{equation}\\] Observe that the objective in (6.18) can be rewritten as \\[ \\mathbb{E}_{x,y,z} \\left\\{ \\Vert x - (G_y - G_z B) y - G_z z - (h - G_z c) \\Vert^2 \\right\\}, \\] when matched with the objective of (6.16) we have \\[ A_y^\\star = G_y^\\star - G_z^\\star B, \\quad A_z^\\star = G_z^\\star, \\quad b^\\star = h^\\star - G_z^\\star c. \\] As a result, the linear least-squares estimator in (6.17) becomes \\[ x = (A_y^\\star + A_z^\\star B) y + A_z^\\star (z - By - c) + b^\\star + A_z^\\star c = A^\\star_y y + A^\\star_z z + b^\\star, \\] which is the same as (6.15). Therefore, we have \\[ x^\\star(y,z) = x^\\star(y, z - z^\\star(y)). \\] From Corollary 6.1, we know \\(y\\) and \\(z - z^\\star(y)\\) is uncorrelated. Consequently, we reduce to the uncorrelated setup in Corollary 6.5 and the result follows. 6.2 Kalman Filter Consider now a linear dynamical system without control \\[\\begin{equation} \\begin{cases} x_{k+1} = A_k x_k + w_k \\\\ y_k = C_k x_k + v_k \\end{cases}, k=0,1,\\dots,N-1, \\tag{6.19} \\end{equation}\\] where \\(x_k \\in \\mathbb{R}^n\\) the state, \\(w_k \\in \\mathbb{R}^n\\) the random disturbance, \\(y_k \\in \\mathbb{R}^d\\) the measurement (output), and \\(v_k \\in \\mathbb{R}^d\\) the measurement noise. We assume \\(x_0,w_0,\\dots,w_{N-1},v_0,\\dots,v_{N-1}\\) are independent random vectors with means \\[ \\mathbb{E}\\{ w_k \\} = \\mathbb{E} \\{ v_k \\} = 0, \\quad k=0,\\dots,N-1, \\] and covariances \\[ \\mathbb{E} \\{ w_k w_k^T \\} = M_k, \\quad \\mathbb{E} \\{ v_k v_k^T \\} = N_k, \\quad k=0,\\dots,N-1, \\] with \\(N_k\\) positive definite for all \\(k\\). We assume the initial state \\(x_0\\) has mean \\(\\mathbb{E}\\{ x_0 \\}\\) and covariance \\[ S = \\mathbb{E} \\{ (x_0 - \\mathbb{E}\\{x_0 \\} ) (x_0 - \\mathbb{E}\\{x_0 \\} )^T \\}. \\] Let \\[ Y_k = (y_0,\\dots,y_k) \\] be the set of measurements up to time \\(k\\), our goal is to obtain the linear least-squares estimate of \\(x_k\\) given \\(Y_k\\), denoted as \\[ x^\\star_{k \\mid k} := x^\\star_k (Y_k), \\] and its error covariance matrix \\[ \\Sigma_{k \\mid k} := \\mathbb{E} \\{ (x_k - x^\\star_{k \\mid k}) (x_k - x^\\star_{k \\mid k})^T\\}. \\] By definition, we have \\[\\begin{equation} x^\\star_{0 \\mid -1} = \\mathbb{E}\\{ x_0 \\}, \\quad \\Sigma_{0 \\mid -1} = S. \\tag{6.20} \\end{equation}\\] The Kalman filter (Rudolph Emil Kalman 1960) will derive a recursion to update \\(x^\\star_{k \\mid k}\\) and \\(\\Sigma_{k \\mid k}\\). Road map. Towards this, let us assume we have computed \\(x^\\star_{k \\mid k-1}\\) and its error covariance \\(\\Sigma_{k \\mid k-1}\\) (the initial condition is given by (6.20)). Our goal is to compute \\(x^\\star_{k+1 \\mid k}\\) and its error covariance \\(\\Sigma_{k+1\\mid k}\\). We will see that, to compute \\(x^\\star_{k+1 \\mid k}\\), we compute \\(x^\\star_{k \\mid k}\\) first, and to compute \\(\\Sigma_{k+1 \\mid k}\\), we compute \\(\\Sigma_{k\\mid k}\\) first, as illustrated below. \\[\\begin{equation} \\begin{split} \\text{Update estimate:}\\quad x^\\star_{k \\mid k-1} \\longrightarrow x^\\star_{k \\mid k} \\longrightarrow x^\\star_{k+1 \\mid k}. \\\\ \\text{Update covariance:}\\quad \\Sigma_{k \\mid k-1} \\longrightarrow \\Sigma_{k \\mid k} \\longrightarrow \\Sigma_{k+1 \\mid k}. \\end{split} \\end{equation}\\] Using the measurement. At time \\(k\\), we receive a new measurement \\[ y_k = C_k x_k + v_k. \\] We have \\[ x^\\star_{k\\mid k} = x^\\star_k (Y_k) = x^\\star_k(Y_{k-1},y_k), \\] by Corollary 6.6, we have \\[\\begin{equation} x^\\star_{k \\mid k} = x^\\star_{k \\mid k-1} + x^\\star_k (y_k - y_k^\\star(Y_{k-1})) - \\mathbb{E}\\{ x_k \\}, \\tag{6.21} \\end{equation}\\] where \\(y_k^\\star(Y_{k-1})\\) is the linear least-squares estimate of \\(y_k\\) given \\(Y_{k-1}\\). Simplify (6.21). First note that by Corollary 6.2 and independence of \\(v_k\\), we have \\[ y_k^\\star(Y_{k-1}) = C_k x^\\star_{k \\mid k-1}. \\] We now need to compute \\(x_k^\\star(y_k - y_k^\\star(Y_{k-1}))\\). We do so by Proposition 6.3. The covariance matrix of \\(y_k - y_k^\\star(Y_{k-1})\\) is, by Corollary 6.2, \\[ \\mathbb{E} \\{ (y_k - y_k^\\star(Y_{k-1}))(y_k - y_k^\\star(Y_{k-1}))^T \\} = C_k \\Sigma_{k \\mid k-1} C_k^T + N_k. \\] The correlation between \\(x_k\\) and \\(y_k - y_k^\\star(Y_{k-1})\\) is \\[\\begin{equation} \\begin{split} \\mathbb{E}\\{ x_k (y_k - y_k^\\star(Y_{k-1}))^T \\} = \\mathbb{E} \\{ x_k(C_k (x_k - x^\\star_{k\\mid k-1}))^T \\} + \\underbrace{ \\mathbb{E}\\{ x_k v_k^T \\} }_{=0 \\text{ by independence of } v_k} \\\\ = \\mathbb{E}\\{(x_k - x^\\star_{k \\mid k-1}) (x_k - x^\\star_{k \\mid k-1})^T \\}C_k^T + \\underbrace{\\mathbb{E}\\{ x^\\star_{k \\mid k-1}(x_k - x^\\star_{k \\mid k-1})^T \\}}_{=0 \\text{ by the orthogonal projection principle}}C_k^T \\\\ = \\Sigma_{k \\mid k-1} C_k^T. \\end{split} \\end{equation}\\] As a result, by Proposition 6.3, we have \\[ x_k^\\star(y_k - y_k^\\star(Y_{k-1})) = \\mathbb{E}\\{x_k \\} + \\Sigma_{k \\mid k-1} C_k^T (C_k \\Sigma_{k \\mid k-1} C_k^T + N_k)^{-1} (y_k - C_k x^\\star_{k \\mid k-1}). \\] Update the estimate. Thus, (6.21) is simplified as \\[\\begin{equation} x^\\star_{k\\mid k} = x^\\star_{k \\mid k-1} + \\Sigma_{k \\mid k-1} C_k^T (C_k \\Sigma_{k \\mid k-1} C_k^T + N_k)^{-1} (y_k - C_k x^\\star_{k \\mid k-1}), \\tag{6.22} \\end{equation}\\] which describes the update from \\(x^\\star_{k \\mid k-1}\\) to \\(x^\\star_{k \\mid k}\\). To update \\(x^\\star_{k+1 \\mid k}\\) from \\(x^\\star_{k \\mid k}\\), by Corollary 6.2, we have \\[\\begin{equation} x^\\star_{k+1 \\mid k} = A_k x^\\star_{k \\mid k}. \\tag{6.23} \\end{equation}\\] Update the covariance. The error covariance \\(\\Sigma_{k \\mid k}\\), by Corollary 6.6, can be written as \\[\\begin{equation} \\Sigma_{k \\mid k} = \\Sigma_{k \\mid k-1} - \\Sigma_{k\\mid k-1} C_k^T (C_k \\Sigma_{k \\mid k-1} C_k^T + N_k)^{-1} C_k \\Sigma_{k \\mid k-1}. \\tag{6.24} \\end{equation}\\] (This requires just a little bit thinking.) To update \\(\\Sigma_{k+1 \\mid k}\\) from \\(\\Sigma_{k \\mid k}\\), we can use Corollary 6.2 and the independence of \\(w_k\\) to obtain \\[\\begin{equation} \\Sigma_{k+1 \\mid k} = A_k \\Sigma_{k \\mid k} A_k^T + M_k. \\tag{6.25} \\end{equation}\\] Equations (6.22)-(6.25) forms the Kalman Filter. Including controls. When the dynamics includes control \\[ x_{k+1} = A_k x_k + B_k u_k + w_k, \\] the only change that needs to be made to the Kalman Filter is that (6.23) should be replaced by \\[\\begin{equation} x^\\star_{k+1 \\mid k} = A_k x^\\star_{k \\mid k} + B_k u_k. \\tag{6.26} \\end{equation}\\] 6.2.1 Steady-State Kalman Filter Combining the two updates on covariance matrices (6.24) and (6.25), we obtain \\[\\begin{equation} \\Sigma_{k+1 \\mid k} = A_k (\\Sigma_{k \\mid k-1} - \\Sigma_{k \\mid k-1} C_k^T (C_k \\Sigma_{k \\mid k-1} C_k^T + N_k)^{-1} C_k \\Sigma_{k \\mid k-1} ) A_k^T + M_k, \\tag{6.27} \\end{equation}\\] which is the same as the discrete-time Riccati equation (2.7) when we study the solution of the finite-horizon LQR problem in Proposition 2.1. When \\(A_k, C_k, M_k, N_k\\) are constant matrices \\[ A_k = A, \\quad C_k = C, \\quad, M_k = M, \\quad N_k = N, \\] the recursion (6.27) tends to the algebraic Riccati equation \\[\\begin{equation} \\Sigma = A(\\Sigma - \\Sigma C^T(C \\Sigma C^T + N)^{-1}C \\Sigma)A^T + M, \\tag{6.28} \\end{equation}\\] for which a unique positive definite solution \\(\\Sigma\\) exists if \\((A,C)\\) is observable and \\((A,D)\\) is controllable with \\(M = D D^T\\). Then, using equation (6.24), we have that \\(\\Sigma_{k \\mid k}\\) tends to \\[ \\bar{\\Sigma} = \\Sigma - \\Sigma C^T (C \\Sigma C^T + N)^{-1} C \\Sigma. \\] This leads to the simple steady-state Kalman filter \\[\\begin{equation} x^\\star_{k \\mid k} = \\underbrace{A x^\\star_{k-1 \\mid k-1}}_{\\text{prediction}} + \\underbrace{\\Sigma C^T(C \\Sigma C^T + N)^{-1} (y_k - CA x_{k-1 \\mid k-1})}_{\\text{feedback correction}}. \\tag{6.29} \\end{equation}\\] 6.2.2 Continuous-time Kalman Filter Consider the continuous-time analog of the linear system in (6.19) \\[\\begin{equation} \\begin{cases} \\dot{x} = A x + Bu + w \\\\ y = C x + v \\end{cases} \\tag{6.30} \\end{equation}\\] where \\(x \\in \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{R}^m\\) the control, \\(y \\in \\mathbb{R}^d\\) the measurement, and \\(w,v\\) are white noises with \\[ \\mathbb{E}\\{w\\}=0, \\quad \\mathbb{E}\\{ w w^T \\} = M, \\quad \\mathbb{E}\\{v\\} = 0, \\quad \\mathbb{E}\\{ v v^T\\} = N. \\] Suppose the initial state \\(x(0)\\) satisfies \\[ \\mathbb{E}\\{ x(0) \\} = \\hat{x}_0, \\quad \\mathbb{E}\\{ (x(0) - \\hat{x}_0) (x(0) - \\hat{x}_0)^T \\} = \\Sigma_0. \\] Then the continuous-time Kalman Filter updates the state estimate as \\[\\begin{equation} \\dot{\\hat{x}} = \\underbrace{A \\hat{x} + Bu}_{\\text{prediction}} + \\underbrace{K (y - C \\hat{x})}_{\\text{feedback correction}}, \\tag{6.31} \\end{equation}\\] where \\(K\\) is a feedback gain computed by \\[ K = \\Sigma C^T N^{-1}, \\] with \\(\\Sigma\\) satisfying the following differential equation \\[\\begin{equation} \\dot{\\Sigma} = A \\Sigma + \\Sigma A^T + M - \\Sigma C^T N^{-1} C \\Sigma. \\tag{6.32} \\end{equation}\\] Similarly, at steady state, the differential equation (6.32) tends to the continuous-time algebraic Riccati equation \\[ 0 = A \\Sigma + \\Sigma A^T + M - \\Sigma C^T N^{-1} C \\Sigma. \\] 6.3 Linear Quadratic Gaussian Control With our knowledge about the Kalman filter, we are ready to solve a particular instance of output-feedback control, known as the Linear Quadratic Gaussian (LQG) control. Consider the discrete-time linear dynamical system: \\[\\begin{equation} \\begin{cases} x_{k+1} = A_k x_k + B_k u_k + w_k \\\\ y_k = C_k x_k + v_k \\end{cases}, \\quad k=0,\\dots,N-1, \\tag{6.33} \\end{equation}\\] where \\(w_k\\) and \\(v_k\\) are white noises with \\[ \\mathbb{E}\\{ w_k \\} = 0, \\mathbb{E}\\{ v_k \\} = 0, \\quad k=0,\\dots,N-1, \\\\ \\mathbb{E} \\{ w_k w_k^T \\} = M_k, \\mathbb{E} \\{ v_k v_k^T \\} = N_k, \\quad k=0,\\dots,N-1. \\] Assume the initial state \\(x_0\\) follows a Gaussian distribution \\(x_0 \\sim \\mathcal{N}(\\bar{x}_0, S)\\). This is the same setup as Section 6.2 Kalman filter. The goal of the LQG control is \\[\\begin{equation} \\min_{u_k,k=0,\\dots,N-1} \\mathbb{E} \\left\\{ x_N^T Q_N x_N + \\sum_{k=0}^{N-1} (x_k^T Q_k x_k + u_k^T R_k u_k) \\right\\}, \\tag{6.34} \\end{equation}\\] where the expectation is taken over the randomness of both \\(w_k\\) and \\(v_k\\). The LQG cost function (6.34) reads exactly the same as the LQR problem in (2.2). However, the key difference between the LQG problem (6.34) and the LQR problem (2.2) is that in LQR the controller has full access to the state \\(x_k\\) of the system, while in LQG the controller only has access to a sequence of measurements \\[ Y_k = \\{y_0,\\dots,y_k \\}. \\] We will now show that the Kalman filter brings us back to the setup of LQR. Let us rewrite the cost \\[ \\mathbb{E}\\{x_k^T Q_k x_k \\} = \\mathbb{E} \\left\\{ \\mathbb{E} \\{ x_k^T Q_k x_k \\mid Y_k \\} \\right\\} = \\mathbb{E} \\left\\{ \\mathbb{E} \\{ (x_k - x^\\star_{k \\mid k} + x^\\star_{k \\mid k})^T Q_k (x_k - x^\\star_{k \\mid k} + x^\\star_{k \\mid k}) \\mid Y_k \\} \\right\\} \\\\ = \\mathbb{E}\\left\\{ \\mathbb{E}\\{ (x_k - x^\\star_{k \\mid k})^T Q_k (x_k - x^\\star_{k \\mid k}) + 2 (x^\\star_{k \\mid k})^T Q_k (x_k - x^\\star_{k \\mid k}) + (x^\\star_{k \\mid k})^T Q_k x^\\star_{k \\mid k} \\mid Y_k\\} \\right\\} \\\\ = \\mathbb{E}\\{ (x^\\star_{k \\mid k})^T Q_k x^\\star_{k \\mid k} \\} + \\underbrace{\\mathbb{E}\\{ \\mathbb{E}\\{ 2 (x^\\star_{k \\mid k})^T Q_k (x_k - x^\\star_{k \\mid k}) \\mid Y_k \\} \\}}_{=0 \\text{ due to orthogonal projection principle}} + \\underbrace{\\mathbb{E}\\{ (x_k - x^\\star_{k \\mid k})^T Q_k (x_k - x^\\star_{k \\mid k}) \\}}_{=\\text{tr}(Q_k \\Sigma_{k \\mid k})}, \\] where \\(x^\\star_{k\\mid k}\\) is the Kalman filter estimate of the state (i.e., the linear least squares estimate, or the conditional expectation). Note that from the Kalman filter we know the error covariance \\(\\Sigma_{k \\mid k}\\) (and hence \\(\\text{tr}(Q_k \\Sigma_{k \\mid k})\\)) is independent from the control. Therefore, the LQG problem (6.34) is equivalent to the following LQR problem \\[\\begin{equation} \\min_{u_k, k=0,\\dots,N-1} \\mathbb{E} \\left\\{ (x^\\star_{N\\mid N})^T Q_N x^\\star_{N\\mid N} + \\sum_{k=0}^{N-1} (x^\\star_{k \\mid k})^T Q_k x^\\star_{k \\mid k} + u_k^T R_k u_k \\right\\}, \\tag{6.35} \\end{equation}\\] for which we know the optimal control is (according to Proposition 2.1) \\[\\begin{equation} u_k^\\star = - \\underbrace{(R_k + B_k^T S_{k+1} B_k)^{-1} B_k^T S_{k+1} A_k}_{K_k} x^\\star_{k \\mid k}, \\tag{6.36} \\end{equation}\\] with \\(S_k\\) computed backwards as \\(S_N = Q_N\\), \\[\\begin{equation} S_k = Q_k + A_k^T \\left[ S_{k+1} - S_{k+1} B_k (R_k + B_k^T S_{k+1} B_k)^{-1} B_k^T S_{k+1} \\right] A_k. \\tag{6.37} \\end{equation}\\] In summary, to implement the optimal controller for LQR problem, we need to Compute \\(S_k\\) backwards in time according to (6.37) Run the Kalman filter (6.22) and (6.23) to obtain \\(x^\\star_{k \\mid k}\\) Compute control according to (6.36) Separation theorem. Our derivation above shows that the optimal output-feedback control for LQG consists of (i) an optimal estimator, the Kalman filter, and (ii) an optimal state-feedback controller, the LQR controller. 6.3.1 Steady-state LQG In steady-state LQG, we consider the dynamical system \\[\\begin{equation} \\begin{cases} x_{k+1} = A x_k + B u_k + w_k \\\\ y_k = C x_k + v_k \\end{cases}, \\quad k=0,\\dots \\end{equation}\\] where \\(\\mathbb{E}\\{ w_k\\} =0, \\mathbb{E}\\{ v_k\\} = 0\\), and \\[ \\mathbb{E}\\{w_k w_k^T \\} = M, \\quad \\mathbb{E}\\{v_k v_k^T \\} = N. \\] The LQG control problem is \\[\\begin{equation} \\min \\mathbb{E} \\left\\{ \\sum_{k=0}^{\\infty} (x_k^T Q_k x_k + u_k^T R_k u_k) \\right\\}. \\end{equation}\\] The optimal controller is \\[ u^\\star_k = - \\left[ (R + B^T S B)^{-1} B^T S A \\right]x^\\star_{k \\mid k}, \\] where \\(S\\) is the solution to the discrete-time algebraic Riccati equation \\[ S = Q + A^T \\left[ S - SB (R + B^T S B)^{-1} B^T S \\right]A, \\] and \\(x^\\star_{k\\mid k}\\) comes from the steady-state Kalman filter \\[ x_{k \\mid k}^\\star = A x^\\star_{k-1\\mid k-1} + B u_{k-1} + \\Sigma C^T (C \\Sigma C^T + N)^{-1} (y_k - C(A x^\\star_{k-1\\mid k-1} + B u_{k-1})), \\] with \\(\\Sigma\\) the solution to another discrete-time algebraic Riccati equation \\[ \\Sigma = M + A\\left[ \\Sigma - \\Sigma C^T (N + C \\Sigma C^T)^{-1} C \\Sigma \\right]A^T. \\] Example 6.1 (LQG Stabilization of Simple Pendulum) Consider the pendulm dynamics that has already been shifted such that \\(x=[\\theta,\\dot{\\theta}]=0\\) represents the upright position that we want to stabilize (cf. Example 2.1) \\[ \\dot{x} = \\begin{bmatrix} x_2 \\\\ \\frac{1}{ml^2} (u - b x_2 + mgl \\sin x_1) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ w \\end{bmatrix}, \\] where \\(w\\) is a random white noise. Linearizing the dynamics around \\(x=0\\) we have \\[ \\dot{x} \\approx \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} &amp; - \\frac{b}{ml^2} \\end{bmatrix}}_{A_c} x + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix}}_{B_c} u + \\begin{bmatrix} 0 \\\\ w \\end{bmatrix}. \\] We convert the continuous-time dynamics to discrete time with a fixed discretization \\(h\\) \\[ x_{k+1} = \\dot{x}_k \\cdot h + x_k = (h \\cdot A_c + I) x_k + (h\\cdot B_c) u_k + h \\begin{bmatrix} 0 \\\\ w \\end{bmatrix} \\\\ = A x_k + B u_k + w_k. \\] We assume we can only observe the angular position of the pendulum \\[ y_k = \\underbrace{\\begin{bmatrix} 1 &amp; 0 \\end{bmatrix}}_{C} x_k + v_k, \\] with \\(v_k\\) some white noise. We start with the true initial state \\(x_0 = [2;-2]\\), but with a noisy estimate \\(x^\\star_{0 \\mid 0} = [0;0]\\). Implementing the steady-state LQG controller produces the following result. Figure 6.1: LQG Stabilization of Simple Pendulum. You can find code for this example here. Closed-loop analysis. Now that we have the Kalman filter and LQR controller implemented, let us take a look at the closed-loop response of the system. To ease our notation, let us denote \\(\\hat{x}_k := x^\\star_{k \\mid k}\\) as the Kalman estimate of state. Then we can write the LQG controller as \\[\\begin{equation} \\begin{cases} \\hat{x}_{k+1} = A \\hat{x}_k + B u_k + L (y_{k+1} - C(A \\hat{x}_k + B u_k) ) &amp; \\text{Kalman filter}\\\\ x_{k+1} = A x_k + B u_k + w_k &amp; \\text{Original dynamics} \\end{cases}, \\tag{6.38} \\end{equation}\\] where \\(L\\) is the Kalman gain. The first equation can be simplified as \\[ \\hat{x}_{k+1} = A \\hat{x}_k + Bu_k + L( C\\underbrace{(A x_k + B u_k + w_k)}_{x_{k+1}} + v_{k+1} - CA\\hat{x}_k - CB u_k )\\\\ = A \\hat{x}_k + Bu_k + L (CA x_k - CA \\hat{x}_k + C w_k + v_{k+1}) \\] Let us denote the estimation error as \\(e_k = x_k - \\hat{x}_k\\), the above equation becomes \\[ \\hat{x}_{k+1} = A \\hat{x}_k + B u_k + LCA e_k + LC w_k + L v_{k+1} \\] Subtracting the equation above from the second equation of (6.38), we have \\[\\begin{equation} e_{k+1} = (A - LCA) e_k + (I - LC) w_k - L v_{k+1}, \\tag{6.39} \\end{equation}\\] which describes the evolution of the error dynamics. Now with \\[ u_k = - K \\hat{x}_k = -K (x_k - e_k) = K e_k - K x_k \\] inserted into the second equation of (6.38), we have \\[\\begin{equation} x_{k+1} = (A - BK) x_k + BK e_k + w_k, \\tag{6.40} \\end{equation}\\] which describes the evolution of the true state dynamics. Combining (6.39) and (6.40), we have the closed-loop system \\[\\begin{equation} \\begin{bmatrix} x_{k+1} \\\\ e_{k+1} \\end{bmatrix} = \\underbrace{\\begin{bmatrix} A - BK &amp; BK \\\\ 0 &amp; A - LCA \\end{bmatrix}}_{A_{\\mathrm{cl}}} \\begin{bmatrix} x_k \\\\ e_k \\end{bmatrix} + \\begin{bmatrix} w_k \\\\ (I - LC) w_k - L v_{k+1} \\end{bmatrix}. \\tag{6.41} \\end{equation}\\] Now observe that if the Kalman gain \\(L\\) is stable, then \\(e_{k}\\) tends to zero with some noise. In this case, if the LQR gain \\(K\\) is stable, then the state \\(x_k\\) also tends to zero with some noise. In fact, since \\(A_{\\mathrm{cl}}\\) is block-diagonal, we know the eigenvalues of \\(A_{\\mathrm{cl}}\\) contains those of \\(A - BK\\) and those of \\(A - LC A\\). 6.3.2 Continuous-time LQG 6.4 Nonlinear Filtering Kalman filter is the optimal filter for linear dynamics, linear measurements, and Gaussian noises. However, interesting real-world systems are often nonlinear \\[\\begin{equation} \\begin{cases} x_{k+1} = f(x_k) + w_k, \\\\ y_k = h(x_k) + v_k \\end{cases}, \\tag{6.42} \\end{equation}\\] where \\(w_k\\) is white disturbance \\(\\mathbb{E}\\{ w_k\\} =0, \\mathbb{E}\\{ w_k w_k^T\\} = M_k\\), \\(v_k\\) is white measurement noise \\(\\mathbb{E}\\{ v_k\\} = 0, \\mathbb{E} \\{v_k v_k^T \\} = N_k\\), and \\(f,g\\) are assumed to be differentiable nonlinear functions. A nonlinear filter follows the general strategy of a Kalman filter \\[\\begin{equation} \\begin{split} \\text{Update estimate:}\\quad \\hat{x}_{k \\mid k-1} \\overset{(3)}{\\longrightarrow} \\hat{x}_{k \\mid k} \\overset{(1)}{\\longrightarrow} \\hat{x}_{k+1 \\mid k}. \\\\ \\text{Update covariance:}\\quad \\Sigma_{k \\mid k-1} \\overset{(4)}{\\longrightarrow} \\Sigma_{k \\mid k} \\overset{(2)}{\\longrightarrow} \\Sigma_{k+1 \\mid k}. \\end{split} \\end{equation}\\] where I have replaced \\(x^\\star\\) with \\(\\hat{x}\\) because the estimator is often not optimal in the case of nonlinear functions. 6.4.1 Extended Kalman Filter The key idea of extended Kalman filter (EKF) is to linearize the nonlinear functions \\(f\\) and \\(h\\). Prediction (1) and (2). To perform prediction update according to the dynamics, we linearize \\(f\\) around \\(\\hat{x}_{k\\mid k}\\) \\[ x_{k+1} = f(x_k) + w_k = f(\\hat{x}_{k\\mid k}) + J_f(\\hat{x}_{k \\mid k})(x_k - \\hat{x}_{k \\mid k}) + w_k + \\text{h.o.t.}, \\] where \\(\\text{h.o.t.}\\) standards for high-order terms, and \\(J_f\\) is the Jacobian of the vector function \\(f\\): \\[ J_f = \\begin{bmatrix} \\frac{\\partial f_1}{x_1} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_n}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_n}{\\partial x_n} \\end{bmatrix}. \\] With this linearization, update (1) of EKF is simply \\[\\begin{equation} \\hat{x}_{k+1 \\mid k} = f(\\hat{x}_{k \\mid k}), \\tag{6.43} \\end{equation}\\] and covariance update (2) is \\[\\begin{equation} \\Sigma_{k+1\\mid k} = M_k + J_f(\\hat{x}_{k \\mid k}) \\Sigma_{k \\mid k} J_f^T (\\hat{x}_{k \\mid k}). \\tag{6.44} \\end{equation}\\] Correction (3) and (4). To perform measurement correction from \\(y_k\\), we linearize \\(h\\) around \\(\\hat{x}_{k \\mid k-1}\\) \\[ y_{k} = h(x_k) + v_k = h(\\hat{x}_{k \\mid k-1}) + J_h(\\hat{x}_{k \\mid k-1}) (x_k - \\hat{x}_{k \\mid k-1}) + v_k + \\text{h.o.t.}. \\] With this linearization, update (3) of EKF is \\[\\begin{equation} \\hspace{-10mm} \\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + \\Sigma_{k\\mid k-1}J_h^T(\\hat{x}_{k\\mid k-1}) ( J_h(\\hat{x}_{k\\mid k-1})\\Sigma_{k \\mid k-1} J_h^T(\\hat{x}_{k\\mid k-1}) + N_k )^{-1}(y_k - h(\\hat{x}_{k\\mid k-1})), \\tag{6.45} \\end{equation}\\] and update (4) of EKF is \\[\\begin{equation} \\hspace{-10mm}\\Sigma_{k \\mid k-1} - \\Sigma_{k \\mid k-1} \\Sigma_{k\\mid k-1}J_h^T(\\hat{x}_{k\\mid k-1}) ( J_h(\\hat{x}_{k\\mid k-1})\\Sigma_{k \\mid k-1} J_h^T(\\hat{x}_{k\\mid k-1}) + N_k )^{-1} J_h(\\hat{x}_{k \\mid k-1}) \\Sigma_{k \\mid k-1}. \\tag{6.46} \\end{equation}\\] Compared with the original Kalman filter in (6.22)-(6.25), we can see that EKF (6.43)-(6.46) (i) replaces \\(A_k\\) with the Jacobian \\(J_f(\\hat{x}_{k \\mid k})\\), (ii) replaces \\(C_k\\) with the Jacobian \\(J_h(\\hat{x}_{k \\mid k-1})\\), and (iii) replaces \\(C_k \\hat{x}_{k \\mid k-1}\\) with \\(h(\\hat{x}_{k \\mid k-1})\\) (those are the expected measurements). When does EKF perform well? Since EKF is based on linearization, it performs well when (i) the functions \\(f,g\\) are locally linear around the linearization points, and (ii) there is less uncertainty in the covariance matrices \\(\\Sigma_{k \\mid k-1},\\Sigma_{k \\mid k}\\). Good illustrations can be found in Fig. 3.5 and Fig. 3.6 of (Thrun, Burgard, and Fox 2005). Guarantees. Under the technical conditions in (Reif et al. 1999), the estimation error of EKF can be guaranteed to be bounded. 6.4.2 Unscented Kalman Filter In nonlinear filtering, usually the distribution of the true state is never a Gaussian distribution. The key idea of unscented Kalman filter is to use a set of so-called “Sigma points” to capture the most important statistical properties of the true distribution, notably the first and second-order moments (i.e., mean and covariance). Sigma points. Given a distribution with mean and covariance \\((\\mu, \\Sigma)\\) (assume the random variable has dimension \\(n\\)), it can be shown that \\((2n+1)\\) Sigma points can be chosen to match the mean and covariance (Van Der Merwe 2004) \\[\\begin{equation} \\begin{split} x^0 = \\mu, \\\\ x^i = \\mu + \\left[\\sqrt{(n+\\lambda)\\Sigma} \\right]_{i}, \\quad i = 1,\\dots,n, \\\\ x^{i} = \\mu - \\left[\\sqrt{(n+\\lambda)\\Sigma} \\right]_{i-n}, \\quad i=n+1,\\dots,2n, \\end{split} \\end{equation}\\] where \\(\\sqrt{(n+\\lambda)\\Sigma}\\) denotes the square root of the matrix \\((n+\\lambda)\\Sigma\\), \\([\\cdot]_i\\) denotes the \\(i\\)-th column of the matrix, and \\[ \\lambda = \\alpha^2(n+\\kappa) - n, \\] is a scaling parameter determined by hyperparameters \\(\\alpha\\) and \\(\\kappa\\) (\\(\\alpha\\) usually set between \\(10^{-2}\\) and \\(1\\), and \\(\\kappa\\) is usually set to either \\(0\\) or \\(3-n\\)). Each of the Sigma point also comes with two weights as \\[ w^0_m = \\frac{\\lambda}{n+\\lambda}, w^0_c = \\frac{\\lambda}{n+\\lambda} + (1 - \\alpha^2 + \\beta), \\\\ w^i_m = w^i_c = \\frac{1}{2(L + \\lambda)}, \\quad i=1,\\dots,2n, \\] where \\(\\beta\\) is an extra hyperparameter (for Gaussian \\(\\beta=2\\)). Now we are ready to use the idea of Sigma points to present the unscented Kalman filter (UKF). Prediction (1) and (2). To perform the prediction step, we generate \\(2n+1\\) Sigma points from the distribution \\((\\hat{x}_{k \\mid k}, \\Sigma_{k \\mid k})\\): \\[ \\bar{x}^0_{k \\mid k}, \\dots, \\bar{x}^{2n}_{k \\mid k}. \\] The estimate \\(\\hat{x}_{k+1\\mid k}\\) is updated as the sample mean of evaluating \\(f\\) on all Sigma points \\[\\begin{equation} \\hat{x}_{k+1 \\mid k} = \\sum_{i=0}^{2n} w^i_m f(\\bar{x}^{i}_{k \\mid k}), \\tag{6.47} \\end{equation}\\] and the covariance estimate is updated as the sample covariance \\[\\begin{equation} \\Sigma_{k+1 \\mid k} = M_k + \\sum_{i=0}^{2n} w^i_c (f(\\bar{x}^{i}_{k \\mid k}) - \\hat{x}_{k+1 \\mid k})(f(\\bar{x}^{i}_{k \\mid k}) - \\hat{x}_{k+1 \\mid k})^T, \\tag{6.48} \\end{equation}\\] where \\(M_k\\) comes from the disturbance \\(w_k\\) in the dynamics. Correction (3) and (4). To understand the UKF measurement correction, we need to present a key observation of the original Kalman filter. In (6.22) and (6.24), the matrix \\[ C_k \\Sigma_{k \\mid k-1} C_k^T + N_k \\] can be interpreted as the covariance of the measurement \\(y_k\\) (conditioned on all past measurements), i.e., \\(\\Sigma_{yy,k}\\), due to the measurement function \\(y_k = C_k x_k + v_k\\), while the matrix \\[ \\Sigma_{k \\mid k-1} C_k^T \\] can be interpreted as the correlation between \\(x_k\\) and \\(y_k\\), i.e., \\(\\Sigma_{xy,k}\\). Now that we have Sigma points in UKF, we can use them to approximate \\(\\Sigma_{yy,k}\\) and \\(\\Sigma_{xy,k}\\). To do so, we first generate \\((2n+1)\\) Sigma points from the distribution \\((\\hat{x}_{k \\mid k-1}, \\Sigma_{k \\mid k-1})\\) \\[ \\bar{x}^0_{k \\mid k-1}, \\dots, \\bar{x}^{2n}_{k \\mid k-1}. \\] We then compute the expected measurements of these Sigma points \\[ \\bar{y}^i_{k \\mid k-1} = h(\\bar{x}^i_{k \\mid k-1}), i=0,\\dots,2n. \\] The sample mean of these measurements is \\[ \\hat{y}_{k \\mid k-1} = \\sum_{i=0}^{2n} w^i_m \\bar{y}^i_{k \\mid k-1}, \\] and the sample covariance is \\[ \\Sigma_{yy,k} = N_k + \\sum_{i=0}^{2n} w^i_c (\\bar{y}^i_{k \\mid k-1} - \\hat{y}_{k \\mid k-1})(\\bar{y}^i_{k \\mid k-1} - \\hat{y}_{k \\mid k-1})^T. \\] The sample correlation between \\(x\\) and \\(y\\) can be computed as \\[ \\Sigma_{xy,k} = \\sum_{i=0}^{2n} w^i_c (\\bar{x}^i_{k \\mid k-1} - \\hat{x}_{k \\mid k-1}) (\\bar{y}^i_{k \\mid k-1} - \\hat{y}_{k \\mid k-1})^T. \\] As a result, the Kalman gain matrix is \\[ L_k = \\Sigma_{xy,k} \\Sigma^{-1}_{yy,k}. \\] The UKF update (3) is therefore \\[\\begin{equation} \\hat{x}_{k \\mid k} = \\hat{x}_{k \\mid k-1} + L_k (y_k - \\hat{y}_{k \\mid k-1}), \\tag{6.49} \\end{equation}\\] and the update (4) is \\[\\begin{equation} \\Sigma_{k \\mid k} = \\Sigma_{k \\mid k-1} - L_k \\Sigma_{yy,k} L_k^T. \\tag{6.50} \\end{equation}\\] Applications of EKF and UKF. Perhaps one of the most of well-known applications of EKF and UKF is simultaneous location and mapping (SLAM) in mobile robotics. Matlab has a nice demo that I recommend you to play with. 6.4.3 Particle Filter Kalman filter, EKF, and UKF all represent the uncertainty of the state as Gaussian distributions, specifically as means and covariances. However, as mentioned before, the true distribution of the state is rarely a Gaussian distribution. How can we faithfully represent non-Gaussian distributions? The UKF does provide us with a possible solution. UKF represents the Gaussian distribution by \\(2n+1\\) Sigma points. What if we generalize this idea by allowing more points, or in other words, represent a distribution by random samples? This is the basic idea of a particle filter, which is arguably one of the most high-impact nonlinear filtering techniques. The idea of a particle filter is simple to state. Consider the dynamical system (6.42), but this time assume we know the distribution of \\(w_k\\) and \\(v_k\\). For example, in the case of a Gaussian distribution \\(\\mathcal{N}(\\mu,\\Sigma)\\) (with dimension \\(n\\)), we know its probability density function is \\[\\begin{equation} \\mathbb{P}(x = \\xi) = \\frac{\\exp\\left( -\\frac{1}{2} (\\xi- \\mu)^T \\Sigma^{-1} (\\xi - \\mu) \\right)}{\\sqrt{(2\\pi)^n \\det \\Sigma}}. \\tag{6.51} \\end{equation}\\] The goal of a nonlinear filter is to estimate the conditional distribution (or sometimes called the belief) of \\(x_k\\) given all previous measurements \\(Y_k = (y_0,\\dots,y_k)\\) \\[ p(x_k \\mid Y_k). \\] Since the conditional distribution generally cannot be written analytically, a particle filter will represent the conditional distribution as a set of samples, i.e., \\[ \\mathcal{X}_{k\\mid k} = \\{x_{k \\mid k}^1,\\dots,x_{k\\mid k}^M \\} \\sim p(x_k \\mid Y_k), \\] where \\(M\\) is the total number of samples. The particle filter will then update the set of samples, using a similar prediction-correction two-step approach: \\[ \\mathcal{X}_{k \\mid k} \\overset{\\text{prediction}}{\\longrightarrow} \\mathcal{X}_{k+1 \\mid k} \\overset{\\text{correction}}{\\longrightarrow} \\mathcal{X}_{k+1 \\mid k+1}, \\] where the prediction step leverages the dynamics, and the correction step leverages the measurement function. Prediction. The prediction step is straightforward \\[ \\mathcal{X}_{k+1\\mid k} = \\{x^i_{k+1\\mid k} \\}_{i=1}^M, \\quad x_{k+1\\mid k}^i = f(x^i_{k\\mid k}) + w_k^i, i=1,\\dots,M, \\] which basically passes each sample in \\(\\mathcal{X}_{k\\mid k}\\) through the nonlinear dynamics \\(f\\) and then add a random disturbance. Note that \\(w_k^i\\) follows the known distribution of the noise that we know how to sample from, for example it can be the Gaussian distribution in (6.51). Correction. The correction step will leverage the new measurement, \\(y_{k+1}\\), to adjust the set of samples. Particularly, we will evaluate the probability of observing \\(y_{k+1}\\) for each of the samples in the set \\(\\mathcal{X}_{k+1 \\mid k}\\), that is \\[ \\alpha_{k+1}^i = p(y_{k+1} \\mid x_{k+1\\mid k}^i), i=1,\\dots,M. \\] This is simply the probability of \\[ \\alpha_{k+1}^i = \\mathbb{P}(v_{k+1} = y_{k+1} - h(x_{k+1\\mid k}^i)), \\] which can be computed since we know the distribution of \\(v_{k+1}\\). For example, if the distribution of \\(v_{k+1}\\) is Gaussian, then we can evaluate the probability using (6.51). Intuitively, the higher \\(\\alpha_{k+1}^i\\) is, the more likely it is for the true state to be \\(x_{k+1\\mid k}^i\\). Therefore, effectively, we have obtained a set of samples together with their probabilities \\[ \\{ (x_{k+1\\mid k}^i, \\alpha_{k+1}^i) \\}_{i=1}^M. \\] The next step is to perform importance sampling. In particular, we will generate a new set of \\(M\\) samples by sampling (with replacement) from the population \\(\\mathcal{X}_{k+1 \\mid k}\\) with the probability of sampling the \\(i\\)-th sample being \\(\\alpha_{k+1}^i\\). This leads to the new sample set \\(\\mathcal{X}_{k+1 \\mid k+1}\\). Hopefully you see that the particle filter is very easy to implement. Let us work on a simple example. Example 6.2 (Robot Localization by Nonlinear Filtering) Consider a 2D robot with state \\(s = (x,y,\\theta)\\) where \\((x,y)\\) is the position and \\(\\theta\\) is the heading. The robot has two controls, the first one being the linear velocity \\(l\\), and the second one being the angular velocity (steering) \\(u\\). Therefore, the dynamics of the robot is \\[ \\begin{bmatrix} x_{k+1} \\\\ y_{k+1} \\\\ \\theta_{k+1} \\end{bmatrix} = \\begin{bmatrix} x_k + (l \\cos \\theta_k)\\Delta t \\\\ y_k + (l \\sin \\theta_k)\\Delta t \\\\ \\theta_k + u \\Delta t \\end{bmatrix} + w_k, \\] where \\(w_k\\) is assumed to be some Gaussian noise, and \\(\\Delta t\\) is time discretization. There are a set of \\(N\\) landmarks in the environment, each with location \\[ (x_{lm,i},y_{lm,i}),i=1,\\dots,N. \\] The robot is equipped with a sensor that measures distance from each landmark \\[ o_k = \\begin{bmatrix} \\vdots \\\\ o_{k,i} \\\\ \\vdots \\end{bmatrix} + v_k, \\quad o_{k,i} = \\sqrt{(x_k - x_{lm,i})^2 + (y_k - y_{lm,i})^2}, i=1,\\dots,N, \\] where \\(v_k\\) is assumed to be known Gaussian noise. We can implement a particle filter for localizing the robot given distance measurements from the landmarks. We set \\(N=20\\) landmarks, in the beginning, the prior estimation of the robot location is completely wrong, as shown in Fig. 6.2. Figure 6.2: Initial estimate of the robot location. After 200 steps of particle filter, we can see the particles cluster around the groundtruth robot location. Figure 6.3: Initial estimate of the robot location. You can play with the code here. Particle filters have many advantages: Ability to approximate non-Gaussian distributions Works for any dynamics model and observation model Good scalability, they are ``embarassingly parallelizable” Easy to implement However, there are several notable issues with particle filters Lack of diversity: over time, especially with uninformative sensor readings, samples tend to congregate (because the resampling step removes particles) Measuring Particle Filter performance is difficult Particle Filters are non-deterministic 6.4.4 Feedback Particle Filter (T. Yang, Mehta, and Meyn 2013) 6.5 State Observer For the system (6.1), let us denote \\(X(x_0,t_0;t;u)\\) the solution at time \\(t\\) with input \\(u\\) and initial condition \\(x_0\\) at time \\(t_0\\); when \\(t_0 = 0\\), we write \\(X(x_0;t;u)\\) \\(Y(x_0,t_0;t;u)\\) the output at time \\(t\\) with input \\(u\\) and initial condition \\(x_0\\) at time \\(t_0\\), i.e., \\(Y(x_0,t_0;t;u) = h(X(x_0,t_0;t;u), u(t))\\); when \\(t_0 = 0\\), we write \\(y_{x_0,u}(t)\\); \\(\\mathcal{X}_0\\) a subset of \\(\\mathbb{X}\\) containing the initial conditions we consider; for any \\(x_0 \\in \\mathcal{X}_0\\), we write \\(\\sigma^+_{\\mathcal{X}}(x_0;u)\\) the maximal time of existence of \\(X(x_0,\\cdot;t;u)\\) in a set \\(\\mathcal{X}\\) \\(\\mathcal{U}\\) the set of all sufficiently many times differentiable inputs \\(u: [0,+\\infty) \\rightarrow \\mathbb{U}\\). The problem of state observation is to produce an estimated state \\(\\hat{x}(t)\\) of the true state \\(X(x_0,t_0;t;u)\\) based on knowledge about the system (6.1) and information about the history of inputs \\(u_{[0,t]}\\) and outputs \\(y_{[0,t]}\\), so that \\(\\hat{x}(t)\\) asymptotically converges to \\(X(x_0,t_0;t;u)\\), for any initial condition \\(x_0 \\in \\mathcal{X}_0\\) and any input \\(u \\in \\mathcal{U}\\). There are multiple ways for solving the problem of state observation (see e.g., (Bernard 2019), (Bernard, Andrieu, and Astolfi 2022)). Here we are particularly interested in the approach using a state observer, i.e., a dynamical system whose internal state evolves according to the history of inputs and outputs, from which a state estimation can be reconstructed that guarantees asymptotic convergence to the true state. We formalize this concept below. Definition 6.1 (State Observer) A state observer for system (6.1) is a couple \\((\\mathcal{F},\\mathcal{T})\\) such that \\(\\mathcal{F}: \\mathbb{R}^{l} \\times \\mathbb{R}^{m} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^l\\) is continuous \\(\\mathcal{T}\\) is a family of continuous functions indexed by \\(u \\in \\mathcal{U}\\) where each \\(\\mathcal{T}_u: \\mathbb{R}^l \\times [0,+\\infty) \\rightarrow \\mathbb{R}^n\\) respects the causality condition \\[ \\forall \\tilde{u}: [0,+\\infty) \\rightarrow \\mathbb{R}^m,\\forall t \\in [0,+\\infty), u_{[0,t]} = \\tilde{u}_{[0,t]} \\Rightarrow \\mathcal{F}_u (\\cdot,t) = \\mathcal{F}_{\\tilde{u}}(\\cdot,t). \\] For any \\(u \\in \\mathcal{U}\\), any \\(z_0 \\in \\mathbb{R}^l\\), and any \\(x_0 \\in \\mathcal{X}_0\\) such that \\(\\sigma^+_{\\mathbb{X}}(x_0;u) = +\\infty\\), any solution \\(Z(z_0;t;u,y_{x_0,u})\\)8 to \\[\\begin{equation} \\dot{z} = \\mathcal{F}(z,u,y_{x_0,u}) \\tag{6.52} \\end{equation}\\] initialized at \\(z_0\\) at time \\(0\\) with input \\(u\\) and \\(y_{x_0,u}\\) exists on \\([0,+\\infty)\\) and satisfies \\[\\begin{equation} \\lim_{t \\rightarrow \\infty} \\Vert \\hat{X}(x_0,z_0;t;u) - X(x_0;t;u) \\Vert = 0, \\tag{6.53} \\end{equation}\\] with \\[\\begin{equation} \\hat{X}(x_0,z_0;t;u) = \\mathcal{T}_u(Z(z_0;t;u,y_{x_0,u}),t). \\tag{6.54} \\end{equation}\\] In words, (i) the state observer maintains an internal state (or latent state) \\(z \\in \\mathbb{R}^l\\) that evolves according to the latent dynamics \\(\\mathcal{F}\\) in (6.52), where \\(u\\) and \\(y_{x_0,u}\\) are inputs; (ii) an estimated state can be reconstructed from the internal state using \\(\\mathcal{T}_u\\) as in (6.54); and (iii) the error between the estimated state and the true state (defined by a proper distance function \\(\\Vert \\cdot \\Vert\\) on \\(\\mathbb{X}\\)) converges to zero. If \\(\\mathcal{T}_u\\) is the same for any \\(u \\in \\mathcal{U}\\) and is also time independent, then we say \\(\\mathcal{T}\\) is stationary.9 In this case, we can simply write the observer (6.52) and (6.54) as \\[\\begin{equation} \\begin{split} \\dot{z} &amp;= \\mathcal{F}(z,u,y) \\\\ \\hat{x} &amp;= \\mathcal{T}(z). \\end{split} \\tag{6.55} \\end{equation}\\] If \\(\\hat{x}\\) can be read off directly from \\(z\\), then we say the observer (6.55) is in the given coordinates. A special case of this is when \\(\\hat{x} = z\\), i.e., the internal state of the observer is the same as the system state. 6.5.1 General Design Strategy Theorem 6.1 (Meta Observer) Let \\(F: \\mathbb{R}^p \\times \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^p\\), \\(H: \\mathbb{R}^p \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^d\\) and \\(\\mathcal{F}: \\mathbb{R}^p \\times \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^p\\) be continuous functions such that \\[\\begin{equation} \\dot{\\hat{\\xi}} = \\mathcal{F}(\\hat{\\xi}, u, \\tilde{y}) \\tag{6.56} \\end{equation}\\] is an observer for \\[\\begin{equation} \\dot{\\xi} = F(\\xi,u,H(\\xi,u)), \\quad \\tilde{y} = H(\\xi,u), \\tag{6.57} \\end{equation}\\] i.e., for any \\(\\xi_0,\\hat{\\xi}_0 \\in \\mathbb{R}^p\\) and any \\(u \\in \\mathcal{U}\\), the solution of the observer (6.56), denoted by \\(\\hat{\\Xi}(\\hat{\\xi}_0;t;u;\\tilde{y}_{\\xi_0,u})\\), and the solution of the true system (6.57), denoted by \\(\\Xi(\\xi_0;t;u)\\), satisfy \\[\\begin{equation} \\lim_{t \\rightarrow \\infty} \\Vert \\hat{\\Xi}(\\hat{\\xi}_0;t;u;\\tilde{y}_{\\xi_0,u}) - \\Xi(\\xi_0;t;u) \\Vert = 0. \\tag{6.58} \\end{equation}\\] Note that the observer (6.56) is stationary and in the given coordinates for system (6.57). Indeed the internal state of the observer is the same as the system state. Now suppose for any \\(u \\in \\mathcal{U}\\), there exists a continuous function (i.e., coordinate transformation) \\(T_u: \\mathbb{R}^n \\times \\mathbb{R} \\rightarrow \\mathbb{R}^p\\) and a subset \\(\\mathcal{X}\\) of \\(\\mathbb{X}\\) such that For any \\(x_0 \\in \\mathcal{X}_0\\) such that \\(\\sigma^+_{\\mathbb{X}}(x_0;u) = + \\infty\\), \\(X(x_0;\\cdot;u)\\) remains in \\(\\mathcal{X}\\) There exists a concave \\(\\mathcal{K}\\)10 function \\(\\rho\\) and a positive number \\(\\bar{t}\\) such that \\[ \\Vert x_a - x_b \\Vert \\leq \\rho (| T_u(x_a,t) - T_u(x_b,t) |), \\quad \\forall x_a,x_b \\in \\mathcal{X}, t \\geq \\bar{t}, \\] i.e., \\(x \\mapsto T_u(x,t)\\) becomes injective on \\(\\mathcal{X}\\),11 uniformly in time and space, after a certain time \\(\\bar{t}\\). \\(T_u\\) transforms the system (6.1) into the system (6.57), i.e., for all \\(x \\in \\mathcal{X}\\) and all \\(t \\geq 0\\), we have \\[\\begin{equation} L_{(f,1)} T_u(x,t) = F(T_u(x,t),u,h(x,u)), \\quad h(x,u) = H(T_u(x,t),u), \\tag{6.59} \\end{equation}\\] where \\(L_{(f,1)} T_u(x,t)\\) is the Lie derivative of \\(T_u\\) along the vector field \\((f,1)\\) \\[ L_{(f,1)} T_u(x,t) = \\lim_{\\tau \\rightarrow 0} \\frac{ T_u (X(x,t;t+\\tau;u),t+\\tau) - T_u(x,t) }{\\tau}. \\] \\(T_u\\) respects the causality condition \\[ \\forall \\tilde{u}: [0,+\\infty) \\rightarrow \\mathbb{R}^m, \\forall t \\in [0,+\\infty), u_{[0,t]} = \\tilde{u}_{[0,t]} \\Rightarrow T_u(\\cdot,t) = T_{\\tilde{u}}(\\cdot,t). \\] Then, for any \\(u \\in \\mathcal{U}\\), there exists a function \\(\\mathcal{T}_u: \\mathbb{R}^p \\times [0,+\\infty) \\rightarrow \\mathcal{X}\\) (satisfying the causality condition) such that for any \\(t \\geq \\bar{t}\\), \\(\\xi \\mapsto \\mathcal{T}_u (\\xi, t)\\) is uniformly continuous on \\(\\mathbb{R}^p\\) and satisfies \\[ \\mathcal{T}_u \\left( T_u(x,t),t \\right) = x, \\forall x \\in \\mathcal{X}. \\] Moreover, denoting \\(\\mathcal{T}\\) the family of functions \\(\\mathcal{T}_u\\) for \\(u \\in \\mathcal{U}\\), the couple \\((\\mathcal{F}, \\mathcal{T})\\) is an observer for the system (6.1) initialized in \\(\\mathcal{X}_0\\). Proof. See Theorem 1.1 in (Bernard 2019). A simpler version of Theorem 6.1 where the coordinate transformation \\(T_u\\) is stationary and fixed for all \\(u\\) is stated below as a corollary. Corollary 6.7 (Meta Observer with Fixed Transformation) Let \\(F: \\mathbb{R}^p \\times \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^p\\), \\(H: \\mathbb{R}^p \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^d\\) and \\(\\mathcal{F}: \\mathbb{R}^p \\times \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^p\\) be continuous functions such that (6.56) is an observer for (6.57). Suppose there exists a continuous coordinate transformation \\(T: \\mathbb{R}^p \\rightarrow \\mathbb{R}^n\\) and a compact subset \\(\\Omega\\) of \\(\\mathbb{R}^n\\) such that For any \\(x_0 \\in \\mathcal{X}_0\\) such that \\(\\sigma^+_{\\mathbb{X}}(x_0;u) = + \\infty\\), \\(X(x_0;\\cdot;u)\\) remains in \\(\\Omega\\) \\(x \\mapsto T(x)\\) is injective on \\(\\Omega\\) \\(T\\) transforms the system (6.1) into system (6.57) \\[ L_f T(x) = F(T(x),u,h(x,u)), \\quad h(x,u) = H(T(x),u), \\] where \\(L_f T(x)\\) is the Lie derivative of \\(T(x)\\) along \\(f\\) \\[ L_f T(x) = \\lim_{\\tau \\rightarrow 0} \\frac{ T(X(x,t;t+\\tau;u)) - T(x)}{\\tau}. \\] Then, there exists a uniformly continuous function \\(\\mathcal{T}:\\mathbb{R}^p \\rightarrow \\mathbb{R}^{n}\\) such that \\[ \\mathcal{T}(T(x)) = x, \\quad \\forall x \\in \\Omega, \\] and \\((\\mathcal{F},\\mathcal{T})\\) is an observer for system (6.1) initialized in \\(\\mathcal{X}_0\\). Theorem 6.1 and Corollary 6.7 suggest the following general observer design strategy: Find an injective coordinate transformation \\(T_u\\) (that may be time-varying and also dependent on \\(u\\)) that transforms the original system (6.1) with coordinate \\(x\\) into a new system (6.57) with coordinate \\(\\xi\\) Design an observer (6.56), \\(\\hat{\\xi}\\), for the new system Compute a left inverse, \\(\\mathcal{T}_u\\), of the transformation \\(T_u\\) to recover a state estimation \\(\\hat{x}\\) of the original system. The transformed systems (6.57) are typically referred to as normal forms, or in my opinion, templates. Of course, the general design strategy is rather conceptual, and in order for it to be practical, we have to answer three questions. What templates do we have, what are their associated observers, and what are the conditions for the observers to be asymptotically converging? What kinds of (nonlinear) systems can be transformed into the templates, and how to perform the transformation? How to invert the coordinate transformation? Is it analytical or does it require numerical approximation? In the following sections, we will study several representative normal forms and answer the above questions. Before presenting the results, let us first introduce several notions of observability. Definition 6.2 (Observability) Consider an open subset \\(\\mathcal{L}\\) of the state space \\(\\mathbb{X} \\subseteq \\mathbb{R}^n\\) of system (6.1). The system (6.1) is said to be Distinguishable on \\(\\mathcal{L}\\) for some input \\(u(t)\\), if for all \\((x_a,x_b) \\in \\mathcal{L} \\times \\mathcal{L}\\), \\[ y_{x_a,u}(t) = y_{x_b,u}(t), \\forall t \\in [0,\\min\\left\\{\\sigma^+_{\\mathbb{X}}(x_a;u), \\sigma^+_{\\mathbb{X}}(x_b;u) \\right\\}] \\Longrightarrow x_a = x_b \\] Instantaneously distinguishable on \\(\\mathcal{L}\\) for some input \\(u(t)\\), if for all \\((x_a,x_b) \\in \\mathcal{L} \\times \\mathcal{L}\\), and for all \\(\\bar{t} \\in (0, \\min\\left\\{\\sigma^+_{\\mathbb{X}}(x_a;u), \\sigma^+_{\\mathbb{X}}(x_b;u) \\right\\})\\), \\[ y_{x_a,u}(t) = y_{x_b,u}(t), \\forall t \\in [0,\\bar{t}) \\Longrightarrow x_a = x_b \\] Uniformly observable on \\(\\mathcal{L}\\) if it is distinguishable on \\(\\mathcal{L}\\) for any input \\(u(t)\\) (not only for \\(u \\in \\mathcal{U}\\)) Uniformly instantaneously observable on \\(\\mathcal{L}\\) if it is instantaneously observable on \\(\\mathcal{L}\\) for any input \\(u(t)\\) (not only for \\(u \\in \\mathcal{U}\\)). Moreover, let \\(\\mathcal{X}\\) be a subset of \\(\\mathbb{X}\\) such that \\(\\mathrm{cl}(\\mathcal{X})\\), i.e., the closure of \\(\\mathcal{X}\\), is contained in \\(\\mathcal{L}\\). Then the system (6.1) is said to be Backward \\(\\mathcal{L}\\)-distinguishable on \\(\\mathcal{X}\\) for some input \\(u(t)\\), if for any \\((x_a,x_b) \\in \\mathcal{X} \\times \\mathcal{X}\\) such that \\(x_a \\neq x_b\\), there exists \\(t \\in (\\max\\left\\{ \\sigma^{-}_{\\mathcal{L}}(x_a;u), \\sigma^{-}_{\\mathcal{L}}(x_b;u) \\right\\},0]\\) such that \\(y_{x_a,u}(t) \\neq y_{x_b,u}(t)\\), or in words similar to the definition of distinguishable on \\(\\mathcal{L}\\), for all \\((x_a,x_b) \\in \\mathcal{X} \\times \\mathcal{X}\\) \\[ y_{x_a,u}(t) = y_{x_b,u}(t), \\forall t \\in (\\max\\left\\{\\sigma^{-}_{\\mathcal{L}}(x_a;u), \\sigma^{-}_{\\mathcal{L}}(x_b;u) \\right\\},0] \\Longrightarrow x_a = x_b. \\] 6.5.2 Luenberger Template Consider an instance of the normal form (6.57) as follows: \\[\\begin{equation} \\dot{\\xi} = A \\xi + B(u,y), \\quad y = C \\xi, \\tag{6.60} \\end{equation}\\] where \\(A,C\\) are constant matrices, and \\(B(u,y)\\) can depend nonlinearly on \\(u\\) and \\(y\\). For this template, we have the well-known Luenberger observer. Theorem 6.2 (Luenberger Observer) If the pair \\((A,C)\\) is detectable (see Theorem C.9), then there exists a matrix \\(K\\) such that \\(A-KC\\) is Hurwitz and the system \\[\\begin{equation} \\dot{\\hat{\\xi}} = A \\hat{\\xi} + B(u,y) + K(y - C \\hat{\\xi}) \\tag{6.61} \\end{equation}\\] is an observer for (6.60). Proof. Define \\(e(t) = \\xi(t) - \\hat{\\xi}(t)\\). In that case, \\[\\begin{equation} \\dot{e}(t) = [A - KC] e(t) \\tag{6.62} \\end{equation}\\] Solving (6.62), we obtain \\[\\begin{equation} e(t) = \\mathrm{exp}[(A - KC)t] e(0) \\end{equation}\\] Then, if the real components of the eigenvalues of \\(A - KC\\) are strictly negative (i.e., \\(A - KC\\) is Hurwitz), then \\(e(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\), independent of the initial error \\(e(0) = \\xi(0) - \\hat{\\xi}(0)\\). From Theorem C.9, we know that \\((A,C)\\) being detectable implies the existence of \\(K\\) such that \\(A - KC\\) is Hurwitz. If one is further interested in estimating the convergence rate of the Luenberger observer, then one can use the result from Corollary C.1. Particularly, one can solve the Lyapunov equation \\[ (A - KC)^T P + P (A - KC) = - I \\] to obtain \\(P\\). Then the convergence rate of \\(\\Vert e \\Vert\\) towards zero is \\(\\frac{0.5}{\\lambda_{\\max}(P)}\\). The Luenberger observer is an elegant result in observer design (and even in control theory) that has far-reaching impact. In my opinion, the essence of observer design is twofold: (i) to simulate the dynamics when the state estimation is correct, and (ii) to correct the state estimation from observation when it is off. These two pieces of ideas are evident in (6.61): the observer is a copy of the original dynamics (\\(A \\hat{\\xi} + B(u,y)\\)) plus a feedback correction from the difference between the “imagined” observation \\(C\\hat{\\xi}\\) and the true observation \\(y\\). You may think the Luenberger template is restricting because it requires the system to be linear (up to the only nonlinearly in \\(B(u,y)\\)). However, it turns out the Luenberger template is already quite useful, as I will show in the following pendulum example. Example 6.3 (Luenberger Observer for A Simple Pendulum) Consider a simple pendulum dynamics model \\[\\begin{equation} x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = \\begin{bmatrix} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix} u, \\quad y = \\theta, \\tag{6.63} \\end{equation}\\] where \\(\\theta\\) the angular position of the pendulum from the vertical line, \\(m &gt; 0\\) the mass of the pendulum, \\(l &gt; 0\\) the length, \\(g\\) the gravitational constant, \\(b &gt; 0\\) the dampling coefficient, and \\(u\\) the control input (torque). We assume we can only observe the angular position of the pendulum in (6.63), e.g., using a camera, but not the angular velocity. Our goal is to construct an observer that can provide a full state estimation. We first note that the pendulum dynamics (6.63) can actually be written in the (linear) Luenberger template (6.60) as12 \\[\\begin{equation} \\begin{split} \\dot{x} &amp; = \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; - \\frac{b}{ml^2} \\end{bmatrix}}_{=:A} x + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{u - mgl \\sin \\theta }{ml^2} \\end{bmatrix}}_{=:B(u,y)} \\\\ y &amp; = \\underbrace{\\begin{bmatrix} 1 &amp; 0 \\end{bmatrix}}_{=:C} x \\end{split}. \\tag{6.64} \\end{equation}\\] In order for us to use the Luenberger observer, we need to check if the pair \\((A,C)\\) is detectable. We can easily find the eigenvalues and eigenvectors of \\(A\\): \\[ A \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 0,\\quad A \\begin{bmatrix} - \\frac{ml^2}{b} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ - \\frac{b}{ml^2} \\end{bmatrix} = - \\frac{b}{ml^2} \\begin{bmatrix} - \\frac{ml^2}{b} \\\\ 1 \\end{bmatrix}. \\] The first eigenvalue has real part equal to \\(0\\). However, \\[ C \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 1 \\neq 0. \\] According to Theorem C.9, we conclude \\((A,C)\\) is detectable. In fact, the pair \\((A,C)\\) is more than just detectable, it is indeed observable (according to Theorem C.7). Therefore, the poles of \\(A - KC\\) can be arbitrarily placed. Finding \\(K\\). Now we need to find \\(K\\). An easy choice of \\(K\\) is \\[ K = \\begin{bmatrix} k \\\\ 0 \\end{bmatrix}, \\quad A - KC = \\begin{bmatrix} - k &amp; 1 \\\\ 0 &amp; - \\frac{b}{ml^2} \\end{bmatrix}. \\] With \\(k &gt; 0\\), we know \\(A- KC\\) is guaranteed to be Hurwitz (the two eigenvalues of \\(A-KC\\) are \\(-k\\) and \\(-b/ml^2\\)), and we have obtained an observer! We can also estimate the convergence rate of this observer. Let us use \\(m=1,g=9.8,l=1,b=0.1\\) as parameters of the pendulm dynamics. According to Theorem 6.2, we solve the Lyapunov equation \\[ (A - KC)^T P + P(A - KC) = -I \\] and \\(\\gamma = \\frac{0.5}{\\lambda_{\\max}(P)}\\) will be our best estimate of the convergence rate (of \\(\\Vert e \\Vert = \\Vert \\hat{x} - x \\Vert\\) towards zero). Table 6.1 below shows the convergence rates computed for different values of \\(k\\). We can see that as \\(k\\) is increased, the convergence rate estimation is also increased. However, it appears that \\(0.1\\) is the best convergence rate we can achieve, regardless of how large \\(k\\) is. Table 6.1: Convergence rate estimation of the Luenberger observer for a simple pendulm. \\(k\\) \\(0.1\\) \\(1\\) \\(10\\) \\(100\\) \\(1000\\) \\(10000\\) \\(\\gamma\\) \\(0.0019\\) \\(0.0523\\) \\(0.0990\\) \\(0.1000\\) \\(0.1000\\) \\(0.1000\\) Optimal \\(K\\). Is it true that \\(0.1\\) is the best convergence rate, or in other words, what is the best \\(K\\) that maximizes the convergence rate \\(\\gamma\\)? A natural way (and my favorite way) to answer this question is to formulate an optimization problem. \\[\\begin{equation} \\begin{split} \\min_{P,K} &amp; \\quad \\lambda_{\\max}(P) \\\\ \\text{subject to} &amp; \\quad (A - KC)^T P + P (A - KC) = -I \\\\ &amp; \\quad P \\succeq 0 \\end{split} \\tag{6.65} \\end{equation}\\] The above formulation seeks the best possible \\(K\\) that minimizes \\(\\lambda_{\\max}(P)\\) which, according to \\(\\gamma = 0.5 / \\lambda_{\\max}(P)\\), also maximizes \\(\\gamma\\). However, problem (6.65) is not a convex formulation due to the bilinear term \\(PK\\). Nevertheless, via a simple change of variable \\(H = PK\\), we arrive at the following convex formulation \\[\\begin{equation} \\begin{split} \\min_{P,H} &amp; \\quad \\lambda_{\\max}(P) \\\\ \\text{subject to} &amp; \\quad A^T P - C^T H^T + PA - H C = - I \\\\ &amp; \\quad P \\succeq 0 \\end{split} \\tag{6.66} \\end{equation}\\] Problem (6.66) is a semidefinite programming problem (SDP), that can be modeled and solved by off-the-shelf tools. We can recover \\(K = P^{-1} H\\) from (6.66) after it is solved. Interestingly, solving problem (6.66) verifies that the minimum \\(\\lambda_{\\max}(P)\\) is \\(5\\) and the maximum converge rate is \\(0.1\\). An optimal solution of (6.66) is \\[ P^\\star = \\begin{bmatrix} 2.4923 &amp; 0 \\\\ 0 &amp; 5 \\end{bmatrix}, \\quad K^\\star = \\begin{bmatrix} 0.2006 \\\\ 0.4985 \\end{bmatrix}. \\] You should check out the Matlab code of this example here. 6.5.3 State-affine Template Consider an instance of the normal form (6.57) where the dynamics is linear in \\(\\xi\\), but the coefficients are time-varying and dependent on the input and output \\[\\begin{equation} \\dot{\\xi} = A(u,y) \\xi + B(u,y), \\quad y = C(u) \\xi. \\tag{6.67} \\end{equation}\\] The difference between the state-affine template (6.67) and the Luenberger template (6.60) is that the linear matrices \\(A,C\\) are allowed to depend nonlinearly on the input \\((u,y)\\). Kalman and Bucy originally proposed an observer for linear time-varying systems (Rudolph E. Kalman and Bucy 1961). The result is later extened by (Besançon, Bornard, and Hammouri 1996) and (Hammouri and Leon Morales 1990) to deal with coefficient matrices dependent on the control. The following theorem is a direct extension of the result from (Besançon, Bornard, and Hammouri 1996) and (Hammouri and Leon Morales 1990) by considering \\((u,y)\\) as an augmented control input. Before presenting the theorem, we need to introduce the following terminology. Definition 6.3 (Linear Time-Varying System) For a linear time-varying system of the form \\[\\begin{equation} \\dot{\\chi} = A(\\nu) \\chi, \\quad y = C(\\nu) \\chi, \\tag{6.68} \\end{equation}\\] with input \\(\\nu\\) and output \\(y\\), we define the transition matrix \\(\\Psi_\\nu\\) as the unique solution to \\[ \\Psi_\\nu (t,t) = I, \\quad \\frac{\\partial \\Psi_\\nu}{\\partial \\tau}(\\tau,t) = A(\\nu(\\tau)) \\Psi_\\nu (\\tau, t). \\] Note that the transition matrix is used to express the solution to (6.68) because it satisfies \\[ \\chi(\\chi_0,t_0;t;\\nu) = \\Psi_\\nu (t,t_0) \\chi_0. \\] the observability grammian as \\[ \\Gamma_\\nu (t_0,t_1) = \\int_{t_0}^{t_1} \\Psi_\\nu (\\tau,t_0)^T C(\\nu(\\tau))^T C(\\nu(\\tau)) \\Psi_\\nu (\\tau,t_0) d\\tau. \\] the backward observability grammian as \\[ \\Gamma_\\nu^b (t_0,t_1) = \\int_{t_0}^{t_1} \\Psi_\\nu (\\tau,t_1)^T C(\\nu(\\tau))^T C(\\nu(\\tau)) \\Psi_\\nu (\\tau,t_1) d\\tau. \\] We now introduce the Kalman-Bucy Observer for the state-affine template (6.67). Theorem 6.3 (Kalman-Bucy Observer) Let \\(y_{\\xi_0,u}(t) = C(u(t)) \\Xi (\\xi_0;t;u)\\) be the output of system (6.67) at time \\(t\\) with initialization \\(\\xi_0\\) and control \\(u\\). Suppose the control \\(u\\) satisfies For any \\(\\xi_0\\), \\(t \\mapsto A(u(t),y_{\\xi_0,u}(t))\\) is bounded by \\(A_{\\max}\\) For any \\(\\xi_0\\), the augmented input \\(\\nu = (u,y_{\\xi_0,u})\\) is regularly persistent for the dynamics \\[\\begin{equation} \\dot{\\chi} = A(\\nu) \\chi , \\quad y = C(\\nu) \\chi \\tag{6.69} \\end{equation}\\] uniformly with respect to \\(\\xi_0\\). That is, there exist strictly positive numbers \\(t_0,\\bar{t}\\), and \\(\\alpha\\) such that for any \\(\\xi_0\\) and any time \\(t \\geq t_0 \\geq \\bar{t}\\), \\[ \\Gamma_v^b (t-\\bar{t}, t) \\succeq \\alpha I, \\] where \\(\\Gamma_v^b\\) is the backward observability grammian associated with system (6.69). Then, given any positive definite matrix \\(P_0\\), there exist \\(\\alpha_1,\\alpha_2 &gt; 0\\) such that for any \\(\\lambda \\geq 2 A_{\\max}\\) and any \\(\\xi_0 \\in \\mathbb{R}^p\\), the matrix differential equation \\[\\begin{equation} \\dot{P} = -\\lambda P - A(u,y)^T P - P A(u,y) + C(u)^T C(u) \\tag{6.70} \\end{equation}\\] initialized at \\(P(0) = P_0\\) admits a unique solution satisfying \\(P(t)=P(t)^T\\) and \\[ \\alpha_2 I \\succeq P(t) \\succeq \\alpha_1 I. \\] Moreover, the system \\[\\begin{equation} \\dot{\\hat{\\xi}} = A(u,y) \\hat{\\xi} + B(u,y) + K (y - C(u)\\hat{\\xi}) \\tag{6.71} \\end{equation}\\] with a time-varying gain matrix \\[\\begin{equation} K = P^{-1} C(u)^T \\tag{6.72} \\end{equation}\\] is an observer for the state-affine system (6.67). Let us work out an example of the Kalman-Bucy Observer for nonlinear systems. Example 6.4 (Kalman-Bucy Observer for A Simple Pendulum) Let us reconsider the pendulum dynamics (6.63) but this time try to design a Kalman-Bucy observer. We first write the pendulum dynamics in a new coordinate system so that it is in the state-affine normal form (6.67). We choose \\(\\xi = [\\mathfrak{s},\\mathfrak{c},\\dot{\\theta}]^T\\) with \\(\\mathfrak{s} = \\sin \\theta\\) and \\(\\mathfrak{c} = \\cos \\theta\\). Clearly, we will be able to observe \\(y = [\\mathfrak{s},\\mathfrak{c}]^T\\) in this new coordinate. The state-affine normal form of the pendulum dynamics reads \\[\\begin{equation} \\begin{split} \\dot{\\xi} = \\begin{bmatrix} \\mathfrak{c} \\dot{\\theta} \\\\ - \\mathfrak{s} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\mathfrak{s} ) + \\frac{1}{ml^2} u \\end{bmatrix} &amp; = \\underbrace{\\begin{bmatrix} 0 &amp; 0 &amp; \\mathfrak{c} \\\\ 0 &amp; 0 &amp; -\\mathfrak{s} \\\\ 0 &amp; 0 &amp; -\\frac{b}{ml^2} \\end{bmatrix}}_{=:A(u,y)} \\xi + \\underbrace{\\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{u - mgl \\mathfrak{s}}{ml^2} \\end{bmatrix}}_{=:B(u,y)} \\\\ y &amp; = \\underbrace{\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}}_{=:C(u)} \\xi \\end{split}. \\tag{6.73} \\end{equation}\\] Note that \\(C(u)\\) is in fact time-invariant, and \\(B(u,y)\\) only depends on \\(u\\); but we adopt the same notation as the general state-affine template (6.67). In order to use the Kalman-Bucy observer in Theorem 6.3, we need to verify the boundedness of \\(A(u,y)\\), and the regular persistence of (6.69). Boundedness of \\(A(u,y)\\). We can easily show the boundedness of \\(A(u,y)\\) by writing \\[ \\Vert A(u,y) \\xi \\Vert = \\Vert \\xi_3 (\\mathfrak{c} - \\mathfrak{s} - b/ml^2) \\Vert \\leq |\\xi_3| \\sqrt{3} \\sqrt{\\mathfrak{c}^2 + \\mathfrak{s}^2 + b^2 / m^2 l^4} \\leq \\Vert \\xi \\Vert \\sqrt{3 + 3b^2 / m^2 l^4}. \\] Therefore, we can take \\(A_{\\max} = \\sqrt{3 + 3b^2 / m^2 l^4}\\). Regular persistence. We write the backward observability grammian of system (6.69) \\[ \\Gamma_\\nu^b(t - \\bar{t},t) = \\int_{t - \\bar{t}}^t \\Psi_\\nu(\\tau,t)^T C^T C \\Psi_\\nu (\\tau, t) d \\tau = \\int_{t - \\bar{t}}^t \\Psi_\\nu(\\tau,t)^T \\underbrace{\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}}_{=:\\tilde{C}} \\Psi_\\nu (\\tau, t) d \\tau. \\] \\(\\Gamma_\\nu^b(t - \\bar{t},t) \\succeq \\alpha I\\) if and only if \\[ w^T \\Gamma_\\nu^b(t - \\bar{t},t) w \\geq \\alpha, \\quad \\forall w \\in \\mathbb{R}^3, \\Vert w \\Vert = 1. \\] With this, we develop \\(w^T \\Gamma_\\nu^b(t - \\bar{t},t) w\\) \\[\\begin{equation} \\begin{split} w^T \\Gamma_\\nu^b(t - \\bar{t},t) w &amp;= \\int_{t - \\bar{t}}^t s^T \\tilde{C} s d\\tau, \\\\ &amp; = \\int_{t - \\bar{t}}^t \\left( s_1^2 + s_2^2 \\right) d\\tau, \\quad s = \\Psi_\\nu (\\tau, t) w \\end{split} \\tag{6.74} \\end{equation}\\] and observe that \\(s = \\Psi_\\nu (\\tau,t) w\\) is equivalent to \\[ w = (\\Psi_\\nu (\\tau,t))^{-1} s = \\Psi_\\nu (t,\\tau) s, \\] that is, \\(w\\) is the solution of \\(\\dot{\\xi} = A(\\nu) \\xi\\) at time \\(t\\) with initial condition \\(s\\) at time \\(\\tau \\leq t\\). Equivalently, this is saying \\(s\\) is the initial condition of \\(\\dot{\\xi} = A(\\nu) \\xi\\) at time \\(\\tau \\leq t\\) such that its solution at time \\(t\\) is \\(w\\). Note that from (6.74) it is clearly that \\(\\int_{t - \\bar{t}}^t \\left( s_1^2 + s_2^2 \\right) d\\tau \\geq 0\\), and \\(\\int_{t - \\bar{t}}^t \\left( s_1^2 + s_2^2 \\right) d\\tau = 0\\) if and only if \\(s_1^2 + s_2^2 = 0\\), or equivalently \\(s_1 = s_2 = 0\\) for any \\(\\tau \\in [t - \\bar{t}, t]\\). We then take a closer look at the system \\(\\dot{\\xi} = A(\\nu) \\xi\\): \\[\\begin{equation} \\begin{split} \\dot{\\xi}_1 &amp;= \\mathfrak{c} \\xi_3 \\\\ \\dot{\\xi}_2 &amp;= -\\mathfrak{s} \\xi_3 \\\\ \\dot{\\xi}_3 &amp;= - \\frac{b}{ml^2} \\xi_3. \\end{split} \\tag{6.75} \\end{equation}\\] If the solution of \\(\\xi_3\\) at time \\(t\\) is \\(w_3\\), then \\[ \\xi_3(\\tau) = w_3 e^{\\frac{b}{ml^2}(t - \\tau)}, \\quad \\tau \\leq t. \\] We can now claim it is impossible that \\(s_1 = s_2 = 0\\) at any time \\(\\tau \\in [t - \\bar{t}, t]\\). We can show this by contradiction. First of all, \\(s_1 = s_2 = 0\\) at \\(\\tau = t\\) implies \\(w_1 = w_2 = 0\\) and hence \\(w_3 = \\pm 1\\). This implies \\(\\xi_3 \\neq 0\\) for any \\(\\tau \\in [t - \\bar{t}, t]\\). Then, \\(s_1 = 0, \\forall \\tau \\in [t - \\bar{t}, t]\\) implies \\(\\dot{\\xi}_1 = 0\\) which, due to \\(\\xi_3 \\neq 0\\), implies \\(\\mathfrak{c} = 0\\) for all \\(\\tau\\). Similarly, \\(s_2 = 0, \\forall \\tau \\in [t - \\bar{t}, t]\\) implies \\(\\dot{\\xi}_2 = 0\\) and \\(\\mathfrak{s} = 0\\). This creates a contradiction because \\(\\mathfrak{c}^2 + \\mathfrak{s}^2 = 1\\) and \\(\\mathfrak{c}, \\mathfrak{s}\\) cannot be simultaneously zero. The above reasoning proves that the backward observability Grammian is positive definite, which is, however, still insufficient for the Kalman-Bucy observer. We need a stronger uniformly positive definite condition on \\(\\Gamma_\\nu^b\\), i.e., to find \\(t_0, \\bar{t}\\) and \\(\\alpha&gt;0\\) so that \\(\\Gamma_\\nu^b(t-\\bar{t},t) \\succeq \\alpha I\\) for all \\(t \\geq t_0\\). If the control \\(u\\) is unbounded, then sadly, one can show that the uniform positive definite condition fails to hold, as left by you to show in the following exercise. Exercise 6.1 (Counterexample for Kalman-Bucy Observer) Show that, if the control \\(u\\) is unbounded, then for any \\(\\alpha &gt; 0\\), \\(t_0 \\geq \\bar{t} &gt; 0\\), there exists \\(t \\geq t_0\\) such that \\(\\Gamma_\\nu^b(t - \\bar{t},t) \\prec \\alpha I\\). (Hint: consider a controller that spins the pendulum faster and faster such that in time \\(\\bar{t}\\) it has rotated \\(2k\\pi\\), in this case the angular velocity becomes unobservable because we are not sure how many rounds the pendulum has rotated.) Fortunately, if the control \\(u\\) is bounded, then we can prove the uniform positive define condition holds for \\(\\Gamma_\\nu^b(t - \\bar{t},t)\\). The following proof is given by Weiyu Li. Without loss of generality, let \\(\\frac{b}{ml^2} = 1\\). Assume \\(u\\) is bounded such that the third entry of \\(B(u,y)\\) in (6.73) is bounded by \\(\\beta &gt; 0\\) \\[ \\left| \\frac{u - mgl \\mathfrak{s}}{ml^2} \\right| \\leq \\beta. \\] Assuming the initial velocity of the pendulum is \\(\\dot{\\theta}(0) = \\dot{\\theta}_0\\), we know \\(\\dot{\\theta}(t)\\) is bounded by \\[ \\dot{\\theta}(t) \\in \\left[ c_1(1-\\beta)e^{-t} - \\beta , c_2(1-\\beta)e^{-t} + \\beta \\right], \\] where \\(c_1,c_2\\) are constants chosen to satisfy the initial condition. Clearly, for all \\(t &gt; 0\\), we see \\(\\dot{\\theta}(t)\\) is bounded, and hence we know \\(\\dot{\\mathfrak{c}}\\) and \\(\\dot{\\mathfrak{s}}\\) are bounded (due to \\(\\mathfrak{c}\\) and \\(\\mathfrak{s}\\) are bounded). Intuitively, what we have just shown says that when the control is bounded, the measurements \\(\\mathfrak{c}\\) and \\(\\mathfrak{s}\\) will have bounded time derivatives. (This will help us analyze the auxiliary system (6.75).) Now back to checking regular persistence of the auxilary system (6.75). We will discuss two cases: (1) \\(w_3^2 &gt; 1 - \\delta\\), and (2) \\(w_3^2 \\leq 1 - \\delta\\), for some constant \\(\\delta &lt; 0.5\\) determined later. \\(w_3^2 &gt; 1 - \\delta &gt; 0.5\\). In this case we have \\(w_1^2 + w_2^2 = 1 - w_3^2 &lt; \\delta\\), and hence \\(w_1^2 &lt; \\delta\\), \\(w_2^2 &lt; \\delta\\). On the other hand, from (6.75) we have \\[ \\dot{\\xi}_1^2(\\tau) + \\dot{\\xi}_2^2(\\tau) = \\xi_3^2 = w_3^2 e^{2 (t - \\tau)} &gt; w_3^2 &gt; 1 - \\delta, \\quad \\forall \\tau &lt; t. \\] Without loss of generality assume \\(\\dot{\\xi}_1(t)^2 &gt; (1-\\delta)/2\\). As \\(\\dot{\\xi}_1 = \\mathfrak{c} \\xi_3\\) and both \\(\\mathfrak{c}\\) and \\(\\xi_3\\) have bounded derivatives, we know \\(\\dot{\\xi}_1\\) will not change sign for some duration \\(T\\) that is independent from the choice of \\(\\delta\\) (because the time derivatives of \\(\\mathfrak{c}\\) and \\(\\xi_3\\) do not depend on \\(\\delta\\)). That is \\(|\\dot{\\xi}_1| &gt; \\sqrt{(1-\\delta)/2} &gt; 1/2\\) for \\(\\tau \\in [t - T,t]\\). Consequently, \\[ |\\xi_1(t - \\tau)| &gt; \\frac{1}{2} \\tau - |w_1| &gt; \\frac{1}{2} \\tau - \\sqrt{\\delta}, \\quad \\tau \\in [0,T]. \\] Choosing \\(\\delta\\) small enough, we have \\(|\\xi_1(t - \\tau)| &gt; 0.25 \\tau\\) for \\(\\tau \\in [0.5T,T]\\). Then we have \\[ \\Gamma_\\nu^b(t - T, t) \\succ [(0.25 \\times 0.5 T)^2 \\times 0.5T]I. \\] \\(w_3^2 \\leq 1-\\delta\\). In this case \\(w_1^2 + w_2^2 = 1 - w_3^2 \\geq \\delta\\), and at least one of \\(w_1\\) and \\(w_2\\) has absolute value larger than \\(\\sqrt{\\delta/2}\\). Because the derivatives of \\(\\xi_1\\) and \\(\\xi_2\\) are both bounded, we know \\(\\xi_1\\) and \\(\\xi_2\\) will remain large for some constant time. Thus there is a uniform lower bound. The intuition of the above proof is simple: when \\(\\xi_1\\) and/or \\(\\xi_2\\) already have large absolute value (case 2), we can find a time window such that \\(\\xi_1\\) and/or \\(\\xi_2\\) remain large in that time window; when \\(\\xi_1\\) and/or \\(\\xi_2\\) are small (case 1), using the observation that their time derivatives are large (because \\(w_3\\) is large), together with the fact that these derivatives remain large (because the derivative of these derivatives are bounded), we can also find a time window that \\(\\xi_1\\) and/or \\(\\xi_2\\) are large (back in time). Therefore, the backward observability Grammian is uniformly positive definite. 6.5.4 Kazantzis-Kravaris-Luenberger (KKL) Template In Luenberger’s original paper about observer design for linear systems (Luenberger 1964), the goal was to transform a linear system \\[ \\dot{x} = F x, \\quad y = C x \\] into a Hurwitz form \\[\\begin{equation} \\dot{\\xi} = A \\xi + B y \\tag{6.76} \\end{equation}\\] with \\(A\\) a Hurwitz (stable) matrix. If such a transformation is available, then the following system \\[ \\dot{\\hat{\\xi}} = A \\hat{\\xi} + B y, \\] which is nothing but a copy of the dynamics (6.76), is in fact an observer. This is because the error \\(e = \\hat{\\xi} - \\xi\\) evolves as \\[ \\dot{e} = A e, \\] which implies that \\(e\\) tends to zero regardless of the initial error \\(e(0)\\). Luenberger proved that when \\((F,C)\\) is observable, a stationary transformation \\(\\xi = T x\\) with \\(p = n\\), i.e., \\(T \\in \\mathbb{R}^{n\\times n}\\), always exists and is unique, for any matrix \\(A\\) that is Hurwitz and \\((A,B)\\) that is controllable. This is based on the fact that \\[\\begin{align} (A T + B C) x =A \\xi + B y =\\dot{\\xi} = T \\dot{x} = TF x, \\forall x \\\\ \\Longleftrightarrow AT + BC = TF, \\end{align}\\] known as the Sylvester equation, admits a unique and invertible solution \\(T\\). A natural extension of Luenberger’s original idea is to find a transformation that converts the nonlinear system (6.1) into the following form \\[\\begin{equation} \\dot{\\xi} = A \\xi + B(u,y), \\quad y = H(\\xi,u), \\tag{6.77} \\end{equation}\\] with \\(A\\) a Hurwitz matrix (but \\(H\\) can be nonlinear, as opposed to the Luenberger template in Theorem 6.2). If such a transformation can be found, then we can design a similar observer that copies the dynamics (6.77) \\[\\begin{equation} \\dot{\\hat{\\xi}} = A \\hat{\\xi} + B(u,y). \\tag{6.78} \\end{equation}\\] We refer to such a nonlinear Luenberger template the Kazantzis-Kravaris-Luenberger (KKL) template, due to the seminal work (Kazantzis and Kravaris 1998). The KKL template, once found, is nice in the sense that (i) the observer (6.78) is a simple copy of the dynamics and also very easy to implement (as opposed to the Kalman-Bucy observer); and (ii) checking if the matrix \\(A\\) is Hurwitz is easy, at least when \\(A\\) has reasonable size, (e.g., compared to checking the regular persistence condition in the state-affine template in Theorem 6.3). However, the KKL template is difficult to realize in the sense that (i) what kind of nonlinear systems can be converted to (6.77), and (ii) for those systems, how do we find the coordinate transformation? Recent works have leveraged deep learning to learn the coordinate transformation, for example in (Janny et al. 2021), (Niazi et al. 2023), (Miao and Gatsis 2023). Before hammering the problem with deep learning, let us look at the fundamentals of the KKL observer. 6.5.4.1 Autonomous Systems Consider the autonomous version of system (6.1) without control \\[\\begin{equation} \\dot{x} = f(x), \\quad y = h(x), \\tag{6.79} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n, y \\in \\mathbb{Y} \\subseteq \\mathbb{R}^d\\). The following result, established by (Andrieu and Praly 2006), states that the KKL observer exists under mild conditions. Theorem 6.4 (KKL Observer for Autonomous Systems) Assum \\(\\mathcal{X}\\) and \\(\\mathcal{L}\\) are open bounded sets in \\(\\mathbb{X}\\) (the state space) such that \\(\\mathrm{cl}(\\mathcal{X})\\) is contained in \\(\\mathcal{L}\\) and the system (6.79) is backward \\(\\mathcal{L}\\)-distinguishable on \\(\\mathcal{X}\\) (cf. Definition 6.2). Then there exists a strictly positive number \\(\\gamma\\) and a set \\(\\mathcal{S}\\) of zero Lebesgue measure in \\(\\mathbb{C}^{n+1}\\) such that denoting \\(\\Omega = \\{ \\lambda \\in \\mathbb{C} \\mid \\mathrm{Re}(\\lambda) &lt; - \\gamma \\}\\), for any \\((\\lambda_1,\\dots,\\lambda_{n+1}) \\in \\Omega^{n+1} \\backslash \\mathcal{S}\\), there exists a function \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^{(n+1)d}\\) uniformly injective on \\(\\mathcal{X}\\) satisfying \\[ L_f T(x) = A T(x) + B(h(x)) \\] with \\[\\begin{align} A = \\tilde{A} \\otimes I_d, \\quad B(y) = (\\tilde{B} \\otimes I_d ) y \\\\ \\tilde{A} = \\begin{bmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_{n+1} \\end{bmatrix} \\quad \\tilde{B} = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}. \\end{align}\\] Moreover, if \\(\\mathcal{X}\\) is backward invariant, then \\(T\\) is unique and defined by \\[\\begin{equation} T(x) = \\int_{-\\infty}^0 e^{-A\\tau} B(h(X(x,\\tau))) d\\tau. \\tag{6.80} \\end{equation}\\] Remark. The function \\(T\\) in Theorem 6.4 takes complex numbers. To simulate the observer \\[ \\dot{\\hat{\\xi}} = A \\hat{\\xi} + B(y), \\] one needs to implement in real numbers, for each \\(\\lambda_i\\) and \\(j \\in [d]\\) \\[ \\dot{\\hat{\\xi}}_{\\lambda_i,j} = \\begin{bmatrix} - \\mathrm{Re}(\\lambda_i) &amp; - \\mathrm{Im}(\\lambda_i) \\\\ \\mathrm{Im}(\\lambda_i) &amp; - \\mathrm{Re}(\\lambda_i) \\end{bmatrix} \\hat{\\xi}_{\\lambda_i,j} + \\begin{bmatrix} y_j \\\\ 0 \\end{bmatrix}. \\] Therefore, the dimension of the observer is \\(2 \\times d (n+1)\\). Theorem 6.4 states that as long as the system (6.79) is backward distinguishable, then there exists a stationary transformation \\(T\\) that can transform the system to a new coordinate system \\(\\xi\\) such that the dynamics in \\(\\xi\\) is Hurwitz. A closer look at the structure of \\(A\\) and \\(B\\) reveals that the coordinate transformation needs to satisfy \\(n+1\\) differential equations of the form \\[ \\frac{\\partial T_{\\lambda}}{\\partial x}(x) \\dot{x} = \\lambda T_{\\lambda} (x) + y \\] where each \\(T_{\\lambda}\\) transforms the state \\(x\\) into a new coordinate having the same dimension of \\(y\\). Clearly, if \\(T = (T_\\lambda)\\), i.e., there is a single \\(\\lambda\\), then \\(T\\) is not uniformly injective (as the dimension of \\(\\xi\\) is \\(d &lt; n\\)). Consequently, by choosing \\[ T = (T_{\\lambda_1},\\dots,T_{\\lambda_{n+1}}), \\] the uniform injectivity of \\(T\\) is ensured. However, the difficulty lies in the computation of \\(T\\) (and \\(T_\\lambda\\)), let alone its inverse (that recovers \\(x\\) from \\(\\xi\\)). Even though \\(\\mathcal{X}\\) is backward invariant, the formulation (6.80) is difficult to compute. I tried very hard to find a coordinate transformation \\(T\\) that can convert the non-controlled pendulum dynamics into the KKL form but did not succeed. You should let me know if you were able to find one! Nevertheless, the following example shows you the flavor of how such a transformation may look like for a different system. Example 6.5 (KKL Observer for an Oscillator with Unknown Frequency) Consider a harmonic oscillator with unknown frequency \\[ \\begin{cases} \\dot{x}_1 = x_2 \\\\ \\dot{x}_2 = - x_1 x_3 \\\\ \\dot{x}_3 = 0 \\end{cases}, \\quad y = x_1 \\] Consider the coordinate transformation \\[ T_{\\lambda_i} (x) = \\frac{\\lambda_i x_1 - x_2}{\\lambda_i^2 + x_3}, \\quad \\lambda_i &gt; 0, i=1,\\dots,p. \\] We have \\[\\begin{align} \\frac{\\partial T_{\\lambda_i}(x)}{\\partial x} \\dot{x} &amp;= \\left\\langle \\begin{bmatrix} \\frac{\\lambda_i}{\\lambda_i^2 + x_3} \\\\ \\frac{-1 }{\\lambda_i^2 + x_3} \\\\ \\frac{x_2 - \\lambda_i x_1}{(\\lambda_i^2 + x_3)^2} \\end{bmatrix}, \\begin{bmatrix} x_2 \\\\ -x_1 x_3 \\\\ 0 \\end{bmatrix} \\right \\rangle = \\frac{\\lambda_i x_2 + x_1 x_3}{\\lambda_i^2 + x_3} \\\\ -\\lambda_i T_{\\lambda_i}(x) + y &amp;= \\frac{-\\lambda_i^2 x_1 + \\lambda_i x_2 + x_1 \\lambda_i^2 + x_1 x_3}{\\lambda_i^2 + x_3} = \\frac{\\lambda_i x_2 + x_1 x_3}{\\lambda_i^2 + x_3} \\end{align}\\] Therefore, with \\[ \\xi = T(x) = [T_{\\lambda_1}(x), T_{\\lambda_2}(x),\\dots,T_{\\lambda_p}(x)]^T, \\] we have \\[ \\dot{\\xi} = \\underbrace{\\begin{bmatrix} - \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; -\\lambda_p \\end{bmatrix}}_{A} \\xi + \\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} y \\] with \\(A\\) clearly Hurwitz. With some extra arguments (cf. Section 8.1.1 in (Bernard 2019)), one can see that the transformation \\(T\\) is injective with \\(p \\geq 4\\) distinct \\(\\lambda_i\\)’s. Therefore, this is a valid KKL observer. The final issue that one needs to think about is, since the observer is estimating \\(\\hat{\\xi}\\), how to recover \\(\\hat{x}\\)? In this example, there is actually no analytical formula for recovering \\(\\hat{x}\\) from \\(\\hat{\\xi}\\). In this case, one approach is to solve the following optimization problem \\[ \\hat{x} = \\arg\\min_{x} \\Vert \\hat{\\xi} - T(x) \\Vert^2, \\] which may be quite expensive. A more general treatment is given in Section 8.2.2 in (Bernard 2019). 6.5.4.2 Controlled Systems 6.5.5 Triangular Template 6.5.6 Design with Convex Optimization Consider a nonlinear system \\[\\begin{equation} \\dot{x} = f(x) + \\psi(u,y), \\quad y = Cx \\tag{6.81} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\), \\(y \\in \\mathbb{R}^d\\), \\(C\\) a constant matrix, and \\(\\psi(u,y)\\) a nonlinear function. We assume that \\(f(x)\\) is a polynomial vector map (i.e., each entry of \\(f\\) is a polynomial function in \\(x\\)). Certainly the formulation in (6.81) is not as general as (6.1), but it is general enough to include many examples in robotics. Recall that I said the essence of observer design is to (i) simulate the dynamics when the state estimation is correct, and (ii) to correct the state estimation from observation when it is off. Therefore, we wish to design an observer for (6.81) in the following form \\[\\begin{equation} \\dot{\\hat{x}} = \\underbrace{f(\\hat{x}) + \\psi(u,y)}_{\\text{dynamics simulation}} + \\underbrace{K(y - \\hat{y},y)(C \\hat{x} - y)}_{\\text{feedback correction}}, \\tag{6.82} \\end{equation}\\] where, compared to the Luenberger observer (6.61), we allow the gain matrix \\(K\\) to be nonlinear functions of the true observation \\(y\\) and the estimated observation \\(\\hat{y}\\). With the observer (6.82), the dynamics on the estimation error \\(e = \\hat{x} - x\\) becomes \\[ \\dot{e} = f(x + e) - f(x) + K(Ce,Cx)C e. \\] If we can find a Lyapunov-like function \\(V(e)\\) so that \\(V(e)\\) is positive definite and \\(\\dot{V}(e)\\) is negative definite, then Lyapunov stability theorem 5.3 tells us that \\(e=0\\) is asymptotically stable. Because we do not know the gain matrix \\(K\\) either, we need to jointly search for \\(V\\) and \\(K\\) (that are polynomials). Mathematically, this is \\[\\begin{equation} \\begin{split} \\text{find} &amp; \\quad V, K \\\\ \\text{subject to} &amp; \\quad V(0) = 0, \\quad V(e) &gt; 0, \\forall e \\neq 0 \\\\ &amp; \\quad \\dot{V}(e) = \\frac{\\partial V}{\\partial e} \\left( f(x + e) - f(x) + K(Ce,Cx)C e \\right) &lt; 0, \\forall e \\neq 0, \\forall x \\in \\mathbb{X} \\\\ &amp; \\quad V(e) \\geq \\epsilon \\Vert e \\Vert^2, \\forall e \\end{split} \\tag{6.83} \\end{equation}\\] where the last constraint is added to make sure \\(V(e)\\) is radially unbounded. Furthermore, if we replace the second constraint by \\(\\dot{V}(e) \\leq - \\lambda V(e)\\), then we can guarantee \\(V(e)\\) converges to zero exponentially. Problem (6.83), however, is not a convex optimization problem, due to the term \\(\\frac{\\partial V}{\\partial e} K\\) being bilinear in the coefficients of \\(V\\) and \\(K\\). Nevertheless, as shown in (Ebenbauer, Renz, and Allgower 2005), we can use a reparameterization trick to formulate a stronger version of (6.83) as follows. \\[\\begin{equation} \\begin{split} \\text{find} &amp; \\quad V, Q(Ce), M(Ce,Cx) \\\\ \\text{subject to} &amp; \\quad V(0) = 0, \\quad V(e) &gt; 0, \\forall e \\neq 0 \\\\ &amp; \\quad \\frac{\\partial V}{\\partial e} = e^T Q(Ce), \\quad Q(Ce) \\succ 0 \\\\ &amp; \\quad e^T Q(Ce) \\left( f(x + e) - f(x) \\right) + e^T M(Ce,Cx) C e &lt; 0, \\forall e \\neq 0, \\forall x \\in \\mathbb{X} \\\\ &amp; \\quad V(e) \\geq \\epsilon \\Vert e \\Vert^2, \\forall e \\end{split} \\tag{6.84} \\end{equation}\\] Clearly, if we can solve problem (6.84), then \\[ K = Q(Ce)^{-1} M(Ce,Cx) \\] is the right gain matrix for the formulation (6.83). Let us bring this idea to action in our pendulum example. Example 6.6 (Pendulum Observer with Convex Optimization) With \\(x = [\\mathfrak{s}, \\mathfrak{c}, \\dot{\\theta}]^T\\) (\\(\\mathfrak{s} = \\sin \\theta, \\mathfrak{c} = \\cos \\theta\\)), we can write the pendulum dynamics as \\[ \\dot{x} = \\underbrace{\\begin{bmatrix} \\mathfrak{c} \\dot{\\theta} \\\\ - \\mathfrak{s} \\dot{\\theta} \\\\ - \\frac{b}{ml^2} \\dot{\\theta} \\end{bmatrix}}_{=:f(x)} + \\underbrace{\\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{u - mgl \\mathfrak{s}}{ml^2} \\end{bmatrix}}_{=: \\psi(u,y)}, \\quad y = \\underbrace{\\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}}_{=:C} x \\] Clearly \\(f(x)\\) is a polynomial. Solving the convex optimization problem (6.84), we obtain a solution \\[ V(e) = 0.5954 e_1^2 + 0.5954 e_2^2 + 0.9431 e_3^2 \\] \\[ Q(Ce) = \\begin{bmatrix} 0.4603 e_2^2 + 1.1909 &amp; -0.4603 e_1 e_2 &amp; 0 \\\\ -0.4603 e_1 e_2 &amp; 0.4603 e_1^2 + 1.1909 &amp; 0 \\\\ 0 &amp; 0 &amp; 1.8863 \\end{bmatrix} \\] \\[ M(Ce,Cx) = \\begin{bmatrix} \\substack{-2.0878 e_1^2 - 0.8667 e_2^2 - \\\\ 0.4588 (y_1^2 + y_2^2) - 0.4885} &amp; - 0.8667 e_1 e_2 \\\\ - 0.8667 e_1 e_2 &amp; \\substack{-0.8667 e_1^2 - 2.0878 e_2^2 - \\\\ 0.4588 (y_1^2 + y_2^2) - 0.4885} \\\\ -1.1909 y_2 &amp; 1.1909 y_1 \\end{bmatrix} \\] Simulating this observer, we verify that the observer is in fact exponentially converging, as shown in Fig. 6.4. The Matlab code for formulating and solving the convex optimization (6.84) can be found here. The code for simulating the observer can be found here. Figure 6.4: Simulation of the pendulum observer design from convex optimization 6.6 Observer Feedback Now that we have good ways to design a state observer, we will see how we can use the observer for feedback control. Example 6.7 (Pendulum Stabilization with A Luenberger Observer) In Example 6.3, we have written the dynamics of a pendulum, and the dynamics of a Luenberger observer as \\[\\begin{align} \\dot{x} &amp;= A x + B(u,y) \\\\ \\dot{\\hat{x}} &amp;= A \\hat{x} + B(u,y) + KC (x - \\hat{x}) \\end{align}\\] We wish to understand (so we can optimize) the behavior of this system under certain control input \\(u\\). To do so, let us denote \\(e = \\hat{x} - x\\), and write the above dynamics as \\[\\begin{align} \\dot{x} &amp;= A x + B(u,Cx) \\\\ \\dot{e} &amp; = (A - KC) e \\end{align}\\] Denoting \\(z = [x,e]^T\\), we have the augmented dynamics \\[ \\dot{z} = \\underbrace{\\begin{bmatrix} A &amp; 0 \\\\ 0 &amp; A - KC \\end{bmatrix}}_{=:F} z + \\underbrace{\\begin{bmatrix} B(u,Dz) \\\\ 0 \\end{bmatrix}}_{=:G(z,u)} \\] We want to stabilize the system at \\(z_0 = [\\pi,0,0,0]^T\\) (the upright position) subject to control bounds \\(u \\in \\mathbb{U} = [-u_{\\max},u_{\\max}]\\). We need to find a control Lyapunov function (CLF), \\(V(z)\\), that satisfies the following constraints: \\[ V(z_0) = 0 \\] \\[ V(z) &gt; 0 \\quad \\forall z \\in \\{z: V(z) &lt; \\rho, z \\neq z_0 \\} \\] \\[ \\inf_{u \\in \\mathbb{U}} [L_F V(z) + L_G V(z)] \\leq 0 \\quad \\forall z \\in \\mathcal{Z} \\] where \\(L_F V\\) and \\(L_G V\\) are the Lie derivatives of \\(V\\) along \\(F\\) and \\(G(z,u)\\), respectively. \\(\\mathcal{Z}\\) is the set of all possible augmented states. The CLF will define the set of admissible control inputs \\(U\\). \\[ U = \\{ u: L_f V(z) + L_g V(z)u \\leq 0 \\} \\] To find the smallest-magnitude control input such that \\(u \\in K\\), we may use a quadratic program: \\[ \\min_{u \\in \\mathcal{U}} ||u||^2 \\] \\[ \\mathrm{s.t.} \\quad L_f V(z) + L_g V(z)u \\leq -c V(z) \\] where \\(c\\) is some positive constant. The challenge now is in choosing a suitable \\(V(z)\\). References Andrieu, Vincent, and Laurent Praly. 2006. “On the Existence of a Kazantzis–Kravaris/Luenberger Observer.” SIAM Journal on Control and Optimization 45 (2): 432–56. Astolfi, Alessandro, and Romeo Ortega. 2003. “Immersion and Invariance: A New Tool for Stabilization and Adaptive Control of Nonlinear Systems.” IEEE Transactions on Automatic Control 48 (4): 590–606. Bernard, Pauline. 2019. Observer Design for Nonlinear Systems. Vol. 479. Springer. Bernard, Pauline, Vincent Andrieu, and Daniele Astolfi. 2022. “Observer Design for Continuous-Time Dynamical Systems.” Annual Reviews in Control. Besançon, Gildas, Guy Bornard, and Hassan Hammouri. 1996. “Observer Synthesis for a Class of Nonlinear Control Systems.” European Journal of Control 2 (3): 176–92. Ebenbauer, Christian, Jonathan Renz, and F Allgower. 2005. “Polynomial Feedback and Observer Design Using Nonquadratic Lyapunov Functions.” In Proceedings of the 44th IEEE Conference on Decision and Control, 7587–92. IEEE. Hammouri, Hassan, and Jesus de Leon Morales. 1990. “Observer Synthesis for State-Affine Systems.” In 29th IEEE Conference on Decision and Control, 784–85. IEEE. Janny, Steeven, Vincent Andrieu, Madiha Nadri, and Christian Wolf. 2021. “Deep Kkl: Data-Driven Output Prediction for Non-Linear Systems.” In 2021 60th IEEE Conference on Decision and Control (CDC), 4376–81. IEEE. Kalman, Rudolph E, and Richard S Bucy. 1961. “New Results in Linear Filtering and Prediction Theory.” Kalman, Rudolph Emil. 1960. “A New Approach to Linear Filtering and Prediction Problems.” Karagiannis, Dimitrios, and Alessandro Astolfi. 2005. “Nonlinear Observer Design Using Invariant Manifolds and Applications.” In Proceedings of the 44th IEEE Conference on Decision and Control, 7775–80. IEEE. Kazantzis, Nikolaos, and Costas Kravaris. 1998. “Nonlinear Observer Design Using Lyapunov’s Auxiliary Theorem.” Systems &amp; Control Letters 34 (5): 241–47. Luenberger, David G. 1964. “Observing the State of a Linear System.” IEEE Transactions on Military Electronics 8 (2): 74–80. Miao, Keyan, and Konstantinos Gatsis. 2023. “Learning Robust State Observers Using Neural ODEs.” In Learning for Dynamics and Control Conference, 208–19. PMLR. Niazi, Muhammad Umar B, John Cao, Xudong Sun, Amritam Das, and Karl Henrik Johansson. 2023. “Learning-Based Design of Luenberger Observers for Autonomous Nonlinear Systems.” In 2023 American Control Conference (ACC), 3048–55. IEEE. Reif, Konrad, Stefan Gunther, Engin Yaz, and Rolf Unbehauen. 1999. “Stochastic Stability of the Discrete-Time Extended Kalman Filter.” IEEE Transactions on Automatic Control 44 (4): 714–28. Thrun, S, W Burgard, and D Fox. 2005. “Probabilistic Robotics.” MIT Press. Van Der Merwe, Rudolph. 2004. Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic State-Space Models. Oregon Health &amp; Science University. Yang, Tao, Prashant G Mehta, and Sean P Meyn. 2013. “Feedback Particle Filter.” IEEE Transactions on Automatic Control 58 (10): 2465–80. We say “any solution” because there may be several solutions to the observer (6.52) due to \\(\\mathcal{F}\\) only being continuous. This is not a problem as long as any such solution satisfies the required convergence property.↩︎ The time dependence of \\(\\mathcal{T}_u\\) enables us to cover the case where the knowledge of the \\(u\\) and \\(y_{x_0,u}\\) is used to construct the estimate from the observer state. In particular, using the output sometimes can reduce the dimension of the observer state (and thus alleviate the computations), thus obtaining a reduced-order observer. For example, see (Karagiannis and Astolfi 2005) and (Astolfi and Ortega 2003).↩︎ A function \\(\\rho: \\mathbb{R}_+ \\rightarrow \\mathbb{R}_+\\) is a \\(\\mathcal{K}\\) function if \\(\\rho(0) = 0\\), \\(\\rho\\) is continuous, and \\(\\rho\\) is increasing.↩︎ An injective function is a function \\(f\\) that maps distinct elements of its domain to distinct elements. That is, \\(f(x_a) = f(x_b)\\) implies \\(x_a = x_b\\), or equivalently, \\(x_a \\neq x_b\\) implies \\(f(x_a) \\neq f(x_b)\\).↩︎ I have to say I was a bit surprised when I arrived at this formulation.↩︎ "],["geometric-vision.html", "Chapter 7 Geometric Vision 7.1 3D Rotations and Poses 7.2 The Pinhole Camera Model 7.3 Camera Pose Estimation 7.4 Point Cloud Registration", " Chapter 7 Geometric Vision In this Chapter, we introduce the fundamentals of geometric vision, a (classical) branch of computer vision that seeks to estimate geometric models (e.g., 3D rotations, translations, and points) from sensor measurements (e.g., images and point clouds). There are two goals for introducing geometric vision. In the output feedback Chapter 6, we see that the full state \\(x\\) of a dynamical system is often not available, and needs to be estimated from the measurement signal \\(y\\) that satisfies \\[ y(t) = h(x(t),u(t)) \\] potentially plus some noise. In Chapter 6, we studied the case where \\(y\\) is part of the state \\(x\\), often the position \\(q\\) of a second-order system \\(x=[q;\\dot{q}]\\). For example, in the pendulm swing-up example, we assume the angular position \\(\\theta\\) is observed, but not the angular velocity \\(\\dot{\\theta}\\). However, in many practical applications, the measurement signal \\(y\\) does not directly tell us the position \\(q\\), and we need to estimate \\(q\\) from \\(y\\). For instance, a quadcopter needs to estimate its position from its onboard cameras. Once we obtain an estimated \\(q\\) from \\(y\\), we can use the observer synthesis methods in Chapter 6 to obtain the full state estimation. The estimation community and the control community are a bit separated (at least in my opinion), despite that they share a lot of common tools, especially optimization. We will see that estimating \\(q\\) from \\(y\\), where \\(y\\) could be a high-dimensional image, is often formulated as an optimization problem that is difficult to solve. However, using the SOS tool we developed in Chapter 5, we can actually solve the optimization problem to global optimality. 7.1 3D Rotations and Poses 7.1.1 Rotation matrices The first part is a quick recap of the basics in linear algebra. Definition 7.1 (Orthogonal Matrix) We call a \\(n\\times n\\) square matrix \\(A\\) orthogonal if the column of \\(A\\) is orthogonal to each other and all the column vectors have unit length. The set of all \\(n\\times n\\) orthogonal matrices is denoted as \\(O(n)\\). Below are some basic properties of orthogonal matrices: Proposition 7.1 (Property of Orthogonal Matrix) Let \\(A\\) be a \\(n\\times n\\) orthogonal matrix. Then: \\(A^T = A^{-1}\\) and \\(A^T\\) is also a orthogonal matrix. For every orthogonal matrices \\(A,B\\), \\(AB\\) is also a orthogonal matrix. \\(\\det(A) = \\pm 1\\). \\(A\\) preserves dot product, i.e. \\(&lt;x,y&gt; = &lt;Ax,Ay&gt;\\), thus preserves the length of a vector, i.e. \\(\\|Ax\\|_2 = \\|x\\|_2\\). All the eigenvalues of \\(A\\) have modulus one. Proof. We only offer the proof of the last property. Consider any eigenvalue \\(\\lambda\\) of \\(A\\), and \\(x\\) be its eigenvector. We have \\(&lt;x,x&gt; = &lt;A^TAx,x&gt;=&lt;Ax,Ax&gt; = |\\lambda|^2&lt;x,x&gt;\\), thus \\(|\\lambda|=1\\). There are two types of orthogonal matrices, categorized by determinant \\(1\\) and \\(-1\\). Those with determinant \\(1\\) are called rotation matrices. The set of rotation matrices is denoted as \\(SO(n)\\) (Special Orthogonal). In the world of robotics and most engineering fields, we care about \\(SO(3)\\) the most. Below are some basic properties of \\(3\\times 3\\) rotation matrices: Proposition 7.2 (Property of Rotation Matrix) Let \\(A\\) be a \\(3\\times 3\\) orthogonal matrix. Then: \\(\\det(A) = \\pm 1\\) For every rotation matrices \\(A,B\\), \\(A^T\\),\\(AB\\) are also rotation matrices. \\(A\\) always has an eigenvalue \\(1\\). If \\(A\\) is not identity, \\(A\\) either has two conjugate complex eigenvalues not equal to 1, or has two eigenvalues -1. Proof. We can only prove the last property. From the property of orthogonal matrix, we know that all eigenvalues have modulus one. First, note that there must exist at least one real eigenvalue. Because eigenvalues with nonzero imaginary parts always come in pair and 3 is an odd number. Then there are two possible cases: (1) All the eigenvalues are real; (2) There is only one eigenvalue. For case one, note that the determinant of a matrix is the product of all eigenvalues, then rotation matrix can’t have all the eigenvalues -1. For case two. The product of the pairing complex eigenvalues is the square of the modulus of the eigenvalue, which is 1. So the real eigenlvalue left must be 1. Finally if all the eigenvalues of \\(A\\) are 1, then \\(A\\) must be identity. 7.1.2 Coordinate Frame Coordinate frames are a set of orthogonal basis (containing three axes) attached to a certain body at a point. It serves as the tool to describe the position of points relative to that body. Conventionally, coordinate frames are right-handed. We will encounter different frames in applications, including: (1) Robot (robot frame “\\(r\\)”), (2) Sensor on the robot (e.g. camera frame ‘\\(c\\)’), (3) A fixed location in the world (world frame “\\(w\\)”) It’s worth metion that, denote \\(\\vec{x},\\vec{y},\\vec{z}\\) as the three axes of the coordinate frame, then the right-handed property can be expressed as: \\(\\vec{x}\\cdot(\\vec{y}\\times \\vec{z}) = 1\\), which is the same as \\(\\det ([\\vec{x},\\vec{y},\\vec{z}]) = 1\\). So the matrix \\([\\vec{x},\\vec{y},\\vec{z}]\\) is a rotation matrix. It’s natural for us to ask three questions: (1) How to express a point in a given frame? (2) How to represent a frame \\(r\\) with respect to a frame \\(w\\)? (3) How to translate the coordinate of a point in different frames? (1) How to express a point in a given frame? Let’s consider a reference frame \\(r\\) and denote the the three axes attached to it as \\(\\vec{x^r},\\vec{y^r},\\vec{z^r}\\). Then for any point \\(p\\), we care about the vector pointing from the origin of the frame to \\(p\\). We slightly abuse the notation and let that vector called \\(p\\).(Since we will only care about the vector) Then, we can express \\(p\\) as the combination of the basis, i.e. \\[p = p^r_x \\vec{x^r}+p^r_y\\vec{y^r}+p^r_z\\vec{z^r} = \\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r}\\end{bmatrix} \\begin{bmatrix} p^r_x\\\\p^r_y\\\\p^r_z \\end{bmatrix}\\] Thus we can fully describe point \\(p\\) with three scalars \\(p^r_x,p^r_y,p^r_z\\), which is called the coordinates of \\(p\\) with respect to the frame \\(r\\). (2) How to represent a frame \\(r\\) with respect to a frame \\(w\\)? Now we consider how to describe frame \\(w\\) in frame \\(r\\). Let’s focus on the simple case where the origin of the two coordinate systems coincide. Then we can express the axes \\(\\vec{x^w},\\vec{y^w},\\vec{z^w}\\) in frame \\(r\\) directly. For example, thanks to the orthogonality, we can get:\\[\\vec{x^w} = &lt;\\vec{x^w},\\vec{x^r}&gt;\\vec{x^r} +&lt;\\vec{x^w},\\vec{y^r}&gt;\\vec{y^r} + &lt;\\vec{x^w},\\vec{z^r}&gt;\\vec{z^r} = \\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r}\\end{bmatrix}\\begin{bmatrix} &lt;\\vec{x^w},\\vec{x^r}&gt;\\\\ &lt;\\vec{x^w},\\vec{y^r}&gt;\\\\&lt;\\vec{x^w},\\vec{z^r}&gt; \\end{bmatrix}\\] So we can get: \\[\\begin{bmatrix} \\vec{x^w},\\vec{y^w},\\vec{z^w} \\end{bmatrix} = \\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r} \\end{bmatrix}\\begin{bmatrix} &amp;&lt;\\vec{x^w},\\vec{x^r}&gt; &amp;&lt;\\vec{y^w},\\vec{x^r}&gt; &amp;&lt;\\vec{z^w},\\vec{x^r}&gt;\\\\ &amp;&lt;\\vec{x^w},\\vec{y^r}&gt; &amp;&lt;\\vec{y^w},\\vec{y^r}&gt; &amp;&lt;\\vec{z^w},\\vec{y^r}&gt;\\\\ &amp;&lt;\\vec{x^w},\\vec{z^r}&gt; &amp;&lt;\\vec{y^w},\\vec{z^r}&gt; &amp;&lt;\\vec{z^w},\\vec{z^r}&gt; \\end{bmatrix} = \\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r} \\end{bmatrix}R^r_w\\] Note that \\(R^r_w = (\\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r} \\end{bmatrix})^T\\begin{bmatrix} \\vec{x^w},\\vec{y^w},\\vec{z^w} \\end{bmatrix}\\) is a rotation matrix. Example 7.1 (A simple example of translation between frames) If we for frame \\(w\\) we have \\[\\begin{bmatrix} \\vec{x^w},\\vec{y^w},\\vec{z^w} \\end{bmatrix} = I_3\\] and for frame \\(r\\) we have \\[\\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r} \\end{bmatrix} = \\begin{bmatrix} &amp;\\cos(\\theta) &amp;-\\sin(\\theta) &amp;0\\\\ &amp;\\sin(\\theta) &amp;\\cos(\\theta) &amp;0\\\\ &amp;0 &amp;0 &amp;1 \\end{bmatrix}\\] Then we can have: \\[\\begin{align} \\begin{bmatrix} \\vec{x^w},\\vec{y^w},\\vec{z^w} \\end{bmatrix} =&amp; \\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r} \\end{bmatrix}\\begin{bmatrix} &amp;&lt;\\vec{x^w},\\vec{x^r}&gt; &amp;&lt;\\vec{y^w},\\vec{x^r}&gt; &amp;&lt;\\vec{z^w},\\vec{x^r}&gt;\\\\ &amp;&lt;\\vec{x^w},\\vec{y^r}&gt; &amp;&lt;\\vec{y^w},\\vec{y^r}&gt; &amp;&lt;\\vec{z^w},\\vec{y^r}&gt;\\\\ &amp;&lt;\\vec{x^w},\\vec{z^r}&gt; &amp;&lt;\\vec{y^w},\\vec{z^r}&gt; &amp;&lt;\\vec{z^w},\\vec{z^r}&gt; \\end{bmatrix} \\\\ =&amp; \\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r} \\end{bmatrix}\\begin{bmatrix} &amp;\\cos(\\theta) &amp;\\sin(\\theta) &amp;0\\\\ &amp;-\\sin(\\theta) &amp;\\cos(\\theta) &amp;0\\\\ &amp;0 &amp;0 &amp;1 \\end{bmatrix} \\end{align}\\] So we can get \\(R_w^r = \\begin{bmatrix} &amp;\\cos(\\theta) &amp;\\sin(\\theta) &amp;0\\\\ &amp;-\\sin(\\theta) &amp;\\cos(\\theta) &amp;0\\\\ &amp;0 &amp;0 &amp;1 \\end{bmatrix}\\) (3) How to translate the coordinate of a point in different frames? First let us consider frame \\(w\\) and \\(r\\) with the same origin. Then for any point \\(\\vec{p}\\), we will have: \\[\\begin{align} &amp;\\vec{p} = \\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r}\\end{bmatrix} \\begin{bmatrix} p^r_x\\\\p^r_y\\\\p^r_z \\end{bmatrix} = \\begin{bmatrix} \\vec{x^w},\\vec{y^w},\\vec{z^w}\\end{bmatrix} \\begin{bmatrix} p^w_x\\\\p^w_y\\\\p^w_z \\end{bmatrix} \\\\ \\Rightarrow&amp; \\vec{p^r} = \\begin{bmatrix} p^r_x\\\\p^r_y\\\\p^r_z \\end{bmatrix} = \\begin{bmatrix} \\vec{x^r}^T\\\\ \\vec{y^r}^T\\\\ \\vec{z^r}^T\\end{bmatrix} \\begin{bmatrix} \\vec{x^w},\\vec{y^w},\\vec{z^w}\\end{bmatrix} \\begin{bmatrix} p^w_x\\\\p^w_y\\\\p^w_z \\end{bmatrix} = R^r_w \\begin{bmatrix} p^w_x\\\\p^w_y\\\\p^w_z \\end{bmatrix} = R^r_w\\vec{p^w} \\end{align}\\] We can find that, we only need to multiply the matrix \\(R^r_w\\) to translate the coordinate in \\(w\\) frame to \\(r\\) frame. Example 7.2 (A simple example of translation between frames (Cont.)) If we for frame \\(w\\) we have \\[\\begin{bmatrix} \\vec{x^w},\\vec{y^w},\\vec{z^w} \\end{bmatrix} = I_3\\] and for frame \\(r\\) we have \\[\\begin{bmatrix} \\vec{x^r},\\vec{y^r},\\vec{z^r} \\end{bmatrix} = \\begin{bmatrix} &amp;\\cos(\\theta) &amp;-\\sin(\\theta) &amp;0\\\\ &amp;\\sin(\\theta) &amp;\\cos(\\theta) &amp;0\\\\ &amp;0 &amp;0 &amp;1 \\end{bmatrix}\\] Assume point \\(p\\) has coordinates \\(\\vec{p^w} = \\begin{bmatrix} \\cos(\\theta)\\\\\\sin(\\theta)\\\\0 \\end{bmatrix}\\) in frame \\(w\\). Then we can have: \\[\\vec{p^r} = R^r_w\\vec{p^w} = \\begin{bmatrix} &amp;\\cos(\\theta) &amp;\\sin(\\theta) &amp;0\\\\ &amp;-\\sin(\\theta) &amp;\\cos(\\theta) &amp;0\\\\ &amp;0 &amp;0 &amp;1 \\end{bmatrix}\\begin{bmatrix} \\cos(\\theta)\\\\\\sin(\\theta)\\\\0 \\end{bmatrix} = \\begin{bmatrix} 1\\\\0\\\\0 \\end{bmatrix}\\] What if the origin of the two frames are not the same? We may think of two ways to do so: (1) First do the translation, then do the rotation, (2) First do the rotation, then do the translation. The most natural way is (1). Here is a quick example: Example 7.3 (A simple example with different origins) Suppose we have two frames: Frame 1 and Frame 2 as depicted below. All the coordinates are in frame 1. \\[ R_1^2 = \\begin{bmatrix}&lt;\\vec{x^1},\\vec{x^2}&gt; &amp;&lt;\\vec{y^1},\\vec{x^2}&gt; \\\\ &lt;\\vec{x^1},\\vec{y^2}&gt; &amp;&lt;\\vec{y^1},\\vec{y^2}&gt; \\end{bmatrix} = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} &amp;\\frac{\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} &amp;\\frac{\\sqrt{2}}{2}\\end{bmatrix}\\] and \\(t_1^2 = \\begin{bmatrix} 0\\\\-2\\end{bmatrix}\\). Figure 7.1: A simple example with different origins We can see that, if we want to use method 1, we can just consecutively do the translation and the rotation. However, if we want to do it reversely, we must multiply the translation by the rotation matrix. Actually it’s easy to show this two methods are equivalent. In conclusion, the formula is as follows: \\[p^2 = R^2_1(p^1+t_1^2) = R^2_1p^1+R^2_1t_1^2\\] 7.1.3 Representations of the rotations Although rotation matrix is enough to characterize a rotation. But it’s not simple enough and not intuitive enough. For example, we can’t explicitly know the rotation angles from the rotation matrix. The rotation matrix has 9 elements, but there are many constraints. So is it possible for us to find a simpler representation for rotation? 7.1.3.1 Euler angles representation Intuitively, we can achieve any rotation by rotating along the x, y, z axes in turn. First let’s introduce some basic rotations along x,y,z axes. Proposition 7.3 (Basic Rotations) Below are the basic rotation matrices along the x, y, z axes, all the rotations are counterclockwise. Rotation along z axes, with angle \\(\\gamma\\) is: \\[\\begin{bmatrix} &amp;\\cos(\\gamma) &amp;\\sin(\\gamma) &amp;0\\\\ &amp;-\\sin(\\gamma) &amp;\\cos(\\gamma) &amp;0\\\\ &amp;0 &amp;0 &amp;1 \\end{bmatrix}\\] Rotation along y axes, with angle \\(\\beta\\) is: \\[\\begin{bmatrix} &amp;\\cos(\\beta) &amp;0 &amp;-\\sin(\\beta)\\\\ &amp;0 &amp;1 &amp;0\\\\ &amp;\\sin(\\beta) &amp;0 &amp;\\cos(\\beta) \\end{bmatrix}\\] Rotation along x axes, with angle \\(\\alpha\\) is: \\[\\begin{bmatrix} &amp;1&amp;0&amp;0 \\\\ &amp;0 &amp;\\cos(\\alpha) &amp;\\sin(\\alpha)\\\\ &amp;0 &amp;-\\sin(\\alpha) &amp;\\cos(\\alpha) \\end{bmatrix}\\] Actually every rotation can be written into combination of no more than three basic rotations, with no two consecutive rotations along the same axis. A popular choice of the sequence is roll-pitch-yaw. Proposition 7.4 (Roll-pitch-yaw angle representation) Any rotation matrix \\(R^w_r\\) can be written into combination of basic rotations in the following order:\\[R^w_r = R_z(\\gamma)R_y(\\beta)R_x(\\alpha)\\] where \\(\\gamma\\) is called the yaw angle, \\(\\beta\\) is called the pitch angle, and \\(\\alpha\\) is called the roll angle. This representation is very intuitive, because it directly tells us the rotation angles. However, the calculation will include trigonometric functions. So it can be hard to calculate and analyze. Moreover, if you want to recover the Euler angles from rotation matrices, there may be problems in certain point. The example below shows the singularities. Example 7.4 (Singularities for Euler angles) In formula \\[R = R_z(\\gamma)R_y(\\beta)R_x(\\alpha)\\] consider \\(\\beta = \\frac{\\pi}{2}\\). Then we will have: \\[R = R_z(\\gamma)R_y(\\frac{\\pi}{2})R_x(\\alpha) = \\begin{bmatrix} &amp;0 &amp;\\sin(\\alpha+\\gamma) &amp;-\\cos(\\alpha+\\gamma)\\\\ &amp;0 &amp;\\cos(\\alpha+\\gamma) &amp;\\sin(\\alpha+\\gamma)\\\\ &amp;1 &amp;0 &amp;0 \\end{bmatrix}\\] So there are ambiguities in choosing \\(\\alpha,\\gamma\\) for the same rotation matrices. 7.1.3.2 Axis-angle representation Another intuitive representation is the axis-angle representation. Imagine a rotation in 3D space, it seems that all the rotations are rotation with respect to an axis (not necessary to be aligned with the x,y,z axes) for some angle. How to find the axis and the corresponding angle? Given rotation angle and axis, how can we get the rotation matrix? Next theorem will give us an explicit formula. Theorem 7.1 (Rodrigues' rotation formula) Given a rotation angle \\(\\theta\\) and a rotation axis \\(u\\) (expressed by a unit vector). The rotation matrix \\(R\\) can be computed as: \\[R = \\cos(\\theta)I_3+\\sin(\\theta)[u]_\\times+(1-\\cos(\\theta))uu^T\\] where \\([u]_\\times = \\begin{bmatrix} &amp;0 &amp;-u_z &amp;u_y\\\\ &amp;u_z &amp;0 &amp;-u_x\\\\ &amp;-u_y &amp;u_x &amp;0 \\end{bmatrix}\\) How can we do it reversely? Intuitively, axis of the rotation is the direction that the rotation preserves. Mathematically speaking, axis of rotation is in the direction of the eigenvector with respect to the eigenvalue of 1. From the discussion above, if rotation is not equal to identity, then there is a unique direction of rotation axis. So we can solve for the rotation axis by calculating \\(u\\) satisfying:\\[Ru = u\\] To get the rotation angle, we notice that if we take trace of both of the sides of Rodrigues’ rotation formula, we can get:\\[\\text{Tr}(R) = 2\\cos(\\theta) + 1\\] Note that if we treat \\(\\theta\\) as the minimal angle of the rotation, we can always restrict the \\(\\theta\\in[0,\\pi]\\) and thus \\(\\theta = \\arccos(\\frac{\\text{Tr}(R) - 1}{2})\\). However, then we will leave the rotation direction to the sign of axis \\(u\\). That is to say: \\(R(u,\\theta)^{-1} = R(-u,\\theta)\\). So we need to double check the two potential solutions to make sure which rotation is the one we want. 7.1.3.3 Quaternion representation W.R. Hamilton first introduced the definition of quaternion representation. Definition 7.2 (Quaternion) A quaternion is represented in the form \\(\\textbf{q} = \\textbf{i}q_1 + \\textbf{j}q_2 + \\textbf{k}q_3 + q_4\\), where \\(q_1,q_2,q_3,q_4\\) are real numbers and \\(\\bf i,j,k\\) satisfying: \\[\\begin{align} &amp;\\textbf{i}^2=\\textbf{j}^2=\\textbf{k}^2=-1\\\\ &amp;\\textbf{ij} = -\\textbf{ji} = \\textbf{k} \\quad \\textbf{jk} = -\\textbf{kj} = \\textbf{i} \\quad \\textbf{ki} = -\\textbf{ik} = \\textbf{j} \\end{align}\\] We can also write the quaternion in the column vector form: \\(\\textbf{q} = \\begin{bmatrix} q_1\\\\q_2\\\\q_3\\\\q_4 \\end{bmatrix}\\). We are particularly interested in unit quaternions, which means the column vector has unit length. Unit quaternions can represent rotations, actually we can get quaternions immediately from the axis-angle representation. The main idea is to use the axis-angle representation but in a more compact format. Concretely speaking, given rotation angle \\(\\theta\\) and rotation axis \\(u\\)(in unit length), the corresponding unit quaternion is as follows: \\[q = \\begin{bmatrix}u\\sin(\\frac{\\theta}{2}) \\\\ \\cos(\\frac{\\theta}{2})\\end{bmatrix}\\] The corresponding rotation matrix is: \\[R(q) = \\begin{bmatrix} q_1^2 - q_2^2 - q_3^2 + q_4^2 &amp;2(q_1q_2-q_3q_4) &amp;2(q_1q_3 +q_2q_4)\\\\ 2(q_1q_2+q_3q_4) &amp; -q_1^2+q_2^2-q_3^2+q_4^2 &amp;2(q_2q_3 - q_1q_4)\\\\ 2(q_1q_3-q_2q_4) &amp;2(q_2q_3+q_1q_4) &amp;-q_1^2-q_2^2+q_3^2+q_4^2 \\end{bmatrix}\\] Quaternion representation doesn’t have singularities. But there is still a little ambiguity that \\(q\\) and \\(-q\\) always represent the same rotation, which means the quaternion is a double cover of the 3D rotation. Actually, quaternions also give us great convenience in calculation, because of its compact representation. For computational use, we care about: (1)How to compose rotations? (2) How to take inverse for quaternions (3) How to rotate a 3D vector using quaternions? (1)How to compose rotations? Consider two quaternions \\(q_a = q_{a,1}i +q_{a,2}j +q_{a,3}k +q_{a,4},\\ q_b = q_{b,1}i +q_{b,2}j +q_{b,3}k +q_{b,4}\\). Then the composition of the corresponding rotation is just the product of two quaternions. Explicitly, \\[\\begin{align} q_c = q_a\\otimes q_b =&amp; (q_{a,4}q_{b,1} -q_{a,3}q_{b,2} + q_{a,2}q_{b,3} + q_{a,1}q_{b,4})i \\\\ +&amp; (q_{a,3}q_{b,1} +q_{a,4}q_{b,2} - q_{a,1}q_{b,3} + q_{a,2}q_{b,4})j \\\\ +&amp; (-q_{a,2}q_{b,1} +q_{a,1}q_{b,2} + q_{a,4}q_{b,3} + q_{a,3}q_{b,4})k \\\\ +&amp; (-q_{a,1}q_{b,1} -q_{a,2}q_{b,2} - q_{a,3}q_{b,3} + q_{a,4}q_{b,4}) \\end{align}\\] If we use vector to represent quaternion, we can claim the following formula: \\[ q_c = \\begin{bmatrix} q_{a,4} &amp; -q_{a,3} &amp; q_{a,2} &amp; q_{a,1}\\\\ q_{a,3} &amp; q_{a,4} &amp;-q_{a,1} &amp;q_{a,2} \\\\ -q_{a,2} &amp; q_{a,1} &amp; q_{a,4} &amp;q_{a,3} \\\\ -q_{a,1} &amp; -q_{a,2} &amp; -q_{a,3} &amp;q_{a,4}\\end{bmatrix} \\begin{bmatrix} q_{b,1}\\\\q_{b,2}\\\\q_{b,3}\\\\q_{b,4}\\end{bmatrix} \\] Similar formula can be derived for \\(q_a\\). (2) How to take inverse for quaternions? Assume that now we have the pose of frame \\(r\\) with respect to a frame \\(w\\), i.e. we know \\(q_r^w\\). How can we get the opposite, i.e. the pose of frame \\(w\\) with respect to frame \\(r\\)? The answer is to take the inverse of the rotation. It’s easy to carry out using rotation matrix, but how shall we proceed using quaternions? Let’s remind ourselves that quaternion is nothing but rearranged axis-angle representation. So naturally the inverse is as follows: \\[q_w^r = \\begin{bmatrix}-u_r^w\\sin(\\frac{\\theta}{2}) \\\\ \\cos(\\frac{\\theta}{2})\\end{bmatrix} = (q_r^w)^{-1}\\] This process is also compatible with the quaternion product. (3) How to rotate a 3D vector using quaternions? One key question is, given coordinates in one frame, and the pose of that frame in another frame, how can we translate the coordinates? Assume there is a point \\(p\\), and we know its coordinates in frame \\(r\\), denoted as \\(p^r\\). Also we know the pose of frame \\(r\\) in frame \\(w\\), denoted as \\(q_r^w\\). For simplicity, we assume that the two frame share the same origin. How can we get the coordinates of \\(p\\) with respect to the frame \\(w\\), i.e. \\(p^w\\)? We have the following formula: \\[p^w = (q^w_r)\\otimes\\begin{bmatrix} p^r\\\\1\\end{bmatrix}\\otimes(q^w_r)^{-1} = \\begin{bmatrix} R^w_rp^r\\\\1\\end{bmatrix}\\] i.e. we can compute the rotation by first stack an extra entry 1 at the end of \\(p^r\\), then let \\(q^w_r\\) act ‘conjugately’ on it. 7.1.4 Miscellaneous topics on rotations 7.1.4.1 Lie group structure of rotations Actually, on one hand, the set of rotation matrices (\\(SO(3)\\)) is closed under matrix multiplication (and some other properties), which gives rise to its algebraic structure (which is called ‘group’). On the other hand, the set of rotation matrices has its own topological structure (which is called ‘manifold’). Lie group is both group and smooth manifold, but we’ll not introduce Lie group formally. Geometrically, thanks to the quaternion representation, we can treat the group \\(SO(3)\\) as a sphere in 4D, but due to \\(R(q) = R(-q)\\), we must image there are portals connecting the antipodal points. Locally we can just think it as a sphere. For a sphere, we can imagine that at each point there is a tangent space. The tangent space at the identity is called Lie algebra. Why identity? There is nothing special with identity, just because: (1) Every group has identity. (2) The tangent space at other points can be obtained from identity. Lie algebra is deeply connected with Lie group. Imagine the sphere case, the sphere itself is curved, but we can use coordinates to translate the sphere into a flat map. Lie algebra is a flat space which we can make use of to study about the complicated curved Lie group. We will have some intuition on it by examining \\(SO(3)\\). Figure 7.2: Sphere and tangent space Lie group \\(SO(3)\\) is related to its Lie algebra \\(\\mathfrak{so}(3)\\). We will state without proof that the Lie algebra \\(\\mathfrak{so}(3)\\) is the set of \\(3\\times 3\\) skew symmetric matrices. Exponential Map and Logarithm Map: Looking at the sphere in the picture, it seems natural for us to bridge the endpoint of the green curved line, which is still on the sphere, with the endpoint of the red line, which is in the tangent space. This retraction is called exponential map. For matrix Lie group (including \\(SO(3)\\)), the exponential map coinsides with the matrix exponential (See A.1.) We will check this on the \\(SO(3)\\) case: Theorem 7.2 (Matrix exponential for rotations) The exponential map \\(\\text{exp:}\\mathfrak{so}(3)\\to SO(3)\\) is well-defined. Proof. Consider a \\(3\\times 3\\) skew symmetric matrix \\(A = \\begin{bmatrix} 0 &amp;-c &amp;b\\\\c &amp;0 &amp;-a\\\\-b &amp;a &amp;0 \\end{bmatrix}\\), define \\(w = \\begin{bmatrix} a\\\\b\\\\c \\end{bmatrix}, \\hat{w} = w/\\|w\\|, \\theta=\\|w\\|\\). Then \\(A = w_\\times\\). Consider \\(\\hat{w}_\\times\\), using the properties of the cross product, we have:\\(\\hat{w}^3_\\times = -\\hat{w}_\\times\\). Thus we can know that: \\[\\begin{align} \\exp(A) &amp;= I + (1-\\frac{\\theta^3}{3!}+\\frac{\\theta^5}{5!}+\\dots)\\hat{w}_\\times + (\\frac{\\theta^2}{2!}-\\frac{\\theta^4}{4!}+\\frac{\\theta^6}{6!})\\hat{w}_\\times^2\\\\ &amp;= I + \\frac{\\sin\\theta}{\\theta}A + \\frac{1-\\cos\\theta}{\\theta^2}A^2 \\end{align}\\] which actually coincide with the Rodrigues’ formula. Thus \\(\\exp(A)\\) is a rotation matrix. Actually the exponential map for \\(SO(3)\\) is surjective. From the derivation above, I believe we can see the similarity of this with the axis-angle representation. In order to map backwards, we want to recover the angle and the axis from the rotation matrix. There are infinitely many possible points due to the periodic property of trigonometric functions. So we may restrict the rotation angle \\(\\theta \\in [0,\\pi)\\)。 Then from the derivation of the axis-angle representation, we can get: \\[\\begin{align} \\theta &amp;= \\arccos(\\frac{\\text{Tr}(R) - 1}{2}) \\end{align}\\] Next is to recover the axis. Note that the whole space of \\(n\\times n\\) matrices is the direct sum of \\(n\\times n\\) symmetric matrices and \\(n\\times n\\) skew symmetric matrices. So we can decompose the rotation matrix into symmetric and skew symmetric parts. From the formula above, we can decompose as: \\[ \\exp(A) = \\underbrace{\\frac{\\sin\\theta}{\\theta}A}_{\\text{skew symmetric}} + \\underbrace{I + \\frac{1-\\cos\\theta}{\\theta^2}A^2}_{\\text{symmetric}} \\] Thus we can recover the skew symmetric matrix \\(A\\) (the axis) by taking the skew symmetric part of \\(\\exp(A)\\). Concretely, we can get: \\(A = \\frac{\\theta}{2\\sin\\theta}(R - R^T)\\). Next we will give an example of how to use exponential map and logarithm map to do interpolation of rotations. Suppose we have two rotations \\(R_1,R_2\\), and we want to find a rotation \\(R\\) that is in between. We can use the following formula: \\[R = R_1\\exp(\\lambda\\log(R_1^TR_2))\\] where \\(\\lambda\\in[0,1]\\). 7.2 The Pinhole Camera Model Next let’s take a look at the pinhole camera model, which is the simplest one. In this model, the picture in 2D is the projection of the corresponding 3D points with respect to the optical center. Figure 7.3: Pinhole Camera Model Consider point \\(p\\) in the camera frame. Suppose the focal length(i.e. the distance from the optical center to the image plane) is \\(f\\). Then the coordinates of the projection of \\(p\\) is: \\[p_m^c = \\begin{bmatrix} u_m^c\\\\v_m^c \\end{bmatrix} = \\begin{bmatrix} f\\frac{p^c_x}{p^c_z}\\\\ f\\frac{p^c_y}{p^c_z} \\end{bmatrix}\\] The unit of these coordinates are meter. We would prefer to write the formula into a matrix form, but the barrier here is the quotient. In order to overcome this, we introduce homogeneous coordinates. In homogeneous coordinates system, if coordinates are multiplied by a non-zero scalar then the resulting coordinates represent the same point. We will add an extra entry to better represent this equivalence. Denote \\[\\tilde{p^c} = \\begin{bmatrix} p_x^c\\\\p_y^c\\\\p_z^c\\\\1 \\end{bmatrix}\\] Then the equivalence can be expressed as \\[\\forall k\\neq0 \\text{,}\\quad \\ \\begin{bmatrix} p_x^c\\\\p_y^c\\\\p_z^c\\\\1 \\end{bmatrix} \\sim \\begin{bmatrix} kp_x^c\\\\kp_y^c\\\\kp_z^c\\\\k \\end{bmatrix}\\] With this notation, the projection equation can be written as: \\[p^c_z\\begin{bmatrix} u_m^c\\\\v_m^c\\\\1 \\end{bmatrix} = \\begin{bmatrix} f &amp;0 &amp;0 \\\\ 0 &amp;f &amp;0\\\\ 0 &amp;0 &amp;1\\end{bmatrix} \\begin{bmatrix} 1 &amp;0 &amp;0 &amp;0 \\\\ 0 &amp;1 &amp;0 &amp;0\\\\ 0 &amp;0 &amp;1 &amp;0\\end{bmatrix} \\begin{bmatrix} p_x^c\\\\p_y^c\\\\p_z^c\\\\1 \\end{bmatrix}\\] Normally when we deal with points in a 2D picture, we will prefer the unit is in pixels instead of meters. Next we will show how to convert the coordinates into pixels. In convention, the origin of the pixel coordinates is at the top-left of the image. So that we can express the coordinates in pixel as follows: \\[\\begin{bmatrix} u^I\\\\v^I\\\\1 \\end{bmatrix} = \\begin{bmatrix} s_x &amp;0 &amp;o_x \\\\ 0 &amp;s_y &amp;o_y\\\\ 0 &amp;0 &amp;1\\end{bmatrix} \\begin{bmatrix} u_m^c\\\\v_m^c\\\\1 \\end{bmatrix}\\] where \\(s_x\\) is the number of horizontal pixels per meter, \\(s_y\\) is the number of vertical pixels per meter, and \\(\\begin{bmatrix} o_x\\\\o_y\\end{bmatrix}\\) is the coordinates of the optical center. Combing the previous result, we can get: \\[\\begin{align} p_z^c\\begin{bmatrix} u^I\\\\v^I\\\\1 \\end{bmatrix} =&amp; \\begin{bmatrix} s_x &amp;0 &amp;o_x \\\\ 0 &amp;s_y &amp;o_y\\\\ 0 &amp;0 &amp;1\\end{bmatrix} \\begin{bmatrix} f &amp;0 &amp;0 \\\\ 0 &amp;f &amp;0\\\\ 0 &amp;0 &amp;1\\end{bmatrix} \\begin{bmatrix} 1 &amp;0 &amp;0 &amp;0 \\\\ 0 &amp;1 &amp;0 &amp;0\\\\ 0 &amp;0 &amp;1 &amp;0\\end{bmatrix} \\begin{bmatrix} p_x^c\\\\p_y^c\\\\p_z^c\\\\1 \\end{bmatrix} \\\\ =&amp; \\underbrace{\\begin{bmatrix} s_xf &amp;0 &amp;o_x \\\\ 0 &amp;s_yf &amp;o_y\\\\ 0 &amp;0 &amp;1\\end{bmatrix}}_{K} \\begin{bmatrix} 1 &amp;0 &amp;0 &amp;0 \\\\ 0 &amp;1 &amp;0 &amp;0\\\\ 0 &amp;0 &amp;1 &amp;0\\end{bmatrix} \\begin{bmatrix} p_x^c\\\\p_y^c\\\\p_z^c\\\\1 \\end{bmatrix}\\end{align}\\] where \\(K\\) is called intrinsic matrix. Notice that, in our derivation above, the homogeneous coordinates of \\(p\\) is in the camera frame. However, it’s not always easy to know the coordinates in the camera frame. For example, sometimes we may have many different camera views, or the camera itself is moving. What we usually have is the coordinate of the 3D points in the world frame. Therefore we will add a procedure to transform the coordinates from world fram to the camera frame. From the previous section, we can know that we just need to do some rotations and translations. Therefore we can obtain the formula: \\[\\begin{align} p_z^c\\begin{bmatrix} u^I\\\\v^I\\\\1 \\end{bmatrix} =&amp; \\begin{bmatrix} s_xf &amp;0 &amp;o_x \\\\ 0 &amp;s_yf &amp;o_y\\\\ 0 &amp;0 &amp;1\\end{bmatrix} \\begin{bmatrix} 1 &amp;0 &amp;0 &amp;0 \\\\ 0 &amp;1 &amp;0 &amp;0\\\\ 0 &amp;0 &amp;1 &amp;0\\end{bmatrix} \\begin{bmatrix} R_w^c &amp;t^c_w \\\\ 0 &amp;1 \\end{bmatrix}\\begin{bmatrix} p_x^w\\\\p_y^w\\\\p_z^w\\\\1 \\end{bmatrix}\\\\ =&amp; \\begin{bmatrix} s_xf &amp;0 &amp;o_x \\\\ 0 &amp;s_yf &amp;o_y\\\\ 0 &amp;0 &amp;1\\end{bmatrix} \\begin{bmatrix} R_w^c &amp;t^c_w\\end{bmatrix}\\begin{bmatrix} p_x^w\\\\p_y^w\\\\p_z^w\\\\1 \\end{bmatrix} \\end{align}\\] where the matrix \\(\\begin{bmatrix} R_w^c &amp;t^c_w\\end{bmatrix}\\) is called the extrinsic matrix. Some pinhole cameras will introduce significant distortion to images. For cameras with wide field of view (FOV), they often suffers from radial distortion. Radial distortion makes straight lines look curved in the image. The farther points are from the center of the image, the larger the radial distortion will be. Radial distortion is primarily dominated by low-order components. An easy model of the distortion (in camera frame) is: \\[u^c = (1+K_1r^2+K_2r^4)u^c_{\\text{distort}}\\quad\\quad v^c = (1+K_1r^2+K_2r^4)v^c_{\\text{distort}}\\] where \\((u^c,v^c)\\) is the undistorted image point, \\((u^c_{\\text{distort}},v^c_{\\text{distort}})\\) is the distorted image point, \\(r^2=(u^c_{\\text{distort}})^2 +(v^c_{\\text{distort}})^2\\), and the \\(K_n\\) is the n-th distortion coefficient. If we want to express in the image frame, we can obtain the following model: \\[u^I = (1+K_1r^2+K_2r^4)(u^I_{\\text{distort}}-o_x)+o_x\\quad\\quad v^I = (1+K_1r^2+K_2r^4)(v^I_{\\text{distort}}-o_y)+o_y\\] where \\(r^2=(u^I_{\\text{distort}}-o_x)^2 +(v^I_{\\text{distort}}-o_y)^2\\). It’s worth to mention that despite the radial distortion, there is another distortion called tangential distortion, which will not be elaborated here. We often assume that the intrinsics of the camera and he distortion coefficients are already known from the previous camera calibration process, i.e. we always assume that the camera is calibrated. There are many existing codes and toolbox by hand if we want to undistort the pictures. Such as OpenCV has undistort function, and Matlab has undistortImage function. They can take in distorted images and return the undistorted ones. 7.3 Camera Pose Estimation Suppose now we have a calibrated camera, but we don’t know the rotation and translation, i.e. the pose of the camera. We wish to find the extrinsics. Now what we have is a set of \\(n\\) 3D points and their corresponding 2D image projections. We wish to estimate the pose of the camera from these corresponding points. 7.3.1 The P3P Problem Let’s start from something simple. Consider there are three points \\(P_1^w,P_2^w,P_3^w\\) in the world frame, and their corresponding projection points on the image plane \\(P^c_{m,1},P^c_{m,2},P^c_{m,3}\\). The problem is: Can we found the coordinates of the points \\(P_1,P_2,P_3\\) in the camera frame? We will introduce a direct way to tackle the problem, called Grunert’s method. Figure 7.4: P3P problem From the coordinates of \\(P^c_{m,1},P^c_{m,2},P^c_{m,3}\\) in the image plane, we can calculate the angles between \\(P_1,P_2,P_3\\). Denote the angle between \\(P_1,P_2\\) as \\(\\gamma\\), the angle between \\(P_2,P_3\\) as \\(\\beta\\), and the angle between \\(P_1,P_3\\) as \\(\\alpha\\). Remember that if \\(P^c_{m,i} = \\begin{bmatrix}u_{m,i}^c\\\\v^c_{m,i}\\\\1\\end{bmatrix}\\), then the camera frame coordinates \\(P^c_i = \\frac{s_i}{\\sqrt{1+(u_{m,i}^c)^2+(v_{m,i}^c)^2}}\\begin{bmatrix}u_{m,i}^c\\\\v_{m,i}^c\\\\1\\end{bmatrix}\\). So what we have to do is to solve for \\(s_i\\), which is the distance from the origin to the point \\(P_i\\). Using law of cosines, we can obtain: \\[\\begin{align} s_1^2 +s_2^2 - 2s_1s_2\\cos\\gamma = c^2\\\\ s_1^2 +s_3^2 - 2s_1s_3\\cos\\beta = b^2\\\\ s_2^2 +s_3^2 - 2s_2s_3\\cos\\alpha = a^2\\\\ \\end{align}\\] Then if we let \\(s_2 = us_1,s_3 = vs_1\\), we can obtain: \\[\\begin{align} s_1^2 = \\frac{c^2}{1+u^2-2u\\cos\\gamma}\\\\ s_1^2= \\frac{b^2}{1+v^2-2v\\cos\\beta}\\\\ s_1^2= \\frac{a^2}{v^2+u^2-2uv\\cos\\alpha}\\\\ \\end{align}\\] Then we can get: \\[ u^2-\\frac{c^2}{b^2} v^2+2 v \\frac{c^2}{b^2} \\cos \\beta-2 u \\cos \\gamma+\\frac{b^2-c^2}{b^2}=0 \\] and \\[ u^2+\\frac{b^2-a^2}{b^2} v^2-2 u v \\cos \\alpha+\\frac{2 a^2}{b^2} v \\cos \\beta-\\frac{a^2}{b^2}=0 \\] Then substituting the later equation back to the former one, we can express \\(u\\) in terms of \\(v\\). Then plug this into the latter equality, we can finally get an equation: \\[ A_4 v^4+A_3 v^3+A_2 v^2+A_1 v+A_0=0 \\] where \\[\\begin{align} &amp; A_4=\\left(\\frac{a^2-c^2}{b^2}-1\\right)^2-\\frac{4 c^2}{b^2} \\cos ^2 \\alpha \\\\ &amp; A_3=4\\left[\\frac{a^2-c^2}{b^2}\\left(1-\\frac{a^2-c^2}{b^2}\\right) \\cos \\beta-\\left(1-\\frac{a^2+c^2}{b^2}\\right) \\cos \\alpha \\cos \\gamma+2 \\frac{c^2}{b^2} \\cos ^2 \\alpha \\cos \\beta\\right] \\\\ &amp; A_2=2[\\left(\\frac{a^2-c^2}{b^2}\\right)^2-1+2\\left(\\frac{a^2-c^2}{b^2}\\right)^2 \\cos ^2 \\beta+2\\left(\\frac{b^2-c^2}{b^2}\\right) \\cos ^2 \\alpha \\\\ &amp; -4\\left(\\frac{a^2+c^2}{b^2}\\right) \\cos \\alpha \\cos \\beta \\cos \\gamma+2\\left(\\frac{b^2-a^2}{b^2}\\right) \\cos ^2 \\gamma] \\\\ &amp; A_1=4\\left[-\\left(\\frac{a^2-c^2}{b^2}\\right)\\left(1+\\frac{a^2-c^2}{b^2}\\right) \\cos \\beta+\\frac{2 a^2}{b^2} \\cos ^2 \\gamma \\cos \\beta-\\left(1-\\left(\\frac{a^2+c^2}{b^2}\\right)\\right) \\cos \\alpha \\cos \\gamma\\right] \\\\ &amp; A_0=\\left(1+\\frac{a^2-c^2}{b^2}\\right)^2-\\frac{4 a^2}{b^2} \\cos ^2 \\gamma \\end{align}\\] From this equation, we can obtain up to four solutions. With each \\(v\\) we can obtain the corresponding \\(u\\) and thus all the other parameters. To eliminate the ambiguity, we can use a 4th point to confirm which one is the right solution. 7.3.2 The PnP Problem 7.3.3 Global Optimality 7.3.4 Handling Outliers 7.4 Point Cloud Registration "],["adaptivecontrol.html", "Chapter 8 Adaptive Control 8.1 Model-Reference Adaptive Control 8.2 Certainty-Equivalent Adaptive Control", " Chapter 8 Adaptive Control 8.1 Model-Reference Adaptive Control Basic flow for designing an adaptive controller Design a control law with variable parameters Design an adaptation law for adjusting the control parameters Analyze the convergence of the closed-loop system The control law design at the first step typically requires the designer to know what a good controller is if the true parameters were actually known, e.g., from feedback linearization (Appendix F), sliding control (Appendix G) etc. The design of the adaptation law typically comes from analyzing the dynamics of the tracking error, which as we will see often appears in the form of Lemma 8.1. The convergence of the closed-loop system is usually analyzed with the help of a Lyapunov-like function introduced in Chapter 5. Lemma 8.1 (Basic Lemma) Let two signals \\(e(t)\\) and \\(\\phi(t)\\) be related by \\[\\begin{equation} e(t) = H(p)[k \\phi(t)^T v(t)] \\tag{8.1} \\end{equation}\\] where \\(e(t)\\) a scalar output signal, \\(H(p)\\) a strictly positive real (SPR) transfer function, \\(k\\) an unknown real number with known sign, \\(\\phi(t) \\in \\mathbb{R}^m\\) a control signal, and \\(v(t) \\in \\mathbb{R}^m\\) a measurable input signal. If the control signal \\(\\phi(t)\\) satisfies \\[\\begin{equation} \\dot{\\phi}(t) = - \\mathrm{sgn}(k) \\gamma e(t) v(t) \\tag{8.2} \\end{equation}\\] with \\(\\gamma &gt; 0\\) a positive constant, then \\(e(t)\\) and \\(\\phi(t)\\) are globally bounded. Moreover, if \\(v(t)\\) is bounded, then \\[ \\lim_{t \\rightarrow \\infty} e(t) = 0. \\] Proof. Let the state-space representation of (8.1) be \\[\\begin{equation} \\dot{x} = A x + b [k \\phi^T v], \\quad e = c^T x. \\tag{8.3} \\end{equation}\\] Since \\(H(p)\\) is SPR, it follows from the Kalman-Yakubovich Lemma E.1 that there exist \\(P,Q \\succ 0\\) such that \\[ A^T P + P A = -Q, \\quad Pb = c. \\] Let \\[ V(x,\\phi) = x^T P x + \\frac{|k|}{\\gamma} \\phi^T \\phi, \\] clearly \\(V\\) is positive definite (i.e., \\(V(0,0)=0\\), and \\(V(x,\\phi) &gt; 0\\) for all \\(x \\neq 0, \\phi \\neq 0\\)). The time derivative of \\(V\\) along the trajectory defined by (8.3) with \\(\\phi\\) chosen as in (8.2) is \\[\\begin{align} \\dot{V} &amp; = \\frac{\\partial V}{\\partial x} \\dot{x} + \\frac{\\partial V}{\\partial \\phi} \\dot{\\phi} \\\\ &amp;= x^T (PA + A^T P) x + 2 x^T P b (k \\phi^T v) + \\frac{2|k|}{\\gamma} \\phi^T (- \\mathrm{sgn}(k) \\gamma e v) \\\\ &amp; = - x^T Q x + 2 (x^T c)(k\\phi^T v) - 2 \\phi^T (e v) \\\\ &amp; = - x^T Q x \\leq 0. \\end{align}\\] As a result, we know \\(x\\) and \\(\\phi\\) must be bounded (\\(V(x(t),\\phi(t)) \\leq V(x(0),\\phi(0))\\) is bounded). Since \\(e = c^T x\\), we know \\(e\\) must be bounded as well. If the input signal \\(v\\) is also bounded, then \\(\\dot{x}\\) is bounded as seen from (8.3). Because \\(\\ddot{V} = -2x^T Q \\dot{x}\\) is now bounded, we know \\(\\dot{V}\\) is uniformly continuous. Therefore, by Barbalat’s stability certificate (Theorem 5.7), we know \\(\\dot{V}\\) tends to zero as \\(t\\) tends to infinity, which implies \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) and hence \\(\\lim_{t \\rightarrow \\infty} e(t) = 0\\). 8.1.1 First-Order Systems Consider the first-order single-input single-output (SISO) system \\[\\begin{equation} \\dot{x} = - a x + b u \\tag{8.4} \\end{equation}\\] where \\(a\\) and \\(b\\) are unknown groundtruth parameters. However, we do assume that the sign of \\(b\\) is known. What if the sign of \\(b\\) is unknown too? Let \\(r(t)\\) be a reference trajectory, e.g., a step function or a sinusoidal function, and \\(x_d(t)\\) be a desired system trajectory that tracks the reference \\[\\begin{equation} \\dot{x}_d = - a_d x_d + b_d r(t), \\tag{8.5} \\end{equation}\\] where \\(a_d,b_d &gt; 0\\) are user-defined constants. Note that the transfer function from \\(r\\) to \\(x_d\\) is \\[ x_d = \\frac{b_d}{p + a_d} r \\] and the system is stable. Review basics of transfer function. The goal of adaptive control is to design a control law and an adaptation law such that the tracking error of the system \\(x(t) - x_d(t)\\) converges to zero. Control law. We design the control law as \\[\\begin{equation} u = \\hat{a}_r(t) r + \\hat{a}_x(t) x \\tag{8.6} \\end{equation}\\] where \\(\\hat{a}_r(t)\\) and \\(\\hat{a}_x(t)\\) are time-varying feedback gains that we wish to adapt. The closed-loop dynamics of system (8.4) with the controller (8.6) is \\[ \\dot{x} = - a x + b (\\hat{a}_r r + \\hat{a}_x x) = - (a - b \\hat{a}_x) x + b \\hat{a}_r r. \\] With the equation above, the reason for choosing the control law (8.6) is clear: if the system parameters \\((a,b)\\) were known, then choosing \\[\\begin{equation} a_r^\\star = \\frac{b_d}{b}, \\quad a_x^\\star = \\frac{a - a_d}{b} \\tag{8.7} \\end{equation}\\] leads to the closed-loop dynamics \\(\\dot{x} = - a_d x + b_d r\\) that is exactly what we want in (8.5). However, in adaptive control, since the true parameters \\((a,b)\\) are not revealed to the control designer, an adaptation law is needed to dynamically adjust the gains \\(\\hat{a}_r\\) and \\(\\hat{a}_x\\) based on the tracking error \\(x(t) - x_d(t)\\). Adaptation law. Let \\(e(t) = x(t) - x_d(t)\\) be the tracking error, and we develop its time derivative \\[\\begin{align} \\dot{e} &amp;= \\dot{x} - \\dot{x}_d \\\\ &amp;= - a_d (x - x_d) + (a_d - a + b\\hat{a}_x)x + (b \\hat{a}_r - b_d) r \\\\ &amp; = - a_d e + b\\underbrace{(\\hat{a}_x - \\hat{a}_x^\\star)}_{=:\\tilde{a}_x} x + b \\underbrace{(\\hat{a}_r - \\hat{a}_r^\\star )}_{=:\\tilde{a}_r} r \\\\ &amp; = - a_d e + b (\\tilde{a}_x x + \\tilde{a}_r r) \\tag{8.8} \\end{align}\\] where \\(\\tilde{a}_x\\) and \\(\\tilde{a}_r\\) are the gain errors w.r.t. the optimal gains in (8.7) if the true parameters were known. The error dynamics (8.8) is equivalent to the following transfer function \\[\\begin{equation} e = \\frac{1}{p + a_d} b(\\tilde{a}_x x + \\tilde{a}_r r) = \\frac{1}{p + a_d} \\left(b \\begin{bmatrix} \\tilde{a}_x \\\\ \\tilde{a}_r \\end{bmatrix}^T \\begin{bmatrix} x \\\\ r \\end{bmatrix} \\right), \\tag{8.9} \\end{equation}\\] which is in the form of (8.1). Therefore, we choose the adaptation law \\[\\begin{equation} \\begin{bmatrix} \\dot{\\tilde{a}}_x \\\\ \\dot{\\tilde{a}}_r \\end{bmatrix} = - \\mathrm{sgn}(b) \\gamma e \\begin{bmatrix} x \\\\ r \\end{bmatrix}. \\tag{8.10} \\end{equation}\\] Tracking convergence. With the control law (8.6) and the adaptation law (8.10), we can prove that the tracking error converges to zero, using Lemma 8.1. With \\(\\tilde{a}=[\\tilde{a}_x, \\tilde{a}_r]^T\\), let \\[\\begin{equation} V(e,\\tilde{a}) = e^2 + \\frac{|b|}{\\gamma} \\tilde{a}^T \\tilde{a} \\tag{8.11} \\end{equation}\\] be a positive definite Lyapunov function candidate with time derivative \\[ \\dot{V} = - 2a_d e^2 \\leq 0. \\] Clearly, \\(e\\) and \\(\\tilde{a}\\) are both bounded. Assuming the reference trajectory \\(r\\) is bounded, we know \\(x_d\\) is bounded (due to (8.5)) and hence \\(x\\) is bounded (due to \\(e = x - x_d\\) being bounded). Consequently, from the error dynamics (8.8) we know \\(\\dot{e}\\) is bounded, which implies \\(\\ddot{V} = -4a_d e \\dot{e}\\) is bounded and \\(\\dot{V}\\) is uniformly continuous. By Barbalat’s stability certificate 5.7, we conlude \\(e(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). It is always better to combine mathematical analysis with intuitive understanding. Can you explain intuitively why the adaptation law (8.10) makes sense? (Hint: think about how the control should react to a negative/positive tracking error.) Parameter convergence. We have shown the control law (8.6) and the adaptation law (8.10) guarantee to track the reference trajectory. However, is it guaranteed that the gains of the controller (8.6) also converge to the optimal gains in (8.7)? We will now show that the answer is indefinite and it depends on the reference trajectory \\(r(t)\\). Because the tracking error \\(e\\) converges to zero, and \\(e\\) is the output of a stable filter (8.9), we know the input \\(b(\\tilde{a}_x x + \\tilde{a}_r r)\\) must also converge to zero. On the other hand, the adaptation law (8.10) shows that both \\(\\dot{\\tilde{a}}_x\\) and \\(\\dot{\\tilde{a}}_r\\) converge to zero (due to \\(e\\) converging to zero and \\(x\\), \\(r\\) being bounded). As a result, we know \\(\\tilde{a} = [\\tilde{a}_x,\\tilde{a}_r]^T\\) converges to a constant that satisfies \\[\\begin{equation} v^T \\tilde{a} = 0, \\quad v = \\begin{bmatrix} x \\\\ r \\end{bmatrix}, \\tag{8.12} \\end{equation}\\] which is a single linear equation of \\(\\tilde{a}\\) with time-varying coeffients. Constant reference: no guaranteed convergence. Suppose \\(r(t) \\equiv r_0 \\neq 0\\) for all \\(t\\). From (8.5) we know \\(x = x_d = \\alpha r_0\\) when \\(t \\rightarrow \\infty\\), where \\(\\alpha\\) is the constant DC gain of the stable filter. Therefore, the linear equation (8.12) reduces to \\[ \\alpha \\tilde{a}_x + \\tilde{a}_r = 0. \\] This implies that \\(\\tilde{a}\\) does not necessarily converge to zero. In fact, it converges to a straight line in the parameter space. Persistent excitation: guaranteed convergence. However, when the signal \\(v\\) satisfies the so-called persistent excitation condition, which states that for any \\(t\\), there exists \\(T, \\beta &gt; 0\\) such that \\[\\begin{equation} \\int_{t}^{t+T} v v^T d\\tau \\geq \\beta I, \\tag{8.13} \\end{equation}\\] then \\(\\tilde{a}\\) is guaranteed to converge to zero. To see this, we multiply (8.12) by \\(v\\) and integrate it from \\(t\\) to \\(t+T\\), which gives rise to \\[ \\left( \\int_{t}^{t+T} vv^T d\\tau \\right) \\tilde{a} = 0. \\] By the persistent excitation condition (8.13), we infer that \\(\\tilde{a} = 0\\) is the only solution. It remains to understand under what conditions of the reference trajectory \\(r(t)\\) can we guarantee the persistent excitation of \\(v\\). We leave it as an exercise for the reader to show, if \\(r(t)\\) contains at least one sinusoidal component, then the persistent excitation condition of \\(v\\) is guaranteed. Exercise 8.1 (Extension to Nonlinear Systems) Design a control law and an adaptation law for the following system \\[ \\dot{x} = - a x - c f(x) + b u \\] with unknown true parameters \\((a,b,c)\\) (assume the sign of \\(b\\) is known) and known nonlinearity \\(f(x)\\) to track a reference trajectory \\(r(t)\\). Analyze the convergence of tracking error and parameter estimation error. 8.1.2 High-Order Systems Consider an \\(n\\)-th order nonlinear system \\[\\begin{equation} q^{(n)} + \\sum_{i=1}^n \\alpha_i f_i(x,t) = bu \\tag{8.14} \\end{equation}\\] where \\(x=[q,\\dot{q},\\ddot{q},\\dots,q^{(n-1)}]^T\\) is the state of the system, \\(f_i\\)’s are known nonlinearities, \\((\\alpha_1,\\dots,\\alpha_n,b)\\) are unknown parameters of the system (with \\(\\mathrm{sgn}(b)\\) known). The goal of adaptive control is to control the system (8.14) trajectory to follow a desired trajectory \\(q_d(t)\\) despite no knowing the true parameters. To facilitate the derivation of the adaptive controller, let us divide both sides of (8.14) by \\(b\\) \\[\\begin{equation} h q^{(n)} + \\sum_{i=1}^n a_i f_i(x,t) = u \\tag{8.15} \\end{equation}\\] where \\(h = 1 / b\\) and \\(a_i = \\alpha_i / b\\). Control law. Recall that the choice of the control law is tyically inspired by the control design if the true system parameters were known. We will borrow ideas from sliding control (Appendix G). Known parameters. Let \\(e = q(t) - q_d(t)\\) be the tracking error, and define the following combined error \\[ s = e^{(n-1)} + \\lambda_{n-2} e^{(n-2)} + \\dots + \\lambda_0 e = \\Delta(p) e \\] where \\(\\Delta(p) = p^{n-1} + \\lambda_{n-2} p^{(n-2)} + \\dots + \\lambda_0\\) is a stable polynomial with user-chosen coeffients \\(\\lambda_0,\\dots,\\lambda_{n-2}\\). The rationale for defining the combined error \\(s\\) is that the convergence of \\(e\\) to zero can be guaranteed by the convergence of \\(s\\) to zero (when \\(\\Delta(p)\\) is stable). Note that \\(s\\) can be equivalently written as \\[\\begin{align} s &amp; = (q^{(n-1)} - q_d^{(n-1)}) + \\lambda_{n-2} e^{(n-2)} + \\dots + \\lambda_0 e \\\\ &amp; = q^{(n-1)} - \\underbrace{ \\left( q_d^{(n-1)} - \\lambda_{n-2} e^{(n-2)} - \\dots - \\lambda_0 e \\right) }_{q_r^{(n-1)}}. \\end{align}\\] Now consider the control law \\[\\begin{equation} u = h q_r^{(n)} - ks + \\sum_{i=1}^n a_i f_i(x,t) \\tag{8.16} \\end{equation}\\] where \\[ q_r^{(n)} = q_d^{(n)} - \\lambda_{n-2} e^{(n-1)} - \\dots - \\lambda_0 \\dot{e} \\] and \\(k\\) is a design constant that has the same sign as \\(h\\). This choice of control, plugged into the system dynamics (8.15), leads to \\[\\begin{align} h q^{(n)} + \\sum_{i=1}^n a_i f_i(x,t) = h q_r^{(n)} - ks + \\sum_{i=1}^n a_i f_i(x,t) \\Longleftrightarrow \\\\ h \\left( q^{(n)} - q_r^{(n)} \\right) + ks = 0 \\Longleftrightarrow \\\\ h \\dot{s} + ks = 0, \\end{align}\\] which guarantees the exponential convergence of \\(s\\) to zero (note that \\(h\\) and \\(k\\) have the same sign), and hence the convergence of \\(e\\) to zero. Unknown parameters. Inspired by the control law with known parameters in (8.16), we design the adapative control law as \\[\\begin{equation} u = \\hat{h} q_r^{(n)} - ks + \\sum_{i=1}^n \\hat{a}_i f_i(x,t), \\tag{8.17} \\end{equation}\\] where the time-varying gains \\(\\hat{h},\\hat{a}_1,\\dots,\\hat{a}_n\\) will be adjusted by an adaptation law. Adaptation law. Inserting the adapative control law (8.17) into the system dynamics (8.15), we obtain \\[\\begin{align} h \\dot{s} + ks = \\tilde{h} q_r^{(n)} + \\sum_{i=1}^n \\tilde{a}_i f_i (x,t) \\Longleftrightarrow \\\\ s = \\frac{1}{p + k/h} \\frac{1}{h} \\underbrace{ \\left( \\begin{bmatrix} \\tilde{h} \\\\ \\tilde{a}_1 \\\\ \\vdots \\\\ \\tilde{a}_n \\end{bmatrix}^T \\begin{bmatrix} q_r^{(n)} \\\\ f_1(x,t) \\\\ \\vdots \\\\ f_n(x,t) \\end{bmatrix} \\right)}_{=:\\phi^T v} \\tag{8.18} \\end{align}\\] where \\(\\tilde{h} = \\hat{h} - h\\) and \\(\\tilde{a}_i = \\hat{a}_i - a_i,i=1,\\dots,n\\). Again, (8.18) is in the familiar form of (8.1), which naturally leads to the following adaptation law with \\(\\gamma &gt; 0\\) a chosen constant \\[\\begin{equation} \\dot{\\phi} = \\begin{bmatrix} \\dot{\\tilde{h}} \\\\ \\dot{\\tilde{a}}_1 \\\\ \\vdots \\\\ \\dot{\\tilde{a}}_n \\end{bmatrix} = - \\mathrm{sgn}(h) \\gamma s \\begin{bmatrix} q_r^{(n)} \\\\ f_1(x,t) \\\\ \\vdots \\\\ f_n(x,t) \\end{bmatrix}. \\tag{8.19} \\end{equation}\\] Tracking and parameter convergence. With the following Lyapunov function \\[\\begin{equation} V(s,\\phi) = |h| s^2 + \\frac{1}{\\gamma} \\phi^T \\phi, \\quad \\dot{V}(s,\\phi) -2|k| s^2, \\tag{8.20} \\end{equation}\\] the global convergence of \\(s\\) to zero can be easily shown. For parameter convergence, it is easy to see that when \\(v\\) satisfies the persistent excitation condition, we have that \\(\\phi\\) converges to zero. (However, the relationship between the reference trajectory \\(q_d(t)\\) and the persistent excitation of \\(v\\) becomes nontrivial due to the nonlinearities \\(f_i\\).) 8.1.3 Robotic Manipulator So far our focus has been on systems with a single input (\\(u \\in \\mathbb{R}\\)). In the following, we will show that similar techniques can be applied to adapative control of systems with multiple inputs, particularly, trajectory control of a robotic manipulator. Let \\(q \\in \\mathbb{R}^n\\) be the joint angles of a multi-link robotic arm, and \\(\\dot{q} \\in \\mathbb{R}^n\\) be the joint velocities. The dynamics of a robotic manipulator reads \\[\\begin{equation} H(q) \\ddot{q} + C(q,\\dot{q})\\dot{q} + g(q) = \\tau, \\tag{8.21} \\end{equation}\\] where \\(H(q) \\in \\mathbb{S}^{n}_{++}\\) is the manipulator inertia matrix (that is positive definite), \\(C(q,\\dot{q})\\dot{q}\\) is a vector of centripetal and Coriolis torques (with \\(C(q,\\dot{q}) \\in \\mathbb{R}^{n \\times n}\\)), and \\(g(q)\\) denotes gravitational torques. Figure 8.1: Planar two-link manipulator Example 8.1 (Planar Two-link Manipulator) The dynamics of a planar two-link manipulator in Fig. 8.1 is \\[\\begin{equation} \\begin{bmatrix} H_{11} &amp; H_{12} \\\\ H_{21} &amp; H_{22} \\end{bmatrix} \\begin{bmatrix} \\ddot{q}_1 \\\\ \\ddot{q}_2 \\end{bmatrix} + \\begin{bmatrix} - h \\dot{q}_2 &amp; -h (\\dot{q}_1 + \\dot{q}_2) \\\\ h \\dot{q}_1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\dot{q}_1 \\\\ \\dot{q}_2 \\end{bmatrix} = \\begin{bmatrix} \\tau_1 \\\\ \\tau_2 \\end{bmatrix}, \\tag{8.22} \\end{equation}\\] where \\[\\begin{align} H_{11} &amp; = a_1 + 2 a_3 \\cos q_2 + 2 a_4 \\sin q_2 \\\\ H_{12} &amp; = H_{21} = a_2 + a_3 \\cos q_2 + a_4 \\sin q_2 \\\\ H_{22} &amp;= a_2 \\\\ h &amp;= a_3 \\sin q_2 - a_4 \\cos q_2 \\end{align}\\] with \\[\\begin{align} a_1 &amp;= I_1 + m_1 l_{c1}^2 + I_e + m_e l_{ce}^2 + m_e l_1^2 \\\\ a_2 &amp;= I_e + m_e l_{ce}^2 \\\\ a_3 &amp;= m_e l_1 l_{ce} \\cos \\delta_e \\\\ a_4 &amp;= m_e l_1 l_{ce} \\sin \\delta_e. \\end{align}\\] As seen from the above example, the parameters \\(a\\) (which are nonlinear functions of the physical parameters such as mass and length) enter linearly in \\(H\\) and \\(C\\) (\\(g(q)\\) is ignored because the manipulator is on a horizontal plane). The goal of the control design is to have the manipulator track a desired trajectory \\(q_d(t)\\). Known parameters. When the parameters are known, we follow the sliding control design framework. Let \\(\\tilde{q} = q(t) - q_d(t)\\) be the tracking error, and define the combined error \\[ s = \\dot{\\tilde{q}} + \\Lambda \\tilde{q} = \\dot{q} - \\underbrace{\\left( \\dot{q}_d - \\Lambda \\tilde{q} \\right)}_{\\dot{q}_r} \\] where \\(\\Lambda \\in \\mathbb{S}^n_{++}\\) is a user-chosen positive definite matrix (in general we want \\(-\\Lambda\\) to be Hurwitz). In this case, \\(s \\rightarrow 0\\) implies \\(\\tilde{q} \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). Choosing the control law (coming from feedback linearization Appendix F) \\[\\begin{equation} \\tau = H \\ddot{q}_r - K_D s + C \\dot{q} + g(q) \\tag{8.23} \\end{equation}\\] with \\(K_D \\in \\mathbb{S}^n_{++}\\) positive definite leads to the closed-loop dynamics \\[ H \\dot{s} + K_D s = 0 \\Longleftrightarrow \\dot{s} = - H^{-1} K_D s. \\] Because the matrix \\(H^{-1} K_D\\) is the product of two positive definite matrices (recall \\(H\\) is positive definite and so is \\(H^{-1}\\)), it has strictly positive real eigenvalues.13 Hence, \\(- H^{-1} K_D\\) is Hurwitz and \\(s\\) is guaranteed to converge to zero. Control law. A closer look at the controller (8.23) allows us to write it in the following form \\[\\begin{align} \\tau &amp;= H \\ddot{q}_r + C(s + \\dot{q}_r) + g(q) - K_D s \\\\ &amp;= H \\ddot{q}_r + C \\dot{q}_r + g(q) + (C - K_D) s \\\\ &amp;= Y (q,\\dot{q},\\dot{q}_r,\\ddot{q}_r) a + (C - K_D) s \\end{align}\\] where \\(a \\in \\mathbb{R}^m\\) contains all the parameters and \\(Y \\in \\mathbb{R}^{n \\times m}\\) is the matrix that collects all the coeffients of \\(a\\) in \\(H \\ddot{q}_r + C \\dot{q}_r + g(q)\\). As a result, we design the adapative control law to be \\[\\begin{equation} \\tau = Y \\hat{a} - K_D s, \\tag{8.24} \\end{equation}\\] with \\(\\hat{a}\\) the time-varying parameter that we wish to adapt. Note that here we have done something strange: the adapative control law does not exactly follow the controller (8.23) in the known-parameter case.14 We first separated \\(s\\) from \\(\\dot{q}\\) and wrote \\(Ya = H \\ddot{q}_r + C \\dot{q}_r + g\\) instead of \\(Ya = H \\ddot{q}_r + C \\dot{q} + g\\); then we dropped the “\\(C\\)” matrix in front of \\(s\\) in the adapative control law. The reason for doing this will soon become clear when we analyze the tracking convergence. Adaptation law and tracking convergence. Recall that the key of adapative control is to design a control law and an adaptation law such that global converge of the tracking error \\(s\\) can be guaranteed by a Lyapunov function. Looking at the previous Lyapunov functions in (8.11) and (8.20), we see that they both contain a positive definite term in the tracking error \\(s\\) (or \\(e\\) if in first-order systems) and another positive definite term in the parameter error \\(\\tilde{a}\\). This hints us that we may try a Lyapunov candidate function of the following form \\[\\begin{equation} V = \\frac{1}{2} \\left( s^T H s + \\tilde{a} \\Gamma^{-1} \\tilde{a} \\right), \\tag{8.25} \\end{equation}\\] where \\(\\Gamma \\in \\mathbb{S}^m_{++}\\) is a constant positive definite matrix, and \\(\\tilde{a} = \\hat{a} - a\\) is the parameter error. The next step would be to derive the time derivative of \\(V\\), which, as we can expect, will contain a term that involves \\(\\dot{H}\\) and complicates our analysis. Fortunately, the following lemma will help us. Lemma 8.2 For the manipulator dynamics (8.21), there exists a way to define \\(C\\) such that \\(\\dot{H} - 2C\\) is skew-symmetric. Proof. See Section 9.1, page 399-402 in (Slotine, Li, et al. 1991). You should also check if this is true for the planar two-link manipulator dynamics in Example 8.1. With Lemma 8.2, the time derivative of \\(V\\) in (8.25) reads \\[\\begin{align} \\dot{V} &amp; = s^T H \\dot{s} + \\frac{1}{2} s^T \\dot{H} s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\\\ &amp;= s^T (H \\ddot{q} - H \\ddot{q}_r) + \\frac{1}{2} s^T \\dot{H} s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\\\ &amp;= s^T (\\tau - C \\dot{q} - g - H \\ddot{q}_r ) + \\frac{1}{2} s^T \\dot{H} s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\tag{8.26} \\\\ &amp;= s^T (\\tau - H \\ddot{q}_r - C (s + \\dot{q}_r) - g) + \\frac{1}{2} s^T \\dot{H} s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\\\ &amp;= s^T (\\tau - H \\ddot{q}_r - C \\dot{q}_r - g) + \\frac{1}{2} s^T (\\dot{H}- 2C)s + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\tag{8.27}\\\\ &amp;= s^T (\\tau - H \\ddot{q}_r - C \\dot{q}_r - g) + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\\\ &amp;=s^T(Y\\hat{a} - K_D s - Ya) + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} \\tag{8.28}\\\\ &amp;=s^T Y \\tilde{a} + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}} - s^T K_D s \\tag{8.29}, \\end{align}\\] where we used the manipulator dynamics (8.21) to rewrite \\(H\\ddot{q}\\) in (8.26), used \\(\\dot{H} - 2C\\) is skew-symmetric in (8.27), invoked the adapative control law (8.24) and reused \\(Ya = H \\ddot{q}_r + C \\dot{q}_r + g(q)\\) in (8.28). The derivation above explains why the choice of the control law in (8.24) did not exactly follow its counterpart when the parameters are known: we need to use \\(s^T Cs\\) to cancel \\(\\frac{1}{2} s^T \\dot{H} s\\) in (8.27). We then wonder if we can design \\(\\dot{\\tilde{a}}\\) such that \\(\\dot{V}\\) in (8.29) is negative semidefinie? This turns out to be straightforward with the adaptation law \\[\\begin{equation} \\dot{\\tilde{a}} = -\\Gamma Y^T s, \\tag{8.30} \\end{equation}\\] to make \\(s^T Y \\tilde{a} + \\tilde{a}^T \\Gamma^{-1} \\dot{\\tilde{a}}\\) vanish and so \\[ \\dot{V} = - s^T K_D s \\leq 0. \\] We are not done yet. To show \\(s\\) converges to zero (which is implied by \\(\\dot{V}\\) converges to zero), by Barbalat’s stability certificate 5.7, it suffices to show \\[ \\ddot{V} = -2 s^T K_D \\dot{s} \\] is bounded. We already know \\(s\\) and \\(\\tilde{a}\\) are bounded, due to the fact that \\(V\\) in (8.25) is bounded. Therefore, we only need to show \\(\\dot{s}\\) is bounded. To do so, we plug the adapative control law (8.24) into the manipulator dynamics (8.21) and obtain \\[ H \\dot{s} + (C + K_D) s = Y\\tilde{a}, \\] which implies the boundedness of \\(\\dot{s}\\) (note that \\(H\\) is uniformly positive definite, i.e., \\(H \\succeq \\alpha I\\) for some \\(\\alpha &gt; 0\\)). This concludes the analysis of the tracking convergence \\(s \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). 8.2 Certainty-Equivalent Adaptive Control References Slotine, Jean-Jacques E, Weiping Li, et al. 1991. Applied Nonlinear Control. Vol. 199. 1. Prentice hall Englewood Cliffs, NJ. Consider two positive definite matrices \\(A\\) and \\(B\\), let \\(B = B^{1/2}B^{1/2}\\). The product \\(AB\\) can be written as \\(AB = A B^{1/2}B^{1/2} = B^{-1/2} (B^{1/2} A B^{1/2}) B^{1/2}\\). Therefore \\(AB\\) is similar to \\(B^{1/2} A B^{1/2}\\) and is positive definite.↩︎ In fact, one can show that the controller (8.24) with known parameters, i.e., \\(\\tau = Y a - K_D s\\), also guarantees the convergence of \\(s\\) towards zero, though it is different from the feedback linearization controller (8.23). Try proving the convergence with a Lyapunov candidate \\(V = \\frac{1}{2}s^T H s\\).↩︎ "],["psets.html", "Chapter 9 Problem Sets", " Chapter 9 Problem Sets Exercise 9.1 (Inscribed Polygon of Maximal Perimeter) In this exercise, we will use dynamic programming to solve a geometry problem, i.e., to find the \\(N\\)-side polygon inscribed inside a circle with maximum perimeter. We will walk you through the key steps of formulating and solving the problem, while leaving a few mathematical details for you to fill in. Given a circle with radius \\(1\\), we can randomly choose \\(N\\) distinct points on the circle to form a polygon with \\(N\\) vertices and sides, as shown in Fig. 9.1 with \\(N=3,4,5\\). Figure 9.1: Polygons inscribed inside a circle Once the \\(N\\) points are chosen, the \\(N\\)-polygon will have a perimeter, i.e., the sum of the lengths of its edges. What is the configuration of the \\(N\\) points such that the resulting \\(N\\)-polygon has the maximum perimeter? I claim that the answer is when the \\(N\\)-polygon has edges of equal lengths, or in other words, when the \\(N\\) points are placed on the circle evenly. Let us use dynamic programming to prove the claim. To use dynamic programming, we need to definie a dynamical system and an objective function. Figure 9.2: Sequential placement of N points on the circle. Dynamical system. We will use \\(\\{x_1,\\dots,x_N \\}\\) to denote the angular positions of the \\(N\\) points to be placed on the circle (with slight abuse of notation, we will call each of those points \\(x_k\\) as well). In particular, as shown in Fig. 9.2, let us use \\(x_k\\) to denote the angle between the line \\(O-x_k\\) and the vertical line (\\(O\\) is the center of the circle), with zero angle starting at \\(12\\) O’clock and clockwise being positive. Without loss of generality, we assume \\(x_1 = 0\\). (if \\(x_1\\) is nonzero, we can always rotate the entire circle so that \\(x_1 = 0\\)). After the \\(k\\)-th point is placed, we can “control” where the next point \\(x_{k+1}\\) will be, by deciding the incremental angle between \\(x_{k+1}\\) and \\(x_k\\), denoted as \\(u_k &gt; 0\\) in Fig. 9.2. This is simply saying the dynamics is \\[ x_{k+1} = x_k + u_k, \\quad k=1,\\dots,N-1, \\quad x_1 = 0. \\] Cost-to-go. The perimeter of the \\(N\\)-polygon is therefore \\[ g_N(x_N) + \\sum_{k=1}^{N-1} g_k(x_k, u_k), \\] with the terminal cost \\[ g_N(x_N) = 2 \\sin \\left( \\frac{2\\pi - x_N}{2} \\right) \\] the distance between \\(x_N\\) and \\(x_1\\) (see Fig. 9.2), and the running cost \\[ g_k(x_k,u_k) = 2 \\sin \\left( \\frac{u_k}{2} \\right) \\] the distance between \\(x_{k+1}\\) and \\(x_k\\). Dynamic programming. We are now ready to invoke dynamic programming. We start by setting \\[ J_N(x_N) = g_N(x_N) = 2 \\sin \\left( \\frac{2\\pi - x_N}{2} \\right). \\] We then compute \\(J_{N-1}(x_{N-1})\\) as \\[\\begin{equation} J_{N-1}(x_{N-1}) = \\max_{0&lt; u_{N-1} &lt; 2\\pi - x_{N-1}} \\left\\{ \\underbrace{ 2 \\sin \\left( \\frac{u_{N-1}}{2} \\right) + J_N(x_{N-1} + u_{N-1}) }_{Q_{N-1}(x_{N-1}, u_{N-1})} \\right\\}, \\tag{9.1} \\end{equation}\\] where \\(u_{N-1} &lt; 2\\pi - x_{N-1}\\) because we do not want \\(x_N\\) to cross \\(2\\pi\\). Show that \\[ Q_{N-1}(x_{N-1},u_{N-1}) = 2 \\sin \\left( \\frac{u_{N-1}}{2} \\right) + 2 \\sin \\left( \\frac{2\\pi - x_{N-1} - u_{N-1} }{2} \\right), \\] and \\[ \\frac{\\partial Q_{N-1}(x_{N-1},u_{N-1})}{\\partial u_{N-1}} = \\cos \\left( \\frac{u_{N-1}}{2} \\right) - \\cos \\left( \\frac{2\\pi - x_{N-1} - u_{N-1} }{2} \\right). \\] Show that \\(Q_{N-1}(x_{N-1},u_{N-1})\\) is concave (i.e., \\(-Q_{N-1}(x_{N-1},u_{N-1})\\) is convex) in \\(u_{N-1}\\) for every \\(x_{N-1} \\in (0, \\pi)\\) and \\(u_{N-1} \\in (0, 2\\pi - x_{N-1})\\). (Hint: compute the second derivative of \\(Q_{N-1}(x_{N-1},u_{N-1})\\) with respect to \\(u_{N-1}\\) and use Proposition B.2). With a and b, show that the optimal \\(u_{N-1}\\) that solves (9.1) is \\[ u_{N-1}^\\star = \\frac{2\\pi - x_{N-1}}{2}, \\] and therefore \\[ J_{N-1}(x_{N-1}) = 4 \\sin \\left( \\frac{2\\pi - x_{N-1}}{4} \\right). \\] (Hint: the point at which a concave function’s gradient vanishes must be the unique maximizer of that function) Now use induction to show that the \\(k\\)-th step dynamic programming \\[ J_k(x_k) = \\max_{0&lt; u_k &lt; 2\\pi - x_k} \\left\\{ 2 \\sin\\left( \\frac{u_k}{2} \\right) + J_{k+1}(x_k + u_k) \\right\\} \\] admits an optimal control \\[ u_k^\\star = \\frac{2\\pi - x_k}{N-k+1}, \\] and optimal cost-to-go \\[ J_k(x_k) = 2(N-k+1)\\sin\\left( \\frac{2\\pi - x_k}{2(N-k+1)} \\right). \\] Starting from \\(x_1 = 0\\), what is the optimal sequence of controls? Hopefully now you see why my original claim is true! (Bonus) We are not yet done for this exercise. Since you have probably already spent quite some time on this exercise, I will leave the rest of the exercise a bonus. In case you found this simple geometric problem interesting, you should keep reading as we will use numerical techniques to prove the same claim. In Fig. 9.2, by denoting \\[ u_N = 2\\pi - x_N = 2\\pi - (u_1 + \\dots + u_{N-1}) \\] as the angle between the line \\(O-x_{N}\\) and the line \\(O-x_1\\), it is not hard to observe that the perimeter of the \\(N\\)-polygon is \\[ \\sum_{k=1}^N 2 \\sin \\left( \\frac{u_k}{2} \\right). \\] Consequently, to maximize the perimeter, we can formulate the following optimization \\[\\begin{equation} \\begin{split} \\max_{u_1,\\dots,u_N} &amp;\\quad \\sum_{k=1}^N 2 \\sin \\left( \\frac{u_k}{2} \\right) \\\\ \\text{subject to} &amp;\\quad u_k &gt; 0, k=1,\\dots,N \\\\ &amp;\\quad u_1 + \\dots + u_N = 2 \\pi \\end{split} \\tag{9.2} \\end{equation}\\] where \\(u_k\\) can be seen as the angle spanned by the line \\(x_k - x_{k+1}\\) with respect to the center \\(O\\) so that they are positive and sum up to \\(2\\pi\\). Show that the optimization (9.2) is convex. (Hint: first show the feasible set is convex, and then show the objective function is concave over the feasible set.) Now that we have shown (9.2) is a convex optimization problem, we know that pretty much any numerical algorithm will guarantee convergence to the globally optimal solution. It is too much to ask you to implement a numerical algorithm on your own, as that can be a one-semester graduate-level course (Nocedal and Wright 1999). However, Matlab provides a nice interface, fmincon, to many such numerical algorithms, and let me show you how to use fmincon to solve (9.2) so we can numerically prove our claim. I have provided most of the code necessary for solving (9.2) below. Please fill in the definition of the function perimeter(u), and then run the code in Matlab. Show your results for \\(N=3,10,100\\). Do the solutions obtained from fmincon verify our claim? clc; clear; close all; % number of points to be placed N = 10; % define the objective function % fmincon assumes minimization % We minimize the negative perimeter so as to maximize the perimeter objective = @(u) -1*perimeter(u); % choose which algorithm to use for solving options = optimoptions(&#39;fmincon&#39;, &#39;Algorithm&#39;, &#39;interior-point&#39;); % supply an initial guess % since this is a convex problem, we can use any initial guess u0 = rand(N,1); % solve uopt = fmincon(objective,u0,... % objective and initial guess -eye(N),zeros(N,1),... % linear inequality constraints ones(1,N),2*pi,... % linear equality constraints [],[],[],... % we do not have lower/upper bounds and nonlinear constraints options); % plot the solution x = zeros(N,1); for k = 2:N x(k) = x(k-1) + uopt(k-1); end figure; % plot a circle viscircles([0,0],1); hold on % scatter the placed points scatter(cos(x),sin(x),&#39;blue&#39;,&#39;filled&#39;); axis equal; %% helper functions % The objective function function f = perimeter(u) % TODO: define the perimeter function here. end   Exercise 9.2 (LQR with Constraints) In class we worked on the LQR problem where the states and controls are unbounded. This is rarely the case in real life – you only have a limited amount of control power, and you want your states to be bounded (e.g., not entering some dangerous zones). For linear systems with convex constraints on the control and states, the seminal paper (Bemporad et al. 2002) investigates the landscape of the optimal cost-to-go and controller. In this exercise, let us use convex optimization to numerically study a toy problem. Consider a variant of the LQR problem (2.2) where the controls are bounded between \\([-u_{\\max}, u_{\\max}]\\), the system matrices \\(A_k, B_k\\) are constant, and the dynamics is deterministic: \\[\\begin{equation} \\begin{split} J(x_0) = \\min_{u_{0},\\dots,u_{N-1} \\in [-u_{\\max},u_{\\max}]} &amp;\\quad x_N^T Q_N x_N + \\sum_{k=0}^{N-1} (x_k^T Q_k x_k + u_k^T R_k u_k) \\\\ \\text{subject to} &amp;\\quad x_{k+1} = A x_k + B u_k, k=0,\\dots,N-1 \\end{split} \\tag{9.3} \\end{equation}\\] We assume \\(Q_k\\succeq 0\\) for \\(k=0,\\dots,N\\) and \\(R_k \\succ 0\\) for all \\(k=0,\\dots,N-1\\). Show that Problem (9.3), when \\(x_0\\) is given, is a convex optimization problem. Discretize the continuous-time double integrator dynamics \\[ \\ddot{q} = u, \\quad u \\in [-1,1] \\] in the form of \\(x_{k+1} = A x_k + B u_k\\) with a constant \\(dt\\) time discretization. (Hint: take \\(x = [q,\\dot{q}]\\) as the state vector.) Fix \\(N=50\\), \\(dt=0.1\\) and choose your favorite \\(Q_k\\) and \\(R_k\\). Solve the convex optimization (9.3) at a dense grid of \\(x_0\\) (e.g., using CVX or cvxpy). Plot the optimal cost-to-go \\(J(x_0)\\), and the optimal controls \\(u_0(x_0),\\dots,u_{N-1}(x_0)\\). For the optimal controls, you can just plot one of the controls such as \\(u_0(x_0)\\). You may want to use the Matlab function surf. (Hint: you will most likely benefit from Appendix B.2.) Increase \\(N\\) and decrease \\(dt\\), repeat (c). Do you get more fine-grained plots of the optimal cost-to-go and controls? (When you increase \\(N\\), the convex optimization has more variables to optimize, so there is a limit at which the solver takes too much time.) (Bonus) We only have constraints on the control so far. What if you add constraints to the states as well? For example, you can try limiting the velocity \\(\\dot{q}\\) to be at least \\(0.1\\) by adding \\(\\dot{q}_k \\geq 0.1\\) for some k. How will \\(J\\) and \\(u\\) change? (Bonus) Can you write down the KKT optimality conditions of (9.3) and explain what you have observed from the numerical experiments? (Hint: KKT optimality conditions can be found in Theorem B.2.)   Exercise 9.3 (Cart Pole System) In this exercse, let us study the cart-pole system (we saw the video of human-controlled version in our first lecture), another interesting nonlinear control problem, and reinforce our knowledge about LQR. Our task is to balance a pendulum on a cart by horizontally moving the cart. Fig. 9.3 gives an illustration of the system. See this video for an actual robotic implementation. Figure 9.3: Illustration of cart-pole problem With the above illustration, we parameterize the system with two scalars: \\(x\\) represents the current location of the cart, while \\(\\theta\\) is the angle between current pole and the stable equilibrium. Therefore, our goal is to study the motion of the cart-pole system with a horizonal control \\(f\\). We assume hereafter the system is ideal such that there is no friction, and the mass of the pole concentrates at the free end point. (Bonus) For those of you who have background in rigid-body dynamics, this is an opportunity for you to apply your knowledge. However, feel free to skip this subproblem and it won’t affect the rest of the exercise. Denote the mass of cart and pole as \\(m_c\\) and \\(m_p\\), respectively. Derive the equations of motion: \\[\\begin{equation} \\left(m_c+m_p\\right) \\ddot{x}+m_p l \\ddot{\\theta} \\cos \\theta-m_p l \\dot{\\theta}^2 \\sin \\theta=f, \\tag{9.4} \\end{equation}\\] \\[\\begin{equation} m_p l \\ddot{x} \\cos \\theta+m_p l^2 \\ddot{\\theta}+m_p g l \\sin \\theta=0. \\tag{9.5} \\end{equation}\\] (Hints: compute the Lagrangian of the system and the corresponding Lagrangian equations. Analyzing the two objects separately also works.) Translate the equations in (a) into the basic state-space dynamics form \\[\\begin{equation} \\dot{\\mathbf{x}}=F(\\mathbf{x}, \\mathbf{u}). \\tag{9.6} \\end{equation}\\] What are \\(\\mathbf{x},\\mathbf{u},F\\) here? (Hint: try \\(\\mathbf{x}=[x,\\theta,\\dot{x},\\dot{\\theta}]^\\top\\).) Linearize the dynamics in (b) around the unstable equilibrium where \\(\\theta^*=\\pi\\) and \\(x^*=\\dot{x}^*=\\dot{\\theta}^*=0.\\) (i.e., the pole is in the upright position and the cart stay at zero.) The result should be in the form of \\[\\begin{equation} \\dot{\\Delta\\mathbf{x}}=A\\Delta\\mathbf{x}+B\\Delta\\mathbf{u}, \\tag{9.7} \\end{equation}\\] where \\(\\Delta\\mathbf{x} = \\mathbf{x}-\\mathbf{x}^*\\) and \\(\\Delta\\mathbf{u}=\\mathbf{u}-\\mathbf{u}^*\\). Define the linearization error \\(e(\\mathbf{x}, \\mathbf{u}):=\\|F(\\mathbf{x}, \\mathbf{u}) - (A\\Delta\\mathbf{x}+B\\Delta\\mathbf{u})\\|^2\\). Simulate the original system (9.6) and the linearized system (9.7) with the same initial condition. How does the linearization error change over time? Provide at least three different initialization results. (Hints: (i) Sanity check: intuitively the error should not depend on the initial location \\(x\\), and it should have symmetry. Is that true in your simulation? (ii) In the same unstable position, how does push/pull (positive/negative) force change the results?) Convert the continuous-time dynamics in (9.7) to discrete-time with a fixed time-discretization. Then design an LQR controller to stabilize the cart-pole at the unstable equilibrium. Does the LQR controller succeed for all initial conditions you tested? (Hint: try several initial conditions where the end point of the pole is above or below the horizontal line.) You may want to take a look at the LQR example for the simple pendulum in Example 2.1.   Exercise 9.4 (Trajectory Optimization) Let us use this exercise to practice your skills in implementing trajectory optimization. Consider a dynamical system \\[ x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} (1-x_2^2)x_1 - x_2 + u \\\\ x_1 \\end{bmatrix}, \\quad x(0) = x_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. \\] With \\(T=10\\), consider the following optimal control problem \\[\\begin{equation} \\begin{split} \\min_{u(t),t \\in [0,T]} &amp; \\quad \\int_{t=0}^T \\Vert x(t) \\Vert^2 + u(t)^2 dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x,u), \\quad x(0) = x_0 \\\\ &amp; \\quad u(t) \\in [-1,1],\\forall t \\in [0,T]. \\end{split} \\end{equation}\\] Solve the problem using direct multiple shooting with \\(N=50\\) time intervals. Solve the problem using direct collocation with \\(N=50\\). Plot the optimized control signal and resulting state trajectory for both a and b. You probably want to refer to the source codes of Example 3.6 and 3.7.   Exercise 9.5 (Policy Iteration of Shortest Path) In Example 2.2, we used value iteration to solve the shortest path problem with obstacles. In this exercise, we will implement policy iteration. Instead of value iteration that updates the Q-function by \\(Q^{(k+1)} = \\mathcal{T}Q^{(k)}\\) where \\(\\mathcal{T}\\) is the Bellman optimality operator defined in (2.27), we use a different update rule: Initialize the policy \\(\\pi_0\\). For \\(k=0,1,2,\\ldots\\), compute \\(Q_{\\pi_k}\\) via (2.24). Then update policy by greedy method \\(\\pi_{k+1}=\\pi_{Q^{\\pi_k}}\\) (see (2.26) for the definition of \\(\\pi_Q\\)). Now try to solve the following problems. With the same obstacles in Fig. 2.3, implement the above policy iteration algorithm. Can you recover the cost-to-go and shortest path found in the example? We designed the cost function \\(g\\) to be \\(20\\) when there is an obstacle. Is there a lower bound on this value, such that the shortest path with any starting point will still avoid obstacles? If yes, how will the lower bound change with different obstacle structures? (Bonus) With the assumption that the grids are connected, give a conjecture on the maximum of the lower bound. If it is allowed to go diagonally (for example, from (4,4) to (3,5)), how will the results change? (Bonus) Beyond iterative algorithms, there is another formulation of the problem in linear programming. Try to implement the following problem: \\[\\max_{J} \\sum_{x}J(x),\\quad s.t. J(x)\\le g(x,u) + \\gamma\\sum_{x&#39;}P(x&#39;|x,u)J(x&#39;),\\ \\forall x,u.\\] What can you find? (Hint: formulate the problem as a linear programming problem.)   Exercise 9.6 (Verifying Control Lyapunov Function for the Double Integrator) Consider the following discrete-time double integrator dynamics \\[ x_{t+1} = \\underbrace{\\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}}_{A} x_t + \\underbrace{\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}}_{B} u_t, \\] with control constraints \\[ u \\in \\mathcal{U} = [-1, 1]. \\] We do not enforce any state constraint on \\(x\\), that is \\(\\mathcal{X} = \\mathbb{R}^2\\). Receding horizon controller. We aim to regulate the system at the origin, and design the following receding horizon controller (RHC) with horizon \\(N=3\\) \\[\\begin{equation} \\begin{split} \\min_{u(0),\\dots,u(N-1)} &amp; \\quad x(N)^T P x(N) + \\sum_{k=0}^{N-1} x(k)^T Q x(k) + u(k)^T R u(k) \\\\ \\text{subject to} &amp; \\quad u(k) \\in \\mathcal{U}, \\forall k=0,\\dots,N-1 \\\\ &amp; \\quad x(k+1) = A x(k) + B u(k), \\forall k = 0,\\dots,N-1 \\\\ &amp; \\quad x(0) = x_t, \\end{split} \\tag{9.8} \\end{equation}\\] where \\(x_t\\) is the system state at time \\(t\\), and \\(P,Q,R\\) matrices are designed as follows \\[ Q = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}, \\quad R = 1, \\quad P = \\begin{bmatrix} 0.1 &amp; 0 \\\\ 0 &amp; 0.1 \\end{bmatrix}. \\] Implement the RHC in (9.8), and run the RHC with initial system state \\(x_0 = [2;1]\\). Plot the system trajectory on a 2D plane like Fig. 3.24. From Theorem 3.3, we know that a sufficient condition for the stability of RHC is that the terminal cost, in our example is \\(p(x) = x^T P x\\), needs to satisfy (3.40). When \\(p(x)\\) satisfies (3.40), we say \\(p(x)\\) is a control Lyapunov function (CLF). Instantiating (3.40) for this exercise, it becomes (recall that we do not have terminal constraint, i.e., \\(\\mathcal{X}_f = \\mathbb{R}^2\\)): \\[\\begin{equation} \\rho = \\min_{u \\in \\mathcal{U}} \\ \\ (Ax + Bu)^T P (Ax + Bu) - x^T P x + x^T Q x + u^T R u \\leq 0, \\quad \\forall x \\in \\mathbb{R}^2. \\tag{9.9} \\end{equation}\\] Computing \\(\\rho\\) for all \\(x\\) in \\(\\mathbb{R}^2\\) is difficult, so I will ask you to compute \\(\\rho\\) along the state trajectory generated by RHC in (a). Compute and plot the trajectory of \\(\\rho\\). From Proposition 2.2, we know the optimal cost-to-go of the unconstrained infinite-horizon LQR problem with \\(x(0) = x\\) \\[ J_{\\infty}(x) = \\min_{u(0),\\dots} \\ \\ \\sum_{k=0}^{\\infty} u(k)^T R u(k) + x(k)^T Q x(k), \\quad \\text{subject to} \\quad x(k+1) = A x(k) + B u(k), \\] is a quadratic function \\[ J_{\\infty}(x) = x^T S x, \\] where \\(S\\) can be computed by solving the algebraic Riccati equation. Now use Matlab or Python to compute \\(S\\) (e.g., using dlqr in Matlab) for the double integrator. Redo (a) and (b) by using \\(S\\) in (c) as the terminal cost, i.e., setting \\(P=S\\). Plot the state trajectory, as well as the trajectory of \\(\\rho\\). Redo (a) and (b) by using \\(0.1S\\) and \\(10S\\) as the terminal cost, i.e., setting \\(P=0.1S\\) and \\(P=10S\\). Plot the state trajectory, as well as the trajectory of \\(\\rho\\).   Exercise 9.7 (Polyhedral Controllable and Reachable Sets) In this exercise, we aim to compute controllable and reachable sets explicitly, rather than implementing MPT. A bounded polyhedron in \\(\\mathbb{R}^n\\) can be expressed as the convex hull of a finite set of points (vertices) \\(V = \\{V_1, \\ldots, V_k\\}\\subset \\mathbb{R}^n\\). Show that a linear transform of such a polyhedron is simply tranforming its vertices, namely, \\[A\\text{conv}(V)+b = \\text{conv}(AV)+b,\\] where \\(A\\in\\mathbb{R}^{m\\times n},\\ b\\in\\mathbb{R}^m\\). The exact definition of a (closed) polyhedron \\(\\mathcal{P}\\) is a set defined by finite number of linear inequalities \\(\\mathcal{P} = \\{x\\in\\mathbb{R}^n\\mid Hx\\le h \\}\\) where \\(H\\in\\mathbb{R}^{m\\times n},\\ h\\in\\mathbb{R}^m\\). Let \\(A\\in\\mathbb{R}^{n\\times n}\\) be an invertible matrix, \\(b\\in\\mathbb{R}^n\\). Suppose the linear transformed polyhedron \\(A\\mathcal{P}+b\\) has representation \\[\\{y\\in\\mathbb{R}^n\\mid H^\\prime y\\le h^\\prime \\}.\\] What are \\(H^\\prime\\) and \\(h^\\prime\\)? With the same setting as Example 3.10, consider linear system \\[ x_{t+1} = \\begin{bmatrix} 1.5 &amp; 0 \\\\ 1 &amp; -1.5 \\end{bmatrix} x_t + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} u_t \\] with state and control constraints \\[ \\mathcal{X} = [-10,10]^2, \\quad \\mathcal{U} = [-5,5]. \\] Compute \\(\\text{Pre}(\\mathcal{X})\\) and \\(\\text{Suc}(\\mathcal{X})\\) in the form of \\(\\{x\\in\\mathbb{R}^n\\mid Hx\\le h \\}\\). What are the vertices of two sets respectively? (Hints: Denote the dynamics as \\(x_{t+1} = Ax_t+Bu_t\\). For the successor set, \\[\\text{Suc}(\\mathcal{X}) = \\cup_{u\\in\\mathcal{U}}\\{x:\\exists x&#39;\\in\\mathcal{X},\\text{ s.t., }Ax&#39;+Bu = x\\} = \\cup_{u\\in\\mathcal{U}}(A\\mathcal{X} + Bu).\\] What is \\(A\\mathcal{X} + Bu\\) for each \\(u\\)? Will the shape of \\(A\\mathcal{X} + Bu\\) change when changing \\(u\\)? The conclusion in (b) might help. Similarly, for the precursor set, \\[\\text{Pre}(\\mathcal{X}) = \\cup_{u\\in\\mathcal{U}}\\{x:Ax+Bu \\in \\mathcal{X}\\} = \\cup_{u\\in\\mathcal{U}}A^{-1}(\\mathcal{X} - Bu).\\] Mimic the computation of the successor set.) Continued with the results in (c), compute one-step controllable set \\(\\mathcal{K}_1(\\mathcal{X}) = \\text{Pre}(\\mathcal{X}) \\cap \\mathcal{X}\\). (Hints: There are 4 inequalities for each of the two sets. Any redundancy in those 8 inequalities when combining them together?) From the above questions, we get the intuition of how the MPT compute controllable set: Obtain the representation of precursor set; Intersect the precursor set with the feasible domain \\(\\mathcal{X}\\); Remove redundant (useless) inequalities.        Provided the following algorithm that removes redundancy of a polyhedral representation, write the codes to find \\(\\mathcal{K}_1(\\mathcal{X})\\) without using MPT. Does the outcome match your results in (d)? Algorithm: Find redundacy-free representation of a polyhedron Input: \\(H\\in\\mathbb{R}^{m\\times n}\\) and \\(h\\in\\mathbb{R}^m\\) that represents \\(\\mathcal{P} = \\{x\\in\\mathbb{R}^n\\mid Hx\\le h \\}\\) Output: \\(H_0\\in\\mathbb{R}^{m_0\\times n}\\) and \\(h_0\\in\\mathbb{R}^{m_0}\\) that represents \\(\\mathcal{P}\\) without redundancy    \\(\\mathcal{I} \\leftarrow \\{1,\\ldots, m\\}\\)    For \\(i=1\\) to \\(m\\)      \\(\\mathcal{I}\\leftarrow \\mathcal{I}\\setminus\\{i\\}\\)      \\(f^*\\leftarrow\\max_x H_i x,\\ s.t. H_{\\mathcal{I}}x\\le h_{\\mathcal{I}}, H_i x\\le h_i + 1\\)      %% \\(H_i\\) is the \\(i\\)-th row of \\(H\\), \\(H_{\\mathcal{I}}\\) concatenates rows of \\(H\\) with index in \\(\\mathcal{I}\\)      If \\(f^* &gt; h_i\\) Then \\(\\mathcal{I}\\leftarrow \\mathcal{I}\\cup \\{i\\}\\)    \\(H_0\\leftarrow H_{\\mathcal{I}}, h_0\\rightarrow h_{\\mathcal{I}}\\)   Exercise 9.8 (Certified Region of Attraction of Clipped LQR Controller) In this exercise, let us use the simple pendulum as an example to (i) recognize the difficulty of nonlinear control in the presence of control limits, and (ii) appreciate the power of Lyapunov analysis and Sums-of-Squares (SOS) programming. A starting script for this exercise can be found here, where you need to fill out some specific details to finish this exercise. Let us consider the continuous-time pendulum dynamics \\[\\begin{equation} \\dot{x} = \\begin{bmatrix} x_2 \\\\ \\frac{1}{ml^2} (u - b x_2 + mgl \\sin x_1) \\end{bmatrix} \\tag{9.10} \\end{equation}\\] where \\(x_1\\) is the angular position of the pendulum, and \\(x_2\\) is the angular velocity. Note that the dynamics above is written such that \\(x=0\\) represents the upright position. In the case of no control saturation, i.e., \\(u \\in \\mathbb{R}\\), stabilizing the pendulum at \\(x=0\\) is easy, we can linearize the dynamics at \\(x=0\\) and obtain \\[ \\dot{x} \\approx \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} &amp; - \\frac{b}{ml^2} \\end{bmatrix}}_{A} x + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix}}_{B} u. \\] We then solve an infinite-horizon LQR problem \\[ \\min \\int_{t=0}^{\\infty} x(t)^T Q x(t) + u(t)^T R u(t) dt, \\quad \\text{subject to} \\quad x(0) = x_0. \\] The solution to this LQR problem can be written in closed form, see Section 4.5.1. Calling the Matlab lqr function (which is for continuous-time LQR), we can get a feedback controller \\[\\begin{equation} u(x) = - Kx, \\tag{9.11} \\end{equation}\\] with the optimal cost-to-go \\[\\begin{equation} J(x_0) = x_0^T S x_0, \\tag{9.12} \\end{equation}\\] with \\(K\\) and \\(S\\) constant matrices. Control saturation. Assume now our control has hard bounds, i.e., \\[ u \\in [-u_{\\max}, u_{\\max}]. \\] We wish to keep using the LQR controller (9.11). Therefore, our saturated controller will be \\[\\begin{equation} \\bar{u}(x) = \\text{clip}(u(x),-u_{\\max},u_{\\max}) = \\text{clip}(-Kx,-u_{\\max},u_{\\max}), \\tag{9.13} \\end{equation}\\] where the \\(\\text{clip}\\) function is defined as \\[ \\text{clip}(u,-u_{\\max},u_{\\max}) = \\begin{cases} u_{\\max} &amp; \\text{if} \\quad u \\geq u_{\\max} \\\\ u &amp; \\text{if} \\quad -u_{\\max} \\leq u \\leq u_{\\max} \\\\ -u_{\\max} &amp; \\text{if} \\quad u \\leq - u_{\\max} \\end{cases}. \\] A natural question is: can the saturated controller \\(\\bar{u}(t)\\) in (9.13) still stabilize the pendulum? Let’s simulate the pendulum and the controller to investigate this. We will use the parameter \\(m=1,g=9.8,l=1,b=0.1\\), and \\(u_{\\max} = 2\\). For the LQR cost matrix, let us use \\(Q = I\\) and \\(R = 1\\). Compute the LQR gain \\(K\\) and implement the saturated controller (9.13). In a region \\(\\mathcal{X} = [-0.2\\pi, 0.2\\pi] \\times [-0.2\\pi, 0.2\\pi]\\), sample \\(N=1000\\) initial states, and for each initial state, simulate the system under the saturated controller using ode45 or ode89. There will be some initial states from which the pendulum gets stabilized and others from which the pendulum fails to be stabilized. Plot the stabilized initial states as “circles”, and the non-stabilized initial states as “squares” on a 2D plot. (Hint: you should get something like Fig. 9.4.) Figure 9.4: Example of stabilized and non-stabilized initial states. The plot in Fig. 9.4 intuitively makes sense: when the initial state is very close to \\(0\\), the saturated controller can still stabilize the pendulum. Now our goal is to use Sums-of-Squares and Lyapunov analysis to get a certified region of stabilization, also known as the region of attraction (ROA), under the saturated control. Approximate polynomial dynamics. The SOS tool requires the system dynamics to be polynomial. The dynamics in (9.10) is polynomial except the term \\(\\sin x_1\\). Therefore, we perform a Taylor expansion of \\(\\sin x_1\\) \\[ \\sin x_1 = x_1 - \\frac{x_1^3}{3!} + \\frac{x_1^5}{5!} + \\dots, \\] leading to an approximate polynomial dynamics \\[\\begin{equation} \\dot{x} = \\bar{f}(x,u) = \\begin{bmatrix} x_2 \\\\ \\frac{1}{ml^2} \\left(u - b x_2 + mgl \\left(x_1 - \\frac{x_1^3}{3!} + \\frac{x_1^5}{5!} \\right) \\right) \\end{bmatrix}. \\tag{9.14} \\end{equation}\\] Candidate ROA. We want to certify the following candidate ROA \\[ \\Omega_\\rho = \\{x \\in \\mathbb{R}^2 \\mid J(x) :=x^T S x \\leq \\rho \\} \\] for some \\(\\rho &gt; 0\\), and \\(S\\) is exactly the LQR cost-to-go in (9.12). Since \\(J(x)\\) is already positive definite (because \\(S \\succ 0\\)), \\(\\Omega_\\rho\\) will look like an elliptical region around the origin. To certify all initial states inside \\(\\Omega_\\rho\\) can be stabilized, we need \\(\\dot{J}(x)\\) to be negative definite on \\(\\Omega\\), under the saturated LQR controller (9.13) (according to Theorem 5.2), that is to say \\[ \\dot{J}(x) = \\frac{\\partial J}{\\partial x} \\bar{f}(x, \\bar{u}(x)) &lt; 0, \\quad \\forall x \\in \\Omega_\\rho \\backslash \\{ 0 \\}. \\] A sufficient condition for the above equation to hold is \\[\\begin{equation} - \\frac{\\partial J}{\\partial x} \\bar{f}(x, \\bar{u}(x)) - \\epsilon \\Vert x \\Vert^2 \\quad \\text{ is SOS on } \\Omega_\\rho. \\tag{9.15} \\end{equation}\\] for some \\(\\epsilon &gt; 0\\) (do you see this?). The condition (9.15) is almost ready for us to implement in SOSTOOLS. However, there is one last issue. The saturated controller \\(\\bar{u}(x)\\) is not a polynomial! The last trick. Fortunately, we can use a trick here to save us. A closer look at (9.15) and the saturated controller \\(\\bar{u}(x)\\) shows that it is equivalent to asking \\[\\begin{equation} \\begin{split} - \\frac{\\partial J}{\\partial x} \\bar{f}(x, u_{\\max}) - \\epsilon \\Vert x \\Vert^2 \\quad \\text{ is SOS on } \\quad \\Omega_\\rho \\cap \\{ x \\mid u(x) \\geq u_{\\max} \\} \\\\ - \\frac{\\partial J}{\\partial x} \\bar{f}(x, u(x)) - \\epsilon \\Vert x \\Vert^2 \\quad \\text{ is SOS on } \\quad \\Omega_\\rho \\cap \\{ x \\mid -u_{\\max} \\leq u(x) \\leq u_{\\max} \\} \\\\ - \\frac{\\partial J}{\\partial x} \\bar{f}(x, - u_{\\max}) - \\epsilon \\Vert x \\Vert^2 \\quad \\text{ is SOS on } \\quad \\Omega_\\rho \\cap \\{ x \\mid u(x) \\leq -u_{\\max} \\}. \\end{split} \\tag{9.16} \\end{equation}\\] Essentially, (9.16) breaks the SOS condition into three cases, each corresponding to one case in the \\(\\text{clip}\\) function. (1) If \\(u(x) \\geq u_{\\max}\\), then the first equation takes \\(u_{\\max}\\) to be the controller, (2) If \\(u(x) \\leq -u_{\\max}\\), then the third equation takes \\(-u_{\\max}\\) to be the controller, (3) otherwise, the second equation takes \\(u(x)=-Kx\\) to be the controller. Condition (9.16) has three SOS constraints and can be readily implemented using SOSTOOLS. We will use \\(\\epsilon = 0.01\\). Choose \\(\\rho = 1\\), and implement the SOS conditions in (9.16) with a chosen relaxation order \\(\\kappa\\) (in the code I choose \\(\\kappa=4\\)). Is the SOS program feasible (i.e., does the SOS program produce a certificate)? If so, plot the boundary of \\(\\Omega_\\rho\\) on top of the samples plot in Fig. 9.4. Does \\(\\Omega_\\rho\\) agree with the samples? (Hint: you should see a plot similar to Fig. 9.5.) Figure 9.5: Certified Region of Attraction. Now try \\(\\rho = 2,3,4,5\\), for which values of \\(\\rho\\) the SOS program is feasible, and for which values of \\(\\rho\\) the SOS program becomes infeasible?   Exercise 9.9 (Energy pumping of Simple Pendulum) In Exercise 9.8, we have studied the region of attraction of the Clipped LQR Controller. The conclusion we got there is, once we enter the region of attraction \\(\\Omega_\\rho\\) (the green elliptical region in Fig. 9.5), we can switch to the clipped LQR controller and it guarantees stabilization towards the upright position. In this exercise, we are going to design a controller that can swing up the pendulum to enter the region of attraction. The total energy of the pendulum is given by \\[ E = \\frac{1}{2} m l^2 x_2^2 + mgl\\cos x_1,\\] where \\(x_1\\) and \\(x_2\\) are notations in (9.10). With the dynamics in (9.10), one can obtain the change of energy over time: \\[ \\dot E = \\color{red}{h(x,u)}\\cdot x_2.\\] Find the expression of \\(h(x,u)\\). Our desired energy is the one with the upright equilibruim: \\(E^d = mgl.\\) Define the energy of difference \\[\\tilde{E} = E - E^d.\\] Find the expression of \\(\\dot{\\tilde{E}}\\). Now consider the feedback controller of the form \\[ u = -k x_2 \\tilde{E} + b x_2.\\] Find the expression of \\(\\dot{\\tilde{E}}\\). (Bonus) Explain heuristically why the controller works. However, our controller has saturation as before: \\[\\bar{u}(x) = \\text{clip}(u(x),-u_{\\max},u_{\\max}).\\] Set the parameters the same as the previous exercise and \\(k=1\\). Simulate the system under the saturated controller with initial state \\((x_1,x_2)=(\\pi,1).\\) Will the state hit the candidate ROA \\(\\Omega_\\rho\\) with \\(\\rho=2\\)? If yes, stop the system once it hits the region, and draw the trajectory on the Figure similar to Fig. 9.5. Try \\(k=0.01, 0.1,1,10\\), which one hits the region the fastest? Congrats! We have designed a full nonlinear controller (energy pumping + clipped LQR) that guarantees swing-up of the pendulum from any initial state to the upright position. Try the controller for yourself. References Bemporad, Alberto, Manfred Morari, Vivek Dua, and Efstratios N Pistikopoulos. 2002. “The Explicit Linear Quadratic Regulator for Constrained Systems.” Automatica 38 (1): 3–20. Nocedal, Jorge, and Stephen J Wright. 1999. Numerical Optimization. Springer. "],["acknowledgement.html", "Acknowledgement", " Acknowledgement "],["linear-algebra-and-differential-equations.html", "A Linear Algebra and Differential Equations A.1 Linear Algebra A.2 Solving an Ordinary Differential Equation", " A Linear Algebra and Differential Equations In this Chapter, we provide basic concepts in linear algebra and ordinary differential equations (ODE), which can be a cheatsheet for readers. A.1 Linear Algebra Most of the linear algebraic concepts used in this textbook are provided in this section. A.1.1 Matrix Exponential Definition A.1 (Matrix exponential) Given a \\(n\\times n\\) matrix \\(A\\), the matrix exponential of \\(A\\), denoted as \\(e^A\\), is defined as: \\[e^A = \\sum_{p=0}^\\infty \\frac{A^p}{p!}.\\] Note that the matrix exponential is well defined, and every entry converges absolutely. We show some special cases of matrix exponential. Diagonal matrix. Note that if \\(A=\\text{diag}(a_1,a_2,\\ldots,a_n)\\), then \\(A^p=\\text{diag}(a_1^p,a_2^p,\\ldots,a_n^p)\\), and \\(e^A=\\text{diag}(e^{a_1},e^{a_2},\\ldots,e^{a_n})\\). Diagonalizable matrix: Definition A.2 (Diagonablizable matrix) A square matrix \\(A\\) is said to be diagonalizable or non-defective, if there exists an invertible matrix \\(P\\), such that \\(P^{-1}AP\\) is a diagonal matrix. In words, after change of coordination, the matrix becomes diagonal. If a matrix is not diagonalizable, it is defective. With diagonalization \\(A=PDP^{-1}\\) where \\(D\\) is a diagonal matrix (e.g. symmetric matrix is diagonalizable), we have \\(e^A=Pe^DP^{-1}.\\) Specifically if \\(U\\) consists of eigenvectors of \\(A\\) and \\(D\\) is the spectrum, then matrix exponential is the same as taking exponents of eigenvalues of \\(A\\) while keeping the eigenbasis invariant. Exchangable matrices. If \\(A_1\\) and \\(A_2\\) are exchangeable, that is, \\(A_1A_2=A_2A_1\\), then \\(e^{A_1+A_2}=e^{A_1}e^{A_2}\\) (exercise!). Nilpotent matrix. If \\(N\\) is a nilpotent matrix, which means that \\(N^K=0\\) for some integer \\(K\\), then \\(e^N=I+N+\\frac12N^2+\\ldots+\\frac1{(K-1)!}N^{K-1}.\\) Any matrix with Jordan canonical form (*This is out of scope of this textbook. We leave it for readers with interest). Any matrix \\(A\\) can be decomposed as \\(P(D+N)P^{-1}\\), where \\(D\\) is diagonal and exchangeable with the nilpotent matrix \\(N\\). Then based on the above discussions, \\(e^A=P(e^De^N)P^{-1}\\). Projection matrix \\(A^2=A\\). Then \\(e^A=I+(e-1)A\\) (exercise!). A.1.2 Gradients In matrix calculus, index may not be consistent in different references. It should be noted that in neural network literature, gradients may have a transpose on our results here. For any function \\(f(X):\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}\\) of an \\(m\\)-by-\\(n\\) matrix \\(X\\) (if \\(n=1\\) then it is a vector), the gradient \\(\\nabla f(X)\\) is another \\(m\\)-by-\\(n\\) matrix with the \\((i,j)\\)-th entry \\(\\frac{\\partial f(X)}{\\partial X_{ij}}\\). For any function \\(f(x):\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^n\\) that maps an \\(m\\)-dimensional vector \\(x\\) to \\(n\\)-dimensional space, \\(\\nabla f(x)\\) is an \\(m\\)-by-\\(n\\) matrix with the \\((i,j)\\)-th entry \\(\\frac{\\partial (f(x))_j}{\\partial x_i}\\). Some important examples include: Vector inner-product. \\(\\nabla(a^\\top x)=a\\). Quadratic form \\(\\nabla(x^\\top Ax) = (A+A^\\top)x\\). Composition. Suppose \\(f(y):\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^k\\) and \\(g(x):\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^n\\), then \\(\\nabla f(g(x))\\) is an \\(m\\)-by-\\(k\\) matrix \\(\\nabla g(x) \\nabla f(y=g(x))\\). A.2 Solving an Ordinary Differential Equation A differential equation is an equation with a function and its derivatives. Compared to partial differential equation (PDE) that consists of partial derivatives, throughout this textbook we will mainly focus on ordinary differential equations (ODEs), including but not limited to, the solutions, convergence and stability analysis. An example of ODE looks like this: \\[\\begin{equation} x+\\frac{dx}{dt}=5t,\\quad t\\in[0,T]. \\tag{A.1} \\end{equation}\\] Definition A.3 (Ordinary Differential Equation) In general, an ODE of order \\(k\\), or, a \\(k\\)-th order ODE, is in the form of \\[ F(t, x, x^\\prime, \\ldots, x^{(k)})=0.\\] Further, if \\(F\\) is linear in \\(x,x&#39;,\\ldots,x^{(k)}\\), we call it a linear ODE. If a linear ODE with \\(F=x^{(k)} + \\sum_{i=0}^{k-1}a_i(t)x^{(i)}\\) that does not independently relate to \\(t\\), we call it homogeneous. Note that \\(x(t)\\equiv 0\\) will always be a trivial solution for homogeneous ODE. For example, the equation (A.1) is a linear ODE but not homogeneous. The solution, a function \\(x=x(t)\\) is not unique in general, and additional conditions are required. For example, we can have one of the following conditions for the above ODE: Initial condition, e.g., \\(x(0)=0\\) gives the constraint that the function at initial time \\(t=0\\) starts at zero point. End-point condition, e.g., \\(x(T)=0\\) implies that the dynamics should end at zero. (Initial) velocity condition, e.g., \\(\\frac{dx}{dt}|_{t=0}=0\\) suggests that at the start time the “slope”/“velocity” of \\(x\\) is zero. For higher-order ODEs, the conditions may be much more complicated to guarantee uniqueness. A.2.1 Separation of Variables A.2.2 First-order Linear ODE A.2.3 Gronwall Inequality A.2.4 Matlab "],["appconvex.html", "B Convex Analysis and Optimization B.1 Theory B.2 Practice", " B Convex Analysis and Optimization B.1 Theory B.1.1 Sets Convex set is one of the most important concepts in convex optimization. Checking convexity of sets is crucial to determining whether a problem is a convex problem. Here we will present some definitions of some set notations in convex optimization. Definition B.1 (Affine set) A set \\(C\\subset \\mathbb{R}^n\\) is affine if the line through any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in \\mathbb{R}\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition B.2 (Convex set) A set \\(C\\subset \\mathbb{R}^n\\) is convex if the line segment between any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition B.3 (Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a cone if for any \\(x\\in C\\) and any \\(\\theta\\geq 0\\), we have \\(\\theta x \\in C\\). Definition B.4 (Convex Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a convex cone if \\(C\\) is convex and a cone. Below are some important examples of convex sets: Definition B.5 (Hyperplane) A hyperplane is a set of the form \\[\\{x|a^Tx = b\\}\\] Definition B.6 (Halfspaces) A (closed) halfspace is a set of the form \\[\\{x|a^Tx \\leq b\\}\\] Definition B.7 (Balls) A ball is a set of the form \\[B(x,r) = \\{y|\\|y-x\\|_2 \\leq r\\} = \\{x+ru|\\|u\\|_2\\leq 1\\}\\] where \\(r &gt;0\\). Definition B.8 (Ellipsoids) A ellipsoid is a set of the form \\[\\mathcal{E} = \\{y|(y-x)^TP^{-1}(y-x)\\leq 1\\}\\] where \\(P\\) is symmetric and positive definite. Definition B.9 (Polyhedra) A polyhedra is defined as the solution set of a finite number of linear equalities and inequalities: \\[\\mathcal{P} = \\{x|a_j^Tx\\leq b_j, j=1,...,m, c_k^Tx=d_k,k=1,...,p\\}\\] Definition B.10 (Norm ball) A norm ball \\(B\\) of radius \\(r\\) and a center \\(x_c\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[B = \\{x|\\|x-x_c\\|\\leq r\\}\\] Definition B.11 (Norm cone) A norm cone \\(C\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[C = \\{(x,t)|\\|x\\|\\leq t\\}\\subset \\mathbb{R}^{n+1}\\] Simplexes are important family of polyhedra. Suppose the \\(k+1\\) points \\(v_0,...,v_k\\in \\mathbb{R}^n\\) are affinely independent, which means \\(v_1-v_0,...,v_k-v_0\\) are linearly independent. Definition B.12 (Simplex) A simplex \\(C\\) defined by points \\(v_0,...,v_k\\) is: \\[C = \\textbf{conv}\\{v_0,...,v_k\\} = \\{\\theta_0v_0 + ... \\theta_kv_k|\\theta \\succeq 0, \\textbf{1}^T\\theta = 1\\}\\] Extremely important examples of convex sets are positive semidefinite cones: Definition B.13 (Symmetric,positive semidefinite,positive definite matrices) Symmetric matrices: \\(\\textbf{S}^n = \\{X\\in\\mathbb{R}^{n\\times n}| X=X^T\\}\\) Symmetric Positive Semidefinite matrices: \\(\\textbf{S}_+^n = \\{X\\in\\textbf{S}^n| X\\succeq0\\}\\) Symmetric Positive definite matrices: \\(\\textbf{S}_{++}^n = \\{X\\in\\textbf{S}^n| X\\succ0\\}\\) In most scenarios, the set we encounter is more complicated. In general it is extermely hard to determine whether a set in convex or not. But if the set is ‘generated’ by some convex sets, we can easily determine its convexity. So let’s focus on operations that preserve convexity: Proposition B.1 Assume \\(S\\) is convex, \\(S_\\alpha,\\alpha\\in\\mathcal{A}\\) is a family of convex sets. Following operations on convex sets will preserve convexity: Intersection: \\(\\bigcap_{\\alpha\\in\\mathcal{A}}S_\\alpha\\) is convex. Image under affine function: A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is affine if it has the form \\(f(x) = Ax+b\\). The image of \\(S\\) under affine function \\(f\\) is convex. I.e. \\(f(S) = \\{f(x)|x\\in S\\}\\) is convex Image under perspective function: We define the perspective function \\(P:\\mathbb{R}^{n+1}\\), with domain \\(\\textbf{dom}P = \\mathbb{R}^n\\times \\mathbb{R}_{++}\\)(where \\(\\mathbb{R}_{++}=\\{x\\in \\mathbb{R}|x&gt;0\\}\\)) as \\(P(z,t) = z/t\\). The image of \\(S\\) under perspective function is convex. Image under linear-fractional function: We define linear fractional function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) as:\\(f(x) = (Ax+b)/(c^Tx+d)\\) with \\(\\textbf{dom}f = \\{x|c^Tx+d&gt;0\\|\\). The image of \\(S\\) under linear fractional functions is convex. In some cases, the restrictions of interior is too strict. For example, imagine a plane in \\(\\mathbb{R}^3\\). The interior of the plane is \\(\\emptyset\\). But intuitively many property should be extended to this kind of situation. Because the points in the plane also lies ‘inside’ the convex set. Thus, we will define relative interior. First we will define affine hull. Definition B.14 (Affine hull) The affine hull of a set \\(S\\) is the smallest affine set that contains \\(S\\), which can be written as: \\[\\text{aff}(S) = \\{\\sum_{i=1}^k\\alpha_ix_i|k&gt;0,x_i\\in S,\\alpha_i\\in\\mathbb{R},\\sum_{i=1}^k\\alpha_i=1\\}\\] Definition B.15 (Relative Interior) The relative interior of a set \\(S\\) (denoted \\(\\text{relint}(S)\\)) is defined as its interior within the affine hull of \\(S\\). I.e. \\[\\text{relint}(S):=\\{x\\in S: \\text{there exists } \\epsilon&gt;0 \\text{ such that }N_\\epsilon \\cap \\text{aff}(S)\\subset S\\}\\] where \\(N_\\epsilon(x)\\) is a ball of radius \\(\\epsilon\\) centered on \\(x\\). B.1.2 Convex function In this section, let’s define convex functions: Definition B.16 (Convex function) A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if \\(\\textbf{dom}\\ f\\) is convex and \\(\\forall x,y\\in \\textbf{dom}\\ f\\) and with \\(\\theta \\in [0,1]\\), we have:\\[f(\\theta x +(1-\\theta)y)\\leq \\theta f(x) + (1-\\theta)f(y)\\] The function is strictly convex if the inequality holds whenever \\(x\\neq y\\) and \\(\\theta\\in (0,1)\\). If a function is differentiable, it will be easier for us to check its convexity: Proposition B.2 (Conditions for Convex function) 1.(First order condition) Suppose \\(f\\) is differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x,y\\in \\textbf{dom} f\\), \\[f(y)\\geq f(x) +\\nabla f(x)^T(y-x)\\] 2.(Second order conditions) Suppose \\(f\\) is twice differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x\\in \\textbf{dom} f\\), \\[\\nabla^2 f(x) \\succeq \\textbf{0}\\] For the same purpose, some operations that preserve the convexity of the convex functions are presented here: Proposition B.3 (Operations that preserve convexity) Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be a convex function and \\(g_1,...,g_n\\) be convex functions. The following operations will preserve convexity of the function: 1.(Nonnegative weighted sum): A nonnegative weighted sum of convex functions: \\[f = \\omega_1f_1 + ... +\\omega_mf_m\\] 2.(Composition with an affine mapping) Suppose \\(A\\in \\mathbb{R}^{n\\times m}\\) and \\(b\\in \\mathbb{R}^n\\), then \\(g(x) = f(Ax+b)\\) is convex. 3.(Pointwise maximum and supremum) \\(g(x) = \\max\\{g_1(x),...,g_n(x)\\}\\) is convex. If \\(h(x,y)\\) is convex in \\(x\\) for each \\(y\\in\\mathcal{A}\\), then \\(\\sup_{y\\in\\mathcal{A}} h(x,y)\\) is also convex in \\(x\\). 4.(Minimization) If \\(h(x,y)\\) is convex in \\((x,y)\\), and \\(C\\) is a convex nonempty set, then \\(\\inf_{x\\in C} h(x,y)\\) is convex in \\(x\\). 5.(Perspective of a function) The perspective of \\(f\\) is the function \\(h:\\mathbb{R}^{n+1}\\to\\mathbb{R}\\) defined by: \\(h(x,t) = tf(x/t)\\) with domain \\(\\textbf{dom}\\ h=\\{(x,t)|x/t\\in\\textbf{dom} f,t&gt;0\\}\\). And \\(h\\) is convex. B.1.3 Lagrange dual We consider an optimization problem in the standard form (without assuming convexity of anything): \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; h_i(x) = 0\\quad i=1,...,p \\\\ \\end{aligned} \\end{equation}\\] Definition B.17 (Lagrange dual function) The Lagrangian related to the problem above is defined as: \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\] The Lagrange dual function is defined as: \\[g(\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}}L(x,\\lambda,\\nu)\\] When the Lagrangian is unbounded below in \\(x\\), the dual function takes on the value \\(-\\infty\\). Note that since the Lagrange dual function is a pointwise infimum of a family of affine functions of \\((\\lambda,\\nu)\\), so it’s concave. The Lagrange dual function will give us lower bounds of the optimal value of the original problem: \\[g(\\lambda,\\nu)\\leq p^*\\]. We can see that, the dual function can give a nontrivial lower bound only when \\(\\lambda\\succeq 0\\). Thus we can solve the following dual problem to get the best lower bound. Definition B.18 (Lagrange dual problem) The lagrangian dual problem is defined as follows: \\[\\begin{equation} \\begin{aligned} d^* = \\quad \\max_{\\lambda,\\nu} \\quad &amp; g(\\lambda,\\nu)\\\\ \\textrm{s.t.} \\quad &amp; \\lambda\\succeq 0 \\end{aligned} \\end{equation}\\] This is a convex optimization problem. We can easily see that \\[d^*\\leq p^*\\] always hold. This property is called weak duality. If \\[d^*=p^*\\], it’s called strong duality. Strong duality does not hold in general, but it usually holfs for convex problems. We can find conditions that guarantee strong duality in convex problems, which are called constrained qualifications. Slater’s constraint qualification is a useful one. Theorem B.1 (Slater's constraint qualification) Strong duality holds for a convex problem \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; Ax=b \\\\ \\end{aligned} \\end{equation}\\] if it is strictly feasible, i.e. \\[\\exists x\\in\\textbf{relint}\\mathcal{D}:\\quad f_i(x)&lt;0,\\quad i=1...m,\\quad Ax=b\\] And the linear inequalities do not need to hold with strict inequality. B.1.4 KKT condition Note that if strong duality holds, denote \\(x^*\\) to be primal optimal, and \\((\\lambda^*,\\nu^*)\\) to be dual optimal. Then: \\[\\begin{equation} \\begin{aligned} f_0(x^*) = g(\\lambda^*,\\nu^*) = &amp; \\inf_x(f_0(x)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x))\\\\ \\leq &amp; f_0(x^*)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x)\\\\ \\leq &amp; f_0(x^*)\\\\ \\end{aligned} \\end{equation}\\] from this, combining \\(\\lambda^*\\geq 0\\) and \\(f_i(x^*)\\leq 0\\), we can know that: \\(\\lambda_i^*f_i(x^*)=0\\quad i=1\\cdots m\\). This means for \\(\\lambda_i^*\\) and \\(f_i(x^*)\\), one of them must be zero, which is known as complementary slackness). Thus we arrived at the following four conditions, which are called KKT conditions. Theorem B.2 (Karush-Kuhn-Tucker(KKT) Conditions) The following four conditions are called KKT conditions (for a problem with differentiable \\(f_i,h_i\\)) Primal feasible: \\(f_i(x) \\leq 0,i,\\cdots ,m,\\ h_i(x) = 0,i=1,\\cdots ,p\\) Dual feasible: \\(\\lambda\\succeq0\\) Complementary slackness: \\(\\lambda_if_i(x)=0,i=1,\\cdots,m\\) Gradient of Lagrangian with respect to \\(x\\) vanishes:\\(\\nabla f_0(x)+\\sum_{i=1}^m\\lambda_i\\nabla f_i(x)+\\sum_{i=1}^p\\nu_i\\nabla h_i(x) = 0\\) From the discussion above, we know that if strong duality holds and \\(x,\\lambda,\\nu\\) are optimal, then they must satisfy the KKT conditions. Also if \\(x,\\lambda,\\nu\\) satisfy KKT for a convex problem, then they are optimal. However, the converse is not generally true, since KKT condition implies strong duality. If Slater’s condition is satisfied, then \\(x\\) is optimal if and only if there exist \\(\\lambda,\\nu\\) that satisfy KKT conditions. Sometimes, by solving the KKT system, we can derive the closed-form solution of a optimization directly. Also, sometimes we will use the residual of the KKT system as the termination condition. In general, \\(f_i,h_i\\) may not be differentiable. There are also KKT conditions for them, which will include knowledge of subdifferential and will not be included here. B.2 Practice B.2.1 CVX Introduction In the last section, we have learned basic concepts and theorems in convex optimization. In this section, on the other hand, we will introduce you how to model basic convex optimization problems with CVX, an easy-to-use MATLAB package. To install CVX, please refer to this page. Note that every time you what to use the CVX package, you should add it to your MATLAB path. For example, if I install CVX package in the parent directory of my current directory with default directory name cvx, the following line should be added before your CVX codes: addpath(genpath(&quot;../cvx/&quot;)); With CVX, it is incredibly easy for us to define and solve a convex optimization problem. You just need to: define the variables. define the objective function you want to minimize or maximize. define the constraints. After running your codes, the optimal objective value is stored in the variable cvx_optval, and the problem status is stored in the variable cvx_status (when your problem is well-defined, this variable’s value will be Solved). The optimal solutions will be stored in the variables you define. Throughout this section, we will study five types of convex optimization problems: linear programming (LP), quadratic programming (QP), (convex) quadratically constrained quadratic programming (QCQP), second-order cone programming (SOCP), and semidefinite programming (SDP). Given two types of optimization problems \\(A\\) and \\(B\\), we say \\(A &lt; B\\) if \\(A\\) can always be converted to \\(B\\) while the inverse is not true. Under this notation, we have \\[\\begin{equation*} \\text{LP} &lt; \\text{QP} &lt; \\text{QCQP} &lt; \\text{SOCP} &lt; \\text{SDP} \\end{equation*}\\] B.2.2 Linear Programming (LP) Definition. An LP has the following form: \\[\\begin{equation} \\tag{B.1} \\begin{aligned} \\min_{x \\in \\mathbb{R}^n} &amp; \\ c^T x \\\\ \\text{subject to } &amp; A x \\le b \\end{aligned} \\end{equation}\\] where \\(x\\) is the variable, \\(A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m\\), and \\(c \\in \\mathbb{R}^n\\) are the parameters. Note that the constraint \\(A x \\le b\\) already incorporates linear equality constraints. To see this, consider the constraint \\(A&#39; x = b&#39;\\), we can reformulate it as \\(A x \\le b\\) by \\[\\begin{equation*} \\begin{bmatrix} A&#39; \\\\ -A&#39; \\end{bmatrix} x \\le \\begin{bmatrix} b&#39; \\\\ -b&#39; \\end{bmatrix} \\end{equation*}\\] Example. Consider the problem of minimizing a linear function \\(c_1 x_1 + c_2 x_2\\) over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). We can convert it to the standard LP form in (B.1) by simply setting \\(c\\) as \\([c_1, \\ c_2]^T\\) and the linear inequality constraint as \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the LP example setting c1 = 2; c2 = -5; l1 = 3; l2 = 7; % parameters: c, A, b c = [c1; c2]; A = [1, 0; -1, 0; 0, 1; 0, -1]; b = [l1; l1; l2; l2]; %% solve LP cvx_begin variable x(2); % define variables [x1, x2] minimize(c&#39; * x); % define the objective subject to A * x &lt;= b; % define the linear constraint cvx_end B.2.3 Quadratic Programming (QP) Definition. A QP has the following form: \\[\\begin{align} \\tag{B.2} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P x + q^T x \\\\ \\text{subject to } &amp; Gx \\le h \\\\ &amp; Ax = b \\end{align}\\] where \\(P \\in \\mathcal{S}_+^n, q\\in \\mathbb{R}^n, G \\in \\mathbb{R}^{m \\times n}, h\\in \\mathbb{R}^m, A \\in \\mathbb{R}^{p \\times n}, b \\in \\mathbb{R}^p\\). Here \\(\\mathcal{S}_+^n\\) denotes the set of positive semidefinite matrices of size \\(n\\times n\\). Obviously, if we set \\(P\\) as zero, QP will degenerate to LP. Example. Consider the problem of minimizing a quadratic function \\[\\begin{equation*} f(x_1, x_2) = p_1 x_1^2 + 2p_2 x_1 x_2 + p_3 x_2^2 + q_1 x_1 + q_2 x_2 \\end{equation*}\\] over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). Since \\(P = 2 \\begin{bmatrix} p_1 &amp; p_2 \\\\ p_2 &amp; p_3 \\end{bmatrix} \\succeq 0\\), the following two conditions must hold: \\[\\begin{equation*} \\begin{cases} p_1 \\ge 0 \\\\ p_1 p_3 - 4 p_2^2 \\ge 0 \\end{cases} \\end{equation*}\\] Same as in the LP example, \\(G\\) and \\(h\\) can be expressed as: \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the QP example setting p1 = 2; p2 = 0.5; p3 = 4; q1 = -3; q2 = -6.5; l1 = 2; l2 = 2.5; % check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0); tmp2 = (p1*p3 - 4*p2^2 &gt;= 0); if ~(tmp1 &amp;&amp; tmp2) error(&quot;P is not positve semidefinite!&quot;); end % parameters: P, q, G, h P = 2 * [p1, p2; p2, p3]; q = [q1; q2]; G = [1, 0; -1, 0; 0, 1; 0, -1]; h = [l1; l1; l2; l2]; %% Solve the QP problem cvx_begin variable x(2); % define variables [x1; x2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P) + q&#39; * x; minimize(obj); subject to G * x &lt;= h; % define the linear constraint cvx_end B.2.4 Quadratically Constrained Quadratic Programming (QCQP) Definition. An (convex) QCQP has the following form: \\[\\begin{align} \\tag{B.3} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P_0 x + q_0^T x \\\\ \\text{subject to } &amp; \\frac{1}{2} x^T P_i x + q_i^T x + r_i \\le 0, \\ i = 1 \\dots m \\\\ &amp; Ax = b \\end{align}\\] where \\(P_i \\in \\mathcal{S}_+^n, i = 0 \\dots m\\), \\(q_i \\in \\mathbb{R}^n, i = 0 \\dots m\\), \\(A \\in \\mathbb{R}^{p \\times n}\\), and \\(b \\in \\mathbb{R}^p\\). Note that in other literature, you may find a more general form of QCQP: they don’t require \\(P_i\\)’s to be positive semidefinite. Yet in this case, the problem is non-convex and beyond our scope. Example. We study the problem of getting the minimum distance between two ellipses. By convention, when the ellipses overlap, we set the minimum distance as \\(0\\). This problem can be exactly solved by (convex) QCQP. Consider two ellipses of the following form: \\[\\begin{equation*} \\begin{cases} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 \\le 0 \\\\ \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 \\le 0 \\\\ \\end{cases} \\end{equation*}\\] where \\([y_1, z_1]^T\\) and \\([y_2, z_2]^T\\) are arbitrary points inside the two ellipses respectively. Also, two ensure the ellipses are well defined, we should enforce the following properties in \\((K_i, k_i, c_i), i = 1, 2\\): (1) \\(K_i \\succ 0\\); (2) Let \\(K_i = L_i L_i^T\\) be the Cholesky decomposition of \\(K_i\\). Then, ellipse \\(i\\) can be rewritten as: \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^T \\begin{bmatrix} y_i \\\\ z_i \\end{bmatrix} - L_i^{-1} k_i \\parallel^2 \\le \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i \\end{equation*}\\] Thus, \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i &gt; 0 \\end{equation*}\\] With these two assumptions, we want to minimize: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 \\end{equation*}\\] Now, we construct \\(P, q, r\\)’s in QCQP with the above parameters. Define the variable \\(x\\) as \\([y_1, z_1, y_2, z_2]\\). \\(P_0\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 = \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix}^T \\begin{bmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix} \\end{equation*}\\] \\(P_1, q_1, r_1\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 = \\frac{1}{2} x^T \\begin{bmatrix} K_1 &amp; O \\\\ O &amp; O \\end{bmatrix} + \\begin{bmatrix} k_1 \\\\ O \\end{bmatrix}^T x + c_1 \\le 0 \\end{equation*}\\] \\(P_2, q_2, r_2\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 = \\frac{1}{2} x^T \\begin{bmatrix} O &amp; O \\\\ O &amp; K_2 \\end{bmatrix} + \\begin{bmatrix} O \\\\ k_2 \\end{bmatrix}^T x + c_2 \\le 0 \\end{equation*}\\] The corresponding codes are shown below. In this example, we test the minimum distance between a circle \\(y_1^2 + z_1^2 \\le 1\\) and another circle \\((y_2 - 2)^2 + (z_2 - 2)^2 \\le 1\\). You can check whether the result from QCQP aligns with your manual calculation. %% Define the QCQP example setting K1 = eye(2); k1 = zeros(2, 1); c1 = -0.5; K2 = eye(2); k2 = [2; 2]; c2 = 3.5; if ~(if_ellipse(K1, k1, c1) &amp;&amp; if_ellipse(K2, k2, c2)) error(&quot;The example setting is not correct&quot;); end % define parameters P0, P1, P2, q1, q2, r1, r2 P0 = [1,0,-1,0; 0,1,0,-1; -1,0,1,0; 0,-1,0,1]; P1 = zeros(4, 4); P1(1:2, 1:2) = K1; P2 = zeros(4, 4); P2(3:4, 3:4) = K2; q1 = [k1; zeros(2, 1)]; q2 = [zeros(2, 1); k2]; r1 = c1; r2 = c2; %% Solve the QCQP problem cvx_begin variable x(4); % define variables [y1; z1; y2; z2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P0); minimize(obj); subject to 0.5 * quad_form(x, P1) + q1&#39; * x + r1 &lt;= 0; 0.5 * quad_form(x, P2) + q2&#39; * x + r2 &lt;= 0; cvx_end %% detect whether (K, k, c) generates a ellipse function flag = if_ellipse(K, k, c) L = chol(K); radius_square = 0.5 * norm(L \\ k)^2 - c; % L \\ k = inv(L) * k flag = (radius_square &gt; 0); end B.2.5 Second-Order Cone Programming (SOCP) Definition. An SOCP has the following form: \\[\\begin{align} \\tag{B.4} \\min_{x \\in \\mathbb{R}^n} \\ &amp; f^T x \\\\ \\text{subject to } &amp; || A_i x + b_i ||_2 \\le c_i^T x + d_i, \\ i = 1 \\dots m \\\\ &amp; Fx = g \\end{align}\\] where \\(f \\in \\mathbb{R}^n, A_i \\in \\mathbb{R}^{n_i \\times n}, b_i \\in \\mathbb{R}^{n_i}, c_i \\in \\mathbb{R}^n, d_i \\in \\mathbb{R}, F \\in \\mathbb{R}^{p \\times n}\\), and \\(g \\in \\mathbb{R}^p\\). Example. We consider the problem of stochastic linear programming: \\[\\begin{align} \\min_x \\ &amp; c^T x \\\\ \\text{subject to } &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p, \\ i = 1 \\dots m \\\\ &amp; a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i), \\ i = 1 \\dots m \\end{align}\\] Here \\(p\\) should be more than \\(0.5\\). We show that this problem can be converted to a SOCP: Since \\(a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i)\\), then \\((a_i^T x - b_i) \\sim \\mathcal{N}(\\bar{a}_i^T x - b_i, x^T \\Sigma_i x)\\). Standardize it: \\[\\begin{equation*} t := ||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1} \\left\\{ (a_i^T x - b_i) - (\\bar{a}_i^T x - b_i) \\right\\} \\sim \\mathcal{N}(0, 1) \\end{equation*}\\] Then, \\[\\begin{align} \\mathbb{P}(a_i^T x \\le b_i) &amp; = \\mathbb{P}(a_i^T x - b_i \\le 0) \\\\ &amp; = \\mathbb{P}(t \\le -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\\\ &amp; = \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\end{align}\\] Here \\(\\Phi(\\cdot)\\) is the cumulative distribution function of the standard normal distribution: \\[\\begin{equation*} \\Phi(\\xi) = \\int_{-\\infty}^{\\xi} e^{-\\frac{1}{2} t^2} \\ dt \\end{equation*}\\] Thus, \\[\\begin{align} &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p \\\\ \\Longleftrightarrow &amp; \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\ge p \\\\ \\Longleftrightarrow &amp; -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i) \\ge \\Phi^{-1}(p) \\\\ \\Longleftrightarrow &amp; \\Phi^{-1}(p) ||\\Sigma_i^{\\frac{1}{2}} x||_2 \\le b_i - \\bar{a}_i^T x \\end{align}\\] which is exactly the same as inequality constraints in SOCP formulation. (You can see why we enforce \\(p &gt; 0.5\\) here: otherwise \\(\\Phi^{-1}(p)\\) will be negative and the constraint with not be an second-order cone.) In the following code example, we set up four inequality constraints and let \\(\\bar{a}_i^T x \\le b_i, \\ i = 1 \\dots 4\\) form an square located at the origin of size \\(2\\). Then, for convenience, we set \\(\\Sigma_i \\equiv \\sigma^2 I\\). %% Define the SOCP example setting bar_a1 = [1; 0]; b1 = 1; bar_a2 = [0; 1]; b2 = 1; bar_a3 = [-1; 0]; b3 = 1; bar_a4 = [0; -1]; b4 = 1; sigma = 0.1; c = [2; 3]; p = 0.9; % p should be more than 0.5 Phi_inv = norminv(p); % get Phi^{-1}(p) %% Solve the SOCP problem cvx_begin variable x(2); % define variables [x1; x2] minimize(c&#39; * x); subject to sigma*Phi_inv * norm(x) &lt;= b1 - bar_a1&#39; * x; sigma*Phi_inv * norm(x) &lt;= b2 - bar_a2&#39; * x; sigma*Phi_inv * norm(x) &lt;= b3 - bar_a3&#39; * x; sigma*Phi_inv * norm(x) &lt;= b4 - bar_a4&#39; * x; cvx_end B.2.6 Semidefinite Programming (SDP) Definition. An SDP has the following form: \\[\\begin{align} \\tag{B.5} \\min_{X_i, x_i} \\ &amp; \\sum_{i=1}^{n_s} C_i \\cdot X_i + \\sum_{i=1}^{n_u} c_i \\cdot x_i \\\\ \\text{subject to } &amp; \\sum_{i=1}^{n_s} A_{i,j} \\cdot X_i + \\sum_{i=1}^{n_u} a_{i,j} \\cdot x_i = b_j, \\quad j = 1 \\dots m \\\\ &amp; X_i \\in \\mathcal{S}_+^{D_i}, \\quad i = 1 \\dots n_s \\\\ &amp; x_i \\in \\mathbb{R}^{d_i}, \\quad i = 1 \\dots n_u \\end{align}\\] where \\(C_i, A_{i, j} \\in \\mathbb{R}^{D_i \\times D_i}\\), \\(c_i, a_{i, j} \\in \\mathbb{R}^{d_i}\\), and \\(\\cdot\\) means element-wise product. For two square matrices \\(A, B\\), the dot product \\(A \\cdot B\\) is equal to \\(\\text{tr}(A B)\\); for two vectors \\(a, b\\), the dot product \\(a \\cdot b\\) is the same as inner product \\(a^T b\\). Note that actually there are many “standard” forms of SDP. For example, in the convex optimization theory part, you may find an SDP that looks like: \\[\\begin{align} \\min_X \\ &amp; C \\cdot X \\\\ \\text{subject to } &amp; A \\cdot X = b \\\\ &amp; X \\succeq 0 \\end{align}\\] It is convenient for us to analyze the theoretical properties of SDP with this form. Also, in SDP solvers’ User Guide, you may see more complex SDP forms which involve more general convex cones. For example, see MOSEK’s MATLAB API docs. Here we turn to use the form of (B.5) for two reasons: (1) it is general enough: our SDP example below can be converted to this form (also, SDPs from sum-of-squares programming in this book are exactly of the form (B.5)); (2) it is more readable than more complex forms. Example. We consider the problem of finding the minimum eigenvalue for a positive semidefinite matrix \\(S\\). We will show that this problem can be converted to (B.5). Since \\(S\\) is positive semidefinite, the finding procedure can be cast as \\[\\begin{align} \\max_\\lambda &amp; \\ \\lambda \\\\ \\text{subject to } &amp; S - \\lambda I \\succeq 0 \\end{align}\\] Now define an auxiliary matrix \\(X := S - \\lambda I\\). We have \\[\\begin{align} \\min_{\\lambda, X} &amp; \\ -\\lambda \\\\ \\text{subject to } &amp; X + \\lambda I = S \\\\ &amp; X \\succeq 0 \\end{align}\\] It is obvious that the linear matrix equality constraint \\(X + \\lambda I = S\\) can be divided into several linear scalar equality constraints in (B.5). For example, we consider \\(S \\in \\mathbb{S}_+^3\\). Thereby \\(X + \\lambda I = S\\) will lead to \\(6\\) linear equality constraints (We don’t consider \\(X\\) is a symmetric matrix here, since most solvers will implicitly consider this. Thus, only the upper-triangular part of \\(X\\) and \\(S\\) are actually used in the equality construction.): \\[\\begin{align} &amp; \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[0, 0], \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 2] \\\\ &amp; \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[1, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[1, 2], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\cdot X + \\lambda = S[2, 2] \\end{align}\\] Seems tedious? Fortunately, CVX provides a high-level API to handle these linear equality constraints: you just need to write down X + lam * eye(3) == S; % linear equality constraints: X + lam *I = S CVX will autometically convert this high-level constraint to (B.5) and pass them to the underlying solver. To generate a ramdom \\(S \\in \\mathcal{S}_+^3\\), you just need to assign three nonnegative eigenvalues to the program. After that, an random \\(S\\) will be generated by \\(S = Q \\ \\text{diag}(\\lambda_1, \\lambda_2, \\lambda_3) \\ Q^T\\), where \\(Q\\) is random orthonormal matrix. %% Define the SDP example setting lam_list = [0.7; 2.4; 3.7]; S = generate_random_PD_matrix(lam_list); % get a PD matrix S %% Solve the SDP problem cvx_begin variable X(3, 3) symmetric; variable lam; maximize(lam); subject to % here &quot;==&quot; should be read as &quot;is in&quot; X == semidefinite(3); X + lam * eye(3) == S; cvx_end % this function help to generate PD matrix of size 3*3 % if you provide the eigenvalues [lam_1, lam_2, lam_3] function S = generate_random_PD_matrix(lam_list) if ~all(lam_list &gt;= 0) % all eigenvalues &gt;= 0 error(&quot;All eigenvalues must be nonnegative.&quot;); end D = diag(lam_list); % use QR factorization to generate a random orthonormal matrix Q [Q, ~] = qr(rand(3, 3)); S = Q * D * Q&#39;; end B.2.7 CVXPY Introduction and Examples Apart from CVX MATLAB, we also have a Python package called CVXPY, which functions almost the same as CVX MATLAB. To define and solve a convex optimization problem CVXPY, basically, there are three steps (apart from importing necessary packages): Step 1: Define parameters and variables in a certain type of convex problem. Here variables are what you are trying to optimize or “learn”. Parameters are the “coefficients” of variables in the objective and constraints. Step 2: Define the objective function and constraints. Step 3: Solve the problem and get the results. Here we provide the CVXPY codes for the above five convex optimization examples. B.2.7.1 LP import cvxpy as cp import numpy as np ## Define the LP example setting c1 = 2 c2 = -5 l1 = 3 l2 = 7 ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: c, A, b c = np.array([c1, c2]) A = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) b = np.array([l1, l1, l2, l2]) ## Step 2: define objective and constraints obj = cp.Minimize(c.T @ x) constraints = [A @ x &lt;= b] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x B.2.7.2 QP import cvxpy as cp import numpy as np ## Define the LP example setting p1 = 2 p2 = 0.5 p3 = 4 q1 = -3 q2 = -6.5 l1 = 2 l2 = 2.5 # check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0) tmp2 = (p1*p3 - 4*p2**2 &gt;= 0) assert(tmp1 and tmp2, &quot;P is not positve semidefinite!&quot;) ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: P, q, G, h P = 2*np.array([[p1, p2], [p2, p3]]) q = np.array([q1, q2]) G = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) h = np.array([l1, l1, l2, l2]) ## Step 2: define the objective and constraints fx = 0.5 * cp.quad_form(x, P) + q.T @ x obj = cp.Minimize(fx) constraints = [G @ x &lt;= h] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve the problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x B.2.7.3 QCQP import cvxpy as cp import numpy as np from numpy.linalg import cholesky, inv, norm ## Define the QCQP example setting def if_ellipse(K, k, c): # examine whether 0.5*x^T K x + k^T x + c &lt;= 0 is a ellipse # if K is not positive semidefinite, Cholesky will raise an error L = cholesky(K) radius_square = 0.5 * norm(inv(L) @ k)**2 - c return radius_square &gt; 0 K1 = np.eye(2) k1 = np.zeros(2) c1 = -0.5 K2 = np.array([[1, 0], [0, 1]]) k2 = np.array([2, 2]) c2 = 3.5 if not (if_ellipse(K1, k1, c1) and if_ellipse(K2, k2, c2)): raise ValueError(&quot;The example setting is not correct&quot;) ## Step 1: define variables and parameters P0 = np.array([[1,0,-1,0], [0,1,0,-1], [-1,0,1,0], [0,-1,0,1]]) P1 = np.zeros((4,4)) P1[:2, :2] = K1 P2 = np.zeros((4,4)) P2[2:, 2:] = K2 q1 = np.concatenate([k1, np.zeros(2)]) q2 = np.concatenate([np.zeros(2), k2]) r1 = c1 r2 = c2 ## Step 2: define objective and constraints x = cp.Variable(4) # variable: x = [y1, z1, y2, z2]^T fx = 0.5 * cp.quad_form(x, P0) obj = cp.Minimize(fx) con1 = (0.5 * cp.quad_form(x, P1) + q1.T @ x + r1 &lt;= 0) # ellipse 1 con2 = (0.5 * cp.quad_form(x, P2) + q2.T @ x + r2 &lt;= 0) # ellipse 2 constraints = [con1, con2] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x B.2.7.4 SOCP import cvxpy as cp import numpy as np from scipy.stats import norm ## Define the SOCP example setting # define bar_ai, bi (i = 1, 2, 3, 4) bar_a1 = np.array([1, 0]) b1 = 1 bar_a2 = np.array([0, 1]) b2 = 1 bar_a3 = np.array([-1, 0]) b3 = 1 bar_a4 = np.array([0, -1]) b4 = 1 sigma = 0.1 c = np.array([2, 3]) p = 0.9 # p should be more than 0.5 ## Step 1: define variables and parameters Phi_inv = norm.ppf(p) # get Phi^{-1}(p) ## Step 2: define objective and constraints x = cp.Variable(2) # variable: x = [x1, x2]^T obj = cp.Minimize(c.T @ x) # use cp.SOC(t, x) to create the SOC constraint ||x||_2 &lt;= t constraints = [ cp.SOC(b1 - bar_a1.T @ x, sigma*Phi_inv*x), cp.SOC(b2 - bar_a2.T @ x, sigma*Phi_inv*x), cp.SOC(b3 - bar_a3.T @ x, sigma*Phi_inv*x), cp.SOC(b4 - bar_a4.T @ x, sigma*Phi_inv*x), ] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x B.2.7.5 SDP import cvxpy as cp import numpy as np from scipy.stats import ortho_group ## Define the SDP example setting # this function help to generate PD matrix of size 3*3 # if you provide the eigenvalues [lam_1, lam_2, lam_3] def generate_random_PD_matrix(lam_list): assert np.all(lam_list &gt;= 0) # all eigenvalues &gt;= 0 # S = Q @ D @ Q.T D = np.diag(lam_list) Q = ortho_group.rvs(3) return Q @ D @ Q.T lam_list = np.array([0.5, 2.4, 3.7]) S = generate_random_PD_matrix(lam_list) # get a PD matrix S ## Step 1: define variables and parameters # get coefficients for equality constraints A_00 = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_00 @ X) + lam = S_00 A_01 = np.array([[0, 1, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_01 @ X) = S_01 A_02 = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]]) # tr(A_02 @ X) = S_02 A_11 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]) # tr(A_11 @ X) + lam = S_11 A_12 = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 0]]) # tr(A_12 @ X) = S_12 A_22 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1]]) # tr(A_22 @ X) + lam = S_22 ## Step 2: define objective and constraints # define a PD matrix variable X of size 3*3 X = cp.Variable((3, 3), symmetric=True) constraints = [X &gt;&gt; 0] # the operator &gt;&gt; denotes matrix inequality lam = cp.Variable(1) constraints += [ cp.trace(A_00 @ X) + lam == S[0,0], cp.trace(A_01 @ X) == S[0,1], cp.trace(A_02 @ X) == S[0,2], cp.trace(A_11 @ X) + lam == S[1,1], cp.trace(A_12 @ X) == S[1,2], cp.trace(A_22 @ X) + lam == S[2,2], ] obj = cp.Minimize(-lam) prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, lam.value) # optimal lam "],["app-lti-system-theory.html", "C Linear System Theory C.1 Stability C.2 Controllability and Observability C.3 Stabilizability And Detectability", " C Linear System Theory Thanks to Shucheng Kang for writing this Appendix. C.1 Stability C.1.1 Continuous-Time Stability Consider the continuous-time linear time-invariant (LTI) system \\[\\begin{equation} \\dot{x} = A x. \\tag{C.1} \\end{equation}\\] the system is said to be “diagonalizable” if \\(A\\) is diagonalizable. Definition C.1 (Asymptotic and Marginal Stability) The diagonalizable, LTI system (C.1) is “asymptotically stable” if \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “marginally stable” if \\(x(t) \\nrightarrow 0\\) but remains bounded as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “stable” if it is either asymptotically or marginally stable “unstable” if it is not stable One can show that \\(A\\)’s eigenvalues determine the LTI system’s stability, as the following Theorem states: Theorem C.1 (Stability of Continuous-Time LTI System) The diagonalizable15, LTI system (C.1) is asymptotically stable if \\(\\text{Re} (\\lambda_i) &lt; 0\\) for all \\(i\\) marginally stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(\\text{Re} (\\lambda_i) = 0\\) stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) unstable if \\(\\text{Re} (\\lambda_i) &gt; 0\\) for at least one \\(i\\) Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(\\dot{x} = Ax\\) can be restated as \\(\\dot{z} = \\Lambda z\\). Consider the \\(i\\)’s component of \\(z\\): \\[\\begin{equation*} \\dot{z}_i = \\lambda_i z_i \\Longrightarrow z_i(t) = e^{\\lambda_i t} z_i(0) \\end{equation*}\\] Since \\(\\text{Re}(\\lambda_i) &lt; 0\\), \\(z_i(t)\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_i(0)\\). C.1.2 Discrete-Time Stability Now consider the diagonalizable, discrete-time linear time-invariant (LTI) system \\[\\begin{equation} x_{t+1} = A x_t. \\tag{C.2} \\end{equation}\\] Theorem C.2 (Stability of Discrete-Time LTI System) The diagonalizable, discrete-time LTI system (C.2) is asymptotically stable if \\(|\\lambda_i| &lt; 1\\) for all \\(i\\) marginally stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(|\\lambda_i| = 1\\) stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) unstable if \\(|\\lambda_i| &gt; 1\\) for at least one \\(i\\). Note that \\(|\\lambda_i| &lt; 1\\) means the eigenvalue lies strictly inside the unit circle in the complex plane. Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(x_{t+1} = Ax\\) can be restated as \\(z_{t+1} = \\Lambda z_t\\). Expanding the recursion, we have \\[\\begin{equation*} z_{t} = \\Lambda^{t-1} z_0 \\Longrightarrow z_{t,i} = \\lambda_i^{t-1} z_{0,i} \\end{equation*}\\] Since \\(|\\lambda_i| &lt; 1\\), \\(z_{t,i}\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_{0,i}\\). C.1.3 Lyapunov Analysis Theorem C.3 (Lyapunov Equation) The following is equivalent for a linear time-invariant system \\(\\dot{x} = A x\\) The system is globally asymptotically stable, i.e., \\(A\\) is Hurwitz and \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) regardless of the initial condition; For any positive definite matrix \\(Q\\), the unique solution \\(P\\) to the Lyapunov equation \\[\\begin{equation} A^T P + P A = -Q \\tag{C.3} \\end{equation}\\] is positive definite. Proof. (a): \\(2 \\Rightarrow 1\\). Suppose we are given two positive definite matrices \\(P, Q \\succ 0\\) that satisfies the Lyapunov equation (C.3). Define a scalar function \\[ V(x) = x^T P x. \\] It is clear that \\(V &gt; 0\\) for any \\(x \\neq 0\\) and \\(V(x) = 0\\) (i.e., \\(V(x)\\) is positive definite). We also see \\(V(x)\\) is radially unbounded because: \\[ V(x) \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2 \\Rightarrow \\lim_{x \\rightarrow \\infty} V(x) \\rightarrow \\infty. \\] The time derivative of \\(V\\) reads \\[ \\dot{V} = 2 x^T P \\dot{x} = x^T (A^T P + P A) x = - x^T Q x. \\] Clearly, \\(\\dot{V} &lt; 0\\) for any \\(x \\neq 0\\) and \\(\\dot{V}(0) = 0\\). According to Lyapunov’s global stability theorem 5.3, we conclude the linear system \\(\\dot{x} = Ax\\) is globally asymptotically stable at \\(x = 0\\). (b): \\(1 \\Rightarrow 2\\). Suppose \\(A\\) is Hurwitz, we want to show that, for any \\(Q \\succ 0\\), there exists a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (C.3). In fact, consider the matrix \\[ P = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt. \\] Because \\(A\\) is Hurwitz, the integral exists, and clearly \\(P \\succ 0\\) due to \\(Q \\succ 0\\). To show this choice of \\(P\\) satisfies the Lyapunov equation, we write \\[\\begin{align} A^T P + P A &amp;= \\int_{t=0}^{\\infty} \\left( A^T e^{A^T t} Q e^{At} + e^{A^T t} Q e^{At} A \\right) dt \\\\ &amp;=\\int_{t=0}^{\\infty} d \\left( e^{A^T t} Q e^{At} \\right) \\\\ &amp; = e^{A^T t} Q e^{At}\\vert_{t = \\infty} - e^{A^T t} Q e^{At}\\vert_{t = 0} = - Q, \\end{align}\\] where the last equality holds because \\(e^{A \\infty} = 0\\) (recall \\(A\\) is Hurwitz). To show the uniqueness of \\(P\\), we assume that there exists another matrix \\(P&#39;\\) that also satisfies the Lyapunov equation. Therefore, \\[\\begin{align} P&#39; &amp;= e^{A^T t} P&#39; e^{At} \\vert_{t=0} - e^{A^T t} P&#39; e^{At} \\vert_{t=\\infty} \\\\ &amp;= - \\int_{t=0}^{\\infty} d \\left( e^{A^T t} P&#39; e^{At} \\right) \\\\ &amp;= - \\int_{t=0}^{\\infty} e^{A^T t} \\left( A^T P&#39; + P&#39; A \\right) e^{At} dt \\\\ &amp; = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt = P, \\end{align}\\] leading to \\(P&#39; = P\\). Hence, the solution is unique. Convergence rate estimation. We now show that Theorem C.3 can allow us to quantify the convergence rate of a (stable) linear system towards zero. For a Hurwitz linear system \\(\\dot{x} = Ax\\), let us pick a positive definite matrix \\(Q\\). Theorem C.3 tells us we can find a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (C.3). In this case, we can upper bound the scalar function \\(V = x^T P x\\) as \\[ V \\leq \\lambda_{\\max}(P) \\Vert x \\Vert^2. \\] The time derivative of \\(V\\) is \\(\\dot{V} = - x^T Q x\\), which can be upper bounded by \\[\\begin{align} \\dot{V} &amp; \\leq - \\lambda_{\\min} (Q) \\Vert x \\Vert^2 \\\\ &amp; = - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} \\underbrace{ \\left( \\lambda_{\\max} (P) \\Vert x \\Vert^2 \\right)}_{\\geq V} \\\\ &amp; \\leq - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} V. \\end{align}\\] Denoting \\(\\gamma(Q) = \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max}(P)}\\), the above inequality implies \\[ V(0) e^{-\\gamma(Q) t} \\geq V(t) = x^T P x \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2. \\] As a result, \\(\\Vert x \\Vert^2\\) converges to zero exponentially with a rate at least \\(\\gamma(Q)\\), and \\(\\Vert x \\Vert\\) converges to zero exponentially with a rate at least \\(\\gamma(Q) / 2\\). Best convergence rate estimation. I have used \\(\\gamma (Q)\\) to make it explict that the rate \\(\\gamma\\) depends on the choice of \\(Q\\), because \\(P\\) is computed from the Lyapunov equation as an implicit function of \\(Q\\). Naturally, choosing different \\(Q\\) will lead to different \\(\\gamma (Q)\\). So what is the choice of \\(Q\\) that maximizes the convergence rate estimation? Corollary C.1 (Maximum Convergence Rate Estimation) \\(Q = I\\) maximizes the convergence rate estimation. Proof. let us denote \\(P_0\\) as the solution to the Lyapunov equation with \\(Q = I\\) \\[ A^T P_0 + P_0 A = - I. \\] Let \\(P\\) be the solution corresponding to a different choice of \\(Q\\) \\[ A^T P + P A = - Q. \\] Without loss of generality, we can assume \\(\\lambda_{\\min}(Q) = 1\\), because rescaling \\(Q\\) will recale \\(P\\) by the same factor, which does not affect \\(\\gamma(Q)\\). Subtracting the two Lyapunov equations above we get \\[ A^T (P - P_0) + (P - P_0) A = - (Q - I). \\] Since \\(Q - I \\succeq 0\\) (due to \\(\\lambda_{\\min}(Q) = 1\\)), we know \\(P - P_0 \\succeq 0\\) and \\(\\lambda_{\\max} (P) \\geq \\lambda_{\\max} (P_0)\\). As a result, \\[ \\gamma(Q) = \\frac{\\lambda_{\\min}(Q)}{\\lambda_{\\max}(P)} = \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P)} \\leq \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P_0)} = \\gamma(I), \\] and \\(Q = I\\) maximizes the convergence rate estimation. C.2 Controllability and Observability Consider the following linear time-invariant (LTI) system \\[\\begin{equation} \\tag{C.4} \\begin{split} \\dot{x} = A x + B u \\\\ y = C x + D u \\end{split} \\end{equation}\\] where \\(x \\in \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{R}^m\\) the control input, \\(y \\in \\mathbb{R}^p\\) the output, and \\(A,B,C,D\\) are constant matrices with proper sizes. If we know the initial state \\(x(0)\\) and the control inputs \\(u(t)\\) over a period of time \\(t \\in [0, t_1]\\), the system trajectory \\((x(t), y(t))\\) can be determined as \\[\\begin{equation} \\tag{C.5} \\begin{split} x(t) &amp; = e^{At} x(0) + \\int_{0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau \\\\ y(t) &amp; = C x(t) + D u(t) \\end{split} \\end{equation}\\] To study the internal structure of linear systems, two important properties should be considered: controllability and observability. In the following analysis, we will see that they are actually dual concepts. Their definitions (Chen 1984) are given below. Definition C.2 (Controllability) The LTI system (C.4), or the pair \\((A, B)\\), is controllable, if for any initial state \\(x(0) = x_0\\) and final state \\(x_f\\), there exists a sequence of control inputs that transfer the system from \\(x_0\\) to \\(x_f\\) in finite time. Definition C.3 (Observability) The LTI system (C.4), or the pair \\((C, A)\\), is observable, if for any unknown initial state \\(x(0)\\), there exists a finite time \\(t_1 &gt; 0\\), such that knowing \\(y\\) and \\(u\\) over \\([0, t_1]\\) suffices to determine \\(x(0)\\). Sometimes it will become more convenient for us to analyze the system (C.4) under another coordinate basis, i.e., \\(z = T x\\), where the coordinate transformation \\(T\\) is nonsingular (i.e., full-rank). Define \\(A&#39; = TAT^{-1}, B&#39; = PB, C&#39; = CT^{-1}, D&#39; = D\\), we get \\[\\begin{equation*} \\begin{split} \\dot{z} = A&#39; z + B&#39; u \\\\ y = C&#39; z + D&#39; u \\end{split} \\end{equation*}\\] Since the coordinate transformation only changes the system’s coordinate basis, physical properties like controllability and observability will not change. C.2.1 Cayley-Hamilton Theorem In the analysis of controllability and observability, Cayley Hamilton Theorem lays the foundation. The statement of the theory and its (elegant) proof are given blow. Some useful corollaries are also presented. Theorem C.4 (Cayley-Hamilton) Let \\(A \\in \\mathbb{C}^{n \\times n}\\) and denote the characteristic polynomial of \\(A\\) as \\[ \\text{det}(\\lambda I - A) = \\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n \\in \\mathbb{C}[\\lambda], \\] which is a polynomial in a single variable \\(\\lambda\\) with coefficients \\(a_1,\\dots,a_n\\). Then \\[ A^n + a_1 A^{n-1} + \\dots + a_n I = 0 \\] Proof. Define the adjugate of \\(\\lambda I - A\\) as \\[ B = \\text{adj}(\\lambda I - A) \\] From \\(B\\)’s definition, we have \\[\\begin{equation} (\\lambda I - A) B = \\text{det}(\\lambda I - A) I = (\\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n) I \\tag{C.6} \\end{equation}\\] Also, \\(B\\) is a polynomial matrix over \\(\\lambda\\), whose maximum degree is no more than \\(n - 1\\). Therefore, we write \\(B\\) as follows: \\[ B = \\sum_{i=0}^{n-1} \\lambda^i B_i \\] where \\(B_i\\)’s are constant matrices. In this way, we unfold \\((\\lambda I - A)B\\): \\[\\begin{equation} \\tag{C.7} \\begin{split} (\\lambda I - A) B &amp; = (\\lambda I - A) \\sum_{i=0}^{n-1} \\lambda^i B_i \\\\ &amp; = \\lambda^n B_{n-1} + \\sum_{i=1}^{n-1} \\lambda^i (-A B_i + B_{i-1}) - A B_0 \\end{split} \\end{equation}\\] Since \\(\\lambda\\) can be arbitrarily set, matching the coefficients of (C.6) and (C.7), we have \\[\\begin{equation*} \\begin{split} B_{n-1} &amp; = I \\\\ -A B_i + B_{i-1} &amp; = a_{n-i} I, \\quad i = 1 \\dots n - 1 \\\\ -A B_0 &amp; = a_n I \\end{split} \\end{equation*}\\] Thus, we have \\[\\begin{equation*} \\begin{split} &amp; B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I \\\\ = &amp; I \\cdot A^n + \\sum_{i=1}^{n-1} (a_{n-i} I) \\cdot A^i + (a_n I) \\cdot I \\\\ = &amp; A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I \\end{split} \\end{equation*}\\] On the other hand, one can easily check that \\[ B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I = 0 \\] since each term offsets completely. Therefore, \\[ A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I = 0, \\] concluding the proof. Here are some corollaries of the Cayley-Hamilton Theorem. Corollary C.2 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k \\ge n\\), \\(A^k B\\) is a linear combination of \\(B, AB, A^2B, \\dots, A^{n-1}B\\). Proof. Directly from Cayley Hamilton Theorem, \\(A^n\\) can be expressed as a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). By recursion, it is easy to show that for all \\(m &gt; n\\), \\(A^m\\) is also a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). Post-multiply both sides with \\(B\\), we get what we want. Corollary C.3 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k &gt; n\\), the following equality always holds: \\[ \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix}) = \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix}) \\] Proof. First prove LHS \\(\\le\\) RHS. \\(\\forall v \\in \\mathbb{C}^n\\) such that \\[ v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B &amp; \\dots A^{k-1}B \\end{bmatrix} = 0 \\] \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) must hold. Second prove LHS \\(\\ge\\) RHS. For any \\(v \\in \\mathbb{C}^n\\) such that \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) and any \\(k &gt; n\\), by Corollary C.2, there exists a sequence \\(c_i, i = 0 \\dots n-1\\) satisfy the following: \\[ v^* A^k B = v^* \\sum_{i=0}^{n-1} c_i A^i B = 0 \\] Therefore, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = 0\\). Corollary C.4 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}\\), define \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] If \\(\\text{rank}(\\mathcal{C}) = k_1 &lt; n\\), there exist a similarity transformation \\(T\\) such that \\[ T A T^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, T B = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}, \\bar{B}_c \\in \\mathbb{C}^{k_1 \\times m}\\). Moreover, the matrix \\[\\begin{equation*} \\bar{\\mathcal{C}} := \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c \\end{bmatrix} \\end{equation*}\\] has full row rank. Proof. Since \\(\\mathcal{C}\\) is not full row rank, we pick \\(k_1\\) linearly independent columns from \\(\\mathcal{C}\\). Denote them as \\(q_1\\dots q_{k_1}\\), \\(q_i \\in \\mathbb{C}^n\\). Then, we arbitrarily set other \\(n-k_1\\) vectors \\(q_{k_1+1} \\dots q_{n}\\) as long as \\[ Q = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\] is invertible. Define the similarity transformation matrix by \\(T = Q^{-1}\\). Note that \\(A q_i\\) can be seen as a column picked from \\(A^{k} B, k \\in \\left\\{1 \\dots n\\right\\}\\), which is guaranteed to be a linear combination of \\(B, AB, \\dots, A^{n-1}B\\) from Cayley Hamilton Theorem. Thus, \\(A q_i\\) is bound to be a linear transformation of columns from \\(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = \\mathcal{C}\\). Since \\(q_1\\dots q_{k_1}\\) is the largest linearly independent column vector set from \\(\\mathcal{C}\\), this implies \\(A q_i\\) can be expressed as a linear combination of \\(q_1\\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} A Q &amp; = A T^{-1} = A \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} = T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} \\end{split} \\end{equation*}\\] Similarly, \\(B\\) itself is part of \\(\\mathcal{C}\\). Therefore, each column of \\(B\\) is naturally a linear combination of \\(q_1 \\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} B = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{split} = T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] To see \\(\\bar{\\mathcal{C}}\\) has full row rank, note that \\(\\text{rank} \\mathcal{C} = k_1\\) and \\[\\begin{equation*} \\mathcal{C} = T^{-1} \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\end{equation*}\\] Thus, \\[\\text{rank}\\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\end{bmatrix} = k_1. \\] By Corollary C.3, \\(\\text{rank}\\bar{\\mathcal{C}} = k_1\\). The following Corollary is especially useful in the study of pole assignment in the single-input-multiple-output (SIMO) LTI system. Corollary C.5 For any \\(A \\in \\mathbb{C}^{n \\times n}, b \\in \\mathbb{C}^{n}\\), if \\[\\begin{equation*} \\mathcal{C} = \\begin{bmatrix} b &amp; Ab &amp; \\dots &amp; A^{n-1}b \\end{bmatrix} \\in \\mathbb{C}^{n \\times n} \\end{equation*}\\] has full rank, then there exists a similarity transformation \\(T\\) such that \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(a_1, \\dots, a_n\\) are the coefficients of \\(A\\)’s characteristic polynomial: \\[\\begin{equation*} \\det(A - \\lambda I) = \\lambda^{n} + a_1 \\lambda^{n-1} + \\dots + a_n \\lambda \\end{equation*}\\] Proof. Since \\(\\mathcal{C}\\) is invertible, define its inverse \\[\\begin{equation*} \\mathcal{C}^{-1} = \\begin{bmatrix} M_1 \\\\ M_2 \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] where \\(M_i \\in \\mathbb{C}^{1 \\times n}\\). Then, \\[\\begin{equation*} I = \\mathcal{C}^{-1} \\mathcal{C} = \\begin{bmatrix} M_1 b &amp; M_1 Ab &amp; \\dots &amp; M_1 A^{n-1}b \\\\ M_2 b &amp; M_2 Ab &amp; \\dots &amp; M_2 A^{n-1}b \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} \\Longrightarrow \\begin{cases} M_n A^{n-1}b = 1 \\\\ M_n A^ib = 0, \\ i = 0,\\dots, n-2 \\end{cases} \\end{equation*}\\] Now we claim that the transformation matrix \\(T\\) can be constructed as follows: \\[\\begin{equation*} T = \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] We first show \\(T\\) is invertible by calculating \\(T \\mathcal{C}\\): \\[\\begin{equation*} T \\mathcal{C} = \\begin{bmatrix} M_n A^{n-1}b &amp; \\star &amp; \\dots &amp; \\star \\\\ M_n A^{n-2}b &amp; M_n A^{n-1}b &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\star &amp; \\dots &amp; \\star \\\\ 0 &amp; 1 &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\end{equation*}\\] Then we calculate \\(Tb\\) and \\(TA\\): \\[\\begin{equation*} \\begin{split} Tb &amp; = \\begin{bmatrix} M_n A^{n-1}b \\\\ M_n A^{n-2}b \\\\ \\vdots \\\\ M_n b \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ T A &amp; = \\begin{bmatrix} M_n A^n \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} = \\begin{bmatrix} -M_n \\cdot \\sum_{i=0}^{n-1} a_{n-i} A^i \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\vdots \\\\ M_n A \\\\ M_n \\end{bmatrix} = A_1 T \\end{split} \\end{equation*}\\] where the penultimate equality uses Cayley Hamilton Theorem. C.2.2 Equivalent Statements for Controllability There are a few equivalent statements to express an LTI system’s controllability that one should be familiar with: Theorem C.5 (Equivalent Statements for Controllability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((A, B)\\) is controllable. The matrix \\[ W_c(t) := \\int_{0}^{t} e^{A\\tau} B B^* e^{A^* \\tau} d\\tau \\] is positive definite for any \\(t &gt; 0\\). The controllability matrix \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; A^2 B &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] has full row rank. The matrix \\([A - \\lambda I, B]\\) has full row rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(x\\) be any eigenvalue and any corresponding left eigenvector \\(A\\), i.e., \\(x^* A = x^* \\lambda\\), then \\(x^* B \\ne 0\\). The eigenvalues of \\(A+BF\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(F\\). If, in addition, all eigenvalues of \\(A\\) have negative real parts, then the unique solution of \\[ A W_c + W_c A^* = -B B^* \\] is positive definite. The solution is called the controllability Gramian and can be expressed as \\[ W_c = \\int_{0}^{\\infty} e^{A \\tau} B B^* e^{A^* \\tau} d\\tau \\] Proof. (\\(1. \\Rightarrow 2.\\)) Prove by contradiction. Assume that \\((A, B)\\) is controllable but \\(W_c(t_1)\\) is singular for some \\(t_1 &gt; 0\\). This implies there exists a real vector \\(v \\ne 0 \\in \\mathbb{R}^n\\), s.t. \\[ v^* W_c(t_1) v = v^* (\\int_{0}^{t_1} e^{At} B B^* e^{A^*t} dt) v = \\int_{0}^{t_1} v^* (e^{At} B B^* e^{A^*t}) v \\ dt = 0 \\] Since \\(e^{At} BB^* e^{A^*t} \\succeq 0\\) for all \\(t\\), we must have \\[\\begin{equation*} \\begin{split} &amp; v^* (e^{At} B B^* e^{A^*t}) v = \\parallel v^* B e^{At} \\parallel^2 = 0, \\quad \\forall t \\in [0, t_1] \\\\ \\Longrightarrow &amp; v^* B e^{At} = 0, \\quad \\forall t \\in [0, t_1] \\end{split} \\end{equation*}\\] Setting \\(x(t_1) = 0\\), from (C.5), we have \\[ 0 = e^{A t_1} x(0) + \\int_{0}^{t_1} e^{A (t_1 - \\tau)} B u(\\tau) d\\tau = 0 \\] Pre-multiply the above equation by \\(v^*\\), then \\[ 0 = v^* e^{A t_1} x(0) \\] Since \\(x(0)\\) can be chosen arbitrarily, we set \\(x(0) = v e^{-A t_1}\\), which results in \\(v = 0\\). Contradiction! (\\(2. \\Rightarrow 1.\\)) For any \\(x(0) = x_0, t_1 &gt; 0, x(t_1) = x_1\\), since \\(W_c(t_1) \\succ 0\\), we set the control inputs as \\[ u(t) = -B^* e^{A^*(t_1 - t)} W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\] We claim that the picked \\(u(t)\\) satisfies (C.5) by \\[\\begin{equation*} \\begin{split} &amp; e^{At} x_0 + \\int_{0}^{t_1} e^{A(t_1-t)} B u(t) dt \\\\ &amp; = e^{At} x_0 - \\int_{0}^{t_1} e^{A(t_1-t)} B B^* e^{A^*(t_1-t)} dt \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; \\overset{\\tau = t_1-t}{=} e^{At} x_0 - \\underbrace{\\int_{0}^{t_1} e^{A\\tau} BB^* e^{A^*\\tau} d\\tau}_{W_c(t_1)} \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; = e^{At} x_0 - [e^{At_1} x_0 - x_1] = x_1 \\end{split} \\end{equation*}\\] (\\(2. \\Rightarrow 3.\\)) Prove by contradiction. Suppose \\(W_c(t) \\succ 0, \\forall t &gt; 0\\) but \\(\\mathcal{C}\\) is not of full row rank. Then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[ v^* A^k B = 0, \\quad k = 0 \\dots n - 1 \\] By Corollary C.2, we have \\[ v^* A^k B = 0, \\ \\forall k \\in \\mathbb{N} \\Longrightarrow v^* e^{At} B = 0, \\ \\forall t &gt; 0 \\] which implies \\[ v^* W_c(t) v = v^* (\\int_{0}^{t} e^{A\\tau} B B^* e^{A^*\\tau} d\\tau) v = 0, \\quad \\forall t &gt; 0 \\] Contradiction! (\\(3. \\Rightarrow 2.\\)) Prove by contradiction. Suppose \\(\\mathcal{C}\\) has full row rank but \\(W_c(t_1)\\) is singular at some \\(t_1 &gt; 0\\). Then, similar to the proof in (\\(1. \\Rightarrow 2.\\)), there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(F(t) := v^* e^{At} B \\equiv 0, \\forall t \\in [0, t_1]\\). Since \\(F(t)\\) is infinitely differentiable, we get its \\(i\\)’s derivative at \\(t=0\\), where \\(i = 0, 1, \\dots n-1\\). This results in \\[\\begin{equation*} \\left. \\frac{d^i F}{dt^i} \\right|_{t=0} = \\left. v^* A^{i} e^{At} B \\right|_{t=0} = v^* A^i B = 0, \\quad i = 0 \\dots n-1 \\end{equation*}\\] Thus, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\). Contradiction! (\\(3. \\Rightarrow 4.\\)) Proof by contradiction. Suppose \\([A - \\lambda I, B]\\) does not have full row rank for some \\(\\lambda \\in \\mathbb{C}\\). Then, there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(v^* [A - \\lambda I, B] = 0\\). This implies \\(v^* A = v^* \\lambda\\) and \\(v^* B = 0\\). On the other hand, \\[\\begin{equation*} v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; \\lambda B &amp; \\dots &amp; \\lambda^{n-1} B \\end{bmatrix} = 0 \\end{equation*}\\] Contradiction! (\\(4. \\Rightarrow 5.\\)) Proof by contradiction. If there exists a left eigenvector and eigenvalue pair \\((x, \\lambda)\\), s.t. \\(x^* A = \\lambda x^*\\) while \\(x^*B = 0\\), then \\(x^* [A - \\lambda I, B] = 0\\). Contradiction! (\\(5. \\Rightarrow 3.\\)) Proof by contradiction. If the controllability matrix \\(\\mathcal{C}\\) does not have full row rank, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then, from Corollary C.4, there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{R}^{k \\times k}, \\bar{A}_{\\bar{c}} \\in \\mathbb{R}^{(n-k) \\times (n-k)}\\). Now arbitrarily pick one of \\(\\bar{A}_{\\bar{c}}\\)’s left eigenvector \\(x_{\\bar{c}}\\) and its corresponding eigenvalue \\(\\lambda_1\\). Define the vector \\(x = \\begin{bmatrix} 0 \\\\ x_{\\bar{c}} \\end{bmatrix}\\). Then, \\[\\begin{equation*} \\begin{split} x^* (TAT^{-1}) = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\end{bmatrix} \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\bar{A}_{\\bar{c}} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; \\lambda_1 x_{\\bar{c}}^* \\end{bmatrix} = \\lambda_1 x^* \\\\ x^* (TB) &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{x}} \\end{bmatrix} \\begin{bmatrix} B_{\\bar{c}} \\\\ 0 \\end{bmatrix} = 0 \\end{split} \\end{equation*}\\] which implies \\((TAT^{-1}, TB)\\) is not controllable. However, similarity transformation does not change controllability. Contradiction! (\\(6. \\Rightarrow 1.\\)) Prove by contradiction. If \\((A, B)\\) is not controllable, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then from Corollary C.4, there exists a similarity transformation \\(T\\) s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] Now arbitrarily pick \\(F \\in \\mathbb{R}^{m\\times n}\\) and define \\(FT^{-1} = [F_1, F_2]\\), where \\(F_1 \\in \\mathbb{R}^{m\\times k}, F_2 \\in \\mathbb{R}^{m\\times (n-k)}\\). Thus, \\[\\begin{equation*} \\begin{split} \\text{det}(A+BF-\\lambda I) &amp; = \\text{det}\\left( T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} T + T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} F - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det}\\left( T^{-1} \\left\\{ \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} FT^{-1} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right\\} T \\right) \\\\ &amp; = \\text{det}\\left( \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\begin{bmatrix} F_1 &amp; F_2 \\end{bmatrix} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1 &amp; \\bar{A}_{12} + \\bar{B}_c F_2 \\\\ 0 &amp; \\bar{A}_{\\bar{c}} - \\lambda I_2 \\end{bmatrix} \\\\ &amp; = \\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1) \\cdot \\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2) \\end{split} \\end{equation*}\\] where \\(I_1\\) is the identity matrix of size \\(k\\). Similarly, \\(I_2\\) of size \\(n-k\\). Thus, at least \\(n-k\\) eigenvalues of \\(A+BF\\) cannot be freely assigned by choosing \\(F\\). Contradiction! (\\(1. \\Rightarrow 6.\\)) Here we only represent the SIMO case. For the MIMO case, the proof is far more complex. Interesting readers can refer to (Davison and Wonham 1968) (the shortest proof I can find). Since there is only one input, the matrix \\(B\\) degenerate to vector \\(b\\). From Corollary C.5, there exist a similarity transformation matrix \\(T\\), s.t. \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] For any \\(F \\in \\mathbb{C}^{1 \\times n}\\), denote \\(FT^{-1}\\) as \\([f_1, f_2, \\dots, f_n]\\). Calculating the characteristic polynomial of \\(A + bF\\): \\[\\begin{equation*} \\begin{split} \\text{det}(\\lambda I - A - bF) &amp; = \\text{det}(\\lambda I - T^{-1}A_1 T - T^{-1} b_1 F) \\\\ &amp; = \\text{det}(\\lambda I - A_1 - b_1 F T^{-1}) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\lambda + a_1 - f_1 &amp; \\lambda + a_2 - f_2 &amp; \\dots &amp; \\lambda + a_{n-1} - f_{n-1} &amp; \\lambda + a_n - f_n \\\\ -1 &amp; \\lambda &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; -1 &amp; \\lambda \\end{bmatrix} \\\\ &amp; = \\lambda^n + (a_1 - f_1) \\lambda^{n-1} + \\dots + (a_n - f_n) \\end{split} \\end{equation*}\\] By choosing \\([f_1, f_2, \\dots, f_n]\\), \\(A+bF\\)’s eigenvalues can be arbitrarily set. (\\(7. \\Rightarrow 1.\\)) Prove by contradiction. Assume that \\((A, B)\\) is not controllable. Then from 2., there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\) and \\(t_1 &gt; 0\\), \\[\\begin{equation*} F(t) = v^* e^{At} B = 0, \\quad \\forall t \\in [0, t_1] \\end{equation*}\\] Now consider \\(F(z) = v^* e^{Az} B, z\\in \\mathcal{C}\\), which is a vector of analytic function in complex analysis. For a arbitrary \\(t_2 \\in (0, t_1)\\), we have \\(F^{(i)}(t_2) = 0, \\forall i \\in \\mathbb{N}\\). Then, by invoking the fact from complex analysis: “Let \\(G\\) a connected open set and \\(f: G \\rightarrow \\mathbb{C}\\) be analytic, then \\(f \\equiv 0\\) on \\(G\\), if and only if there is a point \\(a \\in G\\) such that \\(f^{(i)}(a) = 0, \\forall n \\in \\mathbb{N}\\)”, we have \\(f(z) \\equiv 0, \\forall z \\in \\mathbb{C}\\). On the other hand, however, \\(W_c \\succ 0\\) implies there exists \\(t_3 &gt; 0\\), such that for the above \\(v\\), we have \\(v^* e^{At_3} B \\ne 0\\). Contradiction! (\\(1. \\Rightarrow 7.\\)) Since \\((A, B)\\) is controllable, from 2., \\(W_c(t) \\succ 0, \\forall t\\). Therefore, \\(W_c \\succ 0\\). The existence and uniqueness of the solution for \\(AW_c + W_cA^* = -BB^*\\) can be obtained directly from the proof of Theorem C.3, by setting \\(Q\\) there to be positive semidefinite. C.2.3 Duality Although controllability and observability seemingly have no direct connections from their definitions C.2 and C.3, the following theorem (Chen 1984) states their tight relations. Theorem C.6 (Theorem of Duality) The pair \\((C,A)\\) is observable if and only if \\((A^*,C^*)\\) is controllable. Proof. We first show that \\((C,A)\\) is observable if and only if the \\(n \\times n\\) matrix \\(W_o(t) = \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau}\\) is positive definite (nonsingular) for any \\(t&gt;0\\): “\\(\\Longleftarrow\\)”: From (C.5), given initial state \\(x(0)\\) and the inputs \\(u(t)\\), \\(y(t)\\) can be expressed as \\[\\begin{equation*} y(t) = Ce^{At} x(0) + C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau + Du(t) \\end{equation*}\\] Define a known function \\(\\bar{y}(t)\\) as \\(y(t) - C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau - Du(t)\\) and we will get \\[\\begin{equation*} Ce^{At} x(0) = \\bar{y}(t) \\end{equation*}\\] Pre-multiply the above equation by \\(e^{A^*t}C^*\\) and integrate it over \\([0,t_1]\\) to yield \\[\\begin{equation*} (\\int_{0}^{t_1} e^{A^*t} C^*C e^{At} dt) x(0) = W_o(t_1) x(0) = \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] Since \\(W_o(t_1) \\succ 0\\), \\[\\begin{equation*} x(0) = W_o(t_1)^{-1} \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] can be observed. “\\(\\Longrightarrow\\)”: Prove by contradiction. Suppose \\((C,A)\\) is observable but there exists \\(t_1 &gt;0\\), s.t. \\(W_o(t_1)\\) is singular. This implies there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* W_o(t_1) v = 0 \\Longrightarrow Ce^{At} v \\equiv 0, \\ \\forall t \\in [0,t_1] \\end{equation*}\\] Similar to the proof of Theorem C.5 (\\(7. \\Rightarrow 1.\\)), we can use conclusions from complex analysis to claim that \\(Ce^{At} v \\equiv 0, \\forall t &gt;0\\). On the other hand, we set \\(u(t) \\equiv 0\\), which results in \\(y(t) = Ce^{At}x(0)\\). In this case \\(x(0) = 0\\) and \\(x(0) = v \\ne 0\\) will lead to the same output responses \\(y(t)\\) over \\(t&gt;0\\), which implies \\((C,A)\\) is not observable. Contradiction! Next we show the duality of controllability and observability: From (1) we know \\((C,A)\\) is controllable if and only of \\[\\begin{equation*} \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau} d\\tau = \\int_{0}^{t} e^{(A^*)\\tau} (C^*)^* (C^*) e^{(A^*)^*\\tau} d\\tau \\end{equation*}\\] is nonsingular for all \\(t &gt;0\\). The latter is exactly the definition of \\((A^*, C^*)\\)’s controllability Gramian \\(W_c(t)\\). C.2.4 Equivalent Statements for Observability With the Theorem of Duality C.6, we can directly write down the equivalent statements of observability without any additional proofs: Theorem C.7 (Equivalent Statements for Observability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((C, A)\\) is observable. The matrix \\[\\begin{equation*} W_o(t) := \\int_{0}^{t} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] is positive definite for any \\(t&gt;0\\). The observability matrix \\[\\begin{equation*} \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^2 \\\\ \\dots \\\\ CA^{n-1} \\end{bmatrix} \\end{equation*}\\] has full column rank. The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full column rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(y\\) be any eigenvalue and any corresponding right eigenvector of \\(A\\), i.e., \\(Ay = \\lambda y\\), then \\(Cy \\ne 0\\). The eigenvalues of \\(A+LC\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(L\\). \\((A^*, C^*)\\) is controllable. If, in addition, all eigenvalues of \\(A\\) have negative parts, then the unique solution of \\[\\begin{equation*} A^* W_o + W_o A = -C^* C \\end{equation*}\\] is positive definite. The solution is called the observability Gramian and can be expressed as \\[\\begin{equation*} W_o = \\int_{0}^{\\infty} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] C.3 Stabilizability And Detectability To define stabilizability and detectability of an LTI system, we first introduce the concept of system mode, which can be naturally derived from the fifth definition of controllability C.5 (observability C.7). Definition C.4 (System Mode) \\(\\lambda\\) is a mode of an LTI system, if it is an eigenvalue of \\(A\\). The mode \\(\\lambda\\) is said to be: stable, if \\(\\text{Re}\\lambda &lt; 0\\), controllable, if \\(x^* B \\ne 0\\) for all left eigenvectors of \\(A\\) associated with \\(\\lambda\\), observable, if \\(C x \\ne 0\\) for all right eigenvectors of \\(A\\) associated with \\(\\lambda\\). Otherwise, the mode is said to be uncontrollable (unobservable). With the concept of system mode, the fifth definition of controllability C.5 (observability C.7) can be restated as An LTI system is controllable (observable) if and only if all modes are controllable (observable). Stabilizability (detectability) is defined similarly via loosening part of controllability (observability) conditions. Definition C.5 (Stabilizability) An LTI system is said to be stabilizable if all of its unstable modes are controllable. Definition C.6 (Detectability) An LTI system is said to be detectable if all of its unstable modes are observable. Like in the case of controllability and observability, duality also holds in stabilizability and detectability. Moreover, similarity transformation will not influence an LTI system’s stabilizability and detectability. C.3.1 Equivalent Statements for Stabilizability Theorem C.8 (Equivalent Statements for Stabilizability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((A,B)\\) is stabilizable. For all \\(\\lambda\\) and \\(x\\) such that \\(x^* A = \\lambda x^*\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(x^* B \\ne 0\\). The matrix \\([A-\\lambda I, B]\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(F\\) such that \\(A+BF\\) are Hurwitz. Proof. (\\(1. \\Leftrightarrow 2.\\)) Directly from stabilizability’s definition. (\\(2. \\Leftrightarrow 3.\\)) If 2. holds but 3. not hold, then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* [A-\\lambda I, B] = 0 \\Leftrightarrow v^* A = \\lambda v^*, v^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Contradiction! Vice versa. (\\(4. \\Rightarrow 2.\\)) Prove by contradiction. Suppose there \\(x \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} x^* [A-\\lambda I, B] = 0 \\Leftrightarrow x^* A = \\lambda x^*, x^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Thus, for any \\(F\\), \\[\\begin{equation*} x^* (A+BF) = \\lambda x^*, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] On the other hand, suppose \\(A+BF\\) has \\(I\\) Jordon blocks, with each equipped with an eigenvalue \\(\\eta_i, i = 1\\dots I\\) (note that \\(\\eta_\\alpha\\) may be equal to \\(\\eta_\\beta\\), i.e., they are equivalent eigenvalues with different Jordon blocks). Since \\(A+BF\\)’s eigenvalues all have negative real parts, \\(\\text{Re} (\\eta_i) &lt; 0, i = 1\\dots I\\). For each \\(\\eta_i,i \\in \\left\\{1\\dots i\\right\\}\\), denote its \\(K_i\\) generalized left eigenvectors as \\(v_{i,1}, v_{i,2}, \\dots v_{i,K_i}\\). By definition, \\(\\sum_{i=1}^{I} K_i = n\\) and \\[\\begin{equation*} \\begin{split} v_{i,1}^* (A+BF) &amp; = v_{i,1}^* \\cdot \\eta_i \\\\ v_{i,2}^* (A+BF) &amp; = v_{i,1}^* + v_{i,2}^* \\cdot \\eta_i \\\\ &amp; \\vdots \\\\ v_{i,K_i}^* (A+BF) &amp; = v_{i,K_i-1}^* + v_{i,K_i}^* \\cdot \\eta_i \\end{split} \\end{equation*}\\] for all \\(i \\in \\left\\{1\\dots i\\right\\}\\). Also, \\(v_{i,k},i=1\\dots I, k=1\\dots K_i\\) are linearly independent and spans \\(\\mathbb{C}^n\\). Therefore, \\[\\begin{equation*} x^* = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\end{equation*}\\] which leads to \\[\\begin{equation*} \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* (A+BF) = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot \\lambda \\cdot v_{i,k}^* \\end{equation*}\\] Since \\(v_{i,k}\\)’s are \\(A+BF\\)’s generalized eigenvectors, we have \\[\\begin{equation*} \\begin{split} &amp; \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\cdot (A+BF) \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\xi_{i,1} \\cdot \\eta_i \\cdot v_{i,1}^* + \\sum_{k=2}^{K_i} \\xi_{i,k} (v_{i,k-1}^* + \\eta_i \\cdot v_{i,k}^* ) \\right\\} \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} (\\xi_{i,k}\\cdot \\eta_i + \\xi_{i,k+1}) v_{i,k}^* + \\xi_{i,K_i} \\cdot \\eta_i \\cdot v_{i,K_i}^* \\right\\} \\end{split} \\end{equation*}\\] Combining the above two equations: \\[\\begin{equation*} \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} \\left[ \\xi_{i,k}\\cdot (\\eta_i - \\lambda) + \\xi_{i,k+1} \\right] v_{i,k}^* + \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) \\cdot v_{i,K_i}^* = 0 \\right\\} \\end{equation*}\\] Since \\(v_{i,k}\\)’s are linearly independent, for any \\(i \\in \\left\\{i\\dots I\\right\\}\\): \\[\\begin{equation*} \\begin{split} \\xi_{i,1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,2} &amp; = 0 \\Rightarrow \\xi_{i,2} = (-1) \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda) \\\\ \\xi_{i,2} \\cdot (\\eta_i - \\lambda) + \\xi_{i,3} &amp; = 0 \\Rightarrow \\xi_{i,3} = (-1)^2 \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^2 \\\\ &amp; \\vdots \\\\ \\xi_{i,K_i-1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,K_i} &amp; = 0 \\Rightarrow \\xi_{i,K_i} = (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i-1} \\\\ \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) &amp; = 0 \\end{split} \\end{equation*}\\] Thus, \\[\\begin{equation*} (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i} = 0 \\end{equation*}\\] Denote \\(\\xi_{i,1}\\) as \\(r_1 e^{\\theta_1}\\), \\((\\eta_i - \\lambda)\\) as \\(r_2 e^{\\theta_2}\\). Since \\(\\text{Re} \\lambda \\ge 0, \\text{Re}(\\eta_i) &lt; 0\\), \\(r_2 &gt; 0\\). On the other hand, the following equation suggests \\[\\begin{equation*} r_1 r_2^{K_i-1} e^{j[\\theta_1 + \\theta_2 (K_i-1)]} = 0 \\end{equation*}\\] Thus, \\(r_1\\) has to be \\(0\\), which implies \\(\\xi_{i,1} = 0\\). By recursion, \\(\\xi_{i,k} = 0, \\forall k = 1\\dots K_i\\). Contradiction! (\\(1. \\Rightarrow 4.\\)) If \\((A,B)\\) is controllable, then from Theorem (thm:lticontrollable)’s sixth definition, we can freely assign the poles of \\(A+BF\\) via choosing \\(F\\) properly. Otherwise, if \\((A,B)\\) is uncontrollable, then from Corollary C.4 and proof of Theorem C.5 (\\(6. \\Rightarrow 1.\\)), there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] and \\[\\begin{equation*} \\text{det}(A+BF-\\lambda I) = \\underbrace{\\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1)}_{\\chi_c(\\lambda)} \\cdot \\underbrace{\\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2)}_{\\chi_{\\bar{c}}(\\lambda)} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}\\), \\(I_1\\) identity matrix of size \\(k_1\\), \\([F_1,F_2] = FT^{-1}\\), and \\(k_1 = \\text{rank} \\mathcal{C}\\). Additionally, \\((\\bar{A}_c, \\bar{B}_c)\\) is controllable. Thus, \\(\\chi_c(\\lambda)\\)’s zeros can be freely assigned by choosing proper \\(F\\), i.e., system modes with \\(\\chi_c(\\lambda)\\) is controllable, regardless of its stability. On the other hand, system modes with \\(\\chi_{\\bar{c}}(\\lambda)\\) must be stable. Otherwise, we cannot affect it by assigning \\(F\\), which is a contradiction to statement (1). Therefore, \\((TAT^{-1}, TB)\\) is stabilizable. Since similarity transformation does not change stabilizability, \\((A,B)\\) is stabilizable. C.3.2 Equivalent Statements for Detectability Thanks to duality, we can directly write down the equivalent statements of observability without any additional proofs: Theorem C.9 (Equivalent Statements for Detectability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((C,A)\\) is detectable. For all \\(\\lambda\\) and \\(x\\) such that \\(A x = \\lambda x\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(C x \\ne 0\\). The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(L\\) such that \\(A+LC\\) are Hurwitz. \\((A^*, C^*)\\) is stabilizable. References Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. when \\(A\\) is not diagonalizable, similar results can be derived via Jordan decomposition.↩︎ "],["algebraic-techniques-and-sum-of-squares.html", "D Algebraic Techniques and Sum-of-Squares D.1 Algebra", " D Algebraic Techniques and Sum-of-Squares D.1 Algebra D.1.1 Polynomials Definition D.1 (Monomial,Polynomial) A monomial in \\(x_1,\\cdots,x_n\\) is a product of the form \\(x_1^{\\alpha_1}\\cdot x_2^{\\alpha_2}\\cdots x_n^{\\alpha_n}\\). The total degree of the monomial is \\(\\alpha_1+\\cdots+\\alpha_n\\). A polynomial \\(f\\) in \\(x_1,\\cdots,x_n\\) with coefficients in \\(\\mathbb{R}\\) is a finite linear combination (with coefficients in \\(\\mathbb{R}\\)) of monomials. We will write a polynomial \\(f\\) in the form: \\(\\Sigma_\\alpha a_\\alpha x^\\alpha\\). where the sum is over a finite number of n-tuples \\(\\alpha = (\\alpha_1,\\cdots,\\alpha_n)\\). The set of all polynomials in \\(x_1,\\cdots,x_n\\) with coefficients in \\(\\mathbb{R}\\) is denoted \\(\\mathbb{R}[x_1,\\cdots,x_n]\\) Definition D.2 (Affine Variety) Let \\(f_1,\\cdots,f_s\\in\\mathbb{R}[x_1,\\cdots,x_n]\\), we set \\[V(f_1,\\cdots,f_s) = \\{(a_1,\\cdots,a_n)\\in\\mathbb{R}^n|f_i(a_1,\\cdots a_n)=0 \\quad \\forall i\\leq i\\leq s\\}\\] We call \\(V(f_1,\\cdots,f_s)\\) the affine variety defined bt \\(f_1,\\cdots,f_s\\) Definition D.3 (Ideal) A subset \\(I\\subset \\mathbb{R}[x_1,\\cdots,x_n]\\) is an ideal if it satisfies: (i) Contains additive identity: \\(0\\in I\\) (ii) Closed under addition: For all \\(f,g\\in I\\), \\(f+g\\in I\\) (iii) Absorption of multiplication: If \\(f\\in I\\) and \\(h\\in \\mathbb{R}[x_1,\\cdots,x_n]\\), then \\(hf\\in I\\) Definition D.4 (Sum of squares,Quadratic Module and Preordering) Sum of squares D.1.2 Representation of nonnegative polynomial: Univariate case Theorem D.1 (Global version) A polynomial \\(p\\in\\mathbb{R}[x]\\) of even degree is nonnegative if and only if it can be written as a sum of squares of other polynomials, i.e., \\(p(x) = \\Sigma^k_{i=1}[h_i(x)]^2\\), with \\(h_i\\in R[x], i = 1,\\cdots, k\\). Theorem D.2 (Compact interval version) A polynomial \\(p\\in\\mathbb{R}[x]\\) of even degree is nonnegative if and only if it can be written as a sum of squares of other polynomials, i.e., \\(p(x) = \\Sigma^k_{i=1}[h_i(x)]^2\\), with \\(h_i\\in R[x], i = 1,\\cdots, k\\). "],["the-kalman-yakubovich-lemma.html", "E The Kalman-Yakubovich Lemma", " E The Kalman-Yakubovich Lemma Lemma E.1 (Kalman-Yakubovich) Consider a controllable linear time-invariant system \\[ \\dot{x} = A x + b u \\\\ y = c^T x. \\] The transfer function \\[ h(p) = c^T (p I - A)^{-1} b \\] is strictly positive real (SPR) if and only if there exist positive definite matrices \\(P\\) and \\(Q\\) such that \\[ A^T P + P A = - Q \\\\ Pb = c. \\] "],["feedbacklinearization.html", "F Feedback Linearization", " F Feedback Linearization "],["slidingcontrol.html", "G Sliding Control", " G Sliding Control "],["references.html", "References", " References Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. “Reinforcement Learning: Theory and Algorithms.” CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 32. Andrieu, Vincent, and Laurent Praly. 2006. “On the Existence of a Kazantzis–Kravaris/Luenberger Observer.” SIAM Journal on Control and Optimization 45 (2): 432–56. Arnold, William F, and Alan J Laub. 1984. “Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.” Proceedings of the IEEE 72 (12): 1746–54. Astolfi, Alessandro, and Romeo Ortega. 2003. “Immersion and Invariance: A New Tool for Stabilization and Adaptive Control of Nonlinear Systems.” IEEE Transactions on Automatic Control 48 (4): 590–606. Bemporad, Alberto, Manfred Morari, Vivek Dua, and Efstratios N Pistikopoulos. 2002. “The Explicit Linear Quadratic Regulator for Constrained Systems.” Automatica 38 (1): 3–20. Bernard, Pauline. 2019. Observer Design for Nonlinear Systems. Vol. 479. Springer. Bernard, Pauline, Vincent Andrieu, and Daniele Astolfi. 2022. “Observer Design for Continuous-Time Dynamical Systems.” Annual Reviews in Control. Bertsekas, Dimitri. 1972. “Infinite Time Reachability of State-Space Regions by Using Feedback Control.” IEEE Transactions on Automatic Control 17 (5): 604–13. ———. 2012. Dynamic Programming and Optimal Control: Volume i. Vol. 1. Athena scientific. Besançon, Gildas, Guy Bornard, and Hassan Hammouri. 1996. “Observer Synthesis for a Class of Nonlinear Control Systems.” European Journal of Control 2 (3): 176–92. Blekherman, Grigoriy, Pablo A Parrilo, and Rekha R Thomas. 2012. Semidefinite Optimization and Convex Algebraic Geometry. SIAM. Borrelli, Francesco, Alberto Bemporad, and Manfred Morari. 2017. Predictive Control for Linear and Hybrid Systems. Cambridge University Press. Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Dai, Hongkai, and Frank Permenter. 2023. “Convex Synthesis and Verification of Control-Lyapunov and Barrier Functions with Input Constraints.” In 2023 American Control Conference (ACC), 4116–23. IEEE. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Dawson, Charles, Sicun Gao, and Chuchu Fan. 2023. “Safe Control with Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction Methods for Robotics and Control.” IEEE Transactions on Robotics. Ebenbauer, Christian, Jonathan Renz, and F Allgower. 2005. “Polynomial Feedback and Observer Design Using Nonquadratic Lyapunov Functions.” In Proceedings of the 44th IEEE Conference on Decision and Control, 7587–92. IEEE. Gilbert, Elmer G, and K Tin Tan. 1991. “Linear Systems with State and Control Constraints: The Theory and Application of Maximal Output Admissible Sets.” IEEE Transactions on Automatic Control 36 (9): 1008–20. Hammouri, Hassan, and Jesus de Leon Morales. 1990. “Observer Synthesis for State-Affine Systems.” In 29th IEEE Conference on Decision and Control, 784–85. IEEE. Janny, Steeven, Vincent Andrieu, Madiha Nadri, and Christian Wolf. 2021. “Deep Kkl: Data-Driven Output Prediction for Non-Linear Systems.” In 2021 60th IEEE Conference on Decision and Control (CDC), 4376–81. IEEE. Kalman, Rudolph E, and Richard S Bucy. 1961. “New Results in Linear Filtering and Prediction Theory.” Kalman, Rudolph Emil. 1960. “A New Approach to Linear Filtering and Prediction Problems.” Kang, Shucheng, Yuxiao Chen, Heng Yang, and Marco Pavone. 2023. “Verification and Synthesis of Robust Control Barrier Functions: Multilevel Polynomial Optimization and Semidefinite Relaxation.” In 2023 62nd IEEE Conference on Decision and Control (CDC). Karagiannis, Dimitrios, and Alessandro Astolfi. 2005. “Nonlinear Observer Design Using Invariant Manifolds and Applications.” In Proceedings of the 44th IEEE Conference on Decision and Control, 7775–80. IEEE. Kazantzis, Nikolaos, and Costas Kravaris. 1998. “Nonlinear Observer Design Using Lyapunov’s Auxiliary Theorem.” Systems &amp; Control Letters 34 (5): 241–47. Kelly, Matthew. 2017. “An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation.” SIAM Review 59 (4): 849–904. Kolmanovsky, Ilya, Elmer G Gilbert, et al. 1998. “Theory and Computation of Disturbance Invariant Sets for Discrete-Time Linear Systems.” Mathematical Problems in Engineering 4: 317–67. Lasserre, Jean B. 2001. “Global Optimization with Polynomials and the Problem of Moments.” SIAM Journal on Optimization 11 (3): 796–817. Lasserre, Jean Bernard. 2009. Moments, Positive Polynomials and Their Applications. Vol. 1. World Scientific. Levine, Nir, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. 2017. “Shallow Updates for Deep Reinforcement Learning.” Advances in Neural Information Processing Systems 30. Luenberger, David G. 1964. “Observing the State of a Linear System.” IEEE Transactions on Military Electronics 8 (2): 74–80. Magron, Victor, and Jie Wang. 2023. Sparse Polynomial Optimization: Theory and Practice. World Scientific. Miao, Keyan, and Konstantinos Gatsis. 2023. “Learning Robust State Observers Using Neural ODEs.” In Learning for Dynamics and Control Conference, 208–19. PMLR. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. Murray, Riley, Venkat Chandrasekaran, and Adam Wierman. 2021. “Signomial and Polynomial Optimization via Relative Entropy and Partial Dualization.” Mathematical Programming Computation 13: 257–95. Niazi, Muhammad Umar B, John Cao, Xudong Sun, Amritam Das, and Karl Henrik Johansson. 2023. “Learning-Based Design of Luenberger Observers for Autonomous Nonlinear Systems.” In 2023 American Control Conference (ACC), 3048–55. IEEE. Nie, Jiawang. 2023. Moment and Polynomial Optimization. SIAM. Nocedal, Jorge, and Stephen J Wright. 1999. Numerical Optimization. Springer. Reif, Konrad, Stefan Gunther, Engin Yaz, and Rolf Unbehauen. 1999. “Stochastic Stability of the Discrete-Time Extended Kalman Filter.” IEEE Transactions on Automatic Control 44 (4): 714–28. Slotine, Jean-Jacques E, Weiping Li, et al. 1991. Applied Nonlinear Control. Vol. 199. 1. Prentice hall Englewood Cliffs, NJ. Sontag, Eduardo D. 1983. “A Lyapunov-Like Characterization of Asymptotic Controllability.” SIAM Journal on Control and Optimization 21 (3): 462–71. Thrun, S, W Burgard, and D Fox. 2005. “Probabilistic Robotics.” MIT Press. Van Der Merwe, Rudolph. 2004. Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic State-Space Models. Oregon Health &amp; Science University. Vinograd, Robert Èlyukimovich. 1957. “Inapplicability of the Method of Characteristic Exponents to the Study of Non-Linear Differential Equations.” Matematicheskii Sbornik 83 (4): 431–38. Wang, Jie. 2022. “Nonnegative Polynomials and Circuit Polynomials.” SIAM Journal on Applied Algebra and Geometry 6 (2): 111–33. Yang, Alan, and Stephen Boyd. 2023. “Value-Gradient Iteration with Quadratic Approximate Value Functions.” arXiv Preprint arXiv:2307.07086. Yang, Heng, and Luca Carlone. 2022. “Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite Relaxations and Scalable Global Optimization.” IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (3): 2816–34. Yang, Heng, Ling Liang, Luca Carlone, and Kim-Chuan Toh. 2022. “An Inexact Projected Gradient Method with Rounding and Lifting by Nonlinear Programming for Solving Rank-One Semidefinite Relaxation of Polynomial Optimization.” Mathematical Programming, 1–64. Yang, Tao, Prashant G Mehta, and Sean P Meyn. 2013. “Feedback Particle Filter.” IEEE Transactions on Automatic Control 58 (10): 2465–80. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
