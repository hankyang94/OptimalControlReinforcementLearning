[["index.html", "Optimal Control and Reinforcement Learning Preface Feedback Offerings", " Optimal Control and Reinforcement Learning Heng Yang 2025-09-17 Preface This is the textbook for Harvard ES/AM 158: Introduction to Optimal Control and Reinforcement Learning. Feedback I would like to invite you to provide feedback to the textbook via inline comments with Hypothesis: Go to Hypothesis and create an account Install the Chrome extension of Hypothesis Provide public comments to textbook contents and I will try to address them Offerings 2025 Fall Time: Mon/Wed 2:15 - 3:30pm Location: SEC 1.413 Instructor: Heng Yang Teaching Fellow: Haoyu Han, Han Qi [Syllabus], [Problem Sets], [Canvas] 2023 Fall The course was previously offered as Introduction to Optimal Control and Estimation. Starting Fall 2025, contents about reinforcement learning have been added to the course. "],["mdp.html", "Chapter 1 Markov Decision Process 1.1 Finite-Horizon MDP 1.2 Infinite-Horizon MDP", " Chapter 1 Markov Decision Process Optimal control (OC) and reinforcement learning (RL) address the problem of making optimal decisions in the presence of a dynamic environment. In optimal control, this dynamic environment is often referred to as a plant or a dynamical system. In reinforcement learning, it is modeled as a Markov decision process (MDP). The goal in both fields is to evaluate and design decision-making strategies that optimize long-term performance: RL typically frames this as maximizing a long-term reward. OC often formulates it as minimizing a long-term cost. The emphasis on long-term evaluation is crucial. Because the environment evolves over time, decisions that appear beneficial in the short term may lead to poor long-term outcomes and thus be suboptimal. With this motivation, we now formalize the framework of Markov Decision Processes (MDPs), which are discrete-time stochastic dynamical systems. 1.1 Finite-Horizon MDP We begin with finite-horizon MDPs and introduce infinite-horizon MDPs in the following section. An abstract definition of the finite-horizon case will be presented first, followed by illustrative examples. A finite-horizon MDP is given by the following tuple: \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, T), \\] where \\(\\mathcal{S}\\): state space (set of all possible states) \\(\\mathcal{A}\\): action space (set of all possible actions) \\(P(s&#39; \\mid s, a)\\): probability of transitioning to state \\(s&#39;\\) from state \\(s\\) under action \\(a\\) (i.e., dynamics) \\(R(s,a)\\): reward of taking action \\(a\\) in state \\(s\\) \\(T\\): horizon, a positive integer For now, let us assume both the state space and the action space are discrete and have a finite number of elements. In particular, denote the number of elements in \\(\\mathcal{S}\\) as \\(|\\mathcal{S}|\\), and the number of elements in \\(\\mathcal{A}\\) as \\(|\\mathcal{A}|\\). This is also referred to as a tabular MDP. Policy. Decision-making in MDPs is represented by policies. A policy is a function that, given any state, outputs a distribution of actions: \\(\\pi: \\mathcal{S} \\mapsto \\Delta(\\mathcal{A})\\). That is, \\(\\pi(a \\mid s)\\) returns the probability of taking action \\(a\\) in state \\(s\\). In finite-horizon MDPs, we consider a tuple of policies: \\[\\begin{equation} \\pi = (\\pi_0, \\dots, \\pi_t, \\dots, \\pi_{T-1}), \\tag{1.1} \\end{equation}\\] where each \\(\\pi_t\\) denotes the policy at step \\(t \\in [0,T-1]\\). Trajectory and Return. Given an initial state \\(s_0 \\in \\mathcal{S}\\) and a policy \\(\\pi\\), the MDP will evolve as Start at state \\(s_0\\) Take action \\(a_0 \\sim \\pi_0(a \\mid s_0)\\) following policy \\(\\pi_0\\) Collect reward \\(r_0 = R(s_0, a_0)\\) (assume \\(R\\) is deterministic) Transition to state \\(s_1 \\sim P(s&#39; \\mid s_0, a_0)\\) following the dynamics Go to step 2 and continue until reaching state \\(s_T\\) This evolution generates a trajectory of states, actions, and rewards: \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T). \\] The cumulative reward of this trajectory is \\(g_0 = \\sum_{t=0}^{T-1} r_t\\), which is called the return of the trajectory. Clearly, \\(g_0\\) is a random variable due to the stochasticity of both the policy and the dynamics. Similarly, if the state at time \\(t\\) is \\(s_t\\), we denote: \\[ g_t = r_t + \\dots + r_{T-1} \\] as the return of the policy starting at \\(s_t\\). 1.1.1 Value Functions State-Value Function. Given a policy \\(\\pi\\) as in (1.1), which states are preferable at time \\(t\\)? The (time-indexed) state-value function assigns to each \\(s\\in\\mathcal{S}\\) the expected return from \\(t\\) onward when starting in \\(s\\) and following \\(\\pi\\) thereafter. Formally, define \\[\\begin{equation} V_t^\\pi(s) := \\mathbb{E} \\left[g_t \\mid s_t=s\\right] = \\mathbb{E} \\left[\\sum_{i=t}^{T-1} R(s_i,a_i) \\middle| s_t=s,a_i\\sim \\pi_i(\\cdot\\mid s_i), s_{i+1}\\sim P(\\cdot\\mid s_i,a_i)\\right]. \\tag{1.2} \\end{equation}\\] The expectation is over the randomness induced by both the policy and the dynamics. Thus, if \\(V_t^\\pi(s_1)&gt;V_t^\\pi(s_2)\\), then at time \\(t\\) under policy \\(\\pi\\) it is better in expectation to be in \\(s_1\\) than in \\(s_2\\) because the former yields a larger expected return. \\(V^{\\pi}_t(s)\\): given policy \\(\\pi\\), how good is it to start in state \\(s\\) at time \\(t\\)? Action-Value Function. Similarly, the action-value function assigns to each state-action pair \\((s,a)\\in\\mathcal{S}\\times\\mathcal{A}\\) the expected return obtained by starting in state \\(s\\), taking action \\(a\\) first, and then following policy \\(\\pi\\) thereafter: \\[\\begin{equation} \\begin{split} Q_t^\\pi(s,a) := &amp; \\mathbb{E} \\left[R(s,a) + g_{t+1} \\mid s_{t+1} \\sim P(\\cdot \\mid s,a)\\right] \\\\ = &amp; \\mathbb{E} \\left[R(s,a) + \\sum_{i=t+1}^{T-1} R(s_i, a_i) \\middle| s_{t+1} \\sim P(\\cdot \\mid s,a) \\right]. \\end{split} \\tag{1.3} \\end{equation}\\] The key distinction is that the action-value function evaluates the return when the first action may deviate from policy \\(\\pi\\), whereas the state-value function assumes strict adherence to \\(\\pi\\). This flexibility makes the action-value function central to improving \\(\\pi\\), since it reveals whether alternative actions can yield higher returns. \\(Q^{\\pi}_t(s,a)\\): At time \\(t\\), how good is it to take action \\(a\\) in state \\(s\\), then follow the policy \\(\\pi\\)? It is easy to verify that the state-value function and the action-value function satisfy: \\[\\begin{align} V_t^{\\pi}(s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi_t(a \\mid s) Q_t^{\\pi}(s,a), \\tag{1.4} \\\\ Q_t^{\\pi}(s,a) &amp; = R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}_{t+1} (s&#39;). \\tag{1.5} \\end{align}\\] From these two equations, we can derive the Bellman Consistency equations. Proposition 1.1 (Bellman Consistency (Finite Horizon)) The state-value function \\(V^{\\pi}_t(\\cdot)\\) in (1.2) satisfies the following recursion: \\[\\begin{equation} \\begin{split} V^{\\pi}_t(s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi_t(a\\mid s) \\left( R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}_{t+1} (s&#39;) \\right) \\\\ &amp; =: \\mathbb{E}_{a \\sim \\pi_t(\\cdot \\mid s)} \\left[ R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} [V^{\\pi}_{t+1}(s&#39;)] \\right]. \\end{split} \\tag{1.6} \\end{equation}\\] Similarly, the action-value function \\(Q^{\\pi}_t(s,a)\\) in (1.3) satisfies the following recursion: \\[\\begin{equation} \\begin{split} Q^{\\pi}_t (s, a) &amp; = R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) \\left( \\sum_{a&#39; \\in \\mathcal{A}} \\pi_{t+1}(a&#39; \\mid s&#39;) Q^{\\pi}_{t+1}(s&#39;, a&#39;)\\right) \\\\ &amp; =: R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[\\mathbb{E}_{a&#39; \\sim \\pi_{t+1}(\\cdot \\mid s&#39;)} [Q^{\\pi}_{t+1}(s&#39;, a&#39;)] \\right]. \\end{split} \\tag{1.7} \\end{equation}\\] 1.1.2 Policy Evaluation The Bellman consistency result in Proposition 1.1 is fundamental because it directly yields an algorithm for evaluating a given policy \\(\\pi\\)—that is, for computing its state-value and action-value functions—provided the transition dynamics of the MDP are known. Policy evaluation for the state-value function proceeds as follows: Initialization: set \\(V^{\\pi}_T(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). Backward recursion: for \\(t = T-1, T-2, \\dots, 0\\), update each \\(s \\in \\mathcal{S}\\) by \\[ V^{\\pi}_{t}(s) = \\mathbb{E}_{a \\sim \\pi_t(\\cdot \\mid s)} \\left[ R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\big[ V^{\\pi}_{t+1}(s&#39;) \\big] \\right]. \\] Similarly, policy evaluation for the action-value function is given by: Initialization: set \\(Q^{\\pi}_T(s,a) = 0\\) for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). Backward recursion: for \\(t = T-1, T-2, \\dots, 0\\), update each \\((s,a) \\in \\mathcal{S}\\times\\mathcal{A}\\) by \\[ Q^{\\pi}_t(s,a) = R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[ \\mathbb{E}_{a&#39; \\sim \\pi_{t+1}(\\cdot \\mid s&#39;)} \\big[ Q^{\\pi}_{t+1}(s&#39;, a&#39;) \\big] \\right]. \\] The essential feature of this algorithm is its backward-in-time recursion: the value functions are first set at the terminal horizon \\(T\\), and then propagated backward step by step through the Bellman consistency equations. Example 1.1 (MDP, Transition Graph, and Policy Evaluation) It is often useful to visualize small MDPs as transition graphs, where states are represented by nodes and actions are represented by directed edges connecting those nodes. As a simple illustrative example, consider a robot navigating on a two-state grid. At each step, the robot can either Stay in its current state or Move to the other state. This finite-horizon MDP is fully specified by the tuple of states, actions, transition dynamics, rewards, and horizon: States: \\(\\mathcal{S} = \\{\\alpha, \\beta \\}\\) Actions: \\(\\mathcal{A} = \\{\\text{Move} , \\text{Stay} \\}\\) Transition dynamics: we can specify the transition dynamics in the following table State \\(s\\) Action \\(a\\) Next State \\(s&#39;\\) Probability \\(P(s&#39; \\mid s, a)\\) \\(\\alpha\\) Stay \\(\\alpha\\) 1 \\(\\alpha\\) Move \\(\\beta\\) 1 \\(\\beta\\) Stay \\(\\beta\\) 1 \\(\\beta\\) Move \\(\\alpha\\) 1 Reward: \\(R(s,a)=1\\) if \\(a = \\text{Move}\\) and \\(R(s,a)=0\\) if \\(a = \\text{Stay}\\) Horizon: \\(T=2\\). This MDP can be represented by the transition graph in Fig. 1.1. Note that for this MDP, the transition dynamics is deterministic. We will see a stochastic MDP soon. Figure 1.1: A Simple Transition Graph. At time \\(t=0\\), if the robot starts at \\(s_0 = \\alpha\\), first chooses action \\(a_0 = \\text{Move}\\), and then chooses action \\(a_1 = \\text{Stay}\\), the resulting trajectory is \\[ \\tau = (\\alpha, \\text{Move}, +1, \\beta, \\text{Stay}, 0, \\beta). \\] The return of this trajectory is: \\[ g_0 = +1 + 0 = +1. \\] Policy Evaluation. Given a policy \\[\\begin{equation} \\pi = (\\pi_0, \\pi_1), \\quad \\pi_0(a \\mid s) = \\begin{cases} 0.5 &amp; a = \\text{Move} \\\\ 0.5 &amp; a = \\text{Stay} \\end{cases}, \\quad \\pi_1( a \\mid s) = \\begin{cases} 0.8 &amp; a = \\text{Move} \\\\ 0.2 &amp; a = \\text{Stay} \\end{cases}. \\end{equation}\\] We can use the Bellman consistency equations to compute the state-value function. We first initialize: \\[ V^{\\pi}_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\] where the first row contains the value at \\(s = \\alpha\\) and the second row contains the value at \\(s = \\beta\\). We then perform the backward recursion for \\(t=1\\). For \\(s = \\alpha\\), we have \\[\\begin{equation} V^{\\pi}_1(\\alpha) = \\begin{bmatrix} \\pi_1(\\text{Move} \\mid \\alpha) \\\\ \\pi_1(\\text{Stay} \\mid \\alpha) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\alpha, \\text{Move}) + V^{\\pi}_2(\\beta) \\\\ R(\\alpha, \\text{Stay}) + V^{\\pi}_2(\\alpha) \\end{bmatrix} = \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 0.8 \\end{equation}\\] For \\(s = \\beta\\), we have \\[\\begin{equation} V^{\\pi}_1(\\beta) = \\begin{bmatrix} \\pi_1(\\text{Move} \\mid \\beta) \\\\ \\pi_1(\\text{Stay} \\mid \\beta) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\beta, \\text{Move}) + V^{\\pi}_2(\\alpha) \\\\ R(\\beta, \\text{Stay}) + V^{\\pi}_2(\\beta) \\end{bmatrix} = \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 0.8. \\end{equation}\\] Therefore, we have \\[ V^{\\pi}_1 = \\begin{bmatrix} 0.8 \\\\ 0.8 \\end{bmatrix}. \\] We then proceed to the backward recursion for \\(t=0\\): \\[\\begin{align} V_0^{\\pi}(\\alpha) &amp; = \\begin{bmatrix} \\pi_0(\\text{Move} \\mid \\alpha) \\\\ \\pi_0(\\text{Stay} \\mid \\alpha) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\alpha, \\text{Move}) + V^{\\pi}_1(\\beta) \\\\ R(\\alpha, \\text{Stay}) + V^{\\pi}_1(\\alpha) \\end{bmatrix} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1.8 \\\\ 0.8 \\end{bmatrix} = 1.3. \\\\ V_0^{\\pi}(\\beta) &amp; = \\begin{bmatrix} \\pi_0(\\text{Move} \\mid \\beta) \\\\ \\pi_0(\\text{Stay} \\mid \\beta) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\beta, \\text{Move}) + V^{\\pi}_0(\\alpha) \\\\ R(\\beta, \\text{Stay}) + V^{\\pi}_0(\\beta) \\end{bmatrix} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1.8 \\\\ 0.8 \\end{bmatrix} = 1.3. \\end{align}\\] Therefore, the state-value function at \\(t=0\\) is \\[ V^{\\pi}_0 = \\begin{bmatrix} 1.3 \\\\ 1.3 \\end{bmatrix}. \\] You are encouraged to carry out the similar calculations for the action-value function. The toy example was small enough to carry out policy evaluation by hand; in realistic MDPs, we will need the help from computers. Consider now an MDP whose transition graph is shown in Fig. 1.2. This example is adapted from here. Figure 1.2: Hangover Transition Graph. This MDP has six states: \\[ \\mathcal{S} = \\{\\text{Hangover}, \\text{Sleep}, \\text{More Sleep}, \\text{Visit Lecture}, \\text{Study}, \\text{Pass Exam} \\}, \\] and two actions: \\[ \\mathcal{A} = \\{\\text{Lazy}, \\text{Productive} \\}. \\] The stochastic transition dynamics are labeled in the transition graph. For example, at state “Hangover”, taking action “Productive” will lead to state “Visit Lecture” with probability \\(0.3\\) and state “Hangover” with probability \\(0.7\\). The rewards of the MDP are defined as: \\[ R(s,a) = \\begin{cases} +1 &amp; s = \\text{Pass Exam} \\\\ -1 &amp; \\text{otherwise}. \\end{cases}. \\] Policy Evaluation. Consider a time-invariant random policy \\[ \\pi = \\{\\pi_0,\\dots,\\pi_{T-1} \\}, \\quad \\pi_t(a \\mid s) = \\begin{cases} \\alpha &amp; a = \\text{Lazy} \\\\ 1 - \\alpha &amp; a = \\text{Productive} \\end{cases}, \\] that takes “Lazy” with probability \\(\\alpha\\) and “Productive” with probability \\(1-\\alpha\\). The following Python code performs policy evaluation for this MDP, with \\(T=10\\) and \\(\\alpha = 0.4\\). # Finite-horizon policy evaluation for the Hangover MDP from collections import defaultdict from typing import Dict, List, Tuple State = str Action = str # --- MDP spec --------------------------------------------------------------- S: List[State] = [ &quot;Hangover&quot;, &quot;Sleep&quot;, &quot;More Sleep&quot;, &quot;Visit Lecture&quot;, &quot;Study&quot;, &quot;Pass Exam&quot; ] A: List[Action] = [&quot;Lazy&quot;, &quot;Productive&quot;] # P[s, a] -&gt; list of (s_next, prob) P: Dict[Tuple[State, Action], List[Tuple[State, float]]] = { # Hangover (&quot;Hangover&quot;, &quot;Lazy&quot;): [(&quot;Sleep&quot;, 1.0)], (&quot;Hangover&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.3), (&quot;Hangover&quot;, 0.7)], # Sleep (&quot;Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Sleep&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.6), (&quot;More Sleep&quot;, 0.4)], # More Sleep (&quot;More Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;More Sleep&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 0.5), (&quot;More Sleep&quot;, 0.5)], # Visit Lecture (&quot;Visit Lecture&quot;, &quot;Lazy&quot;): [(&quot;Study&quot;, 0.8), (&quot;Pass Exam&quot;, 0.2)], (&quot;Visit Lecture&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 1.0)], # Study (&quot;Study&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Study&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 0.9), (&quot;Study&quot;, 0.1)], # Pass Exam (absorbing) (&quot;Pass Exam&quot;, &quot;Lazy&quot;): [(&quot;Pass Exam&quot;, 1.0)], (&quot;Pass Exam&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 1.0)], } def R(s: State, a: Action) -&gt; float: &quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot; return 1.0 if s == &quot;Pass Exam&quot; else -1.0 # --- Policy: time-invariant, state-independent ------------------------------ def pi(a: Action, s: State, alpha: float) -&gt; float: &quot;&quot;&quot;pi(a|s): Lazy with prob alpha, Productive with prob 1-alpha.&quot;&quot;&quot; return alpha if a == &quot;Lazy&quot; else (1.0 - alpha) # --- Policy evaluation ------------------------------------------------------- def policy_evaluation(T: int, alpha: float): &quot;&quot;&quot; Compute {V_t(s)} and {Q_t(s,a)} for t=0..T with terminal condition V_T = Q_T = 0. Returns: V: Dict[int, Dict[State, float]] Q: Dict[int, Dict[Tuple[State, Action], float]] &quot;&quot;&quot; assert T &gt;= 0 # sanity: probabilities sum to 1 for each (s,a) for key, rows in P.items(): total = sum(p for _, p in rows) if abs(total - 1.0) &gt; 1e-9: raise ValueError(f&quot;Probabilities for {key} sum to {total}, not 1.&quot;) V: Dict[int, Dict[State, float]] = defaultdict(dict) Q: Dict[int, Dict[Tuple[State, Action], float]] = defaultdict(dict) # Terminal boundary for s in S: V[T][s] = 0.0 for a in A: Q[T][(s, a)] = 0.0 # Backward recursion for t in range(T - 1, -1, -1): for s in S: # First compute Q_t(s,a) for a in A: exp_next = sum(p * V[t + 1][s_next] for s_next, p in P[(s, a)]) Q[t][(s, a)] = R(s, a) + exp_next # Then V_t(s) = E_{a~pi}[Q_t(s,a)] V[t][s] = sum(pi(a, s, alpha) * Q[t][(s, a)] for a in A) return V, Q # --- Example run ------------------------------------------------------------- if __name__ == &quot;__main__&quot;: T = 10 # horizon alpha = 0.4 # probability of choosing Lazy V, Q = policy_evaluation(T=T, alpha=alpha) # Print V_0 print(f&quot;V_0(s) with T={T}, alpha={alpha}:&quot;) for s in S: print(f&quot; {s:13s}: {V[0][s]: .3f}&quot;) The code returns the following state values at \\(t=0\\): \\[\\begin{equation} V^{\\pi}_0 = \\begin{bmatrix} -3.582 \\\\ -2.306 \\\\ -2.180 \\\\ 1.757 \\\\ 2.939 \\\\ 10 \\end{bmatrix}, \\tag{1.8} \\end{equation}\\] where the ordering of the states follows that defined in \\(\\mathcal{S}\\). You can find the code here. 1.1.3 Principle of Optimality Every policy \\(\\pi\\) induces a value function \\(V_0^{\\pi}\\) that can be evaluated by policy evaluation (assuming the transition dynamics are known). The goal of reinforcement learning is to find an optimal policy that maximizes the value function with respect to a given initial state distribution: \\[\\begin{equation} V^\\star_0 = \\max_{\\pi}\\; \\mathbb{E}_{s_0 \\sim \\mu(\\cdot)} \\big[ V_0^{\\pi}(s_0) \\big], \\tag{1.9} \\end{equation}\\] where we have used the superscript “\\(\\star\\)” to denote the optimality of the value function. \\(V^\\star_0\\) is often known as the optimal value function. At first glance, (1.9) appears daunting: a naive approach would enumerate all stochastic policies \\(\\pi\\), evaluate their value functions, and select the best. A central result in reinforcement learning and optimal control—rooted in the principle of optimality—is that the optimal value functions satisfy a Bellman-style recursion, analogous to Proposition 1.1. This Bellman optimality recursion enables backward computation of the optimal value functions without enumerating policies. Theorem 1.1 (Bellman Optimality (Finite Horizon, State-Value)) Consider a finite-horizon MDP \\(\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,T)\\) with finite state and action sets and bounded rewards. Define the optimal value functions \\(\\{V_t^\\star\\}_{t=0}^{T}\\) by the following Bellman optimality recursion \\[\\begin{equation} \\begin{split} V_T^\\star(s)&amp; \\equiv 0, \\\\ V_t^\\star(s)&amp; = \\max_{a\\in\\mathcal{A}}\\Big\\{ R(s,a)\\;+\\;\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;)\\Big\\},\\ t=T-1,\\ldots,0. \\end{split} \\tag{1.10} \\end{equation}\\] Then, the optimal value functions are optimal in the sense of statewise dominance: \\[\\begin{equation} V_t^{\\star}(s)\\;\\ge\\; V_t^{\\pi}(s) \\quad\\text{for all policies }\\pi,\\; s\\in\\mathcal{S},\\; t=0,\\ldots,T. \\tag{1.11} \\end{equation}\\] Moreover, the deterministic policy \\(\\pi^\\star=(\\pi^\\star_0,\\ldots,\\pi^\\star_{T-1})\\) with \\[\\begin{equation} \\begin{split} \\pi_t^\\star(s)\\;\\in\\;\\arg\\max_{a\\in\\mathcal{A}} \\Big\\{ R(s,a)\\;+\\;\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;)\\Big\\}, \\\\ \\text{for any } s\\in\\mathcal{S},\\; t=0,\\dots,T-1 \\end{split} \\tag{1.12} \\end{equation}\\] is optimal, where ties can be broken by any fixed rule. Proof. We first show that the value functions defined by the Bellman optimality recursion (1.10) are optimal in the sense that they dominate the value functions of any other policy. The proof proceeds by backward induction. Base case (\\(t=T\\)). For every \\(s\\in\\mathcal{S}\\), \\[ V^\\star_T(s)\\;=\\;0\\;=\\;V_T^{\\pi}(s), \\] so \\(V^\\star_T(s)\\ge V_T^{\\pi}(s)\\) holds trivially. Inductive step. Assume \\(V^\\star_{t+1}(s)\\ge V^{\\pi}_{t+1}(s)\\) for all \\(s\\in\\mathcal{S}\\). Then, for any \\(s\\in\\mathcal{S}\\), \\[\\begin{align*} V_t^{\\pi}(s) &amp;= \\sum_{a\\in\\mathcal{A}} \\pi_t(a\\mid s)\\!\\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\pi}(s&#39;)\\right) \\\\ &amp;\\le \\sum_{a\\in\\mathcal{A}} \\pi_t(a\\mid s)\\!\\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\star}(s&#39;)\\right) \\\\ &amp;\\le \\max_{a\\in\\mathcal{A}} \\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\star}(s&#39;)\\right) \\;=\\; V_t^\\star(s), \\end{align*}\\] where the first inequality uses the induction hypothesis and the second uses that an expectation is bounded above by a maximum. Hence \\(V_t^\\star(s)\\ge V_t^{\\pi}(s)\\) for all \\(s\\), completing the induction. Therefore, \\(\\{V_t^\\star\\}_{t=0}^T\\) dominates the value functions attainable by any policy. Next, we show that \\(\\{V_t^\\star\\}\\) is attainable by some policy. Since \\(\\mathcal{A}\\) is finite (tabular setting), the maximizer in the Bellman optimality operator exists for every \\((t,s)\\); thus we can define a (deterministic) greedy policy \\[ \\pi_t^\\star(s)\\;\\in\\;\\arg\\max_{a\\in\\mathcal{A}} \\Big\\{ R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;) \\Big\\}. \\] A simple backward induction then shows \\(V_t^{\\pi^\\star}(s)=V_t^\\star(s)\\) for all \\(t\\) and \\(s\\): at \\(t=T\\) both are \\(0\\), and if \\(V_{t+1}^{\\pi^\\star}=V_{t+1}^\\star\\), then by construction of \\(\\pi_t^\\star\\) the Bellman equality yields \\(V_t^{\\pi^\\star}=V_t^\\star\\). Consequently, the optimal value functions are achieved by the greedy (deterministic) policy \\(\\pi^\\star\\). Corollary 1.1 (Bellman Optimality (Finite Horizon, Action-Value)) Given the optimal (state-)value functions \\(V^{\\star}_{t},t=0,\\dots,T\\), define the optimal action-value function \\[\\begin{equation} Q_t^\\star(s,a)\\;=\\;R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;), \\quad t=0,\\dots,T-1. \\tag{1.13} \\end{equation}\\] Then we have \\[\\begin{equation} V_t^\\star(s)=\\max_{a\\in\\mathcal{A}} Q_t^\\star(s,a),\\qquad \\pi_t^\\star(s)\\in\\arg\\max_{a\\in\\mathcal{A}} Q_t^\\star(s,a). \\tag{1.14} \\end{equation}\\] The optimal action-value functions satisfy: \\[\\begin{equation} \\begin{split} Q_T^\\star(s,a) &amp; \\equiv 0,\\\\ Q_t^\\star(s,a) &amp; = R(s,a) \\;+\\; \\mathbb{E}_{s&#39;\\sim P(\\cdot\\mid s,a)} \\!\\left[ \\max_{a&#39;\\in\\mathcal{A}} Q_{t+1}^\\star(s&#39;,a&#39;) \\right], \\quad t=T-1,\\ldots,0. \\end{split} \\tag{1.15} \\end{equation}\\] 1.1.4 Dynamic Programming The principle of optimality in Theorem 1.1 yields a constructive procedure to compute the optimal value functions and an associated deterministic optimal policy. This backward-induction procedure is the dynamic programming (DP) algorithm. Dynamic programming (finite horizon). Initialization. Set \\(V_T^\\star(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). Backward recursion. For \\(t = T-1, T-2, \\dots, 0\\): Optimal value: for each \\(s \\in \\mathcal{S}\\), \\[ V_t^\\star(s) = \\max_{a \\in \\mathcal{A}} \\left\\{ R(s,a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s,a)} \\big[ V_{t+1}^\\star(s&#39;) \\big] \\right\\}. \\] Greedy policy (deterministic): for each \\(s \\in \\mathcal{S}\\), \\[ \\pi_t^\\star(s) \\in \\arg\\max_{a \\in \\mathcal{A}} \\left\\{ R(s,a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s,a)} \\big[ V_{t+1}^\\star(s&#39;) \\big] \\right\\}. \\] Exercise 1.1 How does dynamic programming look like when applied to the action-value function? Exercise 1.2 What is the computational complexity of dynamic programming? Let us try dynamic programming for the Hangover MDP presented before. Example 1.2 (Dynamic Programming for Hangover MDP) Consider the Hangover MDP defined by the transition graph shown in Fig. 1.2. With slight modification to the policy evaluation code, we can find the optimal value functions and optimal policies. # Dynamic programming (finite-horizon optimal control) for the Hangover MDP from collections import defaultdict from typing import Dict, List, Tuple State = str Action = str # --- MDP spec --------------------------------------------------------------- S: List[State] = [ &quot;Hangover&quot;, &quot;Sleep&quot;, &quot;More Sleep&quot;, &quot;Visit Lecture&quot;, &quot;Study&quot;, &quot;Pass Exam&quot; ] A: List[Action] = [&quot;Lazy&quot;, &quot;Productive&quot;] # P[s, a] -&gt; list of (s_next, prob) P: Dict[Tuple[State, Action], List[Tuple[State, float]]] = { # Hangover (&quot;Hangover&quot;, &quot;Lazy&quot;): [(&quot;Sleep&quot;, 1.0)], (&quot;Hangover&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.3), (&quot;Hangover&quot;, 0.7)], # Sleep (&quot;Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Sleep&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.6), (&quot;More Sleep&quot;, 0.4)], # More Sleep (&quot;More Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;More Sleep&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 0.5), (&quot;More Sleep&quot;, 0.5)], # Visit Lecture (&quot;Visit Lecture&quot;, &quot;Lazy&quot;): [(&quot;Study&quot;, 0.8), (&quot;Pass Exam&quot;, 0.2)], (&quot;Visit Lecture&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 1.0)], # Study (&quot;Study&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Study&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 0.9), (&quot;Study&quot;, 0.1)], # Pass Exam (absorbing) (&quot;Pass Exam&quot;, &quot;Lazy&quot;): [(&quot;Pass Exam&quot;, 1.0)], (&quot;Pass Exam&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 1.0)], } def R(s: State, a: Action) -&gt; float: &quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot; return 1.0 if s == &quot;Pass Exam&quot; else -1.0 # --- Dynamic programming (Bellman optimality) ------------------------------- def dynamic_programming(T: int): &quot;&quot;&quot; Compute optimal finite-horizon tables: - V[t][s] = V_t^*(s) - Q[t][(s,a)] = Q_t^*(s,a) - PI[t][s] = optimal action at (t,s) with terminal condition V_T^* = 0. &quot;&quot;&quot; assert T &gt;= 0 # sanity: probabilities sum to 1 for each (s,a) for key, rows in P.items(): total = sum(p for _, p in rows) if abs(total - 1.0) &gt; 1e-9: raise ValueError(f&quot;Probabilities for {key} sum to {total}, not 1.&quot;) V: Dict[int, Dict[State, float]] = defaultdict(dict) Q: Dict[int, Dict[Tuple[State, Action], float]] = defaultdict(dict) PI: Dict[int, Dict[State, Action]] = defaultdict(dict) # Terminal boundary for s in S: V[T][s] = 0.0 for a in A: Q[T][(s, a)] = 0.0 # Backward recursion (Bellman optimality) for t in range(T - 1, -1, -1): for s in S: # compute Q*_t(s,a) for a in A: exp_next = sum(p * V[t + 1][s_next] for s_next, p in P[(s, a)]) Q[t][(s, a)] = R(s, a) + exp_next # greedy action and optimal value # tie-breaking is deterministic by the order in A best_a = max(A, key=lambda a: Q[t][(s, a)]) PI[t][s] = best_a V[t][s] = Q[t][(s, best_a)] return V, Q, PI # --- Example run ------------------------------------------------------------- if __name__ == &quot;__main__&quot;: T = 10 # horizon V, Q, PI = dynamic_programming(T=T) print(f&quot;Optimal V_0(s) with T={T}:&quot;) for s in S: print(f&quot; {s:13s}: {V[0][s]: .3f}&quot;) print(&quot;\\nGreedy policy at t=0:&quot;) for s in S: print(f&quot; {s:13s}: {PI[0][s]}&quot;) print(&quot;\\nAction value at t=0:&quot;) for s in S: print(f&quot; {s:13s}: {Q[0][s, A[0]]: .3f}, {Q[0][s, A[1]]: .3f}&quot;) The optimal value function at \\(t=0\\) is: \\[\\begin{equation} V^\\star_0 = \\begin{bmatrix} 1.259 \\\\ 3.251 \\\\ 3.787 \\\\ 6.222 \\\\ 7.778 \\\\ 10 \\end{bmatrix}. \\tag{1.16} \\end{equation}\\] Clearly, the optimal value function dominates the value function shown in (1.8) of the random policy at every state. The optimal actions at \\(t=0\\) are: \\[\\begin{equation} \\begin{split} \\text{Hangover} &amp; : \\text{Lazy} \\\\ \\text{Sleep} &amp; : \\text{Productive} \\\\ \\text{More Sleep} &amp; : \\text{Productive} \\\\ \\text{Visit Lecture} &amp; : \\text{Lazy} \\\\ \\text{Study} &amp; : \\text{Productive} \\\\ \\text{Pass Exam} &amp; : \\text{Lazy} \\end{split}. \\end{equation}\\] You can play with the code here. 1.2 Infinite-Horizon MDP In a finite-horizon MDP, the horizon \\(T\\) must be specified in advance in order to carry out policy evaluation and dynamic programming. The finite horizon naturally provides a terminal condition, which serves as the boundary condition that allows backward recursion to proceed. In many practical applications, however, the horizon \\(T\\) is not well defined or is difficult to determine. In such cases, it is often more natural and convenient to adopt the infinite-horizon MDP formulation. An infinite-horizon MDP is given by the following tuple: \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma), \\] where \\(\\mathcal{S}\\), \\(\\mathcal{A}\\), \\(P\\), and \\(R\\) are the same as defined before in a finite-horizon MDP. We still restrict ourselves to the tabular MDP setup where \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) both have a finite number of elements. The key difference between the finite-horizon and infinite-horizon formulations is that the fixed horizon \\(T\\) is replaced by a discount factor \\(\\gamma \\in [0,1)\\). This discount factor weights future rewards less heavily than immediate rewards, as we will see shortly. Stationary Policy. In an infinite-horizon MDP, we focus on stationary policies \\(\\pi: \\mathcal{S} \\mapsto \\Delta(\\mathcal{A})\\), where \\(\\pi(a \\mid s)\\) denotes the probability of taking action \\(a\\) in state \\(s\\). In contrast, in a finite-horizon MDP we considered a tuple of \\(T\\) policies (see (1.1)), where each \\(\\pi_t\\) could vary with time (i.e., policies were non-stationary). Intuitively, in the infinite-horizon setting, it suffices to consider stationary policies because the decision-making problem at time \\(t\\) is equivalent to the problem at time \\(t + k\\) for any \\(k \\in \\mathbb{N}\\), as both face the same infinite horizon. Trajectory and Return. Given an initial state \\(s_0 \\in \\mathcal{S}\\) and a stationary policy \\(\\pi\\), the MDP will evolve as Start at state \\(s_0\\) Take action \\(a_0 \\sim \\pi(\\cdot \\mid s_0)\\) following policy \\(\\pi\\) Collect reward \\(r_0 = R(s_0, a_0)\\) Transition to state \\(s_1 \\sim P(s&#39; \\mid s_0, a_0)\\) following the dynamics Go to step 2 and continue forever This process generates a trajectory of states, actions, and rewards: \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots). \\] The return of a trajectory is defined as \\[ g_0 = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\dots = \\sum_{t=0}^{\\infty} \\gamma^t r_t. \\] Here, the discount factor \\(\\gamma\\) plays a key role: it progressively reduces the weight of rewards received further in the future, making them less influential as \\(t\\) increases. 1.2.1 Value Functions Similar to the case of finite-horizon MDP, we can define the state-value function and the action-value function associated with a policy \\(\\pi\\). State-Value Function. The value of a state \\(s \\in \\mathcal{S}\\) under policy \\(\\pi\\) is the expected discounted return obtained when starting from \\(s\\) at time \\(0\\): \\[\\begin{equation} V^{\\pi}(s) := \\mathbb{E}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\,\\middle|\\, s_0 = s, a_t \\sim \\pi(\\cdot \\mid s_t), s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\right]. \\tag{1.17} \\end{equation}\\] Action-Value Function. The value of a state-action pair \\((s,a) \\in \\mathcal{S} \\times \\mathcal{A}\\) under policy \\(\\pi\\) is the expected discounted return obtained by first taking action \\(a\\) in state \\(s\\), and then following policy \\(\\pi\\) thereafter: \\[\\begin{equation} Q^{\\pi}(s,a) := \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\,\\middle|\\, s_0 = s, a_0 = a, a_t \\sim \\pi(\\cdot \\mid s_t), s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\right]. \\tag{1.18} \\end{equation}\\] Note that a nice feature of having a discount factor \\(\\gamma \\in [0,1)\\) is that both the state-value and the action-value functions are guaranteed to be bounded even if the horizon is unbounded (assuming the reward function is bounded). We can verify the state-value function and the action value function satisfy the following relationship: \\[\\begin{align} V^{\\pi}(s) &amp;= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) Q^{\\pi}(s,a) \\tag{1.19}\\\\ Q^{\\pi}(s,a) &amp; = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}(s&#39;). \\tag{1.20} \\end{align}\\] Combining these two equations, we arrive at the Bellman consistency result for infinite-horizon MDP. Proposition 1.2 (Bellman Consistency (Infinite Horizon)) The state-value function \\(V^{\\pi}\\) in (1.17) satisfies the following recursion: \\[\\begin{equation} \\begin{split} V^{\\pi} (s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi (a\\mid s) \\left( R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi} (s&#39;) \\right) \\\\ &amp; =: \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} \\left[ R(s, a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} [V^{\\pi}(s&#39;)] \\right]. \\end{split} \\tag{1.21} \\end{equation}\\] Similarly, the action-value function \\(Q^{\\pi}(s,a)\\) in (1.18) satisfies the following recursion: \\[\\begin{equation} \\begin{split} Q^{\\pi} (s, a) &amp; = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) \\left( \\sum_{a&#39; \\in \\mathcal{A}} \\pi(a&#39; \\mid s&#39;) Q^{\\pi}(s&#39;, a&#39;)\\right) \\\\ &amp; =: R(s, a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[\\mathbb{E}_{a&#39; \\sim \\pi(\\cdot \\mid s&#39;)} [Q^{\\pi}(s&#39;, a&#39;)] \\right]. \\end{split} \\tag{1.22} \\end{equation}\\] 1.2.2 Policy Evaluation Given a policy \\(\\pi\\), how can we compute its associated state-value and action-value functions? Finite-horizon case. We initialize the terminal value function \\(V_T^{\\pi}(s) = 0\\) for every \\(s \\in \\mathcal{S}\\), and then apply the Bellman Consistency result (Proposition 1.1) to perform backward recursion. Infinite-horizon case. The Bellman Consistency result (Proposition 1.2) takes a different form and does not provide the same simple recipe for backward recursion. System of Linear Equations. A closer look at the Bellman Consistency equation (1.21) for the state-value function shows that it defines a square system of linear equations. Specifically, the value function \\(V^{\\pi}\\) can be represented as a vector with \\(|\\mathcal{S}|\\) variables, and (1.21) provides \\(|\\mathcal{S}|\\) linear equations over these variables. Thus, one way to compute the state-value function is to set up this linear system and solve it. However, doing so typically requires matrix inversion or factorization, which can be computationally expensive. The same reasoning applies to the action-value function \\(Q^{\\pi}\\), which can be represented as a vector of \\(|\\mathcal{S}||\\mathcal{A}|\\) variables constrained by \\(|\\mathcal{S}||\\mathcal{A}|\\) linear equations. The following proposition states that, instead of solving a linear system of equations, one can use a globally convergent iterative scheme, one that is very much like the policy evaluation algorithm for the finite-horizon MDP, to evaluate the state-value function associated with a policy \\(\\pi\\). Proposition 1.3 (Policy Evaluation (Infinite Horizon, State-Value)) Consider an infinite-horizon MDP \\(\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,\\gamma)\\). Fix a policy \\(\\pi\\) and consider the iterative scheme for the state-value function: \\[\\begin{equation} V_{k+1}(s) \\;\\; \\gets \\;\\; \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a) V_k(s&#39;) \\right], \\quad \\forall s \\in \\mathcal{S}. \\tag{1.23} \\end{equation}\\] Then, starting from any initialization \\(V_0 \\in \\mathbb{R}^{|\\mathcal{S}|}\\), the sequence \\(\\{V_k\\}\\) converges to the unique fixed point \\(V^{\\pi}\\), the state-value function associated with policy \\(\\pi\\). Proof. To prove the convergence of the policy evaluation algorithm, we shall introduce the notion of a Bellman operator. Bellman Operator. Any value function \\(V(s)\\) can be interpreted as a vector in \\(\\mathbb{R}^{|\\mathcal{S}|}\\) (recall we are in the tabular MDP case). Given any value function \\(V \\in \\mathbb{R}^{|\\mathcal{S}|}\\), and a policy \\(\\pi\\), define the Bellman operator associated with \\(\\pi\\) as \\(T^{\\pi}: \\mathbb{R}^{|\\mathcal{S}|} \\mapsto \\mathbb{R}^{|\\mathcal{S}|}\\): \\[\\begin{equation} (T^{\\pi} V)(s) := \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a) V(s&#39;) \\right]. \\tag{1.24} \\end{equation}\\] We claim that \\(T^{\\pi}\\) has two important properties. Monotonicity. If \\(V \\leq W\\) (i.e., \\(V(s) \\leq W(s)\\) for any \\(s \\in \\mathcal{S}\\)), then \\(T^{\\pi} V \\leq T^{\\pi}W\\). To see this, observe that \\[\\begin{align*} (T^{\\pi}V)(s) - (T^\\pi W)(s) &amp;= \\sum_{a} \\pi(a \\mid s) \\left(\\gamma \\sum_{s&#39;} P(s&#39; \\mid s, a) (V(s&#39;) - W(s&#39;)) \\right) \\\\ &amp; = \\gamma \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s), s&#39; \\sim P(\\cdot \\mid s,a)}[V(s&#39;) - W(s&#39;)]. \\end{align*}\\] Therefore, if \\(V(s&#39;) - W(s&#39;) \\leq 0\\) for any \\(s&#39; \\in \\mathcal{S}\\), then \\(T^{\\pi}V \\leq T^{\\pi} W\\). \\(\\gamma\\)-Contraction. For any value function \\(V \\in \\mathbb{R}^{|\\mathcal{S}|}\\), define the \\(\\ell_{\\infty}\\) norm (sup norm) as \\[ \\Vert V \\Vert_{\\infty} = \\max_{s \\in \\mathcal{S}} |V(s)|. \\] We claim that the Bellman operator \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction in the sup norm, i.e., \\[\\begin{equation} \\Vert T^\\pi V - T^\\pi W \\Vert_{\\infty} \\leq \\gamma \\Vert V - W \\Vert_{\\infty}, \\quad \\forall V, W \\in \\mathbb{R}^{|\\mathcal{S}|}. \\tag{1.25} \\end{equation}\\] To prove this, observe that for any \\(s \\in \\mathcal{S}\\), we have: \\[\\begin{align*} |(T^\\pi V)(s) - (T^\\pi W)(s)| &amp;= \\left| \\sum_a \\pi(a|s)\\,\\gamma \\sum_{s&#39;} P(s&#39;|s,a)\\big(V(s&#39;) - W(s&#39;)\\big) \\right| \\\\ &amp;\\le \\gamma \\sum_a \\pi(a|s)\\sum_{s&#39;} P(s&#39;|s,a)\\,|V(s&#39;) - W(s&#39;)| \\\\ &amp;\\le \\gamma \\|V - W\\|_\\infty \\sum_a \\pi(a|s)\\sum_{s&#39;} P(s&#39;|s,a) \\\\ &amp;= \\gamma \\|V - W\\|_\\infty. \\end{align*}\\] Taking the maximum over \\(s\\) gives \\[ \\|T^\\pi V - T^\\pi W\\|_\\infty \\le \\gamma \\|V - W\\|_\\infty, \\] so \\(T^\\pi\\) is a \\(\\gamma\\)-contraction in the sup norm. With the Bellman operator defined, we observe that the value function of \\(\\pi\\), denoted \\(V^{\\pi}\\) in (1.21), is a fixed point of \\(T^{\\pi}\\). That is to say \\(V^{\\pi}\\) satisfies: \\[ T^{\\pi} V^{\\pi} = V^{\\pi}. \\] In other words, \\(V^{\\pi}\\) is fixed (remains unchanged) under the Bellman operator. Since \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction, by the Banach Fixed-Point Theorem, we know that there exists a unique fixed point to \\(T^{\\pi}\\), which is \\(V^{\\pi}\\). Moreover, since \\[ \\Vert V_{k} - V^{\\pi} \\Vert_{\\infty} = \\Vert T^{\\pi} V_{k-1} - T^{\\pi} V^{\\pi} \\Vert_{\\infty} \\leq \\gamma \\Vert V_{k-1} - V^{\\pi} \\Vert_{\\infty}, \\] we can deduce the rate of convergence \\[ \\Vert V_{k} - V^{\\pi} \\Vert_{\\infty} \\leq \\gamma^{k} \\Vert V_0 - V^{\\pi} \\Vert_{\\infty}. \\] Therefore, policy evaluation globally converges from any initialization \\(V_0\\) at a linear rate of \\(\\gamma\\). We have a similar policy evaluation algorithm for the action-value function. Proposition 1.4 (Policy Evaluation (Infinite Horizon, Action-Value)) Fix a policy \\(\\pi\\). Consider the iterative scheme on \\(Q:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}\\): \\[\\begin{equation} \\begin{split} Q_{k+1}(s,a) \\;\\gets\\; R(s,a) \\;+\\; \\gamma \\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\!\\left(\\sum_{a&#39;\\in\\mathcal{A}} \\pi(a&#39;\\mid s&#39;)\\, Q_k(s&#39;,a&#39;)\\right), \\\\ \\forall (s,a)\\in\\mathcal{S}\\times\\mathcal{A}. \\end{split} \\tag{1.26} \\end{equation}\\] Then, for any initialization \\(Q_0\\), the sequence \\(\\{Q_k\\}\\) converges to the unique fixed point \\(Q^{\\pi}\\), the action-value function associated with policy \\(\\pi\\). Proof. Define the Bellman operator on action-values \\[ (T^{\\pi}Q)(s,a) := R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\Big(\\sum_{a&#39;} \\pi(a&#39;\\mid s&#39;)\\, Q(s&#39;,a&#39;)\\Big). \\] \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction in the sup-norm on \\(\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}\\); hence by the Banach fixed-point theorem, global convergence holds regardless of initialization. Let us apply policy evaluation to an infinite-horizon MDP. Example 1.3 (Policy Evaluation for Inverted Pendulum) Figure 1.3: Inverted Pendulum. We consider the inverted pendulum with state \\(s=(\\theta, \\dot\\theta)\\) and action (torque) \\(a = u\\), as visualized in Fig. 1.3. Our goal is to swing up the pendulum from any initial state to the upright position \\(s = (0,0)\\). Continuous-Time Dynamics. The continuous-time dynamics of the inverted pendulum is \\[ \\ddot{\\theta} \\;=\\; \\frac{g}{l}\\sin(\\theta) \\;+\\; \\frac{1}{ml^2}u \\;-\\; c\\,\\dot{\\theta}, \\] where \\(m &gt; 0\\) is the mass of the pendulum, \\(l &gt; 0\\) is the length of the pole, \\(c &gt; 0\\) is the damping coefficient, and \\(g\\) is the gravitational constant. Discretization (Euler). With timestep \\(\\Delta t\\), we obtain the following discrete-time dynamics: \\[\\begin{equation} \\begin{split} \\theta_{k+1} &amp;= \\theta_k + \\Delta t \\, \\dot{\\theta}_k, \\\\ \\dot{\\theta}_{k+1} &amp;= \\dot{\\theta}_k + \\Delta t \\Big(\\tfrac{g}{l}\\sin(\\theta_k) + \\tfrac{1}{ml^2}u_k - c\\,\\dot{\\theta}_k\\Big). \\end{split} \\tag{1.27} \\end{equation}\\] We wrap angles to \\([-\\pi,\\pi]\\) via \\(\\operatorname{wrap}(\\theta)=\\mathrm{atan2}(\\sin\\theta,\\cos\\theta)\\). Tabular MDP. We convert the discrete-time dynamics into a tabular MDP. State grid. \\(\\theta \\in [-\\pi,\\pi]\\), \\(\\dot\\theta \\in [-\\pi,\\pi]\\) on uniform grids: \\[ \\mathcal{S}=\\{\\;(\\theta_i,\\dot\\theta_j)\\;:\\; i=1,\\dots,N_\\theta,\\; j=1, \\dots, N_{\\dot\\theta}\\;\\}. \\] Action grid. \\(u \\in [-mgl/2, mgl/2]\\) on \\(N_u\\) uniform points: \\[ \\mathcal{A}=\\{u_\\ell:\\ell=1,\\dots,N_u\\}. \\] Stochastic transition kernel (nearest-3 interpolation). From a grid point \\(s=(\\theta_i,\\dot\\theta_j)\\) and an action \\(u_\\ell\\), compute the next continuous state \\(s^+ = (\\theta^+,\\dot\\theta^+)\\) via the discrete-time dynamics in (1.27). If \\(s^+\\notin\\mathcal{S}\\), choose the three closest grid states \\(\\{s^{(1)},s^{(2)},s^{(3)}\\}\\) by Euclidean distance in \\((\\theta,\\dot\\theta)\\) and assign probabilities \\[ p_r \\propto \\frac{1}{\\|s^+ - s^{(r)}\\|_2 + \\varepsilon},\\quad r=1,2,3, \\qquad \\sum_r p_r=1, \\] so nearer grid points receive higher probability (use a small \\(\\varepsilon&gt;0\\) to avoid division by zero). Reward. A quadratic shaping penalty around the upright equilibrium: \\[ R(s,a) = -\\Big(\\theta^2 + 0.1\\,\\dot\\theta^2 + 0.01\\,u^2\\Big). \\] Discount. \\(\\gamma \\in [0,1)\\). We obtain a discounted, infinite-horizon, tabular MDP. Policy. For policy evaluation, consider \\(\\pi(a\\mid s)\\) be uniform over the discretized actions, i.e., a random policy. Policy Evaluation. The following python script performs policy evaluation. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 41 N_thetadot = 41 N_u = 21 theta_grid = np.linspace(-np.pi, np.pi, N_theta) thetadot_grid = np.linspace(-np.pi, np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- # Pre-compute all grid points for fast nearest neighbor search grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) # sort those 3 by distance for stability nn_idx = nn_idx[np.argsort(dists[nn_idx])] d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): # reward at current (s,a) R[s, a] = reward(th, thd, u) # next continuous state th_n, thd_n = step_euler(th, thd, u) # map to 3 nearest grid states nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ----- Fixed policy: uniform over actions ----- Pi = np.full((S, A), 1.0 / A) # ----- Iterative policy evaluation ----- V = np.zeros(S) # initialization (any vector works) tol = 1e-6 max_iters = 10000 for k in range(max_iters): V_new = np.zeros_like(V) # Compute Bellman update: V_{k+1}(s) = sum_a Pi(s,a)[ R(s,a) + gamma * sum_j P(s,a,j) V_k(ns_j) ] # First, expected next V for each (s,a) EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # shape: (S, A) # Then expectation over actions under Pi V_new = (Pi * (R + gamma * EV_next)).sum(axis=1) # shape: (S,) # Check convergence if np.max(np.abs(V_new - V)) &lt; tol: V = V_new print(f&quot;Converged in {k+1} iterations (sup-norm change &lt; {tol}).&quot;) break V = V_new else: print(f&quot;Reached max_iters={max_iters} without meeting tolerance {tol}.&quot;) V_grid = V.reshape(N_theta, N_thetadot) # V_grid: shape (N_theta, N_thetadot) # theta_grid, thetadot_grid already defined fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; # any matplotlib colormap, e.g., &quot;plasma&quot;, &quot;inferno&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^\\pi(\\theta,\\dot{\\theta})$&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V^\\pi$ (tabular policy evaluation)&quot;) plt.tight_layout() plt.show() Running the code, it shows that policy evaluation converges in 518 iterations under tolerance \\(10^{-6}\\). Fig. 1.4 plots the value function over the state grid. Figure 1.4: Value Function from Policy Evaluation. You can play with the code here. 1.2.3 Principle of Optimality In an infinite-horizon MDP, our goal is to find the optimal policy that maximizes the expected long-term discounted return: \\[ V^\\star := \\max_{\\pi} \\mathbb{E}_{s \\sim \\mu(\\cdot)} [V^\\pi(s)], \\] where \\(\\mu\\) is a given initial distribution. We call \\(V^\\star\\) the optimal value function. Given a policy \\(\\pi\\) and its associated value function \\(V^\\pi\\), how do we know if the policy is already optimal? Theorem 1.2 (Bellman Optimality (Infinite Horizon)) For an infinite-horizon MDP with discount factor \\(\\gamma \\in [0,1)\\), the optimal state-value function \\(V^\\star(s)\\) satisfies the Bellman optimality equation \\[\\begin{equation} V^\\star(s) \\;=\\; \\max_{a \\in \\mathcal{A}} \\Big[\\, R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a)\\, V^\\star(s&#39;) \\,\\Big]. \\tag{1.28} \\end{equation}\\] Define the optimal action-value function as \\[\\begin{equation} Q^\\star(s,a) = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^\\star(s&#39;). \\tag{1.29} \\end{equation}\\] We have that \\(Q^\\star(s,a)\\) satisfies \\[\\begin{equation} Q^\\star(s,a) \\;=\\; R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a)\\, \\left[\\max_{a&#39; \\in \\mathcal{A}} Q^\\star(s&#39;,a&#39;) \\right]. \\tag{1.30} \\end{equation}\\] Moreover, any greedy policy with respect to \\(V^\\star\\) (equivalently, to \\(Q^\\star\\)) is optimal: \\[\\begin{equation} \\begin{split} \\pi^\\star(s) &amp; \\in \\arg\\max_{a \\in \\mathcal{A}} \\Big[\\, R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39; \\mid s,a)\\, V^\\star(s&#39;) \\,\\Big] \\quad\\Longleftrightarrow\\quad \\\\ \\pi^\\star(s) &amp; \\in \\arg\\max_{a \\in \\mathcal{A}} Q^\\star(s,a). \\end{split} \\tag{1.31} \\end{equation}\\] Proof. We will first show that \\(V^\\star\\) has statewise dominance over all other policies, and then show that \\(V^\\star\\) can be attained by the greedy policy. Claim. For any discounted MDP with \\(\\gamma \\in [0,1)\\) and any policy \\(\\pi\\), \\[ V^\\star(s) \\;\\ge\\; V^{\\pi}(s)\\qquad \\forall s\\in\\mathcal{S}, \\] where \\(V^\\star\\) is the unique solution of the Bellman optimality equation and \\(V^\\pi\\) solves the Bellman consistency equation for \\(\\pi\\). Proof via Bellman Operators. Define the Bellman operators \\[ (T^\\pi V)(s) := \\sum_{a}\\pi(a\\mid s)\\Big[ R(s,a)+\\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)V(s&#39;) \\Big], \\] \\[ (T^\\star V)(s) := \\max_{a}\\Big[ R(s,a)+\\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)V(s&#39;) \\Big]. \\] Key facts: (Monotonicity) If \\(V \\ge W\\) componentwise, then \\(T^\\pi V \\ge T^\\pi W\\) and \\(T^\\star V \\ge T^\\star W\\). (Dominance of \\(T^*\\)) For any \\(V\\) and any \\(\\pi\\), \\[ T^\\star V \\;\\ge\\; T^\\pi V \\] because the max over actions is at least the \\(\\pi\\)-weighted average. (Fixed points) \\(V^\\pi = T^\\pi V^\\pi\\) and \\(V^\\star = T^\\star V^\\star\\). (Contraction) Each \\(T^\\pi\\) and \\(T^\\star\\) is a \\(\\gamma\\)-contraction in the sup-norm; hence their fixed points are unique. Now start from \\(V^\\pi\\). Using (2), \\[ V^\\pi = T^\\pi V^\\pi \\;\\le\\; T^\\star V^\\pi. \\] Applying \\(T^\\star\\) repeatedly and using (1), \\[ V^\\pi \\;\\le\\; T^\\star V^\\pi \\;\\le\\; (T^\\star)^2 V^\\pi \\;\\le\\; \\cdots \\] The sequence \\((T^\\star)^k V^\\pi\\) converges (by contraction) to the unique fixed point of \\(T^\\star\\), namely \\(V^\\star\\). Taking limits preserves the inequality, yielding \\(V^\\pi \\le V^\\star\\) statewise. The Bellman optimality condition tells us, if a policy \\(\\pi\\) is already greedy with respect to its value function \\(V^\\pi\\), then \\(\\pi\\) is the optimal policy and \\(V^\\pi\\) is the optimal value function. In the next, we introduce two algorithms that can guarantee finding the optimal policy and the optimal value function. The first algorithm, policy iteration (PI), iterates over the space of policies; while the second algorithm, value iteration (VI), iterates over the space of value functions. 1.2.4 Policy Improvement The policy evaluation algorithm enables us to compute the value functions associated with a given policy \\(\\pi\\). The next result, known as the Policy Improvement Lemma, shows that once we have \\(V^{\\pi}\\), constructing a greedy policy with respect to \\(V^{\\pi}\\) guarantees performance that is at least as good as \\(\\pi\\), and strictly better in some states unless \\(\\pi\\) is already greedy with respect to \\(V^{\\pi}\\). Lemma 1.1 (Policy Improvement) Let \\(\\pi\\) be any policy and let \\(V^{\\pi}\\) be its state-value function. Define a new policy \\(\\pi&#39;\\) such that for each state \\(s\\), \\[ \\pi&#39;(s) \\in \\arg\\max_{a \\in \\mathcal{A}} \\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39; \\mid s,a) V^{\\pi}(s&#39;) \\Big]. \\] Then for all states \\(s \\in \\mathcal{S}\\), \\[ V^{\\pi&#39;}(s) \\;\\ge\\; V^{\\pi}(s). \\] Moreover, the inequality is strict for some state \\(s\\) unless \\(\\pi\\) is already greedy with respect to \\(V^\\pi\\) (which implies optimality). Proof. Let \\(V^{\\pi}\\) be the value function of a policy \\(\\pi\\), and define a new (possibly stochastic) policy \\(\\pi&#39;\\) that is greedy w.r.t. \\(V^{\\pi}\\): \\[ \\pi&#39;(\\cdot \\mid s) \\in \\arg\\max_{\\mu \\in \\Delta(\\mathcal{A})} \\sum_{a}\\mu(a)\\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V^{\\pi}(s&#39;)\\Big]. \\] Define the Bellman operators \\[\\begin{align*} (T^{\\pi}V)(s) &amp; := \\sum_a \\pi(a\\mid s)\\Big[R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;\\mid s,a)V(s&#39;)\\Big],\\\\ (T^{\\pi&#39;}V)(s) &amp; := \\sum_a \\pi&#39;(a\\mid s)\\Big[\\cdots\\Big]. \\end{align*}\\] Step 1: One-step improvement at \\(V^{\\pi}\\). By greediness of \\(\\pi&#39;\\) w.r.t. \\(V^{\\pi}\\), \\[ (T^{\\pi&#39;} V^{\\pi})(s) = \\max_{\\mu}\\sum_a \\mu(a)\\Big[R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;\\mid s,a)V^{\\pi}(s&#39;)\\Big] \\;\\;\\ge\\;\\; (T^{\\pi} V^{\\pi})(s) = V^{\\pi}(s), \\] for all \\(s\\). Hence \\[\\begin{equation} T^{\\pi&#39;} V^{\\pi} \\;\\ge\\; V^{\\pi}\\quad\\text{(componentwise).} \\tag{1.32} \\end{equation}\\] Step 2: Monotonicity + contraction yield global improvement. The operator \\(T^{\\pi&#39;}\\) is monotone (order-preserving) and a \\(\\gamma\\)-contraction in the sup-norm. Apply \\(T^{\\pi&#39;}\\) repeatedly to both sides of (1.32): \\[ (T^{\\pi&#39;})^k V^{\\pi} \\;\\ge\\; (T^{\\pi&#39;})^{k-1} V^{\\pi} \\;\\ge\\; \\cdots \\;\\ge\\; V^{\\pi},\\qquad k=1,2,\\dots \\] By contraction, \\((T^{\\pi&#39;})^k V^{\\pi} \\to V^{\\pi&#39;}\\), the unique fixed point of \\(T^{\\pi&#39;}\\). Taking limits preserves the inequality, so \\[ V^{\\pi&#39;} \\;\\ge\\; V^{\\pi}\\quad\\text{statewise.} \\] Strict improvement condition. If there exists a state \\(s\\) such that \\[ (T^{\\pi&#39;} V^{\\pi})(s) \\;&gt;\\; V^{\\pi}(s), \\] then by monotonicity we have a strict increase at that state after one iteration, and the limit remains strictly larger at that state (or at any state that can reach it with positive probability under \\(\\pi&#39;\\)). This happens precisely when \\(\\pi&#39;\\) selects, with positive probability, an action \\(a\\) for which \\[ Q^{\\pi}(s,a)=R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V^{\\pi}(s&#39;) \\;&gt;\\; V^{\\pi}(s), \\] i.e., when \\(\\pi\\) was not already greedy (optimal) at \\(s\\). 1.2.5 Policy Iteration The policy improvement lemma and the principle of optimality, combined together, leads to the first algorithm that guarantees convergence to an optimal policy. This algorithm is called policy iteration. Theorem 1.3 (Convergence of Policy Iteration) Consider a discounted MDP with finite state and action sets and \\(\\gamma\\in[0,1)\\). Let \\(\\{\\pi_k\\}_{k\\ge0}\\) be the sequence produced by Policy Iteration (PI): Policy evaluation: compute \\(V^{\\pi_k}\\) such that \\(V^{\\pi_k}=T^{\\pi_k}V^{\\pi_k}\\). Policy improvement: choose \\(\\pi_{k+1}\\) greedy w.r.t. \\(V^{\\pi_k}\\): \\[ \\pi_{k+1}(s) \\in \\arg\\max_{a}\\Big[ R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;|s,a)\\,V^{\\pi_k}(s&#39;)\\Big]. \\] Then: \\(V^{\\pi_{k+1}} \\ge V^{\\pi_k}\\) componentwise, and the inequality is strict for some state unless \\(\\pi_{k+1}=\\pi_k\\). If \\(\\pi_{k+1}=\\pi_k\\), then \\(V^{\\pi_k}\\) satisfies the Bellman optimality equation; hence \\(\\pi_k\\) is optimal and \\(V^{\\pi_k}=V^*\\). Because the number of stationary policies is finite, PI terminates in finitely many iterations at an optimal policy \\(\\pi^*\\) with value \\(V^*\\). \\(\\Vert V^{\\pi_{k+1}} - V^\\star \\Vert_{\\infty} \\leq \\gamma \\Vert V^{\\pi_k} - V^\\star \\Vert_{\\infty}\\), for any \\(k\\) (i.e., contraction). Proof. By the policy improvement lemma, we have \\[ V^{\\pi_{k+1}} \\geq V^{\\pi_k}. \\] By monotonicity of the Bellman operator \\(T^{\\pi_{k+1}}\\), we have \\[ V^{\\pi_{k+1}} = T^{\\pi_{k+1}} V^{\\pi_{k+1}} \\geq T^{\\pi_{k+1}} V^{\\pi_k}. \\] By definition of the Bellman optimality operator, we have \\[ T^{\\pi_{k+1}} V^{\\pi_k} = T^\\star V^{\\pi_k}. \\] Therefore, \\[ 0 \\geq V^{\\pi_{k+1}} - V^\\star \\geq T^{\\pi_{k+1}} V^{\\pi_k} - V^\\star = T^\\star V^{\\pi_k} - T^\\star V^\\star \\] As a result, \\[ \\Vert V^{\\pi_{k+1}} - V^\\star \\Vert_{\\infty} \\leq \\Vert T^\\star V^{\\pi_k} - T^\\star V^\\star \\Vert_{\\infty} \\leq \\gamma \\Vert V^{\\pi_k} - V^\\star \\Vert_{\\infty}. \\] This proves the contraction result (d). Let us apply Policy Iteration to the inverted pendulum problem. Example 1.4 (Policy Iteration for Inverted Pendulum) The following code performs policy iteration for the inverted pendulum problem. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 101 N_thetadot = 101 N_u = 51 theta_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_theta) thetadot_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) nn_idx = nn_idx[np.argsort(dists[nn_idx])] # sort those 3 by distance d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): # reward at current (s,a) R[s, a] = reward(th, thd, u) # next continuous state th_n, thd_n = step_euler(th, thd, u) # map to 3 nearest grid states nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ======================= # POLICY ITERATION # ======================= # Represent policy as a deterministic action index per state: pi[s] in {0..A-1} # Start from uniform-random policy (deterministic tie-breaker: middle action) pi = np.full(S, A // 2, dtype=int) def policy_evaluation(pi, V_init=None, tol=1e-6, max_iters=10000): &quot;&quot;&quot;Iterative policy evaluation for deterministic pi (action index per state).&quot;&quot;&quot; V = np.zeros(S) if V_init is None else V_init.copy() for k in range(max_iters): # For each state s, use chosen action a = pi[s] a = pi # shape (S,) # Expected next value under chosen action EV_next = (NS_prob[np.arange(S), a] * V[NS_idx[np.arange(S), a]]).sum(axis=1) # (S,) V_new = R[np.arange(S), a] + gamma * EV_next if np.max(np.abs(V_new - V)) &lt; tol: # print(f&quot;Policy evaluation converged in {k+1} iterations.&quot;) return V_new V = V_new # print(&quot;Policy evaluation reached max_iters without meeting tolerance.&quot;) return V def policy_improvement(V, pi_old=None): &quot;&quot;&quot;Greedy improvement: pi&#39;(s) = argmax_a [ R(s,a) + gamma * E[V(s&#39;)] ].&quot;&quot;&quot; # Compute Q(s,a) = R + gamma * sum_j P(s,a,j) V(ns_j) EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # (S, A) Q = R + gamma * EV_next # (S, A) pi_new = np.argmax(Q, axis=1).astype(int) # greedy deterministic policy stable = (pi_old is not None) and np.array_equal(pi_new, pi_old) return pi_new, stable # Main PI loop max_pi_iters = 100 V = np.zeros(S) for it in range(max_pi_iters): # Policy evaluation V = policy_evaluation(pi, V_init=V, tol=1e-6, max_iters=10000) # Policy improvement pi_new, stable = policy_improvement(V, pi_old=pi) print(f&quot;[PI] Iter {it+1}: policy changed = {not stable}&quot;) pi = pi_new if stable: print(&quot;Policy iteration converged: policy stable.&quot;) break else: print(&quot;Reached max_pi_iters without policy stability (may still be near-optimal).&quot;) # ----- Visualization ----- V_grid = V.reshape(N_theta, N_thetadot) fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^{\\pi}(\\theta,\\dot{\\theta})$ (final PI)&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V$ after Policy Iteration&quot;) plt.tight_layout() plt.show() # Visualize the greedy action *value* (torque) pi_grid = pi.reshape(N_theta, N_thetadot) # action indices action_values = u_grid[pi_grid] # map indices -&gt; torques plt.figure(figsize=(7,5), dpi=120) im = plt.imshow(action_values, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;coolwarm&quot;) # diverging colormap good for ± torque cbar = plt.colorbar(im) cbar.set_label(&quot;Greedy action value (torque)&quot;) plt.xlabel(r&quot;$\\dot{\\theta}$&quot;) plt.ylabel(r&quot;$\\theta$&quot;) plt.title(&quot;Greedy policy (torque) after PI&quot;) plt.tight_layout() plt.show() Running the code produces the optimal value function shown in Fig. 1.5 and the optimal policy shown in Fig. 1.6. Figure 1.5: Optimal Value Function after Policy Iteration Figure 1.6: Optimal Policy after Policy Iteration We can apply the optimal policy to the pendulum with an initial state of \\((-\\pi, 0)\\) (i.e., the bottomright position). Fig. 1.7 plots the rollout trajectory of \\(\\theta, \\dot{\\theta}, u\\). We can see that the optimal policy is capable of performing “bang-bang” control to accumulate energy before swinging up. Fig. 1.8 overlays the trajectory on top of the optimal value function. You can play with the code here. Figure 1.7: Optimal Trajectory of Pendulum Swing-Up Figure 1.8: Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function 1.2.6 Value Iteration Policy iteration—as the name suggests—iterates on policies: it alternates between (1) policy evaluation (computing \\(V^{\\pi}\\) for the current policy \\(\\pi\\)) and (2) policy improvement (making \\(\\pi\\) greedy w.r.t. \\(V^{\\pi}\\)). An alternative, often very effective, method is value iteration. Unlike policy iteration, value iteration does not explicitly maintain a policy during its updates; it iterates directly on the value function toward the fixed point of the Bellman optimality* operator. Once the value function has (approximately) converged, the optimal policy is obtained by a single greedy extraction step. Note that intermediate value iterates need not correspond to the value of any actual policy. The value iteration (VI) algorithm works as follows: Initialization. Choose any \\(V_0:\\mathcal{S}\\to\\mathbb{R}\\) (e.g., \\(V_0 \\equiv 0\\)). Iteration. For \\(k=0,1,2,\\dots\\), \\[ V_{k+1}(s) \\;\\leftarrow\\; \\max_{a\\in\\mathcal{A}} \\Big[\\, R(s,a) \\;+\\; \\gamma \\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\; V_k(s&#39;) \\,\\Big], \\quad \\forall s\\in\\mathcal{S}. \\] Stopping rule. Stop when \\(\\lVert V_{k+1}-V_k\\rVert_\\infty \\le \\varepsilon\\) (or any chosen tolerance). Policy extraction (greedy): \\[ \\pi_{k+1}(s) \\in \\arg\\max_{a\\in\\mathcal{A}} \\Big[\\, R(s,a) \\;+\\; \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\; V_{k+1}(s&#39;) \\,\\Big]. \\] The following theorem states the convergence of value iteration. Theorem 1.4 (Convergence of Value Iteration) Let \\(T^\\star\\) be the Bellman optimality operator, \\[ (T^\\star V)(s) := \\max_{a}\\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V(s&#39;) \\Big]. \\] For \\(\\gamma\\in[0,1)\\) and finite \\(\\mathcal{S},\\mathcal{A}\\), \\(T^\\star\\) is a \\(\\gamma\\)-contraction in the sup-norm. Hence, for any \\(V_0\\), \\[ V_k \\;=\\; (T^\\star )^k V_0 \\;\\xrightarrow[k\\to\\infty]{}\\; V^*, \\] the unique fixed point of \\(T^\\star\\). Moreover, the greedy policy \\(\\pi_k\\) extracted from \\(V_k\\) converges to an optimal policy \\(\\pi^\\star\\). In addition, after \\(k\\) iterations, we have \\[ \\lVert V_k - V^* \\rVert_\\infty \\;\\le\\; \\gamma^k \\, \\lVert V_0 - V^* \\rVert_\\infty. \\] Finally, we apply value iteration to the inverted pendulum problem. Example 1.5 (Value Iteration for Inverted Pendulum) The following code performs value iteration for the inverted pendulum problem. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 101 N_thetadot = 101 N_u = 51 theta_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_theta) thetadot_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) nn_idx = nn_idx[np.argsort(dists[nn_idx])] # sort those 3 by distance d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): R[s, a] = reward(th, thd, u) th_n, thd_n = step_euler(th, thd, u) nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ======================= # VALUE ITERATION # ======================= # Bellman optimality update: # V_{k+1}(s) = max_a [ R(s,a) + gamma * sum_j P(s,a,j) * V_k(ns_j) ] V = np.zeros(S) tol = 1e-6 max_vi_iters = 1000 for k in range(max_vi_iters): # Expected next V for every (s,a), given current V_k EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # shape (S, A) Q = R + gamma * EV_next # shape (S, A) V_new = np.max(Q, axis=1) # greedy backup over actions delta = np.max(np.abs(V_new - V)) # Optional: a stopping rule aligned with policy loss bound could scale tol # e.g., stop when delta &lt;= tol * (1 - gamma) / (2 * gamma) if delta &lt; tol: V = V_new print(f&quot;Value Iteration converged in {k+1} iterations (sup-norm change {delta:.2e}).&quot;) break V = V_new else: print(f&quot;Reached max_vi_iters={max_vi_iters} (last sup-norm change {delta:.2e}).&quot;) # Greedy policy extraction from the final V EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # recompute with final V Q = R + gamma * EV_next pi = np.argmax(Q, axis=1) # deterministic greedy policy (indices) # ----- Visualization: Value function ----- V_grid = V.reshape(N_theta, N_thetadot) fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^*(\\theta,\\dot{\\theta})$ (Value Iteration)&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V$ after Value Iteration&quot;) plt.tight_layout() plt.show() # ----- Visualization: Greedy torque field ----- pi_grid = pi.reshape(N_theta, N_thetadot) # action indices action_values = u_grid[pi_grid] # map indices -&gt; torques plt.figure(figsize=(7,5), dpi=120) im = plt.imshow( action_values, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;coolwarm&quot; # good for ± torque ) cbar = plt.colorbar(im) cbar.set_label(&quot;Greedy action value (torque)&quot;) plt.xlabel(r&quot;$\\dot{\\theta}$&quot;) plt.ylabel(r&quot;$\\theta$&quot;) plt.title(&quot;Greedy policy (torque) extracted from Value Iteration&quot;) plt.tight_layout() plt.show() Try it for yourself here! You should obtain the same results as policy iteration. "],["value-rl.html", "Chapter 2 Value-based Reinforcement Learning 2.1 Tabular Methods 2.2 Function Approximation", " Chapter 2 Value-based Reinforcement Learning In Chapter 1, we introduced algorithms for policy evaluation, policy improvement, and computing optimal policies in the tabular setting when the model is known. These dynamic-programming methods are grounded in Bellman consistency and optimality and come with strong convergence guarantees. A key limitation of the methods in Chapter 1 is that they require the transition dynamics \\(P(s&#39; \\mid s, a)\\) to be known. While in some applications modeling the dynamics is feasible (e.g., the inverted pendulum), in many others it is costly or impractical to obtain an accurate model of the environment (e.g., a humanoid robot interacting with everyday objects). This motivates relaxing the known-dynamics assumption and asking whether we can design algorithms that learn purely from interaction—i.e., by collecting data through environment interaction. This brings us to model-free reinforcement learning. In this chapter we focus on value-based RL methods. The central idea is to learn the value functions—\\(V(s)\\) and \\(Q(s,a)\\)—from interaction with the environment and then leverage these estimates to derive (approximately) optimal policies. We begin with tabular methods and then move to function-approximation approaches (e.g., neural networks) for problems where a tabular representation is intractable. 2.1 Tabular Methods Consider an infinite-horizon Markov decision process (MDP) \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma), \\] with a discount factor \\(\\gamma \\in [0,1)\\). We focus on the tabular setting where both the state space \\(\\mathcal{S}\\) and the action space \\(\\mathcal{A}\\) are finite, with cardinalities \\(|\\mathcal{S}|\\) and \\(|\\mathcal{A}|\\), respectively. A policy is a stationary stochastic mapping \\[ \\pi: \\mathcal{S} \\to \\Delta(\\mathcal{A}), \\] where \\(\\pi(a \\mid s)\\) denotes the probability of selecting action \\(a\\) in state \\(s\\). Unlike in Chapter 1, here we do not assume knowledge of the transition dynamics \\(P\\) or the reward function \\(R\\) (other than that \\(R\\) is deterministic). Instead, we assume we can interact with the environment and obtain trajectories of the form \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots), \\] by following a policy \\(\\pi\\). 2.1.1 Policy Evaluation We first consider the problem of estimating the value function of a given policy \\(\\pi\\). Recall the definition of the state-value function associated with \\(\\pi\\) is: \\[\\begin{equation} V^{\\pi}(s) = \\mathbb{E}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right], \\tag{2.1} \\end{equation}\\] where the expectation is taken over the randomness of both the policy \\(\\pi\\) and the transition dynamics \\(P\\). 2.1.1.1 Monte Carlo Estimation The basic idea of Monte Carlo (MC) estimation is to approximate the value function \\(V^\\pi\\) by averaging empirical returns observed from sampled trajectories generated under policy \\(\\pi\\). Since the return is defined as the discounted sum of future rewards, MC methods replace the expectation in the definition of \\(V^\\pi\\) with an average over sampled trajectories. Episodic Assumption. To make Monte Carlo methods well-defined, we restrict attention to the episodic setup, where each trajectory terminates upon reaching a terminal state (and the rewards thereafter are always zero). This ensures that the return is finite and can be computed exactly for each trajectory. Concretely, if an episode terminates at time \\(T\\), the return starting from time \\(t\\) is \\[\\begin{equation} g_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k} = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t-1} r_{T-1}. \\tag{2.2} \\end{equation}\\] Algorithmic Form. Let \\(\\mathcal{D}(s)\\) denote the set of all time indices at which state \\(s\\) is visited across sampled episodes. Then the Monte Carlo estimate of the value function is \\[\\begin{equation} \\hat{V}(s) = \\frac{1}{|\\mathcal{D}(s)|} \\sum_{t \\in \\mathcal{D}(s)} g_t. \\tag{2.3} \\end{equation}\\] There are two common variants: First-visit MC: use only the first occurrence of \\(s\\) in each episode. Every-visit MC: use all occurrences of \\(s\\) within an episode. Both variants converge to the same value function in the limit of infinitely many episodes. Incremental Implementation. Monte Carlo can be written as an incremental stochastic-approximation update that uses the return \\(g_t\\) as the target and a diminishing step size. Let \\(N(s)\\) be the number of (first- or every-) visits to state \\(s\\) that have been used to update \\(\\hat V(s)\\) so far, and let \\(g_t\\) be the return computed at a particular visit time \\(t\\in\\mathcal{D}(s)\\). Then the MC update is \\[\\begin{equation} \\hat V(s) \\;\\leftarrow\\; \\hat V(s) + \\alpha_{N(s)}\\,\\big( g_t - \\hat V(s) \\big), \\qquad \\alpha_{N(s)} &gt; 0 \\text{ diminishing.} \\tag{2.4} \\end{equation}\\] A canonical choice is the sample-average step size \\(\\alpha_{N(s)} = 1/N(s)\\), which yields the recurrence \\[\\begin{align} \\hat V_{N}(s) = \\hat V_{N-1}(s) + \\tfrac{1}{N}\\big(g_t - \\hat V_{N-1}(s)\\big) &amp; = \\Big(1-\\tfrac{1}{N}\\Big)\\hat V_{N-1}(s) + \\tfrac{1}{N}\\, g_t \\\\ &amp; = \\frac{N-1}{N} \\frac{1}{N-1} \\sum_{i=1}^{N-1} g_{t,i} + \\frac{1}{N} g_t \\\\ &amp; = \\frac{1}{N} \\sum_{i=1}^N g_{t,i} \\end{align}\\] so that \\(\\hat V_{N}(s)\\) equals the average of the \\(N\\) observed returns for \\(s\\) (i.e., Eq. (2.3)). In the above equation, I have used \\(g_{t,i}\\) to denote the \\(i\\)-th return before \\(g_t\\) was collected (and \\(g_t = g_{t,N}\\)). More generally, any diminishing schedule satisfying \\[ \\sum_{n=1}^\\infty \\alpha_n = \\infty, \\qquad \\sum_{n=1}^\\infty \\alpha_n^2 &lt; \\infty \\] (e.g., \\(\\alpha_n = c/(n+t_0)^p\\) with \\(1/2 &lt; p \\le 1\\)) also ensures consistency in the tabular setting. In first-visit MC, \\(N(s)\\) increases by one per episode at most; in every-visit MC, \\(N(s)\\) increases at each occurrence of \\(s\\) within an episode. Theoretical Guarantees. Unbiasedness: For any state \\(s\\), the return \\(g_t\\) is an unbiased sample of \\(V^\\pi(s)\\). \\[ \\mathbb{E}[g_t \\mid s_t = s] = V^\\pi(s). \\] Consistency: By the law of large numbers, as the number of episodes grows, \\[ \\hat{V}(s) \\xrightarrow{\\text{a.s.}} V^\\pi(s). \\] Asymptotic Normality: The MC estimator converges at rate \\(O(1/\\sqrt{N})\\), where \\(N\\) is the number of episodes used for the estimation. Limitations. Despite its conceptual simplicity, MC estimation suffers from several drawbacks: It requires episodes to terminate, making it unsuitable for continuing tasks without artificial truncation. It can only update value estimates after an episode ends, which is data-inefficient. While unbiased, MC estimates often have high variance, leading to slow convergence. These limitations motivate the study of Temporal-Difference (TD) learning, which updates value estimates online and can handle continuing tasks. 2.1.1.2 Temporal-Difference Learning While Monte Carlo methods estimate value functions by averaging full returns from complete episodes, Temporal-Difference (TD) learning provides an alternative approach that updates value estimates incrementally after each step of interaction with the environment. The key idea is to combine the sampling of Monte Carlo with the bootstrapping of dynamic programming. High-Level Intuition. TD learning avoids waiting until the end of an episode by using the Bellman consistency equation as a basis for updates. Recall that for any policy \\(\\pi\\), the Bellman consistency equation reads: \\[\\begin{equation} V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} \\left[ R(s,a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(s&#39; \\mid s, a)} V(s&#39;) \\right]. \\tag{2.5} \\end{equation}\\] At a high level, TD learning turns the expectation in Bellman equation into sampling. At each step, it updates the current estimate of the value function toward a one-step bootstrap target: the immediate reward plus the discounted value of the next state. This makes TD methods more data-efficient and applicable to continuing tasks without terminal states. Algorithmic Form. Suppose the agent is in state \\(s_t\\), takes action \\(a_t \\sim \\pi(\\cdot \\mid s_t)\\), receives reward \\(r_t\\), and transitions to \\(s_{t+1}\\). The TD(0) update rule is \\[\\begin{equation} \\hat{V}(s_t) \\;\\leftarrow\\; \\hat{V}(s_t) + \\alpha \\big[ r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t) \\big], \\tag{2.6} \\end{equation}\\] where \\(\\alpha \\in (0,1]\\) is the learning rate. The term inside the brackets, \\[\\begin{equation} \\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t), \\tag{2.7} \\end{equation}\\] is called the TD error. It measures the discrepancy between the current value estimate and the bootstrap target. The algorithm updates \\(\\hat{V}(s_t)\\) in the direction of reducing this error. Theoretical Guarantees. Convergence in the Tabular Case: If each state is visited infinitely often and the learning rate sequence satisfies \\[ \\sum_t \\alpha_t = \\infty, \\; \\sum_t \\alpha_t^2 &lt; \\infty \\] then TD(0) converges almost surely to the true value function \\(V^\\pi\\). For example, choosing \\(\\alpha_t = 1/(t+1)\\) satisfies this condition. Bias–Variance Tradeoff: The TD target uses the current estimate \\(\\hat{V}(s_{t+1})\\) rather than the true value, which introduces bias. However, it has significantly lower variance than Monte Carlo estimates, often leading to faster convergence in practice. To see this, note that for TD(0), the target is a one-step bootstrap: \\[ Y_t = r_t + \\gamma \\hat{V}(s_{t+1}). \\] This replaces the true value \\(V^\\pi(s_{t+1})\\) with the current estimate \\(\\hat{V}(s_{t+1})\\). As a result, \\(Y_t\\) is biased relative to the true return. However, since it depends only on the immediate reward and the next state, the variance of \\(Y_t\\) is much lower than that of the Monte Carlo target. Limitations. TD(0) relies on bootstrapping, which introduces bias relative to Monte Carlo methods. Convergence can be slow if the learning rate is not chosen carefully. In summary, Temporal-Difference learning addresses the major limitations of Monte Carlo estimation: it works in continuing tasks, updates online at each step, and is generally more sample-efficient. However, it trades away unbiasedness for bias–variance efficiency, motivating further extensions such as multi-step TD and TD(\\(\\lambda\\)). 2.1.1.3 Multi-Step TD Learning Monte Carlo methods use the full return \\(g_t\\), while TD(0) uses a one-step bootstrap. Multi-step TD learning generalizes these two extremes by using \\(n\\)-step returns as targets. In this way, multi-step TD interpolates between Monte Carlo and TD(0). High-Level Intuition. The motivation is to balance the high variance of Monte Carlo with the bias of TD(0). Instead of waiting for a full return (MC) or using only one step of bootstrapping (TD(0)), multi-step TD uses partial returns spanning \\(n\\) steps of real rewards, followed by a bootstrap. This provides a flexible tradeoff between bias and variance. Algorithmic Form. The \\(n\\)-step return starting from time \\(t\\) is defined as \\[\\begin{equation} g_t^{(n)} = r_t + \\gamma r_{t+1} + \\dots + \\gamma^{n-1} r_{t+n-1} + \\gamma^n \\hat{V}(s_{t+n}). \\tag{2.8} \\end{equation}\\] The \\(n\\)-step TD update is \\[\\begin{equation} \\hat{V}(s_t) \\;\\leftarrow\\; \\hat{V}(s_t) + \\alpha \\big[ g_t^{(n)} - \\hat{V}(s_t) \\big], \\tag{2.9} \\end{equation}\\] where \\(g_t^{(n)}\\) replaces the one-step target in TD(0) (2.6). For \\(n=1\\): the method reduces to TD(0). For \\(n=T-t\\) (the full episode length): the method reduces to Monte Carlo. Theoretical Guarantees. Convergence in the Tabular Case: With suitable learning rates and sufficient exploration, \\(n\\)-step TD converges to \\(V^\\pi\\). Bias–Variance Tradeoff: Larger \\(n\\): lower bias, higher variance (closer to Monte Carlo). Smaller \\(n\\): higher bias, lower variance (closer to TD(0)). Intermediate \\(n\\) provides a balance that often yields faster learning in practice. Limitations. Choosing the right \\(n\\) is problem-dependent: too small and bias dominates; too large and variance grows. Requires storing \\(n\\)-step reward sequences before updating, which can increase memory and computation. In summary, multi-step TD unifies Monte Carlo and TD(0) by introducing \\(n\\)-step returns. It allows practitioners to tune the bias–variance tradeoff by selecting \\(n\\). Later, we will see how TD(\\(\\lambda\\)) averages over all \\(n\\)-step returns in a principled way, further smoothing this tradeoff. 2.1.1.4 Eligibility Traces and TD(\\(\\lambda\\)) So far, we have seen that Monte Carlo methods use full returns \\(g_t\\), while TD(0) uses a one-step bootstrap. Multi-step TD methods generalize between these two extremes by using \\(n\\)-step returns. However, a natural question arises: can we combine information from all possible \\(n\\)-step returns in a principled way? This motivates TD(\\(\\lambda\\)), which blends multi-step TD methods into a single algorithm using eligibility traces. High-Level Intuition. TD(\\(\\lambda\\)) introduces a parameter \\(\\lambda \\in [0,1]\\) that controls the weighting of \\(n\\)-step returns: \\(\\lambda = 0\\): reduces to TD(0), relying only on one-step bootstrapping. \\(\\lambda = 1\\): reduces to Monte Carlo, relying on full returns. \\(0 &lt; \\lambda &lt; 1\\): interpolates smoothly between these two extremes by averaging all \\(n\\)-step returns with exponentially decaying weights. Formally, the \\(\\lambda\\)-return is \\[\\begin{equation} g_t^{(\\lambda)} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} g_t^{(n)}, \\tag{2.10} \\end{equation}\\] where \\(g_t^{(n)}\\) is the \\(n\\)-step return defined in (2.8). Remark. To make the \\(\\lambda\\)-return well defined, we consider two cases. Episodic Case: Well-posed. If an episode terminates at time \\(T\\), let \\(N=T-t\\) be the remaining steps. Then \\[\\begin{equation} g_t^{(\\lambda)} \\;=\\; (1-\\lambda)\\sum_{n=1}^{N-1}\\lambda^{\\,n-1} \\, g_t^{(n)} \\;+\\; \\lambda^{\\,N-1}\\, g_t^{(N)}, \\tag{2.11} \\end{equation}\\] where \\(g_t^{(n)}\\) is the \\(n\\)-step return (Eq. (2.8)) and \\(g_t^{(N)}\\) is the full Monte Carlo return (Eq. (2.2)). This expression is well-defined for all \\(\\lambda\\in[0,1]\\). Note that the weights form a convex combination: \\[ (1-\\lambda)\\sum_{n=1}^{N-1}\\lambda^{n-1} + \\lambda^{N-1} = 1-\\lambda^{N-1}+\\lambda^{N-1} = 1. \\] Continuing Case: Limit. Taking \\(\\lambda\\uparrow 1\\) in (2.11) gives \\[ \\lim_{\\lambda\\uparrow 1} g_t^{(\\lambda)} = g_t^{(N)} = g_t, \\] so the \\(\\lambda\\)-return reduces to the Monte Carlo return at \\(\\lambda=1\\). For continuing tasks (no terminal \\(T\\)), \\(\\lambda=1\\) is conventionally defined by this same limiting argument, yielding the infinite-horizon discounted return when \\(\\gamma&lt;1\\). Eligibility Traces. Naively computing \\(g_t^{(\\lambda)}\\) would require storing and combining infinitely many \\(n\\)-step returns, which is impractical. Instead, TD(\\(\\lambda\\)) uses eligibility traces to implement this efficiently online. An eligibility trace is a temporary record that tracks how much each state is “eligible” for updates based on how recently and frequently it has been visited. Specifically, for each state \\(s\\), we maintain a trace \\(z_t(s)\\) that evolves as \\[\\begin{equation} z_t(s) = \\gamma \\lambda z_{t-1}(s) + \\mathbf{1}\\{s_t = s\\}, \\tag{2.12} \\end{equation}\\] where \\(\\mathbf{1}\\{s_t = s\\}\\) is an indicator that equals 1 if state \\(s\\) is visited at time \\(t\\), and 0 otherwise. TD(\\(\\lambda\\)) Update Rule. At each time step \\(t\\), we compute the TD error \\[ \\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t), \\] as in (2.7). Then, for each state \\(s\\), we update \\[\\begin{equation} \\hat{V}(s) \\;\\leftarrow\\; \\hat{V}(s) + \\alpha \\, \\delta_t \\, z_t(s). \\tag{2.13} \\end{equation}\\] Thus, all states with nonzero eligibility traces are updated simultaneously, with the magnitude of the update determined by both the TD error and the eligibility trace. See Proposition 2.1 below for a justification. Theoretical Guarantees. In the tabular case, TD(\\(\\lambda\\)) converges almost surely to the true value function \\(V^\\pi\\) under the usual stochastic approximation conditions (sufficient exploration, decaying step sizes). The parameter \\(\\lambda\\) directly controls the bias–variance tradeoff: Smaller \\(\\lambda\\): more bootstrapping, more bias but lower variance. Larger \\(\\lambda\\): less bootstrapping, less bias but higher variance. TD(\\(\\lambda\\)) can be shown to converge to the fixed point of the \\(\\lambda\\)-operator, which is itself a contraction mapping. In summary, eligibility traces provide an elegant mechanism to combine the advantages of Monte Carlo and TD learning. TD(\\(\\lambda\\)) introduces a spectrum of algorithms: at one end TD(0), at the other Monte Carlo, and in between a family of methods balancing bias and variance. In practice, intermediate values such as \\(\\lambda \\approx 0.9\\) often work well. Proposition 2.1 (Forward–Backward Equivalence) Consider one episode \\(s_0,a_0,r_0,\\ldots,s_T\\) with \\(\\hat V(s_T)=0\\). Let the forward view apply updates at the end of the episode: \\[ \\hat V(s_t) \\leftarrow \\hat V(s_t) + \\alpha \\big[g_t^{(\\lambda)}-\\hat V(s_t)\\big], \\quad t=0,\\ldots,T-1, \\] where \\(g_t^{(\\lambda)}\\) is the \\(\\lambda\\)-return in (2.10) with the \\(n\\)-step returns \\(g_t^{(n)}\\) from (2.8), and where \\(\\hat V\\) is kept fixed while computing all \\(g_t^{(\\lambda)}\\). Let the backward view run through the episode once, using the TD error \\(\\delta_t\\) from (2.7) and eligibility traces \\(z_t(s)\\) from (2.12), and then apply the cumulative update \\[ \\Delta_{\\text{back}} \\hat V(s) \\;=\\; \\alpha \\sum_{t=0}^{T-1} \\delta_t\\, z_t(s). \\] Then, for every state \\(s\\), \\[ \\Delta_{\\text{back}} \\hat V(s) \\;=\\; \\alpha \\sum_{t:\\, s_t=s}\\big[g_t^{(\\lambda)}-\\hat V(s_t)\\big], \\] i.e., the net parameter change produced by (2.13) equals that of the \\(\\lambda\\)-return updates. Proof. Fix a state \\(s\\). Using (2.12), \\[ z_t(s)=\\sum_{k=0}^{t}(\\gamma\\lambda)^{\\,t-k}\\,\\mathbf{1}\\{s_k=s\\}. \\] Hence \\[ \\sum_{t=0}^{T-1}\\delta_t z_t(s) =\\sum_{t=0}^{T-1}\\delta_t \\sum_{k=0}^{t}(\\gamma\\lambda)^{\\,t-k}\\mathbf{1}\\{s_k=s\\} =\\sum_{k:\\,s_k=s}\\; \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{\\,t-k}\\delta_t . \\tag{1} \\] Write \\(\\delta_t=r_t+\\gamma\\hat V(s_{t+1})-\\hat V(s_t)\\) and split the inner sum: \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = \\underbrace{\\sum_{t=k}^{T-1} \\gamma^{t-k}\\lambda^{t-k} r_t}_{\\text{(A)}} + \\underbrace{\\sum_{t=k}^{T-1}\\gamma^{t-k}\\lambda^{t-k}(\\gamma\\hat V(s_{t+1})-\\hat V(s_t))}_{\\text{(B)}}. \\] Term (B) telescopes. Shifting index in the first part of (B), \\[ \\sum_{t=k}^{T-1}\\gamma^{t-k}\\lambda^{t-k}\\gamma \\hat V(s_{t+1}) = \\sum_{t=k+1}^{T}\\gamma^{t-k}\\lambda^{t-1-k}\\hat V(s_t). \\] Therefore \\[ \\text{(B)}= -\\hat V(s_k) + \\sum_{t=k+1}^{T-1}\\gamma^{t-k}\\lambda^{t-1-k}(1-\\lambda)\\hat V(s_t) + \\underbrace{\\gamma^{T-k}\\lambda^{T-1-k}\\hat V(s_T)}_{=\\,0}. \\tag{2} \\] Combining (A) and (2), and reindexing with \\(n=t-k\\), \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = -\\hat V(s_k) + \\sum_{n=0}^{T-1-k}\\gamma^{n}\\lambda^{n} r_{k+n} + (1-\\lambda)\\sum_{n=1}^{T-1-k}\\gamma^{n}\\lambda^{n-1}\\hat V(s_{k+n}). \\tag{3} \\] On the other hand, expanding the \\(\\lambda\\)-return (2.10), \\[ \\begin{aligned} g_k^{(\\lambda)} &amp;=(1-\\lambda)\\sum_{n=1}^{T-k}\\lambda^{n-1} \\Bigg(\\sum_{m=0}^{n-1}\\gamma^{m} r_{k+m} + \\gamma^{n}\\hat V(s_{k+n})\\Bigg)\\\\ &amp;= \\sum_{n=0}^{T-1-k}\\gamma^{n}\\lambda^{n} r_{k+n} + (1-\\lambda)\\sum_{n=1}^{T-1-k}\\gamma^{n}\\lambda^{n-1}\\hat V(s_{k+n}), \\end{aligned} \\tag{4} \\] where we used that \\(\\hat V(s_T)=0\\). Comparing (3) and (4) yields \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = g_k^{(\\lambda)} - \\hat V(s_k). \\tag{5} \\] Substituting (5) into (1) and multiplying by \\(\\alpha\\) completes the proof. Example 2.1 (Policy Evaluation (MC and TD Family)) We consider the classic random-walk MDP with terminal states: States: \\(\\{0,1,2,3,4,5,6\\}\\), where \\(0\\) and \\(6\\) are terminal; nonterminal states are \\(1{:}5\\). Actions: \\(\\{-1,+1\\}\\) (“Left”/“Right”). Dynamics: From a nonterminal state \\(s\\in\\{1,\\dots,5\\}\\), action \\(-1\\) moves to \\(s-1\\), and action \\(+1\\) moves to \\(s+1\\). Rewards: Transitioning into state \\(6\\) yields reward \\(+1\\); all other transitions yield \\(0\\). Discount: \\(\\gamma=1\\) (episodic task). Episodes start at state \\(s_0=3\\) and terminate upon reaching \\(\\{0,6\\}\\). We evaluate the equiprobable policy \\(\\pi\\) that chooses Left/Right with probability \\(1/2\\) each at every nonterminal state. Under this policy, the true state-value function on nonterminal states \\(s\\in\\{1,\\dots,5\\}\\) is \\[\\begin{equation} V^\\pi(s) \\;=\\; \\frac{s}{6}. \\tag{2.14} \\end{equation}\\] We compare four tabular policy-evaluation methods: Monte Carlo (MC), first-visit — episodic, sample-average updates. TD(0) — one-step bootstrap. \\(n\\)-step TD — here we use \\(n=3\\) (intermediate between MC and TD(0)). TD(\\(\\lambda\\)) — accumulating eligibility traces (we illustrate with \\(\\lambda=0.9\\)). All methods estimate \\(V^\\pi\\) from trajectories generated by \\(\\pi\\). Diminishing Step Sizes. To ensure convergence in the tabular setting and to make the comparison fair (since MC uses sample averages), the TD-family methods use per-state diminishing step sizes of the form \\[\\begin{equation} \\alpha_t(s) \\;=\\; \\frac{c}{\\big(N_t(s)+t_0\\big)^p}, \\qquad \\tfrac{1}{2} &lt; p \\le 1, \\tag{2.15} \\end{equation}\\] where \\(N_t(s)\\) counts how many times \\(V(s)\\) has been updated up to time \\(t\\). A common choice is \\(p=1\\) with moderate \\(c&gt;0\\) and \\(t_0&gt;0\\). Error Metric. We report the mean-squared error (MSE) over nonterminal states after each episode: \\[\\begin{equation} \\mathrm{MSE}_t \\;=\\; \\frac{1}{5}\\sum_{s=1}^{5}\\big(\\hat V_t(s)-V^\\pi(s)\\big)^2, \\tag{2.16} \\end{equation}\\] where \\(V^\\pi\\) is given by (2.14). Curves are averaged over multiple random seeds. Fig. 2.1 shows the MSE versus episodes for MC, TD(0), 3-step TD, and TD(\\(\\lambda\\)) under the diminishing step-size schedule (2.15). You are encouraged to play with the parameters of these algorithms in the code here. Figure 2.1: Policy Evaluation, MC versus TD Family 2.2 Function Approximation "],["policy-gradient.html", "Chapter 3 Policy Gradients", " Chapter 3 Policy Gradients "],["appconvex.html", "A Convex Analysis and Optimization A.1 Theory A.2 Practice", " A Convex Analysis and Optimization A.1 Theory A.1.1 Sets Convex set is one of the most important concepts in convex optimization. Checking convexity of sets is crucial to determining whether a problem is a convex problem. Here we will present some definitions of some set notations in convex optimization. Definition A.1 (Affine set) A set \\(C\\subset \\mathbb{R}^n\\) is affine if the line through any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in \\mathbb{R}\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition A.2 (Convex set) A set \\(C\\subset \\mathbb{R}^n\\) is convex if the line segment between any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition A.3 (Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a cone if for any \\(x\\in C\\) and any \\(\\theta\\geq 0\\), we have \\(\\theta x \\in C\\). Definition A.4 (Convex Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a convex cone if \\(C\\) is convex and a cone. Below are some important examples of convex sets: Definition A.5 (Hyperplane) A hyperplane is a set of the form \\[\\{x|a^Tx = b\\}\\] Definition A.6 (Halfspaces) A (closed) halfspace is a set of the form \\[\\{x|a^Tx \\leq b\\}\\] Definition A.7 (Balls) A ball is a set of the form \\[B(x,r) = \\{y|\\|y-x\\|_2 \\leq r\\} = \\{x+ru|\\|u\\|_2\\leq 1\\}\\] where \\(r &gt;0\\). Definition A.8 (Ellipsoids) A ellipsoid is a set of the form \\[\\mathcal{E} = \\{y|(y-x)^TP^{-1}(y-x)\\leq 1\\}\\] where \\(P\\) is symmetric and positive definite. Definition A.9 (Polyhedra) A polyhedra is defined as the solution set of a finite number of linear equalities and inequalities: \\[\\mathcal{P} = \\{x|a_j^Tx\\leq b_j, j=1,...,m, c_k^Tx=d_k,k=1,...,p\\}\\] Definition A.10 (Norm ball) A norm ball \\(B\\) of radius \\(r\\) and a center \\(x_c\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[B = \\{x|\\|x-x_c\\|\\leq r\\}\\] Definition A.11 (Norm cone) A norm cone \\(C\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[C = \\{(x,t)|\\|x\\|\\leq t\\}\\subset \\mathbb{R}^{n+1}\\] Simplexes are important family of polyhedra. Suppose the \\(k+1\\) points \\(v_0,...,v_k\\in \\mathbb{R}^n\\) are affinely independent, which means \\(v_1-v_0,...,v_k-v_0\\) are linearly independent. Definition A.12 (Simplex) A simplex \\(C\\) defined by points \\(v_0,...,v_k\\) is: \\[C = \\textbf{conv}\\{v_0,...,v_k\\} = \\{\\theta_0v_0 + ... \\theta_kv_k|\\theta \\succeq 0, \\textbf{1}^T\\theta = 1\\}\\] Extremely important examples of convex sets are positive semidefinite cones: Definition A.13 (Symmetric,positive semidefinite,positive definite matrices) Symmetric matrices: \\(\\textbf{S}^n = \\{X\\in\\mathbb{R}^{n\\times n}| X=X^T\\}\\) Symmetric Positive Semidefinite matrices: \\(\\textbf{S}_+^n = \\{X\\in\\textbf{S}^n| X\\succeq0\\}\\) Symmetric Positive definite matrices: \\(\\textbf{S}_{++}^n = \\{X\\in\\textbf{S}^n| X\\succ0\\}\\) In most scenarios, the set we encounter is more complicated. In general it is extermely hard to determine whether a set in convex or not. But if the set is ‘generated’ by some convex sets, we can easily determine its convexity. So let’s focus on operations that preserve convexity: Proposition A.1 Assume \\(S\\) is convex, \\(S_\\alpha,\\alpha\\in\\mathcal{A}\\) is a family of convex sets. Following operations on convex sets will preserve convexity: Intersection: \\(\\bigcap_{\\alpha\\in\\mathcal{A}}S_\\alpha\\) is convex. Image under affine function: A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is affine if it has the form \\(f(x) = Ax+b\\). The image of \\(S\\) under affine function \\(f\\) is convex. I.e. \\(f(S) = \\{f(x)|x\\in S\\}\\) is convex Image under perspective function: We define the perspective function \\(P:\\mathbb{R}^{n+1}\\), with domain \\(\\textbf{dom}P = \\mathbb{R}^n\\times \\mathbb{R}_{++}\\)(where \\(\\mathbb{R}_{++}=\\{x\\in \\mathbb{R}|x&gt;0\\}\\)) as \\(P(z,t) = z/t\\). The image of \\(S\\) under perspective function is convex. Image under linear-fractional function: We define linear fractional function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) as:\\(f(x) = (Ax+b)/(c^Tx+d)\\) with \\(\\textbf{dom}f = \\{x|c^Tx+d&gt;0\\|\\). The image of \\(S\\) under linear fractional functions is convex. In some cases, the restrictions of interior is too strict. For example, imagine a plane in \\(\\mathbb{R}^3\\). The interior of the plane is \\(\\emptyset\\). But intuitively many property should be extended to this kind of situation. Because the points in the plane also lies ‘inside’ the convex set. Thus, we will define relative interior. First we will define affine hull. Definition A.14 (Affine hull) The affine hull of a set \\(S\\) is the smallest affine set that contains \\(S\\), which can be written as: \\[\\text{aff}(S) = \\{\\sum_{i=1}^k\\alpha_ix_i|k&gt;0,x_i\\in S,\\alpha_i\\in\\mathbb{R},\\sum_{i=1}^k\\alpha_i=1\\}\\] Definition A.15 (Relative Interior) The relative interior of a set \\(S\\) (denoted \\(\\text{relint}(S)\\)) is defined as its interior within the affine hull of \\(S\\). I.e. \\[\\text{relint}(S):=\\{x\\in S: \\text{there exists } \\epsilon&gt;0 \\text{ such that }N_\\epsilon \\cap \\text{aff}(S)\\subset S\\}\\] where \\(N_\\epsilon(x)\\) is a ball of radius \\(\\epsilon\\) centered on \\(x\\). A.1.2 Convex function In this section, let’s define convex functions: Definition A.16 (Convex function) A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if \\(\\textbf{dom}\\ f\\) is convex and \\(\\forall x,y\\in \\textbf{dom}\\ f\\) and with \\(\\theta \\in [0,1]\\), we have:\\[f(\\theta x +(1-\\theta)y)\\leq \\theta f(x) + (1-\\theta)f(y)\\] The function is strictly convex if the inequality holds whenever \\(x\\neq y\\) and \\(\\theta\\in (0,1)\\). If a function is differentiable, it will be easier for us to check its convexity: Proposition A.2 (Conditions for Convex function) 1.(First order condition) Suppose \\(f\\) is differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x,y\\in \\textbf{dom} f\\), \\[f(y)\\geq f(x) +\\nabla f(x)^T(y-x)\\] 2.(Second order conditions) Suppose \\(f\\) is twice differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x\\in \\textbf{dom} f\\), \\[\\nabla^2 f(x) \\succeq \\textbf{0}\\] For the same purpose, some operations that preserve the convexity of the convex functions are presented here: Proposition A.3 (Operations that preserve convexity) Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be a convex function and \\(g_1,...,g_n\\) be convex functions. The following operations will preserve convexity of the function: 1.(Nonnegative weighted sum): A nonnegative weighted sum of convex functions: \\[f = \\omega_1f_1 + ... +\\omega_mf_m\\] 2.(Composition with an affine mapping) Suppose \\(A\\in \\mathbb{R}^{n\\times m}\\) and \\(b\\in \\mathbb{R}^n\\), then \\(g(x) = f(Ax+b)\\) is convex. 3.(Pointwise maximum and supremum) \\(g(x) = \\max\\{g_1(x),...,g_n(x)\\}\\) is convex. If \\(h(x,y)\\) is convex in \\(x\\) for each \\(y\\in\\mathcal{A}\\), then \\(\\sup_{y\\in\\mathcal{A}} h(x,y)\\) is also convex in \\(x\\). 4.(Minimization) If \\(h(x,y)\\) is convex in \\((x,y)\\), and \\(C\\) is a convex nonempty set, then \\(\\inf_{x\\in C} h(x,y)\\) is convex in \\(x\\). 5.(Perspective of a function) The perspective of \\(f\\) is the function \\(h:\\mathbb{R}^{n+1}\\to\\mathbb{R}\\) defined by: \\(h(x,t) = tf(x/t)\\) with domain \\(\\textbf{dom}\\ h=\\{(x,t)|x/t\\in\\textbf{dom} f,t&gt;0\\}\\). And \\(h\\) is convex. A.1.3 Lagrange dual We consider an optimization problem in the standard form (without assuming convexity of anything): \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; h_i(x) = 0\\quad i=1,...,p \\\\ \\end{aligned} \\end{equation}\\] Definition A.17 (Lagrange dual function) The Lagrangian related to the problem above is defined as: \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\] The Lagrange dual function is defined as: \\[g(\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}}L(x,\\lambda,\\nu)\\] When the Lagrangian is unbounded below in \\(x\\), the dual function takes on the value \\(-\\infty\\). Note that since the Lagrange dual function is a pointwise infimum of a family of affine functions of \\((\\lambda,\\nu)\\), so it’s concave. The Lagrange dual function will give us lower bounds of the optimal value of the original problem: \\[g(\\lambda,\\nu)\\leq p^*\\]. We can see that, the dual function can give a nontrivial lower bound only when \\(\\lambda\\succeq 0\\). Thus we can solve the following dual problem to get the best lower bound. Definition A.18 (Lagrange dual problem) The lagrangian dual problem is defined as follows: \\[\\begin{equation} \\begin{aligned} d^* = \\quad \\max_{\\lambda,\\nu} \\quad &amp; g(\\lambda,\\nu)\\\\ \\textrm{s.t.} \\quad &amp; \\lambda\\succeq 0 \\end{aligned} \\end{equation}\\] This is a convex optimization problem. We can easily see that \\[d^*\\leq p^*\\] always hold. This property is called weak duality. If \\[d^*=p^*\\], it’s called strong duality. Strong duality does not hold in general, but it usually holfs for convex problems. We can find conditions that guarantee strong duality in convex problems, which are called constrained qualifications. Slater’s constraint qualification is a useful one. Theorem A.1 (Slater's constraint qualification) Strong duality holds for a convex problem \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; Ax=b \\\\ \\end{aligned} \\end{equation}\\] if it is strictly feasible, i.e. \\[\\exists x\\in\\textbf{relint}\\mathcal{D}:\\quad f_i(x)&lt;0,\\quad i=1...m,\\quad Ax=b\\] And the linear inequalities do not need to hold with strict inequality. A.1.4 KKT condition Note that if strong duality holds, denote \\(x^*\\) to be primal optimal, and \\((\\lambda^*,\\nu^*)\\) to be dual optimal. Then: \\[\\begin{equation} \\begin{aligned} f_0(x^*) = g(\\lambda^*,\\nu^*) = &amp; \\inf_x(f_0(x)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x))\\\\ \\leq &amp; f_0(x^*)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x)\\\\ \\leq &amp; f_0(x^*)\\\\ \\end{aligned} \\end{equation}\\] from this, combining \\(\\lambda^*\\geq 0\\) and \\(f_i(x^*)\\leq 0\\), we can know that: \\(\\lambda_i^*f_i(x^*)=0\\quad i=1\\cdots m\\). This means for \\(\\lambda_i^*\\) and \\(f_i(x^*)\\), one of them must be zero, which is known as complementary slackness). Thus we arrived at the following four conditions, which are called KKT conditions. Theorem A.2 (Karush-Kuhn-Tucker(KKT) Conditions) The following four conditions are called KKT conditions (for a problem with differentiable \\(f_i,h_i\\)) Primal feasible: \\(f_i(x) \\leq 0,i,\\cdots ,m,\\ h_i(x) = 0,i=1,\\cdots ,p\\) Dual feasible: \\(\\lambda\\succeq0\\) Complementary slackness: \\(\\lambda_if_i(x)=0,i=1,\\cdots,m\\) Gradient of Lagrangian with respect to \\(x\\) vanishes:\\(\\nabla f_0(x)+\\sum_{i=1}^m\\lambda_i\\nabla f_i(x)+\\sum_{i=1}^p\\nu_i\\nabla h_i(x) = 0\\) From the discussion above, we know that if strong duality holds and \\(x,\\lambda,\\nu\\) are optimal, then they must satisfy the KKT conditions. Also if \\(x,\\lambda,\\nu\\) satisfy KKT for a convex problem, then they are optimal. However, the converse is not generally true, since KKT condition implies strong duality. If Slater’s condition is satisfied, then \\(x\\) is optimal if and only if there exist \\(\\lambda,\\nu\\) that satisfy KKT conditions. Sometimes, by solving the KKT system, we can derive the closed-form solution of a optimization directly. Also, sometimes we will use the residual of the KKT system as the termination condition. In general, \\(f_i,h_i\\) may not be differentiable. There are also KKT conditions for them, which will include knowledge of subdifferential and will not be included here. A.2 Practice A.2.1 CVX Introduction In the last section, we have learned basic concepts and theorems in convex optimization. In this section, on the other hand, we will introduce you how to model basic convex optimization problems with CVX, an easy-to-use MATLAB package. To install CVX, please refer to this page. Note that every time you what to use the CVX package, you should add it to your MATLAB path. For example, if I install CVX package in the parent directory of my current directory with default directory name cvx, the following line should be added before your CVX codes: addpath(genpath(&quot;../cvx/&quot;)); With CVX, it is incredibly easy for us to define and solve a convex optimization problem. You just need to: define the variables. define the objective function you want to minimize or maximize. define the constraints. After running your codes, the optimal objective value is stored in the variable cvx_optval, and the problem status is stored in the variable cvx_status (when your problem is well-defined, this variable’s value will be Solved). The optimal solutions will be stored in the variables you define. Throughout this section, we will study five types of convex optimization problems: linear programming (LP), quadratic programming (QP), (convex) quadratically constrained quadratic programming (QCQP), second-order cone programming (SOCP), and semidefinite programming (SDP). Given two types of optimization problems \\(A\\) and \\(B\\), we say \\(A &lt; B\\) if \\(A\\) can always be converted to \\(B\\) while the inverse is not true. Under this notation, we have \\[\\begin{equation*} \\text{LP} &lt; \\text{QP} &lt; \\text{QCQP} &lt; \\text{SOCP} &lt; \\text{SDP} \\end{equation*}\\] A.2.2 Linear Programming (LP) Definition. An LP has the following form: \\[\\begin{equation} \\tag{A.1} \\begin{aligned} \\min_{x \\in \\mathbb{R}^n} &amp; \\ c^T x \\\\ \\text{subject to } &amp; A x \\le b \\end{aligned} \\end{equation}\\] where \\(x\\) is the variable, \\(A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m\\), and \\(c \\in \\mathbb{R}^n\\) are the parameters. Note that the constraint \\(A x \\le b\\) already incorporates linear equality constraints. To see this, consider the constraint \\(A&#39; x = b&#39;\\), we can reformulate it as \\(A x \\le b\\) by \\[\\begin{equation*} \\begin{bmatrix} A&#39; \\\\ -A&#39; \\end{bmatrix} x \\le \\begin{bmatrix} b&#39; \\\\ -b&#39; \\end{bmatrix} \\end{equation*}\\] Example. Consider the problem of minimizing a linear function \\(c_1 x_1 + c_2 x_2\\) over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). We can convert it to the standard LP form in (A.1) by simply setting \\(c\\) as \\([c_1, \\ c_2]^T\\) and the linear inequality constraint as \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the LP example setting c1 = 2; c2 = -5; l1 = 3; l2 = 7; % parameters: c, A, b c = [c1; c2]; A = [1, 0; -1, 0; 0, 1; 0, -1]; b = [l1; l1; l2; l2]; %% solve LP cvx_begin variable x(2); % define variables [x1, x2] minimize(c&#39; * x); % define the objective subject to A * x &lt;= b; % define the linear constraint cvx_end A.2.3 Quadratic Programming (QP) Definition. A QP has the following form: \\[\\begin{align} \\tag{A.2} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P x + q^T x \\\\ \\text{subject to } &amp; Gx \\le h \\\\ &amp; Ax = b \\end{align}\\] where \\(P \\in \\mathcal{S}_+^n, q\\in \\mathbb{R}^n, G \\in \\mathbb{R}^{m \\times n}, h\\in \\mathbb{R}^m, A \\in \\mathbb{R}^{p \\times n}, b \\in \\mathbb{R}^p\\). Here \\(\\mathcal{S}_+^n\\) denotes the set of positive semidefinite matrices of size \\(n\\times n\\). Obviously, if we set \\(P\\) as zero, QP will degenerate to LP. Example. Consider the problem of minimizing a quadratic function \\[\\begin{equation*} f(x_1, x_2) = p_1 x_1^2 + 2p_2 x_1 x_2 + p_3 x_2^2 + q_1 x_1 + q_2 x_2 \\end{equation*}\\] over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). Since \\(P = 2 \\begin{bmatrix} p_1 &amp; p_2 \\\\ p_2 &amp; p_3 \\end{bmatrix} \\succeq 0\\), the following two conditions must hold: \\[\\begin{equation*} \\begin{cases} p_1 \\ge 0 \\\\ p_1 p_3 - 4 p_2^2 \\ge 0 \\end{cases} \\end{equation*}\\] Same as in the LP example, \\(G\\) and \\(h\\) can be expressed as: \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the QP example setting p1 = 2; p2 = 0.5; p3 = 4; q1 = -3; q2 = -6.5; l1 = 2; l2 = 2.5; % check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0); tmp2 = (p1*p3 - 4*p2^2 &gt;= 0); if ~(tmp1 &amp;&amp; tmp2) error(&quot;P is not positve semidefinite!&quot;); end % parameters: P, q, G, h P = 2 * [p1, p2; p2, p3]; q = [q1; q2]; G = [1, 0; -1, 0; 0, 1; 0, -1]; h = [l1; l1; l2; l2]; %% Solve the QP problem cvx_begin variable x(2); % define variables [x1; x2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P) + q&#39; * x; minimize(obj); subject to G * x &lt;= h; % define the linear constraint cvx_end A.2.4 Quadratically Constrained Quadratic Programming (QCQP) Definition. An (convex) QCQP has the following form: \\[\\begin{align} \\tag{A.3} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P_0 x + q_0^T x \\\\ \\text{subject to } &amp; \\frac{1}{2} x^T P_i x + q_i^T x + r_i \\le 0, \\ i = 1 \\dots m \\\\ &amp; Ax = b \\end{align}\\] where \\(P_i \\in \\mathcal{S}_+^n, i = 0 \\dots m\\), \\(q_i \\in \\mathbb{R}^n, i = 0 \\dots m\\), \\(A \\in \\mathbb{R}^{p \\times n}\\), and \\(b \\in \\mathbb{R}^p\\). Note that in other literature, you may find a more general form of QCQP: they don’t require \\(P_i\\)’s to be positive semidefinite. Yet in this case, the problem is non-convex and beyond our scope. Example. We study the problem of getting the minimum distance between two ellipses. By convention, when the ellipses overlap, we set the minimum distance as \\(0\\). This problem can be exactly solved by (convex) QCQP. Consider two ellipses of the following form: \\[\\begin{equation*} \\begin{cases} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 \\le 0 \\\\ \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 \\le 0 \\\\ \\end{cases} \\end{equation*}\\] where \\([y_1, z_1]^T\\) and \\([y_2, z_2]^T\\) are arbitrary points inside the two ellipses respectively. Also, two ensure the ellipses are well defined, we should enforce the following properties in \\((K_i, k_i, c_i), i = 1, 2\\): (1) \\(K_i \\succ 0\\); (2) Let \\(K_i = L_i L_i^T\\) be the Cholesky decomposition of \\(K_i\\). Then, ellipse \\(i\\) can be rewritten as: \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^T \\begin{bmatrix} y_i \\\\ z_i \\end{bmatrix} - L_i^{-1} k_i \\parallel^2 \\le \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i \\end{equation*}\\] Thus, \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i &gt; 0 \\end{equation*}\\] With these two assumptions, we want to minimize: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 \\end{equation*}\\] Now, we construct \\(P, q, r\\)’s in QCQP with the above parameters. Define the variable \\(x\\) as \\([y_1, z_1, y_2, z_2]\\). \\(P_0\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 = \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix}^T \\begin{bmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix} \\end{equation*}\\] \\(P_1, q_1, r_1\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 = \\frac{1}{2} x^T \\begin{bmatrix} K_1 &amp; O \\\\ O &amp; O \\end{bmatrix} + \\begin{bmatrix} k_1 \\\\ O \\end{bmatrix}^T x + c_1 \\le 0 \\end{equation*}\\] \\(P_2, q_2, r_2\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 = \\frac{1}{2} x^T \\begin{bmatrix} O &amp; O \\\\ O &amp; K_2 \\end{bmatrix} + \\begin{bmatrix} O \\\\ k_2 \\end{bmatrix}^T x + c_2 \\le 0 \\end{equation*}\\] The corresponding codes are shown below. In this example, we test the minimum distance between a circle \\(y_1^2 + z_1^2 \\le 1\\) and another circle \\((y_2 - 2)^2 + (z_2 - 2)^2 \\le 1\\). You can check whether the result from QCQP aligns with your manual calculation. %% Define the QCQP example setting K1 = eye(2); k1 = zeros(2, 1); c1 = -0.5; K2 = eye(2); k2 = [2; 2]; c2 = 3.5; if ~(if_ellipse(K1, k1, c1) &amp;&amp; if_ellipse(K2, k2, c2)) error(&quot;The example setting is not correct&quot;); end % define parameters P0, P1, P2, q1, q2, r1, r2 P0 = [1,0,-1,0; 0,1,0,-1; -1,0,1,0; 0,-1,0,1]; P1 = zeros(4, 4); P1(1:2, 1:2) = K1; P2 = zeros(4, 4); P2(3:4, 3:4) = K2; q1 = [k1; zeros(2, 1)]; q2 = [zeros(2, 1); k2]; r1 = c1; r2 = c2; %% Solve the QCQP problem cvx_begin variable x(4); % define variables [y1; z1; y2; z2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P0); minimize(obj); subject to 0.5 * quad_form(x, P1) + q1&#39; * x + r1 &lt;= 0; 0.5 * quad_form(x, P2) + q2&#39; * x + r2 &lt;= 0; cvx_end %% detect whether (K, k, c) generates a ellipse function flag = if_ellipse(K, k, c) L = chol(K); radius_square = 0.5 * norm(L \\ k)^2 - c; % L \\ k = inv(L) * k flag = (radius_square &gt; 0); end A.2.5 Second-Order Cone Programming (SOCP) Definition. An SOCP has the following form: \\[\\begin{align} \\tag{A.4} \\min_{x \\in \\mathbb{R}^n} \\ &amp; f^T x \\\\ \\text{subject to } &amp; || A_i x + b_i ||_2 \\le c_i^T x + d_i, \\ i = 1 \\dots m \\\\ &amp; Fx = g \\end{align}\\] where \\(f \\in \\mathbb{R}^n, A_i \\in \\mathbb{R}^{n_i \\times n}, b_i \\in \\mathbb{R}^{n_i}, c_i \\in \\mathbb{R}^n, d_i \\in \\mathbb{R}, F \\in \\mathbb{R}^{p \\times n}\\), and \\(g \\in \\mathbb{R}^p\\). Example. We consider the problem of stochastic linear programming: \\[\\begin{align} \\min_x \\ &amp; c^T x \\\\ \\text{subject to } &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p, \\ i = 1 \\dots m \\\\ &amp; a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i), \\ i = 1 \\dots m \\end{align}\\] Here \\(p\\) should be more than \\(0.5\\). We show that this problem can be converted to a SOCP: Since \\(a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i)\\), then \\((a_i^T x - b_i) \\sim \\mathcal{N}(\\bar{a}_i^T x - b_i, x^T \\Sigma_i x)\\). Standardize it: \\[\\begin{equation*} t := ||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1} \\left\\{ (a_i^T x - b_i) - (\\bar{a}_i^T x - b_i) \\right\\} \\sim \\mathcal{N}(0, 1) \\end{equation*}\\] Then, \\[\\begin{align} \\mathbb{P}(a_i^T x \\le b_i) &amp; = \\mathbb{P}(a_i^T x - b_i \\le 0) \\\\ &amp; = \\mathbb{P}(t \\le -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\\\ &amp; = \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\end{align}\\] Here \\(\\Phi(\\cdot)\\) is the cumulative distribution function of the standard normal distribution: \\[\\begin{equation*} \\Phi(\\xi) = \\int_{-\\infty}^{\\xi} e^{-\\frac{1}{2} t^2} \\ dt \\end{equation*}\\] Thus, \\[\\begin{align} &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p \\\\ \\Longleftrightarrow &amp; \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\ge p \\\\ \\Longleftrightarrow &amp; -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i) \\ge \\Phi^{-1}(p) \\\\ \\Longleftrightarrow &amp; \\Phi^{-1}(p) ||\\Sigma_i^{\\frac{1}{2}} x||_2 \\le b_i - \\bar{a}_i^T x \\end{align}\\] which is exactly the same as inequality constraints in SOCP formulation. (You can see why we enforce \\(p &gt; 0.5\\) here: otherwise \\(\\Phi^{-1}(p)\\) will be negative and the constraint with not be an second-order cone.) In the following code example, we set up four inequality constraints and let \\(\\bar{a}_i^T x \\le b_i, \\ i = 1 \\dots 4\\) form an square located at the origin of size \\(2\\). Then, for convenience, we set \\(\\Sigma_i \\equiv \\sigma^2 I\\). %% Define the SOCP example setting bar_a1 = [1; 0]; b1 = 1; bar_a2 = [0; 1]; b2 = 1; bar_a3 = [-1; 0]; b3 = 1; bar_a4 = [0; -1]; b4 = 1; sigma = 0.1; c = [2; 3]; p = 0.9; % p should be more than 0.5 Phi_inv = norminv(p); % get Phi^{-1}(p) %% Solve the SOCP problem cvx_begin variable x(2); % define variables [x1; x2] minimize(c&#39; * x); subject to sigma*Phi_inv * norm(x) &lt;= b1 - bar_a1&#39; * x; sigma*Phi_inv * norm(x) &lt;= b2 - bar_a2&#39; * x; sigma*Phi_inv * norm(x) &lt;= b3 - bar_a3&#39; * x; sigma*Phi_inv * norm(x) &lt;= b4 - bar_a4&#39; * x; cvx_end A.2.6 Semidefinite Programming (SDP) Definition. An SDP has the following form: \\[\\begin{align} \\tag{A.5} \\min_{X_i, x_i} \\ &amp; \\sum_{i=1}^{n_s} C_i \\cdot X_i + \\sum_{i=1}^{n_u} c_i \\cdot x_i \\\\ \\text{subject to } &amp; \\sum_{i=1}^{n_s} A_{i,j} \\cdot X_i + \\sum_{i=1}^{n_u} a_{i,j} \\cdot x_i = b_j, \\quad j = 1 \\dots m \\\\ &amp; X_i \\in \\mathcal{S}_+^{D_i}, \\quad i = 1 \\dots n_s \\\\ &amp; x_i \\in \\mathbb{R}^{d_i}, \\quad i = 1 \\dots n_u \\end{align}\\] where \\(C_i, A_{i, j} \\in \\mathbb{R}^{D_i \\times D_i}\\), \\(c_i, a_{i, j} \\in \\mathbb{R}^{d_i}\\), and \\(\\cdot\\) means element-wise product. For two square matrices \\(A, B\\), the dot product \\(A \\cdot B\\) is equal to \\(\\text{tr}(A B)\\); for two vectors \\(a, b\\), the dot product \\(a \\cdot b\\) is the same as inner product \\(a^T b\\). Note that actually there are many “standard” forms of SDP. For example, in the convex optimization theory part, you may find an SDP that looks like: \\[\\begin{align} \\min_X \\ &amp; C \\cdot X \\\\ \\text{subject to } &amp; A \\cdot X = b \\\\ &amp; X \\succeq 0 \\end{align}\\] It is convenient for us to analyze the theoretical properties of SDP with this form. Also, in SDP solvers’ User Guide, you may see more complex SDP forms which involve more general convex cones. For example, see MOSEK’s MATLAB API docs. Here we turn to use the form of (A.5) for two reasons: (1) it is general enough: our SDP example below can be converted to this form (also, SDPs from sum-of-squares programming in this book are exactly of the form (A.5)); (2) it is more readable than more complex forms. Example. We consider the problem of finding the minimum eigenvalue for a positive semidefinite matrix \\(S\\). We will show that this problem can be converted to (A.5). Since \\(S\\) is positive semidefinite, the finding procedure can be cast as \\[\\begin{align} \\max_\\lambda &amp; \\ \\lambda \\\\ \\text{subject to } &amp; S - \\lambda I \\succeq 0 \\end{align}\\] Now define an auxiliary matrix \\(X := S - \\lambda I\\). We have \\[\\begin{align} \\min_{\\lambda, X} &amp; \\ -\\lambda \\\\ \\text{subject to } &amp; X + \\lambda I = S \\\\ &amp; X \\succeq 0 \\end{align}\\] It is obvious that the linear matrix equality constraint \\(X + \\lambda I = S\\) can be divided into several linear scalar equality constraints in (A.5). For example, we consider \\(S \\in \\mathbb{S}_+^3\\). Thereby \\(X + \\lambda I = S\\) will lead to \\(6\\) linear equality constraints (We don’t consider \\(X\\) is a symmetric matrix here, since most solvers will implicitly consider this. Thus, only the upper-triangular part of \\(X\\) and \\(S\\) are actually used in the equality construction.): \\[\\begin{align} &amp; \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[0, 0], \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 2] \\\\ &amp; \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[1, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[1, 2], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\cdot X + \\lambda = S[2, 2] \\end{align}\\] Seems tedious? Fortunately, CVX provides a high-level API to handle these linear equality constraints: you just need to write down X + lam * eye(3) == S; % linear equality constraints: X + lam *I = S CVX will autometically convert this high-level constraint to (A.5) and pass them to the underlying solver. To generate a ramdom \\(S \\in \\mathcal{S}_+^3\\), you just need to assign three nonnegative eigenvalues to the program. After that, an random \\(S\\) will be generated by \\(S = Q \\ \\text{diag}(\\lambda_1, \\lambda_2, \\lambda_3) \\ Q^T\\), where \\(Q\\) is random orthonormal matrix. %% Define the SDP example setting lam_list = [0.7; 2.4; 3.7]; S = generate_random_PD_matrix(lam_list); % get a PD matrix S %% Solve the SDP problem cvx_begin variable X(3, 3) symmetric; variable lam; maximize(lam); subject to % here &quot;==&quot; should be read as &quot;is in&quot; X == semidefinite(3); X + lam * eye(3) == S; cvx_end % this function help to generate PD matrix of size 3*3 % if you provide the eigenvalues [lam_1, lam_2, lam_3] function S = generate_random_PD_matrix(lam_list) if ~all(lam_list &gt;= 0) % all eigenvalues &gt;= 0 error(&quot;All eigenvalues must be nonnegative.&quot;); end D = diag(lam_list); % use QR factorization to generate a random orthonormal matrix Q [Q, ~] = qr(rand(3, 3)); S = Q * D * Q&#39;; end A.2.7 CVXPY Introduction and Examples Apart from CVX MATLAB, we also have a Python package called CVXPY, which functions almost the same as CVX MATLAB. To define and solve a convex optimization problem CVXPY, basically, there are three steps (apart from importing necessary packages): Step 1: Define parameters and variables in a certain type of convex problem. Here variables are what you are trying to optimize or “learn”. Parameters are the “coefficients” of variables in the objective and constraints. Step 2: Define the objective function and constraints. Step 3: Solve the problem and get the results. Here we provide the CVXPY codes for the above five convex optimization examples. A.2.7.1 LP import cvxpy as cp import numpy as np ## Define the LP example setting c1 = 2 c2 = -5 l1 = 3 l2 = 7 ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: c, A, b c = np.array([c1, c2]) A = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) b = np.array([l1, l1, l2, l2]) ## Step 2: define objective and constraints obj = cp.Minimize(c.T @ x) constraints = [A @ x &lt;= b] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.2 QP import cvxpy as cp import numpy as np ## Define the LP example setting p1 = 2 p2 = 0.5 p3 = 4 q1 = -3 q2 = -6.5 l1 = 2 l2 = 2.5 # check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0) tmp2 = (p1*p3 - 4*p2**2 &gt;= 0) assert(tmp1 and tmp2, &quot;P is not positve semidefinite!&quot;) ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: P, q, G, h P = 2*np.array([[p1, p2], [p2, p3]]) q = np.array([q1, q2]) G = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) h = np.array([l1, l1, l2, l2]) ## Step 2: define the objective and constraints fx = 0.5 * cp.quad_form(x, P) + q.T @ x obj = cp.Minimize(fx) constraints = [G @ x &lt;= h] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve the problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.3 QCQP import cvxpy as cp import numpy as np from numpy.linalg import cholesky, inv, norm ## Define the QCQP example setting def if_ellipse(K, k, c): # examine whether 0.5*x^T K x + k^T x + c &lt;= 0 is a ellipse # if K is not positive semidefinite, Cholesky will raise an error L = cholesky(K) radius_square = 0.5 * norm(inv(L) @ k)**2 - c return radius_square &gt; 0 K1 = np.eye(2) k1 = np.zeros(2) c1 = -0.5 K2 = np.array([[1, 0], [0, 1]]) k2 = np.array([2, 2]) c2 = 3.5 if not (if_ellipse(K1, k1, c1) and if_ellipse(K2, k2, c2)): raise ValueError(&quot;The example setting is not correct&quot;) ## Step 1: define variables and parameters P0 = np.array([[1,0,-1,0], [0,1,0,-1], [-1,0,1,0], [0,-1,0,1]]) P1 = np.zeros((4,4)) P1[:2, :2] = K1 P2 = np.zeros((4,4)) P2[2:, 2:] = K2 q1 = np.concatenate([k1, np.zeros(2)]) q2 = np.concatenate([np.zeros(2), k2]) r1 = c1 r2 = c2 ## Step 2: define objective and constraints x = cp.Variable(4) # variable: x = [y1, z1, y2, z2]^T fx = 0.5 * cp.quad_form(x, P0) obj = cp.Minimize(fx) con1 = (0.5 * cp.quad_form(x, P1) + q1.T @ x + r1 &lt;= 0) # ellipse 1 con2 = (0.5 * cp.quad_form(x, P2) + q2.T @ x + r2 &lt;= 0) # ellipse 2 constraints = [con1, con2] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.4 SOCP import cvxpy as cp import numpy as np from scipy.stats import norm ## Define the SOCP example setting # define bar_ai, bi (i = 1, 2, 3, 4) bar_a1 = np.array([1, 0]) b1 = 1 bar_a2 = np.array([0, 1]) b2 = 1 bar_a3 = np.array([-1, 0]) b3 = 1 bar_a4 = np.array([0, -1]) b4 = 1 sigma = 0.1 c = np.array([2, 3]) p = 0.9 # p should be more than 0.5 ## Step 1: define variables and parameters Phi_inv = norm.ppf(p) # get Phi^{-1}(p) ## Step 2: define objective and constraints x = cp.Variable(2) # variable: x = [x1, x2]^T obj = cp.Minimize(c.T @ x) # use cp.SOC(t, x) to create the SOC constraint ||x||_2 &lt;= t constraints = [ cp.SOC(b1 - bar_a1.T @ x, sigma*Phi_inv*x), cp.SOC(b2 - bar_a2.T @ x, sigma*Phi_inv*x), cp.SOC(b3 - bar_a3.T @ x, sigma*Phi_inv*x), cp.SOC(b4 - bar_a4.T @ x, sigma*Phi_inv*x), ] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.5 SDP import cvxpy as cp import numpy as np from scipy.stats import ortho_group ## Define the SDP example setting # this function help to generate PD matrix of size 3*3 # if you provide the eigenvalues [lam_1, lam_2, lam_3] def generate_random_PD_matrix(lam_list): assert np.all(lam_list &gt;= 0) # all eigenvalues &gt;= 0 # S = Q @ D @ Q.T D = np.diag(lam_list) Q = ortho_group.rvs(3) return Q @ D @ Q.T lam_list = np.array([0.5, 2.4, 3.7]) S = generate_random_PD_matrix(lam_list) # get a PD matrix S ## Step 1: define variables and parameters # get coefficients for equality constraints A_00 = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_00 @ X) + lam = S_00 A_01 = np.array([[0, 1, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_01 @ X) = S_01 A_02 = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]]) # tr(A_02 @ X) = S_02 A_11 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]) # tr(A_11 @ X) + lam = S_11 A_12 = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 0]]) # tr(A_12 @ X) = S_12 A_22 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1]]) # tr(A_22 @ X) + lam = S_22 ## Step 2: define objective and constraints # define a PD matrix variable X of size 3*3 X = cp.Variable((3, 3), symmetric=True) constraints = [X &gt;&gt; 0] # the operator &gt;&gt; denotes matrix inequality lam = cp.Variable(1) constraints += [ cp.trace(A_00 @ X) + lam == S[0,0], cp.trace(A_01 @ X) == S[0,1], cp.trace(A_02 @ X) == S[0,2], cp.trace(A_11 @ X) + lam == S[1,1], cp.trace(A_12 @ X) == S[1,2], cp.trace(A_22 @ X) + lam == S[2,2], ] obj = cp.Minimize(-lam) prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, lam.value) # optimal lam "],["app-lti-system-theory.html", "B Linear System Theory B.1 Stability B.2 Controllability and Observability B.3 Stabilizability And Detectability", " B Linear System Theory Thanks to Shucheng Kang for writing this Appendix. B.1 Stability B.1.1 Continuous-Time Stability Consider the continuous-time linear time-invariant (LTI) system \\[\\begin{equation} \\dot{x} = A x. \\tag{B.1} \\end{equation}\\] the system is said to be “diagonalizable” if \\(A\\) is diagonalizable. Definition B.1 (Asymptotic and Marginal Stability) The diagonalizable, LTI system (B.1) is “asymptotically stable” if \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “marginally stable” if \\(x(t) \\nrightarrow 0\\) but remains bounded as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “stable” if it is either asymptotically or marginally stable “unstable” if it is not stable One can show that \\(A\\)’s eigenvalues determine the LTI system’s stability, as the following Theorem states: Theorem B.1 (Stability of Continuous-Time LTI System) The diagonalizable1, LTI system (B.1) is asymptotically stable if \\(\\text{Re} (\\lambda_i) &lt; 0\\) for all \\(i\\) marginally stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(\\text{Re} (\\lambda_i) = 0\\) stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) unstable if \\(\\text{Re} (\\lambda_i) &gt; 0\\) for at least one \\(i\\) Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(\\dot{x} = Ax\\) can be restated as \\(\\dot{z} = \\Lambda z\\). Consider the \\(i\\)’s component of \\(z\\): \\[\\begin{equation*} \\dot{z}_i = \\lambda_i z_i \\Longrightarrow z_i(t) = e^{\\lambda_i t} z_i(0) \\end{equation*}\\] Since \\(\\text{Re}(\\lambda_i) &lt; 0\\), \\(z_i(t)\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_i(0)\\). B.1.2 Discrete-Time Stability Now consider the diagonalizable, discrete-time linear time-invariant (LTI) system \\[\\begin{equation} x_{t+1} = A x_t. \\tag{B.2} \\end{equation}\\] Theorem B.2 (Stability of Discrete-Time LTI System) The diagonalizable, discrete-time LTI system (B.2) is asymptotically stable if \\(|\\lambda_i| &lt; 1\\) for all \\(i\\) marginally stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(|\\lambda_i| = 1\\) stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) unstable if \\(|\\lambda_i| &gt; 1\\) for at least one \\(i\\). Note that \\(|\\lambda_i| &lt; 1\\) means the eigenvalue lies strictly inside the unit circle in the complex plane. Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(x_{t+1} = Ax\\) can be restated as \\(z_{t+1} = \\Lambda z_t\\). Expanding the recursion, we have \\[\\begin{equation*} z_{t} = \\Lambda^{t-1} z_0 \\Longrightarrow z_{t,i} = \\lambda_i^{t-1} z_{0,i} \\end{equation*}\\] Since \\(|\\lambda_i| &lt; 1\\), \\(z_{t,i}\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_{0,i}\\). B.1.3 Lyapunov Analysis Theorem B.3 (Lyapunov Equation) The following is equivalent for a linear time-invariant system \\(\\dot{x} = A x\\) The system is globally asymptotically stable, i.e., \\(A\\) is Hurwitz and \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) regardless of the initial condition; For any positive definite matrix \\(Q\\), the unique solution \\(P\\) to the Lyapunov equation \\[\\begin{equation} A^T P + P A = -Q \\tag{B.3} \\end{equation}\\] is positive definite. Proof. (a): \\(2 \\Rightarrow 1\\). Suppose we are given two positive definite matrices \\(P, Q \\succ 0\\) that satisfies the Lyapunov equation (B.3). Define a scalar function \\[ V(x) = x^T P x. \\] It is clear that \\(V &gt; 0\\) for any \\(x \\neq 0\\) and \\(V(x) = 0\\) (i.e., \\(V(x)\\) is positive definite). We also see \\(V(x)\\) is radially unbounded because: \\[ V(x) \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2 \\Rightarrow \\lim_{x \\rightarrow \\infty} V(x) \\rightarrow \\infty. \\] The time derivative of \\(V\\) reads \\[ \\dot{V} = 2 x^T P \\dot{x} = x^T (A^T P + P A) x = - x^T Q x. \\] Clearly, \\(\\dot{V} &lt; 0\\) for any \\(x \\neq 0\\) and \\(\\dot{V}(0) = 0\\). According to Lyapunov’s global stability theorem ??, we conclude the linear system \\(\\dot{x} = Ax\\) is globally asymptotically stable at \\(x = 0\\). (b): \\(1 \\Rightarrow 2\\). Suppose \\(A\\) is Hurwitz, we want to show that, for any \\(Q \\succ 0\\), there exists a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (B.3). In fact, consider the matrix \\[ P = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt. \\] Because \\(A\\) is Hurwitz, the integral exists, and clearly \\(P \\succ 0\\) due to \\(Q \\succ 0\\). To show this choice of \\(P\\) satisfies the Lyapunov equation, we write \\[\\begin{align} A^T P + P A &amp;= \\int_{t=0}^{\\infty} \\left( A^T e^{A^T t} Q e^{At} + e^{A^T t} Q e^{At} A \\right) dt \\\\ &amp;=\\int_{t=0}^{\\infty} d \\left( e^{A^T t} Q e^{At} \\right) \\\\ &amp; = e^{A^T t} Q e^{At}\\vert_{t = \\infty} - e^{A^T t} Q e^{At}\\vert_{t = 0} = - Q, \\end{align}\\] where the last equality holds because \\(e^{A \\infty} = 0\\) (recall \\(A\\) is Hurwitz). To show the uniqueness of \\(P\\), we assume that there exists another matrix \\(P&#39;\\) that also satisfies the Lyapunov equation. Therefore, \\[\\begin{align} P&#39; &amp;= e^{A^T t} P&#39; e^{At} \\vert_{t=0} - e^{A^T t} P&#39; e^{At} \\vert_{t=\\infty} \\\\ &amp;= - \\int_{t=0}^{\\infty} d \\left( e^{A^T t} P&#39; e^{At} \\right) \\\\ &amp;= - \\int_{t=0}^{\\infty} e^{A^T t} \\left( A^T P&#39; + P&#39; A \\right) e^{At} dt \\\\ &amp; = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt = P, \\end{align}\\] leading to \\(P&#39; = P\\). Hence, the solution is unique. Convergence rate estimation. We now show that Theorem B.3 can allow us to quantify the convergence rate of a (stable) linear system towards zero. For a Hurwitz linear system \\(\\dot{x} = Ax\\), let us pick a positive definite matrix \\(Q\\). Theorem B.3 tells us we can find a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (B.3). In this case, we can upper bound the scalar function \\(V = x^T P x\\) as \\[ V \\leq \\lambda_{\\max}(P) \\Vert x \\Vert^2. \\] The time derivative of \\(V\\) is \\(\\dot{V} = - x^T Q x\\), which can be upper bounded by \\[\\begin{align} \\dot{V} &amp; \\leq - \\lambda_{\\min} (Q) \\Vert x \\Vert^2 \\\\ &amp; = - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} \\underbrace{ \\left( \\lambda_{\\max} (P) \\Vert x \\Vert^2 \\right)}_{\\geq V} \\\\ &amp; \\leq - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} V. \\end{align}\\] Denoting \\(\\gamma(Q) = \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max}(P)}\\), the above inequality implies \\[ V(0) e^{-\\gamma(Q) t} \\geq V(t) = x^T P x \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2. \\] As a result, \\(\\Vert x \\Vert^2\\) converges to zero exponentially with a rate at least \\(\\gamma(Q)\\), and \\(\\Vert x \\Vert\\) converges to zero exponentially with a rate at least \\(\\gamma(Q) / 2\\). Best convergence rate estimation. I have used \\(\\gamma (Q)\\) to make it explict that the rate \\(\\gamma\\) depends on the choice of \\(Q\\), because \\(P\\) is computed from the Lyapunov equation as an implicit function of \\(Q\\). Naturally, choosing different \\(Q\\) will lead to different \\(\\gamma (Q)\\). So what is the choice of \\(Q\\) that maximizes the convergence rate estimation? Corollary B.1 (Maximum Convergence Rate Estimation) \\(Q = I\\) maximizes the convergence rate estimation. Proof. let us denote \\(P_0\\) as the solution to the Lyapunov equation with \\(Q = I\\) \\[ A^T P_0 + P_0 A = - I. \\] Let \\(P\\) be the solution corresponding to a different choice of \\(Q\\) \\[ A^T P + P A = - Q. \\] Without loss of generality, we can assume \\(\\lambda_{\\min}(Q) = 1\\), because rescaling \\(Q\\) will recale \\(P\\) by the same factor, which does not affect \\(\\gamma(Q)\\). Subtracting the two Lyapunov equations above we get \\[ A^T (P - P_0) + (P - P_0) A = - (Q - I). \\] Since \\(Q - I \\succeq 0\\) (due to \\(\\lambda_{\\min}(Q) = 1\\)), we know \\(P - P_0 \\succeq 0\\) and \\(\\lambda_{\\max} (P) \\geq \\lambda_{\\max} (P_0)\\). As a result, \\[ \\gamma(Q) = \\frac{\\lambda_{\\min}(Q)}{\\lambda_{\\max}(P)} = \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P)} \\leq \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P_0)} = \\gamma(I), \\] and \\(Q = I\\) maximizes the convergence rate estimation. B.2 Controllability and Observability Consider the following linear time-invariant (LTI) system \\[\\begin{equation} \\tag{B.4} \\begin{split} \\dot{x} = A x + B u \\\\ y = C x + D u \\end{split} \\end{equation}\\] where \\(x \\in \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{R}^m\\) the control input, \\(y \\in \\mathbb{R}^p\\) the output, and \\(A,B,C,D\\) are constant matrices with proper sizes. If we know the initial state \\(x(0)\\) and the control inputs \\(u(t)\\) over a period of time \\(t \\in [0, t_1]\\), the system trajectory \\((x(t), y(t))\\) can be determined as \\[\\begin{equation} \\tag{B.5} \\begin{split} x(t) &amp; = e^{At} x(0) + \\int_{0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau \\\\ y(t) &amp; = C x(t) + D u(t) \\end{split} \\end{equation}\\] To study the internal structure of linear systems, two important properties should be considered: controllability and observability. In the following analysis, we will see that they are actually dual concepts. Their definitions (Chen 1984) are given below. Definition B.2 (Controllability) The LTI system (B.4), or the pair \\((A, B)\\), is controllable, if for any initial state \\(x(0) = x_0\\) and final state \\(x_f\\), there exists a sequence of control inputs that transfer the system from \\(x_0\\) to \\(x_f\\) in finite time. Definition B.3 (Observability) The LTI system (B.4), or the pair \\((C, A)\\), is observable, if for any unknown initial state \\(x(0)\\), there exists a finite time \\(t_1 &gt; 0\\), such that knowing \\(y\\) and \\(u\\) over \\([0, t_1]\\) suffices to determine \\(x(0)\\). Sometimes it will become more convenient for us to analyze the system (B.4) under another coordinate basis, i.e., \\(z = T x\\), where the coordinate transformation \\(T\\) is nonsingular (i.e., full-rank). Define \\(A&#39; = TAT^{-1}, B&#39; = PB, C&#39; = CT^{-1}, D&#39; = D\\), we get \\[\\begin{equation*} \\begin{split} \\dot{z} = A&#39; z + B&#39; u \\\\ y = C&#39; z + D&#39; u \\end{split} \\end{equation*}\\] Since the coordinate transformation only changes the system’s coordinate basis, physical properties like controllability and observability will not change. B.2.1 Cayley-Hamilton Theorem In the analysis of controllability and observability, Cayley Hamilton Theorem lays the foundation. The statement of the theory and its (elegant) proof are given blow. Some useful corollaries are also presented. Theorem B.4 (Cayley-Hamilton) Let \\(A \\in \\mathbb{C}^{n \\times n}\\) and denote the characteristic polynomial of \\(A\\) as \\[ \\text{det}(\\lambda I - A) = \\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n \\in \\mathbb{C}[\\lambda], \\] which is a polynomial in a single variable \\(\\lambda\\) with coefficients \\(a_1,\\dots,a_n\\). Then \\[ A^n + a_1 A^{n-1} + \\dots + a_n I = 0 \\] Proof. Define the adjugate of \\(\\lambda I - A\\) as \\[ B = \\text{adj}(\\lambda I - A) \\] From \\(B\\)’s definition, we have \\[\\begin{equation} (\\lambda I - A) B = \\text{det}(\\lambda I - A) I = (\\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n) I \\tag{B.6} \\end{equation}\\] Also, \\(B\\) is a polynomial matrix over \\(\\lambda\\), whose maximum degree is no more than \\(n - 1\\). Therefore, we write \\(B\\) as follows: \\[ B = \\sum_{i=0}^{n-1} \\lambda^i B_i \\] where \\(B_i\\)’s are constant matrices. In this way, we unfold \\((\\lambda I - A)B\\): \\[\\begin{equation} \\tag{B.7} \\begin{split} (\\lambda I - A) B &amp; = (\\lambda I - A) \\sum_{i=0}^{n-1} \\lambda^i B_i \\\\ &amp; = \\lambda^n B_{n-1} + \\sum_{i=1}^{n-1} \\lambda^i (-A B_i + B_{i-1}) - A B_0 \\end{split} \\end{equation}\\] Since \\(\\lambda\\) can be arbitrarily set, matching the coefficients of (B.6) and (B.7), we have \\[\\begin{equation*} \\begin{split} B_{n-1} &amp; = I \\\\ -A B_i + B_{i-1} &amp; = a_{n-i} I, \\quad i = 1 \\dots n - 1 \\\\ -A B_0 &amp; = a_n I \\end{split} \\end{equation*}\\] Thus, we have \\[\\begin{equation*} \\begin{split} &amp; B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I \\\\ = &amp; I \\cdot A^n + \\sum_{i=1}^{n-1} (a_{n-i} I) \\cdot A^i + (a_n I) \\cdot I \\\\ = &amp; A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I \\end{split} \\end{equation*}\\] On the other hand, one can easily check that \\[ B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I = 0 \\] since each term offsets completely. Therefore, \\[ A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I = 0, \\] concluding the proof. Here are some corollaries of the Cayley-Hamilton Theorem. Corollary B.2 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k \\ge n\\), \\(A^k B\\) is a linear combination of \\(B, AB, A^2B, \\dots, A^{n-1}B\\). Proof. Directly from Cayley Hamilton Theorem, \\(A^n\\) can be expressed as a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). By recursion, it is easy to show that for all \\(m &gt; n\\), \\(A^m\\) is also a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). Post-multiply both sides with \\(B\\), we get what we want. Corollary B.3 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k &gt; n\\), the following equality always holds: \\[ \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix}) = \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix}) \\] Proof. First prove LHS \\(\\le\\) RHS. \\(\\forall v \\in \\mathbb{C}^n\\) such that \\[ v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B &amp; \\dots A^{k-1}B \\end{bmatrix} = 0 \\] \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) must hold. Second prove LHS \\(\\ge\\) RHS. For any \\(v \\in \\mathbb{C}^n\\) such that \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) and any \\(k &gt; n\\), by Corollary B.2, there exists a sequence \\(c_i, i = 0 \\dots n-1\\) satisfy the following: \\[ v^* A^k B = v^* \\sum_{i=0}^{n-1} c_i A^i B = 0 \\] Therefore, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = 0\\). Corollary B.4 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}\\), define \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] If \\(\\text{rank}(\\mathcal{C}) = k_1 &lt; n\\), there exist a similarity transformation \\(T\\) such that \\[ T A T^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, T B = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}, \\bar{B}_c \\in \\mathbb{C}^{k_1 \\times m}\\). Moreover, the matrix \\[\\begin{equation*} \\bar{\\mathcal{C}} := \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c \\end{bmatrix} \\end{equation*}\\] has full row rank. Proof. Since \\(\\mathcal{C}\\) is not full row rank, we pick \\(k_1\\) linearly independent columns from \\(\\mathcal{C}\\). Denote them as \\(q_1\\dots q_{k_1}\\), \\(q_i \\in \\mathbb{C}^n\\). Then, we arbitrarily set other \\(n-k_1\\) vectors \\(q_{k_1+1} \\dots q_{n}\\) as long as \\[ Q = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\] is invertible. Define the similarity transformation matrix by \\(T = Q^{-1}\\). Note that \\(A q_i\\) can be seen as a column picked from \\(A^{k} B, k \\in \\left\\{1 \\dots n\\right\\}\\), which is guaranteed to be a linear combination of \\(B, AB, \\dots, A^{n-1}B\\) from Cayley Hamilton Theorem. Thus, \\(A q_i\\) is bound to be a linear transformation of columns from \\(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = \\mathcal{C}\\). Since \\(q_1\\dots q_{k_1}\\) is the largest linearly independent column vector set from \\(\\mathcal{C}\\), this implies \\(A q_i\\) can be expressed as a linear combination of \\(q_1\\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} A Q &amp; = A T^{-1} = A \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} = T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} \\end{split} \\end{equation*}\\] Similarly, \\(B\\) itself is part of \\(\\mathcal{C}\\). Therefore, each column of \\(B\\) is naturally a linear combination of \\(q_1 \\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} B = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{split} = T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] To see \\(\\bar{\\mathcal{C}}\\) has full row rank, note that \\(\\text{rank} \\mathcal{C} = k_1\\) and \\[\\begin{equation*} \\mathcal{C} = T^{-1} \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\end{equation*}\\] Thus, \\[\\text{rank}\\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\end{bmatrix} = k_1. \\] By Corollary B.3, \\(\\text{rank}\\bar{\\mathcal{C}} = k_1\\). The following Corollary is especially useful in the study of pole assignment in the single-input-multiple-output (SIMO) LTI system. Corollary B.5 For any \\(A \\in \\mathbb{C}^{n \\times n}, b \\in \\mathbb{C}^{n}\\), if \\[\\begin{equation*} \\mathcal{C} = \\begin{bmatrix} b &amp; Ab &amp; \\dots &amp; A^{n-1}b \\end{bmatrix} \\in \\mathbb{C}^{n \\times n} \\end{equation*}\\] has full rank, then there exists a similarity transformation \\(T\\) such that \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(a_1, \\dots, a_n\\) are the coefficients of \\(A\\)’s characteristic polynomial: \\[\\begin{equation*} \\det(A - \\lambda I) = \\lambda^{n} + a_1 \\lambda^{n-1} + \\dots + a_n \\lambda \\end{equation*}\\] Proof. Since \\(\\mathcal{C}\\) is invertible, define its inverse \\[\\begin{equation*} \\mathcal{C}^{-1} = \\begin{bmatrix} M_1 \\\\ M_2 \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] where \\(M_i \\in \\mathbb{C}^{1 \\times n}\\). Then, \\[\\begin{equation*} I = \\mathcal{C}^{-1} \\mathcal{C} = \\begin{bmatrix} M_1 b &amp; M_1 Ab &amp; \\dots &amp; M_1 A^{n-1}b \\\\ M_2 b &amp; M_2 Ab &amp; \\dots &amp; M_2 A^{n-1}b \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} \\Longrightarrow \\begin{cases} M_n A^{n-1}b = 1 \\\\ M_n A^ib = 0, \\ i = 0,\\dots, n-2 \\end{cases} \\end{equation*}\\] Now we claim that the transformation matrix \\(T\\) can be constructed as follows: \\[\\begin{equation*} T = \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] We first show \\(T\\) is invertible by calculating \\(T \\mathcal{C}\\): \\[\\begin{equation*} T \\mathcal{C} = \\begin{bmatrix} M_n A^{n-1}b &amp; \\star &amp; \\dots &amp; \\star \\\\ M_n A^{n-2}b &amp; M_n A^{n-1}b &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\star &amp; \\dots &amp; \\star \\\\ 0 &amp; 1 &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\end{equation*}\\] Then we calculate \\(Tb\\) and \\(TA\\): \\[\\begin{equation*} \\begin{split} Tb &amp; = \\begin{bmatrix} M_n A^{n-1}b \\\\ M_n A^{n-2}b \\\\ \\vdots \\\\ M_n b \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ T A &amp; = \\begin{bmatrix} M_n A^n \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} = \\begin{bmatrix} -M_n \\cdot \\sum_{i=0}^{n-1} a_{n-i} A^i \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\vdots \\\\ M_n A \\\\ M_n \\end{bmatrix} = A_1 T \\end{split} \\end{equation*}\\] where the penultimate equality uses Cayley Hamilton Theorem. B.2.2 Equivalent Statements for Controllability There are a few equivalent statements to express an LTI system’s controllability that one should be familiar with: Theorem B.5 (Equivalent Statements for Controllability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((A, B)\\) is controllable. The matrix \\[ W_c(t) := \\int_{0}^{t} e^{A\\tau} B B^* e^{A^* \\tau} d\\tau \\] is positive definite for any \\(t &gt; 0\\). The controllability matrix \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; A^2 B &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] has full row rank. The matrix \\([A - \\lambda I, B]\\) has full row rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(x\\) be any eigenvalue and any corresponding left eigenvector \\(A\\), i.e., \\(x^* A = x^* \\lambda\\), then \\(x^* B \\ne 0\\). The eigenvalues of \\(A+BF\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(F\\). If, in addition, all eigenvalues of \\(A\\) have negative real parts, then the unique solution of \\[ A W_c + W_c A^* = -B B^* \\] is positive definite. The solution is called the controllability Gramian and can be expressed as \\[ W_c = \\int_{0}^{\\infty} e^{A \\tau} B B^* e^{A^* \\tau} d\\tau \\] Proof. (\\(1. \\Rightarrow 2.\\)) Prove by contradiction. Assume that \\((A, B)\\) is controllable but \\(W_c(t_1)\\) is singular for some \\(t_1 &gt; 0\\). This implies there exists a real vector \\(v \\ne 0 \\in \\mathbb{R}^n\\), s.t. \\[ v^* W_c(t_1) v = v^* (\\int_{0}^{t_1} e^{At} B B^* e^{A^*t} dt) v = \\int_{0}^{t_1} v^* (e^{At} B B^* e^{A^*t}) v \\ dt = 0 \\] Since \\(e^{At} BB^* e^{A^*t} \\succeq 0\\) for all \\(t\\), we must have \\[\\begin{equation*} \\begin{split} &amp; v^* (e^{At} B B^* e^{A^*t}) v = \\parallel v^* B e^{At} \\parallel^2 = 0, \\quad \\forall t \\in [0, t_1] \\\\ \\Longrightarrow &amp; v^* B e^{At} = 0, \\quad \\forall t \\in [0, t_1] \\end{split} \\end{equation*}\\] Setting \\(x(t_1) = 0\\), from (B.5), we have \\[ 0 = e^{A t_1} x(0) + \\int_{0}^{t_1} e^{A (t_1 - \\tau)} B u(\\tau) d\\tau = 0 \\] Pre-multiply the above equation by \\(v^*\\), then \\[ 0 = v^* e^{A t_1} x(0) \\] Since \\(x(0)\\) can be chosen arbitrarily, we set \\(x(0) = v e^{-A t_1}\\), which results in \\(v = 0\\). Contradiction! (\\(2. \\Rightarrow 1.\\)) For any \\(x(0) = x_0, t_1 &gt; 0, x(t_1) = x_1\\), since \\(W_c(t_1) \\succ 0\\), we set the control inputs as \\[ u(t) = -B^* e^{A^*(t_1 - t)} W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\] We claim that the picked \\(u(t)\\) satisfies (B.5) by \\[\\begin{equation*} \\begin{split} &amp; e^{At} x_0 + \\int_{0}^{t_1} e^{A(t_1-t)} B u(t) dt \\\\ &amp; = e^{At} x_0 - \\int_{0}^{t_1} e^{A(t_1-t)} B B^* e^{A^*(t_1-t)} dt \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; \\overset{\\tau = t_1-t}{=} e^{At} x_0 - \\underbrace{\\int_{0}^{t_1} e^{A\\tau} BB^* e^{A^*\\tau} d\\tau}_{W_c(t_1)} \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; = e^{At} x_0 - [e^{At_1} x_0 - x_1] = x_1 \\end{split} \\end{equation*}\\] (\\(2. \\Rightarrow 3.\\)) Prove by contradiction. Suppose \\(W_c(t) \\succ 0, \\forall t &gt; 0\\) but \\(\\mathcal{C}\\) is not of full row rank. Then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[ v^* A^k B = 0, \\quad k = 0 \\dots n - 1 \\] By Corollary B.2, we have \\[ v^* A^k B = 0, \\ \\forall k \\in \\mathbb{N} \\Longrightarrow v^* e^{At} B = 0, \\ \\forall t &gt; 0 \\] which implies \\[ v^* W_c(t) v = v^* (\\int_{0}^{t} e^{A\\tau} B B^* e^{A^*\\tau} d\\tau) v = 0, \\quad \\forall t &gt; 0 \\] Contradiction! (\\(3. \\Rightarrow 2.\\)) Prove by contradiction. Suppose \\(\\mathcal{C}\\) has full row rank but \\(W_c(t_1)\\) is singular at some \\(t_1 &gt; 0\\). Then, similar to the proof in (\\(1. \\Rightarrow 2.\\)), there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(F(t) := v^* e^{At} B \\equiv 0, \\forall t \\in [0, t_1]\\). Since \\(F(t)\\) is infinitely differentiable, we get its \\(i\\)’s derivative at \\(t=0\\), where \\(i = 0, 1, \\dots n-1\\). This results in \\[\\begin{equation*} \\left. \\frac{d^i F}{dt^i} \\right|_{t=0} = \\left. v^* A^{i} e^{At} B \\right|_{t=0} = v^* A^i B = 0, \\quad i = 0 \\dots n-1 \\end{equation*}\\] Thus, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\). Contradiction! (\\(3. \\Rightarrow 4.\\)) Proof by contradiction. Suppose \\([A - \\lambda I, B]\\) does not have full row rank for some \\(\\lambda \\in \\mathbb{C}\\). Then, there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(v^* [A - \\lambda I, B] = 0\\). This implies \\(v^* A = v^* \\lambda\\) and \\(v^* B = 0\\). On the other hand, \\[\\begin{equation*} v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; \\lambda B &amp; \\dots &amp; \\lambda^{n-1} B \\end{bmatrix} = 0 \\end{equation*}\\] Contradiction! (\\(4. \\Rightarrow 5.\\)) Proof by contradiction. If there exists a left eigenvector and eigenvalue pair \\((x, \\lambda)\\), s.t. \\(x^* A = \\lambda x^*\\) while \\(x^*B = 0\\), then \\(x^* [A - \\lambda I, B] = 0\\). Contradiction! (\\(5. \\Rightarrow 3.\\)) Proof by contradiction. If the controllability matrix \\(\\mathcal{C}\\) does not have full row rank, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then, from Corollary B.4, there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{R}^{k \\times k}, \\bar{A}_{\\bar{c}} \\in \\mathbb{R}^{(n-k) \\times (n-k)}\\). Now arbitrarily pick one of \\(\\bar{A}_{\\bar{c}}\\)’s left eigenvector \\(x_{\\bar{c}}\\) and its corresponding eigenvalue \\(\\lambda_1\\). Define the vector \\(x = \\begin{bmatrix} 0 \\\\ x_{\\bar{c}} \\end{bmatrix}\\). Then, \\[\\begin{equation*} \\begin{split} x^* (TAT^{-1}) = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\end{bmatrix} \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\bar{A}_{\\bar{c}} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; \\lambda_1 x_{\\bar{c}}^* \\end{bmatrix} = \\lambda_1 x^* \\\\ x^* (TB) &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{x}} \\end{bmatrix} \\begin{bmatrix} B_{\\bar{c}} \\\\ 0 \\end{bmatrix} = 0 \\end{split} \\end{equation*}\\] which implies \\((TAT^{-1}, TB)\\) is not controllable. However, similarity transformation does not change controllability. Contradiction! (\\(6. \\Rightarrow 1.\\)) Prove by contradiction. If \\((A, B)\\) is not controllable, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then from Corollary B.4, there exists a similarity transformation \\(T\\) s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] Now arbitrarily pick \\(F \\in \\mathbb{R}^{m\\times n}\\) and define \\(FT^{-1} = [F_1, F_2]\\), where \\(F_1 \\in \\mathbb{R}^{m\\times k}, F_2 \\in \\mathbb{R}^{m\\times (n-k)}\\). Thus, \\[\\begin{equation*} \\begin{split} \\text{det}(A+BF-\\lambda I) &amp; = \\text{det}\\left( T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} T + T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} F - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det}\\left( T^{-1} \\left\\{ \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} FT^{-1} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right\\} T \\right) \\\\ &amp; = \\text{det}\\left( \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\begin{bmatrix} F_1 &amp; F_2 \\end{bmatrix} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1 &amp; \\bar{A}_{12} + \\bar{B}_c F_2 \\\\ 0 &amp; \\bar{A}_{\\bar{c}} - \\lambda I_2 \\end{bmatrix} \\\\ &amp; = \\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1) \\cdot \\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2) \\end{split} \\end{equation*}\\] where \\(I_1\\) is the identity matrix of size \\(k\\). Similarly, \\(I_2\\) of size \\(n-k\\). Thus, at least \\(n-k\\) eigenvalues of \\(A+BF\\) cannot be freely assigned by choosing \\(F\\). Contradiction! (\\(1. \\Rightarrow 6.\\)) Here we only represent the SIMO case. For the MIMO case, the proof is far more complex. Interesting readers can refer to (Davison and Wonham 1968) (the shortest proof I can find). Since there is only one input, the matrix \\(B\\) degenerate to vector \\(b\\). From Corollary B.5, there exist a similarity transformation matrix \\(T\\), s.t. \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] For any \\(F \\in \\mathbb{C}^{1 \\times n}\\), denote \\(FT^{-1}\\) as \\([f_1, f_2, \\dots, f_n]\\). Calculating the characteristic polynomial of \\(A + bF\\): \\[\\begin{equation*} \\begin{split} \\text{det}(\\lambda I - A - bF) &amp; = \\text{det}(\\lambda I - T^{-1}A_1 T - T^{-1} b_1 F) \\\\ &amp; = \\text{det}(\\lambda I - A_1 - b_1 F T^{-1}) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\lambda + a_1 - f_1 &amp; \\lambda + a_2 - f_2 &amp; \\dots &amp; \\lambda + a_{n-1} - f_{n-1} &amp; \\lambda + a_n - f_n \\\\ -1 &amp; \\lambda &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; -1 &amp; \\lambda \\end{bmatrix} \\\\ &amp; = \\lambda^n + (a_1 - f_1) \\lambda^{n-1} + \\dots + (a_n - f_n) \\end{split} \\end{equation*}\\] By choosing \\([f_1, f_2, \\dots, f_n]\\), \\(A+bF\\)’s eigenvalues can be arbitrarily set. (\\(7. \\Rightarrow 1.\\)) Prove by contradiction. Assume that \\((A, B)\\) is not controllable. Then from 2., there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\) and \\(t_1 &gt; 0\\), \\[\\begin{equation*} F(t) = v^* e^{At} B = 0, \\quad \\forall t \\in [0, t_1] \\end{equation*}\\] Now consider \\(F(z) = v^* e^{Az} B, z\\in \\mathcal{C}\\), which is a vector of analytic function in complex analysis. For a arbitrary \\(t_2 \\in (0, t_1)\\), we have \\(F^{(i)}(t_2) = 0, \\forall i \\in \\mathbb{N}\\). Then, by invoking the fact from complex analysis: “Let \\(G\\) a connected open set and \\(f: G \\rightarrow \\mathbb{C}\\) be analytic, then \\(f \\equiv 0\\) on \\(G\\), if and only if there is a point \\(a \\in G\\) such that \\(f^{(i)}(a) = 0, \\forall n \\in \\mathbb{N}\\)”, we have \\(f(z) \\equiv 0, \\forall z \\in \\mathbb{C}\\). On the other hand, however, \\(W_c \\succ 0\\) implies there exists \\(t_3 &gt; 0\\), such that for the above \\(v\\), we have \\(v^* e^{At_3} B \\ne 0\\). Contradiction! (\\(1. \\Rightarrow 7.\\)) Since \\((A, B)\\) is controllable, from 2., \\(W_c(t) \\succ 0, \\forall t\\). Therefore, \\(W_c \\succ 0\\). The existence and uniqueness of the solution for \\(AW_c + W_cA^* = -BB^*\\) can be obtained directly from the proof of Theorem B.3, by setting \\(Q\\) there to be positive semidefinite. B.2.3 Duality Although controllability and observability seemingly have no direct connections from their definitions B.2 and B.3, the following theorem (Chen 1984) states their tight relations. Theorem B.6 (Theorem of Duality) The pair \\((C,A)\\) is observable if and only if \\((A^*,C^*)\\) is controllable. Proof. We first show that \\((C,A)\\) is observable if and only if the \\(n \\times n\\) matrix \\(W_o(t) = \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau}\\) is positive definite (nonsingular) for any \\(t&gt;0\\): “\\(\\Longleftarrow\\)”: From (B.5), given initial state \\(x(0)\\) and the inputs \\(u(t)\\), \\(y(t)\\) can be expressed as \\[\\begin{equation*} y(t) = Ce^{At} x(0) + C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau + Du(t) \\end{equation*}\\] Define a known function \\(\\bar{y}(t)\\) as \\(y(t) - C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau - Du(t)\\) and we will get \\[\\begin{equation*} Ce^{At} x(0) = \\bar{y}(t) \\end{equation*}\\] Pre-multiply the above equation by \\(e^{A^*t}C^*\\) and integrate it over \\([0,t_1]\\) to yield \\[\\begin{equation*} (\\int_{0}^{t_1} e^{A^*t} C^*C e^{At} dt) x(0) = W_o(t_1) x(0) = \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] Since \\(W_o(t_1) \\succ 0\\), \\[\\begin{equation*} x(0) = W_o(t_1)^{-1} \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] can be observed. “\\(\\Longrightarrow\\)”: Prove by contradiction. Suppose \\((C,A)\\) is observable but there exists \\(t_1 &gt;0\\), s.t. \\(W_o(t_1)\\) is singular. This implies there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* W_o(t_1) v = 0 \\Longrightarrow Ce^{At} v \\equiv 0, \\ \\forall t \\in [0,t_1] \\end{equation*}\\] Similar to the proof of Theorem B.5 (\\(7. \\Rightarrow 1.\\)), we can use conclusions from complex analysis to claim that \\(Ce^{At} v \\equiv 0, \\forall t &gt;0\\). On the other hand, we set \\(u(t) \\equiv 0\\), which results in \\(y(t) = Ce^{At}x(0)\\). In this case \\(x(0) = 0\\) and \\(x(0) = v \\ne 0\\) will lead to the same output responses \\(y(t)\\) over \\(t&gt;0\\), which implies \\((C,A)\\) is not observable. Contradiction! Next we show the duality of controllability and observability: From (1) we know \\((C,A)\\) is controllable if and only of \\[\\begin{equation*} \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau} d\\tau = \\int_{0}^{t} e^{(A^*)\\tau} (C^*)^* (C^*) e^{(A^*)^*\\tau} d\\tau \\end{equation*}\\] is nonsingular for all \\(t &gt;0\\). The latter is exactly the definition of \\((A^*, C^*)\\)’s controllability Gramian \\(W_c(t)\\). B.2.4 Equivalent Statements for Observability With the Theorem of Duality B.6, we can directly write down the equivalent statements of observability without any additional proofs: Theorem B.7 (Equivalent Statements for Observability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((C, A)\\) is observable. The matrix \\[\\begin{equation*} W_o(t) := \\int_{0}^{t} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] is positive definite for any \\(t&gt;0\\). The observability matrix \\[\\begin{equation*} \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^2 \\\\ \\dots \\\\ CA^{n-1} \\end{bmatrix} \\end{equation*}\\] has full column rank. The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full column rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(y\\) be any eigenvalue and any corresponding right eigenvector of \\(A\\), i.e., \\(Ay = \\lambda y\\), then \\(Cy \\ne 0\\). The eigenvalues of \\(A+LC\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(L\\). \\((A^*, C^*)\\) is controllable. If, in addition, all eigenvalues of \\(A\\) have negative parts, then the unique solution of \\[\\begin{equation*} A^* W_o + W_o A = -C^* C \\end{equation*}\\] is positive definite. The solution is called the observability Gramian and can be expressed as \\[\\begin{equation*} W_o = \\int_{0}^{\\infty} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] B.3 Stabilizability And Detectability To define stabilizability and detectability of an LTI system, we first introduce the concept of system mode, which can be naturally derived from the fifth definition of controllability B.5 (observability B.7). Definition B.4 (System Mode) \\(\\lambda\\) is a mode of an LTI system, if it is an eigenvalue of \\(A\\). The mode \\(\\lambda\\) is said to be: stable, if \\(\\text{Re}\\lambda &lt; 0\\), controllable, if \\(x^* B \\ne 0\\) for all left eigenvectors of \\(A\\) associated with \\(\\lambda\\), observable, if \\(C x \\ne 0\\) for all right eigenvectors of \\(A\\) associated with \\(\\lambda\\). Otherwise, the mode is said to be uncontrollable (unobservable). With the concept of system mode, the fifth definition of controllability B.5 (observability B.7) can be restated as An LTI system is controllable (observable) if and only if all modes are controllable (observable). Stabilizability (detectability) is defined similarly via loosening part of controllability (observability) conditions. Definition B.5 (Stabilizability) An LTI system is said to be stabilizable if all of its unstable modes are controllable. Definition B.6 (Detectability) An LTI system is said to be detectable if all of its unstable modes are observable. Like in the case of controllability and observability, duality also holds in stabilizability and detectability. Moreover, similarity transformation will not influence an LTI system’s stabilizability and detectability. B.3.1 Equivalent Statements for Stabilizability Theorem B.8 (Equivalent Statements for Stabilizability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((A,B)\\) is stabilizable. For all \\(\\lambda\\) and \\(x\\) such that \\(x^* A = \\lambda x^*\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(x^* B \\ne 0\\). The matrix \\([A-\\lambda I, B]\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(F\\) such that \\(A+BF\\) are Hurwitz. Proof. (\\(1. \\Leftrightarrow 2.\\)) Directly from stabilizability’s definition. (\\(2. \\Leftrightarrow 3.\\)) If 2. holds but 3. not hold, then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* [A-\\lambda I, B] = 0 \\Leftrightarrow v^* A = \\lambda v^*, v^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Contradiction! Vice versa. (\\(4. \\Rightarrow 2.\\)) Prove by contradiction. Suppose there \\(x \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} x^* [A-\\lambda I, B] = 0 \\Leftrightarrow x^* A = \\lambda x^*, x^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Thus, for any \\(F\\), \\[\\begin{equation*} x^* (A+BF) = \\lambda x^*, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] On the other hand, suppose \\(A+BF\\) has \\(I\\) Jordon blocks, with each equipped with an eigenvalue \\(\\eta_i, i = 1\\dots I\\) (note that \\(\\eta_\\alpha\\) may be equal to \\(\\eta_\\beta\\), i.e., they are equivalent eigenvalues with different Jordon blocks). Since \\(A+BF\\)’s eigenvalues all have negative real parts, \\(\\text{Re} (\\eta_i) &lt; 0, i = 1\\dots I\\). For each \\(\\eta_i,i \\in \\left\\{1\\dots i\\right\\}\\), denote its \\(K_i\\) generalized left eigenvectors as \\(v_{i,1}, v_{i,2}, \\dots v_{i,K_i}\\). By definition, \\(\\sum_{i=1}^{I} K_i = n\\) and \\[\\begin{equation*} \\begin{split} v_{i,1}^* (A+BF) &amp; = v_{i,1}^* \\cdot \\eta_i \\\\ v_{i,2}^* (A+BF) &amp; = v_{i,1}^* + v_{i,2}^* \\cdot \\eta_i \\\\ &amp; \\vdots \\\\ v_{i,K_i}^* (A+BF) &amp; = v_{i,K_i-1}^* + v_{i,K_i}^* \\cdot \\eta_i \\end{split} \\end{equation*}\\] for all \\(i \\in \\left\\{1\\dots i\\right\\}\\). Also, \\(v_{i,k},i=1\\dots I, k=1\\dots K_i\\) are linearly independent and spans \\(\\mathbb{C}^n\\). Therefore, \\[\\begin{equation*} x^* = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\end{equation*}\\] which leads to \\[\\begin{equation*} \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* (A+BF) = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot \\lambda \\cdot v_{i,k}^* \\end{equation*}\\] Since \\(v_{i,k}\\)’s are \\(A+BF\\)’s generalized eigenvectors, we have \\[\\begin{equation*} \\begin{split} &amp; \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\cdot (A+BF) \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\xi_{i,1} \\cdot \\eta_i \\cdot v_{i,1}^* + \\sum_{k=2}^{K_i} \\xi_{i,k} (v_{i,k-1}^* + \\eta_i \\cdot v_{i,k}^* ) \\right\\} \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} (\\xi_{i,k}\\cdot \\eta_i + \\xi_{i,k+1}) v_{i,k}^* + \\xi_{i,K_i} \\cdot \\eta_i \\cdot v_{i,K_i}^* \\right\\} \\end{split} \\end{equation*}\\] Combining the above two equations: \\[\\begin{equation*} \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} \\left[ \\xi_{i,k}\\cdot (\\eta_i - \\lambda) + \\xi_{i,k+1} \\right] v_{i,k}^* + \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) \\cdot v_{i,K_i}^* = 0 \\right\\} \\end{equation*}\\] Since \\(v_{i,k}\\)’s are linearly independent, for any \\(i \\in \\left\\{i\\dots I\\right\\}\\): \\[\\begin{equation*} \\begin{split} \\xi_{i,1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,2} &amp; = 0 \\Rightarrow \\xi_{i,2} = (-1) \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda) \\\\ \\xi_{i,2} \\cdot (\\eta_i - \\lambda) + \\xi_{i,3} &amp; = 0 \\Rightarrow \\xi_{i,3} = (-1)^2 \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^2 \\\\ &amp; \\vdots \\\\ \\xi_{i,K_i-1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,K_i} &amp; = 0 \\Rightarrow \\xi_{i,K_i} = (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i-1} \\\\ \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) &amp; = 0 \\end{split} \\end{equation*}\\] Thus, \\[\\begin{equation*} (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i} = 0 \\end{equation*}\\] Denote \\(\\xi_{i,1}\\) as \\(r_1 e^{\\theta_1}\\), \\((\\eta_i - \\lambda)\\) as \\(r_2 e^{\\theta_2}\\). Since \\(\\text{Re} \\lambda \\ge 0, \\text{Re}(\\eta_i) &lt; 0\\), \\(r_2 &gt; 0\\). On the other hand, the following equation suggests \\[\\begin{equation*} r_1 r_2^{K_i-1} e^{j[\\theta_1 + \\theta_2 (K_i-1)]} = 0 \\end{equation*}\\] Thus, \\(r_1\\) has to be \\(0\\), which implies \\(\\xi_{i,1} = 0\\). By recursion, \\(\\xi_{i,k} = 0, \\forall k = 1\\dots K_i\\). Contradiction! (\\(1. \\Rightarrow 4.\\)) If \\((A,B)\\) is controllable, then from Theorem (thm:lticontrollable)’s sixth definition, we can freely assign the poles of \\(A+BF\\) via choosing \\(F\\) properly. Otherwise, if \\((A,B)\\) is uncontrollable, then from Corollary B.4 and proof of Theorem B.5 (\\(6. \\Rightarrow 1.\\)), there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] and \\[\\begin{equation*} \\text{det}(A+BF-\\lambda I) = \\underbrace{\\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1)}_{\\chi_c(\\lambda)} \\cdot \\underbrace{\\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2)}_{\\chi_{\\bar{c}}(\\lambda)} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}\\), \\(I_1\\) identity matrix of size \\(k_1\\), \\([F_1,F_2] = FT^{-1}\\), and \\(k_1 = \\text{rank} \\mathcal{C}\\). Additionally, \\((\\bar{A}_c, \\bar{B}_c)\\) is controllable. Thus, \\(\\chi_c(\\lambda)\\)’s zeros can be freely assigned by choosing proper \\(F\\), i.e., system modes with \\(\\chi_c(\\lambda)\\) is controllable, regardless of its stability. On the other hand, system modes with \\(\\chi_{\\bar{c}}(\\lambda)\\) must be stable. Otherwise, we cannot affect it by assigning \\(F\\), which is a contradiction to statement (1). Therefore, \\((TAT^{-1}, TB)\\) is stabilizable. Since similarity transformation does not change stabilizability, \\((A,B)\\) is stabilizable. B.3.2 Equivalent Statements for Detectability Thanks to duality, we can directly write down the equivalent statements of observability without any additional proofs: Theorem B.9 (Equivalent Statements for Detectability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((C,A)\\) is detectable. For all \\(\\lambda\\) and \\(x\\) such that \\(A x = \\lambda x\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(C x \\ne 0\\). The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(L\\) such that \\(A+LC\\) are Hurwitz. \\((A^*, C^*)\\) is stabilizable. References Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. when \\(A\\) is not diagonalizable, similar results can be derived via Jordan decomposition.↩︎ "],["references.html", "References", " References Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
