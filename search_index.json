[["index.html", "Optimal Control and Reinforcement Learning Preface Feedback Offerings", " Optimal Control and Reinforcement Learning Heng Yang 2025-11-24 Preface This is the textbook for Harvard ES/AM 158: Introduction to Optimal Control and Reinforcement Learning. Feedback I would like to invite you to provide feedback to the textbook via inline comments with Hypothesis: Go to Hypothesis and create an account Install the Chrome extension of Hypothesis Provide public comments to textbook contents and I will try to address them Offerings 2025 Fall Time: Mon/Wed 2:15 - 3:30pm Location: SEC 1.413 Instructor: Heng Yang Teaching Fellow: Haoyu Han, Han Qi [Syllabus], [Problem Sets], [Canvas] 2023 Fall The course was previously offered as Introduction to Optimal Control and Estimation. Starting Fall 2025, contents about reinforcement learning have been added to the course. "],["mdp.html", "Chapter 1 Markov Decision Process 1.1 Finite-Horizon MDP 1.2 Infinite-Horizon MDP", " Chapter 1 Markov Decision Process Optimal control (OC) and reinforcement learning (RL) address the problem of making optimal decisions in the presence of a dynamic environment. In optimal control, this dynamic environment is often referred to as a plant or a dynamical system. In reinforcement learning, it is modeled as a Markov decision process (MDP). The goal in both fields is to evaluate and design decision-making strategies that optimize long-term performance: RL typically frames this as maximizing a long-term reward. OC often formulates it as minimizing a long-term cost. The emphasis on long-term evaluation is crucial. Because the environment evolves over time, decisions that appear beneficial in the short term may lead to poor long-term outcomes and thus be suboptimal. With this motivation, we now formalize the framework of Markov Decision Processes (MDPs), which are discrete-time stochastic dynamical systems. 1.1 Finite-Horizon MDP We begin with finite-horizon MDPs and introduce infinite-horizon MDPs in the following section. An abstract definition of the finite-horizon case will be presented first, followed by illustrative examples. A finite-horizon MDP is given by the following tuple: \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, T), \\] where \\(\\mathcal{S}\\): state space (set of all possible states) \\(\\mathcal{A}\\): action space (set of all possible actions) \\(P(s&#39; \\mid s, a)\\): probability of transitioning to state \\(s&#39;\\) from state \\(s\\) under action \\(a\\) (i.e., dynamics) \\(R(s,a)\\): reward of taking action \\(a\\) in state \\(s\\) \\(T\\): horizon, a positive integer For now, let us assume both the state space and the action space are discrete and have a finite number of elements. In particular, denote the number of elements in \\(\\mathcal{S}\\) as \\(|\\mathcal{S}|\\), and the number of elements in \\(\\mathcal{A}\\) as \\(|\\mathcal{A}|\\). This is also referred to as a tabular MDP. Policy. Decision-making in MDPs is represented by policies. A policy is a function that, given any state, outputs a distribution of actions: \\(\\pi: \\mathcal{S} \\mapsto \\Delta(\\mathcal{A})\\). That is, \\(\\pi(a \\mid s)\\) returns the probability of taking action \\(a\\) in state \\(s\\). In finite-horizon MDPs, we consider a tuple of policies: \\[\\begin{equation} \\pi = (\\pi_0, \\dots, \\pi_t, \\dots, \\pi_{T-1}), \\tag{1.1} \\end{equation}\\] where each \\(\\pi_t\\) denotes the policy at step \\(t \\in [0,T-1]\\). Trajectory and Return. Given an initial state \\(s_0 \\in \\mathcal{S}\\) and a policy \\(\\pi\\), the MDP will evolve as Start at state \\(s_0\\) Take action \\(a_0 \\sim \\pi_0(a \\mid s_0)\\) following policy \\(\\pi_0\\) Collect reward \\(r_0 = R(s_0, a_0)\\) (assume \\(R\\) is deterministic) Transition to state \\(s_1 \\sim P(s&#39; \\mid s_0, a_0)\\) following the dynamics Go to step 2 and continue until reaching state \\(s_T\\) This evolution generates a trajectory of states, actions, and rewards: \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T). \\] The cumulative reward of this trajectory is \\(g_0 = \\sum_{t=0}^{T-1} r_t\\), which is called the return of the trajectory. Clearly, \\(g_0\\) is a random variable due to the stochasticity of both the policy and the dynamics. Similarly, if the state at time \\(t\\) is \\(s_t\\), we denote: \\[ g_t = r_t + \\dots + r_{T-1} \\] as the return of the policy starting at \\(s_t\\). 1.1.1 Value Functions State-Value Function. Given a policy \\(\\pi\\) as in (1.1), which states are preferable at time \\(t\\)? The (time-indexed) state-value function assigns to each \\(s\\in\\mathcal{S}\\) the expected return from \\(t\\) onward when starting in \\(s\\) and following \\(\\pi\\) thereafter. Formally, define \\[\\begin{equation} V_t^\\pi(s) := \\mathbb{E} \\left[g_t \\mid s_t=s\\right] = \\mathbb{E} \\left[\\sum_{i=t}^{T-1} R(s_i,a_i) \\middle| s_t=s,a_i\\sim \\pi_i(\\cdot\\mid s_i), s_{i+1}\\sim P(\\cdot\\mid s_i,a_i)\\right]. \\tag{1.2} \\end{equation}\\] The expectation is over the randomness induced by both the policy and the dynamics. Thus, if \\(V_t^\\pi(s_1)&gt;V_t^\\pi(s_2)\\), then at time \\(t\\) under policy \\(\\pi\\) it is better in expectation to be in \\(s_1\\) than in \\(s_2\\) because the former yields a larger expected return. \\(V^{\\pi}_t(s)\\): given policy \\(\\pi\\), how good is it to start in state \\(s\\) at time \\(t\\)? Action-Value Function. Similarly, the action-value function assigns to each state-action pair \\((s,a)\\in\\mathcal{S}\\times\\mathcal{A}\\) the expected return obtained by starting in state \\(s\\), taking action \\(a\\) first, and then following policy \\(\\pi\\) thereafter: \\[\\begin{equation} \\begin{split} Q_t^\\pi(s,a) := &amp; \\mathbb{E} \\left[R(s,a) + g_{t+1} \\mid s_{t+1} \\sim P(\\cdot \\mid s,a)\\right] \\\\ = &amp; \\mathbb{E} \\left[R(s,a) + \\sum_{i=t+1}^{T-1} R(s_i, a_i) \\middle| s_{t+1} \\sim P(\\cdot \\mid s,a) \\right]. \\end{split} \\tag{1.3} \\end{equation}\\] The key distinction is that the action-value function evaluates the return when the first action may deviate from policy \\(\\pi\\), whereas the state-value function assumes strict adherence to \\(\\pi\\). This flexibility makes the action-value function central to improving \\(\\pi\\), since it reveals whether alternative actions can yield higher returns. \\(Q^{\\pi}_t(s,a)\\): At time \\(t\\), how good is it to take action \\(a\\) in state \\(s\\), then follow the policy \\(\\pi\\)? It is easy to verify that the state-value function and the action-value function satisfy: \\[\\begin{align} V_t^{\\pi}(s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi_t(a \\mid s) Q_t^{\\pi}(s,a), \\tag{1.4} \\\\ Q_t^{\\pi}(s,a) &amp; = R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}_{t+1} (s&#39;). \\tag{1.5} \\end{align}\\] From these two equations, we can derive the Bellman Consistency equations. Proposition 1.1 (Bellman Consistency (Finite Horizon)) The state-value function \\(V^{\\pi}_t(\\cdot)\\) in (1.2) satisfies the following recursion: \\[\\begin{equation} \\begin{split} V^{\\pi}_t(s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi_t(a\\mid s) \\left( R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}_{t+1} (s&#39;) \\right) \\\\ &amp; =: \\mathbb{E}_{a \\sim \\pi_t(\\cdot \\mid s)} \\left[ R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} [V^{\\pi}_{t+1}(s&#39;)] \\right]. \\end{split} \\tag{1.6} \\end{equation}\\] Similarly, the action-value function \\(Q^{\\pi}_t(s,a)\\) in (1.3) satisfies the following recursion: \\[\\begin{equation} \\begin{split} Q^{\\pi}_t (s, a) &amp; = R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) \\left( \\sum_{a&#39; \\in \\mathcal{A}} \\pi_{t+1}(a&#39; \\mid s&#39;) Q^{\\pi}_{t+1}(s&#39;, a&#39;)\\right) \\\\ &amp; =: R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[\\mathbb{E}_{a&#39; \\sim \\pi_{t+1}(\\cdot \\mid s&#39;)} [Q^{\\pi}_{t+1}(s&#39;, a&#39;)] \\right]. \\end{split} \\tag{1.7} \\end{equation}\\] 1.1.2 Policy Evaluation The Bellman consistency result in Proposition 1.1 is fundamental because it directly yields an algorithm for evaluating a given policy \\(\\pi\\)—that is, for computing its state-value and action-value functions—provided the transition dynamics of the MDP are known. Policy evaluation for the state-value function proceeds as follows: Initialization: set \\(V^{\\pi}_T(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). Backward recursion: for \\(t = T-1, T-2, \\dots, 0\\), update each \\(s \\in \\mathcal{S}\\) by \\[ V^{\\pi}_{t}(s) = \\mathbb{E}_{a \\sim \\pi_t(\\cdot \\mid s)} \\left[ R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\big[ V^{\\pi}_{t+1}(s&#39;) \\big] \\right]. \\] Similarly, policy evaluation for the action-value function is given by: Initialization: set \\(Q^{\\pi}_T(s,a) = 0\\) for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). Backward recursion: for \\(t = T-1, T-2, \\dots, 0\\), update each \\((s,a) \\in \\mathcal{S}\\times\\mathcal{A}\\) by \\[ Q^{\\pi}_t(s,a) = R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[ \\mathbb{E}_{a&#39; \\sim \\pi_{t+1}(\\cdot \\mid s&#39;)} \\big[ Q^{\\pi}_{t+1}(s&#39;, a&#39;) \\big] \\right]. \\] The essential feature of this algorithm is its backward-in-time recursion: the value functions are first set at the terminal horizon \\(T\\), and then propagated backward step by step through the Bellman consistency equations. Example 1.1 (MDP, Transition Graph, and Policy Evaluation) It is often useful to visualize small MDPs as transition graphs, where states are represented by nodes and actions are represented by directed edges connecting those nodes. As a simple illustrative example, consider a robot navigating on a two-state grid. At each step, the robot can either Stay in its current state or Move to the other state. This finite-horizon MDP is fully specified by the tuple of states, actions, transition dynamics, rewards, and horizon: States: \\(\\mathcal{S} = \\{\\alpha, \\beta \\}\\) Actions: \\(\\mathcal{A} = \\{\\text{Move} , \\text{Stay} \\}\\) Transition dynamics: we can specify the transition dynamics in the following table State \\(s\\) Action \\(a\\) Next State \\(s&#39;\\) Probability \\(P(s&#39; \\mid s, a)\\) \\(\\alpha\\) Stay \\(\\alpha\\) 1 \\(\\alpha\\) Move \\(\\beta\\) 1 \\(\\beta\\) Stay \\(\\beta\\) 1 \\(\\beta\\) Move \\(\\alpha\\) 1 Reward: \\(R(s,a)=1\\) if \\(a = \\text{Move}\\) and \\(R(s,a)=0\\) if \\(a = \\text{Stay}\\) Horizon: \\(T=2\\). This MDP can be represented by the transition graph in Fig. 1.1. Note that for this MDP, the transition dynamics is deterministic. We will see a stochastic MDP soon. Figure 1.1: A Simple Transition Graph. At time \\(t=0\\), if the robot starts at \\(s_0 = \\alpha\\), first chooses action \\(a_0 = \\text{Move}\\), and then chooses action \\(a_1 = \\text{Stay}\\), the resulting trajectory is \\[ \\tau = (\\alpha, \\text{Move}, +1, \\beta, \\text{Stay}, 0, \\beta). \\] The return of this trajectory is: \\[ g_0 = +1 + 0 = +1. \\] Policy Evaluation. Given a policy \\[\\begin{equation} \\pi = (\\pi_0, \\pi_1), \\quad \\pi_0(a \\mid s) = \\begin{cases} 0.5 &amp; a = \\text{Move} \\\\ 0.5 &amp; a = \\text{Stay} \\end{cases}, \\quad \\pi_1( a \\mid s) = \\begin{cases} 0.8 &amp; a = \\text{Move} \\\\ 0.2 &amp; a = \\text{Stay} \\end{cases}. \\end{equation}\\] We can use the Bellman consistency equations to compute the state-value function. We first initialize: \\[ V^{\\pi}_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\] where the first row contains the value at \\(s = \\alpha\\) and the second row contains the value at \\(s = \\beta\\). We then perform the backward recursion for \\(t=1\\). For \\(s = \\alpha\\), we have \\[\\begin{equation} V^{\\pi}_1(\\alpha) = \\begin{bmatrix} \\pi_1(\\text{Move} \\mid \\alpha) \\\\ \\pi_1(\\text{Stay} \\mid \\alpha) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\alpha, \\text{Move}) + V^{\\pi}_2(\\beta) \\\\ R(\\alpha, \\text{Stay}) + V^{\\pi}_2(\\alpha) \\end{bmatrix} = \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 0.8 \\end{equation}\\] For \\(s = \\beta\\), we have \\[\\begin{equation} V^{\\pi}_1(\\beta) = \\begin{bmatrix} \\pi_1(\\text{Move} \\mid \\beta) \\\\ \\pi_1(\\text{Stay} \\mid \\beta) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\beta, \\text{Move}) + V^{\\pi}_2(\\alpha) \\\\ R(\\beta, \\text{Stay}) + V^{\\pi}_2(\\beta) \\end{bmatrix} = \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 0.8. \\end{equation}\\] Therefore, we have \\[ V^{\\pi}_1 = \\begin{bmatrix} 0.8 \\\\ 0.8 \\end{bmatrix}. \\] We then proceed to the backward recursion for \\(t=0\\): \\[\\begin{align} V_0^{\\pi}(\\alpha) &amp; = \\begin{bmatrix} \\pi_0(\\text{Move} \\mid \\alpha) \\\\ \\pi_0(\\text{Stay} \\mid \\alpha) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\alpha, \\text{Move}) + V^{\\pi}_1(\\beta) \\\\ R(\\alpha, \\text{Stay}) + V^{\\pi}_1(\\alpha) \\end{bmatrix} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1.8 \\\\ 0.8 \\end{bmatrix} = 1.3. \\\\ V_0^{\\pi}(\\beta) &amp; = \\begin{bmatrix} \\pi_0(\\text{Move} \\mid \\beta) \\\\ \\pi_0(\\text{Stay} \\mid \\beta) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\beta, \\text{Move}) + V^{\\pi}_0(\\alpha) \\\\ R(\\beta, \\text{Stay}) + V^{\\pi}_0(\\beta) \\end{bmatrix} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1.8 \\\\ 0.8 \\end{bmatrix} = 1.3. \\end{align}\\] Therefore, the state-value function at \\(t=0\\) is \\[ V^{\\pi}_0 = \\begin{bmatrix} 1.3 \\\\ 1.3 \\end{bmatrix}. \\] You are encouraged to carry out the similar calculations for the action-value function. The toy example was small enough to carry out policy evaluation by hand; in realistic MDPs, we will need the help from computers. Consider now an MDP whose transition graph is shown in Fig. 1.2. This example is adapted from here. Figure 1.2: Hangover Transition Graph. This MDP has six states: \\[ \\mathcal{S} = \\{\\text{Hangover}, \\text{Sleep}, \\text{More Sleep}, \\text{Visit Lecture}, \\text{Study}, \\text{Pass Exam} \\}, \\] and two actions: \\[ \\mathcal{A} = \\{\\text{Lazy}, \\text{Productive} \\}. \\] The stochastic transition dynamics are labeled in the transition graph. For example, at state “Hangover”, taking action “Productive” will lead to state “Visit Lecture” with probability \\(0.3\\) and state “Hangover” with probability \\(0.7\\). The rewards of the MDP are defined as: \\[ R(s,a) = \\begin{cases} +1 &amp; s = \\text{Pass Exam} \\\\ -1 &amp; \\text{otherwise}. \\end{cases}. \\] Policy Evaluation. Consider a time-invariant random policy \\[ \\pi = \\{\\pi_0,\\dots,\\pi_{T-1} \\}, \\quad \\pi_t(a \\mid s) = \\begin{cases} \\alpha &amp; a = \\text{Lazy} \\\\ 1 - \\alpha &amp; a = \\text{Productive} \\end{cases}, \\] that takes “Lazy” with probability \\(\\alpha\\) and “Productive” with probability \\(1-\\alpha\\). The following Python code performs policy evaluation for this MDP, with \\(T=10\\) and \\(\\alpha = 0.4\\). # Finite-horizon policy evaluation for the Hangover MDP from collections import defaultdict from typing import Dict, List, Tuple State = str Action = str # --- MDP spec --------------------------------------------------------------- S: List[State] = [ &quot;Hangover&quot;, &quot;Sleep&quot;, &quot;More Sleep&quot;, &quot;Visit Lecture&quot;, &quot;Study&quot;, &quot;Pass Exam&quot; ] A: List[Action] = [&quot;Lazy&quot;, &quot;Productive&quot;] # P[s, a] -&gt; list of (s_next, prob) P: Dict[Tuple[State, Action], List[Tuple[State, float]]] = { # Hangover (&quot;Hangover&quot;, &quot;Lazy&quot;): [(&quot;Sleep&quot;, 1.0)], (&quot;Hangover&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.3), (&quot;Hangover&quot;, 0.7)], # Sleep (&quot;Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Sleep&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.6), (&quot;More Sleep&quot;, 0.4)], # More Sleep (&quot;More Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;More Sleep&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 0.5), (&quot;More Sleep&quot;, 0.5)], # Visit Lecture (&quot;Visit Lecture&quot;, &quot;Lazy&quot;): [(&quot;Study&quot;, 0.8), (&quot;Pass Exam&quot;, 0.2)], (&quot;Visit Lecture&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 1.0)], # Study (&quot;Study&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Study&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 0.9), (&quot;Study&quot;, 0.1)], # Pass Exam (absorbing) (&quot;Pass Exam&quot;, &quot;Lazy&quot;): [(&quot;Pass Exam&quot;, 1.0)], (&quot;Pass Exam&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 1.0)], } def R(s: State, a: Action) -&gt; float: &quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot; return 1.0 if s == &quot;Pass Exam&quot; else -1.0 # --- Policy: time-invariant, state-independent ------------------------------ def pi(a: Action, s: State, alpha: float) -&gt; float: &quot;&quot;&quot;pi(a|s): Lazy with prob alpha, Productive with prob 1-alpha.&quot;&quot;&quot; return alpha if a == &quot;Lazy&quot; else (1.0 - alpha) # --- Policy evaluation ------------------------------------------------------- def policy_evaluation(T: int, alpha: float): &quot;&quot;&quot; Compute {V_t(s)} and {Q_t(s,a)} for t=0..T with terminal condition V_T = Q_T = 0. Returns: V: Dict[int, Dict[State, float]] Q: Dict[int, Dict[Tuple[State, Action], float]] &quot;&quot;&quot; assert T &gt;= 0 # sanity: probabilities sum to 1 for each (s,a) for key, rows in P.items(): total = sum(p for _, p in rows) if abs(total - 1.0) &gt; 1e-9: raise ValueError(f&quot;Probabilities for {key} sum to {total}, not 1.&quot;) V: Dict[int, Dict[State, float]] = defaultdict(dict) Q: Dict[int, Dict[Tuple[State, Action], float]] = defaultdict(dict) # Terminal boundary for s in S: V[T][s] = 0.0 for a in A: Q[T][(s, a)] = 0.0 # Backward recursion for t in range(T - 1, -1, -1): for s in S: # First compute Q_t(s,a) for a in A: exp_next = sum(p * V[t + 1][s_next] for s_next, p in P[(s, a)]) Q[t][(s, a)] = R(s, a) + exp_next # Then V_t(s) = E_{a~pi}[Q_t(s,a)] V[t][s] = sum(pi(a, s, alpha) * Q[t][(s, a)] for a in A) return V, Q # --- Example run ------------------------------------------------------------- if __name__ == &quot;__main__&quot;: T = 10 # horizon alpha = 0.4 # probability of choosing Lazy V, Q = policy_evaluation(T=T, alpha=alpha) # Print V_0 print(f&quot;V_0(s) with T={T}, alpha={alpha}:&quot;) for s in S: print(f&quot; {s:13s}: {V[0][s]: .3f}&quot;) The code returns the following state values at \\(t=0\\): \\[\\begin{equation} V^{\\pi}_0 = \\begin{bmatrix} -3.582 \\\\ -2.306 \\\\ -2.180 \\\\ 1.757 \\\\ 2.939 \\\\ 10 \\end{bmatrix}, \\tag{1.8} \\end{equation}\\] where the ordering of the states follows that defined in \\(\\mathcal{S}\\). You can find the code here. 1.1.3 Principle of Optimality Every policy \\(\\pi\\) induces a value function \\(V_0^{\\pi}\\) that can be evaluated by policy evaluation (assuming the transition dynamics are known). The goal of reinforcement learning is to find an optimal policy that maximizes the value function with respect to a given initial state distribution: \\[\\begin{equation} V^\\star_0 = \\max_{\\pi}\\; \\mathbb{E}_{s_0 \\sim \\mu(\\cdot)} \\big[ V_0^{\\pi}(s_0) \\big], \\tag{1.9} \\end{equation}\\] where we have used the superscript “\\(\\star\\)” to denote the optimality of the value function. \\(V^\\star_0\\) is often known as the optimal value function. At first glance, (1.9) appears daunting: a naive approach would enumerate all stochastic policies \\(\\pi\\), evaluate their value functions, and select the best. A central result in reinforcement learning and optimal control—rooted in the principle of optimality—is that the optimal value functions satisfy a Bellman-style recursion, analogous to Proposition 1.1. This Bellman optimality recursion enables backward computation of the optimal value functions without enumerating policies. Theorem 1.1 (Bellman Optimality (Finite Horizon, State-Value)) Consider a finite-horizon MDP \\(\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,T)\\) with finite state and action sets and bounded rewards. Define the optimal value functions \\(\\{V_t^\\star\\}_{t=0}^{T}\\) by the following Bellman optimality recursion \\[\\begin{equation} \\begin{split} V_T^\\star(s)&amp; \\equiv 0, \\\\ V_t^\\star(s)&amp; = \\max_{a\\in\\mathcal{A}}\\Big\\{ R(s,a)\\;+\\;\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;)\\Big\\},\\ t=T-1,\\ldots,0. \\end{split} \\tag{1.10} \\end{equation}\\] Then, the optimal value functions are optimal in the sense of statewise dominance: \\[\\begin{equation} V_t^{\\star}(s)\\;\\ge\\; V_t^{\\pi}(s) \\quad\\text{for all policies }\\pi,\\; s\\in\\mathcal{S},\\; t=0,\\ldots,T. \\tag{1.11} \\end{equation}\\] Moreover, the deterministic policy \\(\\pi^\\star=(\\pi^\\star_0,\\ldots,\\pi^\\star_{T-1})\\) with \\[\\begin{equation} \\begin{split} \\pi_t^\\star(s)\\;\\in\\;\\arg\\max_{a\\in\\mathcal{A}} \\Big\\{ R(s,a)\\;+\\;\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;)\\Big\\}, \\\\ \\text{for any } s\\in\\mathcal{S},\\; t=0,\\dots,T-1 \\end{split} \\tag{1.12} \\end{equation}\\] is optimal, where ties can be broken by any fixed rule. Proof. We first show that the value functions defined by the Bellman optimality recursion (1.10) are optimal in the sense that they dominate the value functions of any other policy. The proof proceeds by backward induction. Base case (\\(t=T\\)). For every \\(s\\in\\mathcal{S}\\), \\[ V^\\star_T(s)\\;=\\;0\\;=\\;V_T^{\\pi}(s), \\] so \\(V^\\star_T(s)\\ge V_T^{\\pi}(s)\\) holds trivially. Inductive step. Assume \\(V^\\star_{t+1}(s)\\ge V^{\\pi}_{t+1}(s)\\) for all \\(s\\in\\mathcal{S}\\). Then, for any \\(s\\in\\mathcal{S}\\), \\[\\begin{align*} V_t^{\\pi}(s) &amp;= \\sum_{a\\in\\mathcal{A}} \\pi_t(a\\mid s)\\!\\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\pi}(s&#39;)\\right) \\\\ &amp;\\le \\sum_{a\\in\\mathcal{A}} \\pi_t(a\\mid s)\\!\\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\star}(s&#39;)\\right) \\\\ &amp;\\le \\max_{a\\in\\mathcal{A}} \\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\star}(s&#39;)\\right) \\;=\\; V_t^\\star(s), \\end{align*}\\] where the first inequality uses the induction hypothesis and the second uses that an expectation is bounded above by a maximum. Hence \\(V_t^\\star(s)\\ge V_t^{\\pi}(s)\\) for all \\(s\\), completing the induction. Therefore, \\(\\{V_t^\\star\\}_{t=0}^T\\) dominates the value functions attainable by any policy. Next, we show that \\(\\{V_t^\\star\\}\\) is attainable by some policy. Since \\(\\mathcal{A}\\) is finite (tabular setting), the maximizer in the Bellman optimality operator exists for every \\((t,s)\\); thus we can define a (deterministic) greedy policy \\[ \\pi_t^\\star(s)\\;\\in\\;\\arg\\max_{a\\in\\mathcal{A}} \\Big\\{ R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;) \\Big\\}. \\] A simple backward induction then shows \\(V_t^{\\pi^\\star}(s)=V_t^\\star(s)\\) for all \\(t\\) and \\(s\\): at \\(t=T\\) both are \\(0\\), and if \\(V_{t+1}^{\\pi^\\star}=V_{t+1}^\\star\\), then by construction of \\(\\pi_t^\\star\\) the Bellman equality yields \\(V_t^{\\pi^\\star}=V_t^\\star\\). Consequently, the optimal value functions are achieved by the greedy (deterministic) policy \\(\\pi^\\star\\). Corollary 1.1 (Bellman Optimality (Finite Horizon, Action-Value)) Given the optimal (state-)value functions \\(V^{\\star}_{t},t=0,\\dots,T\\), define the optimal action-value function \\[\\begin{equation} Q_t^\\star(s,a)\\;=\\;R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;), \\quad t=0,\\dots,T-1. \\tag{1.13} \\end{equation}\\] Then we have \\[\\begin{equation} V_t^\\star(s)=\\max_{a\\in\\mathcal{A}} Q_t^\\star(s,a),\\qquad \\pi_t^\\star(s)\\in\\arg\\max_{a\\in\\mathcal{A}} Q_t^\\star(s,a). \\tag{1.14} \\end{equation}\\] The optimal action-value functions satisfy: \\[\\begin{equation} \\begin{split} Q_T^\\star(s,a) &amp; \\equiv 0,\\\\ Q_t^\\star(s,a) &amp; = R(s,a) \\;+\\; \\mathbb{E}_{s&#39;\\sim P(\\cdot\\mid s,a)} \\!\\left[ \\max_{a&#39;\\in\\mathcal{A}} Q_{t+1}^\\star(s&#39;,a&#39;) \\right], \\quad t=T-1,\\ldots,0. \\end{split} \\tag{1.15} \\end{equation}\\] 1.1.4 Dynamic Programming The principle of optimality in Theorem 1.1 yields a constructive procedure to compute the optimal value functions and an associated deterministic optimal policy. This backward-induction procedure is the dynamic programming (DP) algorithm. Dynamic programming (finite horizon). Initialization. Set \\(V_T^\\star(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). Backward recursion. For \\(t = T-1, T-2, \\dots, 0\\): Optimal value: for each \\(s \\in \\mathcal{S}\\), \\[ V_t^\\star(s) = \\max_{a \\in \\mathcal{A}} \\left\\{ R(s,a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s,a)} \\big[ V_{t+1}^\\star(s&#39;) \\big] \\right\\}. \\] Greedy policy (deterministic): for each \\(s \\in \\mathcal{S}\\), \\[ \\pi_t^\\star(s) \\in \\arg\\max_{a \\in \\mathcal{A}} \\left\\{ R(s,a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s,a)} \\big[ V_{t+1}^\\star(s&#39;) \\big] \\right\\}. \\] Exercise 1.1 How does dynamic programming look like when applied to the action-value function? Exercise 1.2 What is the computational complexity of dynamic programming? Let us try dynamic programming for the Hangover MDP presented before. Example 1.2 (Dynamic Programming for Hangover MDP) Consider the Hangover MDP defined by the transition graph shown in Fig. 1.2. With slight modification to the policy evaluation code, we can find the optimal value functions and optimal policies. # Dynamic programming (finite-horizon optimal control) for the Hangover MDP from collections import defaultdict from typing import Dict, List, Tuple State = str Action = str # --- MDP spec --------------------------------------------------------------- S: List[State] = [ &quot;Hangover&quot;, &quot;Sleep&quot;, &quot;More Sleep&quot;, &quot;Visit Lecture&quot;, &quot;Study&quot;, &quot;Pass Exam&quot; ] A: List[Action] = [&quot;Lazy&quot;, &quot;Productive&quot;] # P[s, a] -&gt; list of (s_next, prob) P: Dict[Tuple[State, Action], List[Tuple[State, float]]] = { # Hangover (&quot;Hangover&quot;, &quot;Lazy&quot;): [(&quot;Sleep&quot;, 1.0)], (&quot;Hangover&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.3), (&quot;Hangover&quot;, 0.7)], # Sleep (&quot;Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Sleep&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.6), (&quot;More Sleep&quot;, 0.4)], # More Sleep (&quot;More Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;More Sleep&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 0.5), (&quot;More Sleep&quot;, 0.5)], # Visit Lecture (&quot;Visit Lecture&quot;, &quot;Lazy&quot;): [(&quot;Study&quot;, 0.8), (&quot;Pass Exam&quot;, 0.2)], (&quot;Visit Lecture&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 1.0)], # Study (&quot;Study&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Study&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 0.9), (&quot;Study&quot;, 0.1)], # Pass Exam (absorbing) (&quot;Pass Exam&quot;, &quot;Lazy&quot;): [(&quot;Pass Exam&quot;, 1.0)], (&quot;Pass Exam&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 1.0)], } def R(s: State, a: Action) -&gt; float: &quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot; return 1.0 if s == &quot;Pass Exam&quot; else -1.0 # --- Dynamic programming (Bellman optimality) ------------------------------- def dynamic_programming(T: int): &quot;&quot;&quot; Compute optimal finite-horizon tables: - V[t][s] = V_t^*(s) - Q[t][(s,a)] = Q_t^*(s,a) - PI[t][s] = optimal action at (t,s) with terminal condition V_T^* = 0. &quot;&quot;&quot; assert T &gt;= 0 # sanity: probabilities sum to 1 for each (s,a) for key, rows in P.items(): total = sum(p for _, p in rows) if abs(total - 1.0) &gt; 1e-9: raise ValueError(f&quot;Probabilities for {key} sum to {total}, not 1.&quot;) V: Dict[int, Dict[State, float]] = defaultdict(dict) Q: Dict[int, Dict[Tuple[State, Action], float]] = defaultdict(dict) PI: Dict[int, Dict[State, Action]] = defaultdict(dict) # Terminal boundary for s in S: V[T][s] = 0.0 for a in A: Q[T][(s, a)] = 0.0 # Backward recursion (Bellman optimality) for t in range(T - 1, -1, -1): for s in S: # compute Q*_t(s,a) for a in A: exp_next = sum(p * V[t + 1][s_next] for s_next, p in P[(s, a)]) Q[t][(s, a)] = R(s, a) + exp_next # greedy action and optimal value # tie-breaking is deterministic by the order in A best_a = max(A, key=lambda a: Q[t][(s, a)]) PI[t][s] = best_a V[t][s] = Q[t][(s, best_a)] return V, Q, PI # --- Example run ------------------------------------------------------------- if __name__ == &quot;__main__&quot;: T = 10 # horizon V, Q, PI = dynamic_programming(T=T) print(f&quot;Optimal V_0(s) with T={T}:&quot;) for s in S: print(f&quot; {s:13s}: {V[0][s]: .3f}&quot;) print(&quot;\\nGreedy policy at t=0:&quot;) for s in S: print(f&quot; {s:13s}: {PI[0][s]}&quot;) print(&quot;\\nAction value at t=0:&quot;) for s in S: print(f&quot; {s:13s}: {Q[0][s, A[0]]: .3f}, {Q[0][s, A[1]]: .3f}&quot;) The optimal value function at \\(t=0\\) is: \\[\\begin{equation} V^\\star_0 = \\begin{bmatrix} 1.259 \\\\ 3.251 \\\\ 3.787 \\\\ 6.222 \\\\ 7.778 \\\\ 10 \\end{bmatrix}. \\tag{1.16} \\end{equation}\\] Clearly, the optimal value function dominates the value function shown in (1.8) of the random policy at every state. The optimal actions at \\(t=0\\) are: \\[\\begin{equation} \\begin{split} \\text{Hangover} &amp; : \\text{Lazy} \\\\ \\text{Sleep} &amp; : \\text{Productive} \\\\ \\text{More Sleep} &amp; : \\text{Productive} \\\\ \\text{Visit Lecture} &amp; : \\text{Lazy} \\\\ \\text{Study} &amp; : \\text{Productive} \\\\ \\text{Pass Exam} &amp; : \\text{Lazy} \\end{split}. \\end{equation}\\] You can play with the code here. 1.2 Infinite-Horizon MDP In a finite-horizon MDP, the horizon \\(T\\) must be specified in advance in order to carry out policy evaluation and dynamic programming. The finite horizon naturally provides a terminal condition, which serves as the boundary condition that allows backward recursion to proceed. In many practical applications, however, the horizon \\(T\\) is not well defined or is difficult to determine. In such cases, it is often more natural and convenient to adopt the infinite-horizon MDP formulation. An infinite-horizon MDP is given by the following tuple: \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma), \\] where \\(\\mathcal{S}\\), \\(\\mathcal{A}\\), \\(P\\), and \\(R\\) are the same as defined before in a finite-horizon MDP. We still restrict ourselves to the tabular MDP setup where \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) both have a finite number of elements. The key difference between the finite-horizon and infinite-horizon formulations is that the fixed horizon \\(T\\) is replaced by a discount factor \\(\\gamma \\in [0,1)\\). This discount factor weights future rewards less heavily than immediate rewards, as we will see shortly. Stationary Policy. In an infinite-horizon MDP, we focus on stationary policies \\(\\pi: \\mathcal{S} \\mapsto \\Delta(\\mathcal{A})\\), where \\(\\pi(a \\mid s)\\) denotes the probability of taking action \\(a\\) in state \\(s\\). In contrast, in a finite-horizon MDP we considered a tuple of \\(T\\) policies (see (1.1)), where each \\(\\pi_t\\) could vary with time (i.e., policies were non-stationary). Intuitively, in the infinite-horizon setting, it suffices to consider stationary policies because the decision-making problem at time \\(t\\) is equivalent to the problem at time \\(t + k\\) for any \\(k \\in \\mathbb{N}\\), as both face the same infinite horizon. Trajectory and Return. Given an initial state \\(s_0 \\in \\mathcal{S}\\) and a stationary policy \\(\\pi\\), the MDP will evolve as Start at state \\(s_0\\) Take action \\(a_0 \\sim \\pi(\\cdot \\mid s_0)\\) following policy \\(\\pi\\) Collect reward \\(r_0 = R(s_0, a_0)\\) Transition to state \\(s_1 \\sim P(s&#39; \\mid s_0, a_0)\\) following the dynamics Go to step 2 and continue forever This process generates a trajectory of states, actions, and rewards: \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots). \\] The return of a trajectory is defined as \\[ g_0 = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\dots = \\sum_{t=0}^{\\infty} \\gamma^t r_t. \\] Here, the discount factor \\(\\gamma\\) plays a key role: it progressively reduces the weight of rewards received further in the future, making them less influential as \\(t\\) increases. 1.2.1 Value Functions Similar to the case of finite-horizon MDP, we can define the state-value function and the action-value function associated with a policy \\(\\pi\\). State-Value Function. The value of a state \\(s \\in \\mathcal{S}\\) under policy \\(\\pi\\) is the expected discounted return obtained when starting from \\(s\\) at time \\(0\\): \\[\\begin{equation} V^{\\pi}(s) := \\mathbb{E}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\,\\middle|\\, s_0 = s, a_t \\sim \\pi(\\cdot \\mid s_t), s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\right]. \\tag{1.17} \\end{equation}\\] Action-Value Function. The value of a state-action pair \\((s,a) \\in \\mathcal{S} \\times \\mathcal{A}\\) under policy \\(\\pi\\) is the expected discounted return obtained by first taking action \\(a\\) in state \\(s\\), and then following policy \\(\\pi\\) thereafter: \\[\\begin{equation} Q^{\\pi}(s,a) := \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\,\\middle|\\, s_0 = s, a_0 = a, a_t \\sim \\pi(\\cdot \\mid s_t), s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\right]. \\tag{1.18} \\end{equation}\\] Note that a nice feature of having a discount factor \\(\\gamma \\in [0,1)\\) is that both the state-value and the action-value functions are guaranteed to be bounded even if the horizon is unbounded (assuming the reward function is bounded). We can verify the state-value function and the action value function satisfy the following relationship: \\[\\begin{align} V^{\\pi}(s) &amp;= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) Q^{\\pi}(s,a) \\tag{1.19}\\\\ Q^{\\pi}(s,a) &amp; = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}(s&#39;). \\tag{1.20} \\end{align}\\] Combining these two equations, we arrive at the Bellman consistency result for infinite-horizon MDP. Proposition 1.2 (Bellman Consistency (Infinite Horizon)) The state-value function \\(V^{\\pi}\\) in (1.17) satisfies the following recursion: \\[\\begin{equation} \\begin{split} V^{\\pi} (s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi (a\\mid s) \\left( R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi} (s&#39;) \\right) \\\\ &amp; =: \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} \\left[ R(s, a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} [V^{\\pi}(s&#39;)] \\right]. \\end{split} \\tag{1.21} \\end{equation}\\] Similarly, the action-value function \\(Q^{\\pi}(s,a)\\) in (1.18) satisfies the following recursion: \\[\\begin{equation} \\begin{split} Q^{\\pi} (s, a) &amp; = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) \\left( \\sum_{a&#39; \\in \\mathcal{A}} \\pi(a&#39; \\mid s&#39;) Q^{\\pi}(s&#39;, a&#39;)\\right) \\\\ &amp; =: R(s, a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[\\mathbb{E}_{a&#39; \\sim \\pi(\\cdot \\mid s&#39;)} [Q^{\\pi}(s&#39;, a&#39;)] \\right]. \\end{split} \\tag{1.22} \\end{equation}\\] 1.2.2 Policy Evaluation Given a policy \\(\\pi\\), how can we compute its associated state-value and action-value functions? Finite-horizon case. We initialize the terminal value function \\(V_T^{\\pi}(s) = 0\\) for every \\(s \\in \\mathcal{S}\\), and then apply the Bellman Consistency result (Proposition 1.1) to perform backward recursion. Infinite-horizon case. The Bellman Consistency result (Proposition 1.2) takes a different form and does not provide the same simple recipe for backward recursion. System of Linear Equations. A closer look at the Bellman Consistency equation (1.21) for the state-value function shows that it defines a square system of linear equations. Specifically, the value function \\(V^{\\pi}\\) can be represented as a vector with \\(|\\mathcal{S}|\\) variables, and (1.21) provides \\(|\\mathcal{S}|\\) linear equations over these variables. Thus, one way to compute the state-value function is to set up this linear system and solve it. However, doing so typically requires matrix inversion or factorization, which can be computationally expensive. The same reasoning applies to the action-value function \\(Q^{\\pi}\\), which can be represented as a vector of \\(|\\mathcal{S}||\\mathcal{A}|\\) variables constrained by \\(|\\mathcal{S}||\\mathcal{A}|\\) linear equations. The following proposition states that, instead of solving a linear system of equations, one can use a globally convergent iterative scheme, one that is very much like the policy evaluation algorithm for the finite-horizon MDP, to evaluate the state-value function associated with a policy \\(\\pi\\). Proposition 1.3 (Policy Evaluation (Infinite Horizon, State-Value)) Consider an infinite-horizon MDP \\(\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,\\gamma)\\). Fix a policy \\(\\pi\\) and consider the iterative scheme for the state-value function: \\[\\begin{equation} V_{k+1}(s) \\;\\; \\gets \\;\\; \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a) V_k(s&#39;) \\right], \\quad \\forall s \\in \\mathcal{S}. \\tag{1.23} \\end{equation}\\] Then, starting from any initialization \\(V_0 \\in \\mathbb{R}^{|\\mathcal{S}|}\\), the sequence \\(\\{V_k\\}\\) converges to the unique fixed point \\(V^{\\pi}\\), the state-value function associated with policy \\(\\pi\\). Proof. To prove the convergence of the policy evaluation algorithm, we shall introduce the notion of a Bellman operator. Bellman Operator. Any value function \\(V(s)\\) can be interpreted as a vector in \\(\\mathbb{R}^{|\\mathcal{S}|}\\) (recall we are in the tabular MDP case). Given any value function \\(V \\in \\mathbb{R}^{|\\mathcal{S}|}\\), and a policy \\(\\pi\\), define the Bellman operator associated with \\(\\pi\\) as \\(T^{\\pi}: \\mathbb{R}^{|\\mathcal{S}|} \\mapsto \\mathbb{R}^{|\\mathcal{S}|}\\): \\[\\begin{equation} (T^{\\pi} V)(s) := \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a) V(s&#39;) \\right]. \\tag{1.24} \\end{equation}\\] We claim that \\(T^{\\pi}\\) has two important properties. Monotonicity. If \\(V \\leq W\\) (i.e., \\(V(s) \\leq W(s)\\) for any \\(s \\in \\mathcal{S}\\)), then \\(T^{\\pi} V \\leq T^{\\pi}W\\). To see this, observe that \\[\\begin{align*} (T^{\\pi}V)(s) - (T^\\pi W)(s) &amp;= \\sum_{a} \\pi(a \\mid s) \\left(\\gamma \\sum_{s&#39;} P(s&#39; \\mid s, a) (V(s&#39;) - W(s&#39;)) \\right) \\\\ &amp; = \\gamma \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s), s&#39; \\sim P(\\cdot \\mid s,a)}[V(s&#39;) - W(s&#39;)]. \\end{align*}\\] Therefore, if \\(V(s&#39;) - W(s&#39;) \\leq 0\\) for any \\(s&#39; \\in \\mathcal{S}\\), then \\(T^{\\pi}V \\leq T^{\\pi} W\\). \\(\\gamma\\)-Contraction. For any value function \\(V \\in \\mathbb{R}^{|\\mathcal{S}|}\\), define the \\(\\ell_{\\infty}\\) norm (sup norm) as \\[ \\Vert V \\Vert_{\\infty} = \\max_{s \\in \\mathcal{S}} |V(s)|. \\] We claim that the Bellman operator \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction in the sup norm, i.e., \\[\\begin{equation} \\Vert T^\\pi V - T^\\pi W \\Vert_{\\infty} \\leq \\gamma \\Vert V - W \\Vert_{\\infty}, \\quad \\forall V, W \\in \\mathbb{R}^{|\\mathcal{S}|}. \\tag{1.25} \\end{equation}\\] To prove this, observe that for any \\(s \\in \\mathcal{S}\\), we have: \\[\\begin{align*} |(T^\\pi V)(s) - (T^\\pi W)(s)| &amp;= \\left| \\sum_a \\pi(a|s)\\,\\gamma \\sum_{s&#39;} P(s&#39;|s,a)\\big(V(s&#39;) - W(s&#39;)\\big) \\right| \\\\ &amp;\\le \\gamma \\sum_a \\pi(a|s)\\sum_{s&#39;} P(s&#39;|s,a)\\,|V(s&#39;) - W(s&#39;)| \\\\ &amp;\\le \\gamma \\|V - W\\|_\\infty \\sum_a \\pi(a|s)\\sum_{s&#39;} P(s&#39;|s,a) \\\\ &amp;= \\gamma \\|V - W\\|_\\infty. \\end{align*}\\] Taking the maximum over \\(s\\) gives \\[ \\|T^\\pi V - T^\\pi W\\|_\\infty \\le \\gamma \\|V - W\\|_\\infty, \\] so \\(T^\\pi\\) is a \\(\\gamma\\)-contraction in the sup norm. With the Bellman operator defined, we observe that the value function of \\(\\pi\\), denoted \\(V^{\\pi}\\) in (1.21), is a fixed point of \\(T^{\\pi}\\). That is to say \\(V^{\\pi}\\) satisfies: \\[ T^{\\pi} V^{\\pi} = V^{\\pi}. \\] In other words, \\(V^{\\pi}\\) is fixed (remains unchanged) under the Bellman operator. Since \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction, by the Banach Fixed-Point Theorem, we know that there exists a unique fixed point to \\(T^{\\pi}\\), which is \\(V^{\\pi}\\). Moreover, since \\[ \\Vert V_{k} - V^{\\pi} \\Vert_{\\infty} = \\Vert T^{\\pi} V_{k-1} - T^{\\pi} V^{\\pi} \\Vert_{\\infty} \\leq \\gamma \\Vert V_{k-1} - V^{\\pi} \\Vert_{\\infty}, \\] we can deduce the rate of convergence \\[ \\Vert V_{k} - V^{\\pi} \\Vert_{\\infty} \\leq \\gamma^{k} \\Vert V_0 - V^{\\pi} \\Vert_{\\infty}. \\] Therefore, policy evaluation globally converges from any initialization \\(V_0\\) at a linear rate of \\(\\gamma\\). We have a similar policy evaluation algorithm for the action-value function. Proposition 1.4 (Policy Evaluation (Infinite Horizon, Action-Value)) Fix a policy \\(\\pi\\). Consider the iterative scheme on \\(Q:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}\\): \\[\\begin{equation} \\begin{split} Q_{k+1}(s,a) \\;\\gets\\; R(s,a) \\;+\\; \\gamma \\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\!\\left(\\sum_{a&#39;\\in\\mathcal{A}} \\pi(a&#39;\\mid s&#39;)\\, Q_k(s&#39;,a&#39;)\\right), \\\\ \\forall (s,a)\\in\\mathcal{S}\\times\\mathcal{A}. \\end{split} \\tag{1.26} \\end{equation}\\] Then, for any initialization \\(Q_0\\), the sequence \\(\\{Q_k\\}\\) converges to the unique fixed point \\(Q^{\\pi}\\), the action-value function associated with policy \\(\\pi\\). Proof. Define the Bellman operator on action-values \\[ (T^{\\pi}Q)(s,a) := R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\Big(\\sum_{a&#39;} \\pi(a&#39;\\mid s&#39;)\\, Q(s&#39;,a&#39;)\\Big). \\] \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction in the sup-norm on \\(\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}\\); hence by the Banach fixed-point theorem, global convergence holds regardless of initialization. Let us apply policy evaluation to an infinite-horizon MDP. Example 1.3 (Policy Evaluation for Inverted Pendulum) Figure 1.3: Inverted Pendulum. We consider the inverted pendulum with state \\(s=(\\theta, \\dot\\theta)\\) and action (torque) \\(a = u\\), as visualized in Fig. 1.3. Our goal is to swing up the pendulum from any initial state to the upright position \\(s = (0,0)\\). Continuous-Time Dynamics. The continuous-time dynamics of the inverted pendulum is \\[ \\ddot{\\theta} \\;=\\; \\frac{g}{l}\\sin(\\theta) \\;+\\; \\frac{1}{ml^2}u \\;-\\; c\\,\\dot{\\theta}, \\] where \\(m &gt; 0\\) is the mass of the pendulum, \\(l &gt; 0\\) is the length of the pole, \\(c &gt; 0\\) is the damping coefficient, and \\(g\\) is the gravitational constant. Discretization (Euler). With timestep \\(\\Delta t\\), we obtain the following discrete-time dynamics: \\[\\begin{equation} \\begin{split} \\theta_{k+1} &amp;= \\theta_k + \\Delta t \\, \\dot{\\theta}_k, \\\\ \\dot{\\theta}_{k+1} &amp;= \\dot{\\theta}_k + \\Delta t \\Big(\\tfrac{g}{l}\\sin(\\theta_k) + \\tfrac{1}{ml^2}u_k - c\\,\\dot{\\theta}_k\\Big). \\end{split} \\tag{1.27} \\end{equation}\\] We wrap angles to \\([-\\pi,\\pi]\\) via \\(\\operatorname{wrap}(\\theta)=\\mathrm{atan2}(\\sin\\theta,\\cos\\theta)\\). Tabular MDP. We convert the discrete-time dynamics into a tabular MDP. State grid. \\(\\theta \\in [-\\pi,\\pi]\\), \\(\\dot\\theta \\in [-\\pi,\\pi]\\) on uniform grids: \\[ \\mathcal{S}=\\{\\;(\\theta_i,\\dot\\theta_j)\\;:\\; i=1,\\dots,N_\\theta,\\; j=1, \\dots, N_{\\dot\\theta}\\;\\}. \\] Action grid. \\(u \\in [-mgl/2, mgl/2]\\) on \\(N_u\\) uniform points: \\[ \\mathcal{A}=\\{u_\\ell:\\ell=1,\\dots,N_u\\}. \\] Stochastic transition kernel (nearest-3 interpolation). From a grid point \\(s=(\\theta_i,\\dot\\theta_j)\\) and an action \\(u_\\ell\\), compute the next continuous state \\(s^+ = (\\theta^+,\\dot\\theta^+)\\) via the discrete-time dynamics in (1.27). If \\(s^+\\notin\\mathcal{S}\\), choose the three closest grid states \\(\\{s^{(1)},s^{(2)},s^{(3)}\\}\\) by Euclidean distance in \\((\\theta,\\dot\\theta)\\) and assign probabilities \\[ p_r \\propto \\frac{1}{\\|s^+ - s^{(r)}\\|_2 + \\varepsilon},\\quad r=1,2,3, \\qquad \\sum_r p_r=1, \\] so nearer grid points receive higher probability (use a small \\(\\varepsilon&gt;0\\) to avoid division by zero). Reward. A quadratic shaping penalty around the upright equilibrium: \\[ R(s,a) = -\\Big(\\theta^2 + 0.1\\,\\dot\\theta^2 + 0.01\\,u^2\\Big). \\] Discount. \\(\\gamma \\in [0,1)\\). We obtain a discounted, infinite-horizon, tabular MDP. Policy. For policy evaluation, consider \\(\\pi(a\\mid s)\\) be uniform over the discretized actions, i.e., a random policy. Policy Evaluation. The following python script performs policy evaluation. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 41 N_thetadot = 41 N_u = 21 theta_grid = np.linspace(-np.pi, np.pi, N_theta) thetadot_grid = np.linspace(-np.pi, np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- # Pre-compute all grid points for fast nearest neighbor search grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) # sort those 3 by distance for stability nn_idx = nn_idx[np.argsort(dists[nn_idx])] d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): # reward at current (s,a) R[s, a] = reward(th, thd, u) # next continuous state th_n, thd_n = step_euler(th, thd, u) # map to 3 nearest grid states nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ----- Fixed policy: uniform over actions ----- Pi = np.full((S, A), 1.0 / A) # ----- Iterative policy evaluation ----- V = np.zeros(S) # initialization (any vector works) tol = 1e-6 max_iters = 10000 for k in range(max_iters): V_new = np.zeros_like(V) # Compute Bellman update: V_{k+1}(s) = sum_a Pi(s,a)[ R(s,a) + gamma * sum_j P(s,a,j) V_k(ns_j) ] # First, expected next V for each (s,a) EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # shape: (S, A) # Then expectation over actions under Pi V_new = (Pi * (R + gamma * EV_next)).sum(axis=1) # shape: (S,) # Check convergence if np.max(np.abs(V_new - V)) &lt; tol: V = V_new print(f&quot;Converged in {k+1} iterations (sup-norm change &lt; {tol}).&quot;) break V = V_new else: print(f&quot;Reached max_iters={max_iters} without meeting tolerance {tol}.&quot;) V_grid = V.reshape(N_theta, N_thetadot) # V_grid: shape (N_theta, N_thetadot) # theta_grid, thetadot_grid already defined fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; # any matplotlib colormap, e.g., &quot;plasma&quot;, &quot;inferno&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^\\pi(\\theta,\\dot{\\theta})$&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V^\\pi$ (tabular policy evaluation)&quot;) plt.tight_layout() plt.show() Running the code, it shows that policy evaluation converges in 518 iterations under tolerance \\(10^{-6}\\). Fig. 1.4 plots the value function over the state grid. Figure 1.4: Value Function from Policy Evaluation. You can play with the code here. 1.2.3 Principle of Optimality In an infinite-horizon MDP, our goal is to find the optimal policy that maximizes the expected long-term discounted return: \\[ V^\\star := \\max_{\\pi} \\mathbb{E}_{s \\sim \\mu(\\cdot)} [V^\\pi(s)], \\] where \\(\\mu\\) is a given initial distribution. We call \\(V^\\star\\) the optimal value function. Given a policy \\(\\pi\\) and its associated value function \\(V^\\pi\\), how do we know if the policy is already optimal? Theorem 1.2 (Bellman Optimality (Infinite Horizon)) For an infinite-horizon MDP with discount factor \\(\\gamma \\in [0,1)\\), the optimal state-value function \\(V^\\star(s)\\) satisfies the Bellman optimality equation \\[\\begin{equation} V^\\star(s) \\;=\\; \\max_{a \\in \\mathcal{A}} \\Big[\\, R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a)\\, V^\\star(s&#39;) \\,\\Big]. \\tag{1.28} \\end{equation}\\] Define the optimal action-value function as \\[\\begin{equation} Q^\\star(s,a) = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^\\star(s&#39;). \\tag{1.29} \\end{equation}\\] We have that \\(Q^\\star(s,a)\\) satisfies \\[\\begin{equation} Q^\\star(s,a) \\;=\\; R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a)\\, \\left[\\max_{a&#39; \\in \\mathcal{A}} Q^\\star(s&#39;,a&#39;) \\right]. \\tag{1.30} \\end{equation}\\] Moreover, any greedy policy with respect to \\(V^\\star\\) (equivalently, to \\(Q^\\star\\)) is optimal: \\[\\begin{equation} \\begin{split} \\pi^\\star(s) &amp; \\in \\arg\\max_{a \\in \\mathcal{A}} \\Big[\\, R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39; \\mid s,a)\\, V^\\star(s&#39;) \\,\\Big] \\quad\\Longleftrightarrow\\quad \\\\ \\pi^\\star(s) &amp; \\in \\arg\\max_{a \\in \\mathcal{A}} Q^\\star(s,a). \\end{split} \\tag{1.31} \\end{equation}\\] Proof. We will first show that \\(V^\\star\\) has statewise dominance over all other policies, and then show that \\(V^\\star\\) can be attained by the greedy policy. Claim. For any discounted MDP with \\(\\gamma \\in [0,1)\\) and any policy \\(\\pi\\), \\[ V^\\star(s) \\;\\ge\\; V^{\\pi}(s)\\qquad \\forall s\\in\\mathcal{S}, \\] where \\(V^\\star\\) is the unique solution of the Bellman optimality equation and \\(V^\\pi\\) solves the Bellman consistency equation for \\(\\pi\\). Proof via Bellman Operators. Define the Bellman operators \\[ (T^\\pi V)(s) := \\sum_{a}\\pi(a\\mid s)\\Big[ R(s,a)+\\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)V(s&#39;) \\Big], \\] \\[ (T^\\star V)(s) := \\max_{a}\\Big[ R(s,a)+\\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)V(s&#39;) \\Big]. \\] Key facts: (Monotonicity) If \\(V \\ge W\\) componentwise, then \\(T^\\pi V \\ge T^\\pi W\\) and \\(T^\\star V \\ge T^\\star W\\). (Dominance of \\(T^*\\)) For any \\(V\\) and any \\(\\pi\\), \\[ T^\\star V \\;\\ge\\; T^\\pi V \\] because the max over actions is at least the \\(\\pi\\)-weighted average. (Fixed points) \\(V^\\pi = T^\\pi V^\\pi\\) and \\(V^\\star = T^\\star V^\\star\\). (Contraction) Each \\(T^\\pi\\) and \\(T^\\star\\) is a \\(\\gamma\\)-contraction in the sup-norm; hence their fixed points are unique. Now start from \\(V^\\pi\\). Using (2), \\[ V^\\pi = T^\\pi V^\\pi \\;\\le\\; T^\\star V^\\pi. \\] Applying \\(T^\\star\\) repeatedly and using (1), \\[ V^\\pi \\;\\le\\; T^\\star V^\\pi \\;\\le\\; (T^\\star)^2 V^\\pi \\;\\le\\; \\cdots \\] The sequence \\((T^\\star)^k V^\\pi\\) converges (by contraction) to the unique fixed point of \\(T^\\star\\), namely \\(V^\\star\\). Taking limits preserves the inequality, yielding \\(V^\\pi \\le V^\\star\\) statewise. The Bellman optimality condition tells us, if a policy \\(\\pi\\) is already greedy with respect to its value function \\(V^\\pi\\), then \\(\\pi\\) is the optimal policy and \\(V^\\pi\\) is the optimal value function. In the next, we introduce two algorithms that can guarantee finding the optimal policy and the optimal value function. The first algorithm, policy iteration (PI), iterates over the space of policies; while the second algorithm, value iteration (VI), iterates over the space of value functions. 1.2.4 Policy Improvement The policy evaluation algorithm enables us to compute the value functions associated with a given policy \\(\\pi\\). The next result, known as the Policy Improvement Lemma, shows that once we have \\(V^{\\pi}\\), constructing a greedy policy with respect to \\(V^{\\pi}\\) guarantees performance that is at least as good as \\(\\pi\\), and strictly better in some states unless \\(\\pi\\) is already greedy with respect to \\(V^{\\pi}\\). Lemma 1.1 (Policy Improvement) Let \\(\\pi\\) be any policy and let \\(V^{\\pi}\\) be its state-value function. Define a new policy \\(\\pi&#39;\\) such that for each state \\(s\\), \\[ \\pi&#39;(s) \\in \\arg\\max_{a \\in \\mathcal{A}} \\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39; \\mid s,a) V^{\\pi}(s&#39;) \\Big]. \\] Then for all states \\(s \\in \\mathcal{S}\\), \\[ V^{\\pi&#39;}(s) \\;\\ge\\; V^{\\pi}(s). \\] Moreover, the inequality is strict for some state \\(s\\) unless \\(\\pi\\) is already greedy with respect to \\(V^\\pi\\) (which implies optimality). Proof. Let \\(V^{\\pi}\\) be the value function of a policy \\(\\pi\\), and define a new (possibly stochastic) policy \\(\\pi&#39;\\) that is greedy w.r.t. \\(V^{\\pi}\\): \\[ \\pi&#39;(\\cdot \\mid s) \\in \\arg\\max_{\\mu \\in \\Delta(\\mathcal{A})} \\sum_{a}\\mu(a)\\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V^{\\pi}(s&#39;)\\Big]. \\] Define the Bellman operators \\[\\begin{align*} (T^{\\pi}V)(s) &amp; := \\sum_a \\pi(a\\mid s)\\Big[R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;\\mid s,a)V(s&#39;)\\Big],\\\\ (T^{\\pi&#39;}V)(s) &amp; := \\sum_a \\pi&#39;(a\\mid s)\\Big[\\cdots\\Big]. \\end{align*}\\] Step 1: One-step improvement at \\(V^{\\pi}\\). By greediness of \\(\\pi&#39;\\) w.r.t. \\(V^{\\pi}\\), \\[ (T^{\\pi&#39;} V^{\\pi})(s) = \\max_{\\mu}\\sum_a \\mu(a)\\Big[R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;\\mid s,a)V^{\\pi}(s&#39;)\\Big] \\;\\;\\ge\\;\\; (T^{\\pi} V^{\\pi})(s) = V^{\\pi}(s), \\] for all \\(s\\). Hence \\[\\begin{equation} T^{\\pi&#39;} V^{\\pi} \\;\\ge\\; V^{\\pi}\\quad\\text{(componentwise).} \\tag{1.32} \\end{equation}\\] Step 2: Monotonicity + contraction yield global improvement. The operator \\(T^{\\pi&#39;}\\) is monotone (order-preserving) and a \\(\\gamma\\)-contraction in the sup-norm. Apply \\(T^{\\pi&#39;}\\) repeatedly to both sides of (1.32): \\[ (T^{\\pi&#39;})^k V^{\\pi} \\;\\ge\\; (T^{\\pi&#39;})^{k-1} V^{\\pi} \\;\\ge\\; \\cdots \\;\\ge\\; V^{\\pi},\\qquad k=1,2,\\dots \\] By contraction, \\((T^{\\pi&#39;})^k V^{\\pi} \\to V^{\\pi&#39;}\\), the unique fixed point of \\(T^{\\pi&#39;}\\). Taking limits preserves the inequality, so \\[ V^{\\pi&#39;} \\;\\ge\\; V^{\\pi}\\quad\\text{statewise.} \\] Strict improvement condition. If there exists a state \\(s\\) such that \\[ (T^{\\pi&#39;} V^{\\pi})(s) \\;&gt;\\; V^{\\pi}(s), \\] then by monotonicity we have a strict increase at that state after one iteration, and the limit remains strictly larger at that state (or at any state that can reach it with positive probability under \\(\\pi&#39;\\)). This happens precisely when \\(\\pi&#39;\\) selects, with positive probability, an action \\(a\\) for which \\[ Q^{\\pi}(s,a)=R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V^{\\pi}(s&#39;) \\;&gt;\\; V^{\\pi}(s), \\] i.e., when \\(\\pi\\) was not already greedy (optimal) at \\(s\\). 1.2.5 Policy Iteration The policy improvement lemma and the principle of optimality, combined together, leads to the first algorithm that guarantees convergence to an optimal policy. This algorithm is called policy iteration. Theorem 1.3 (Convergence of Policy Iteration) Consider a discounted MDP with finite state and action sets and \\(\\gamma\\in[0,1)\\). Let \\(\\{\\pi_k\\}_{k\\ge0}\\) be the sequence produced by Policy Iteration (PI): Policy evaluation: compute \\(V^{\\pi_k}\\) such that \\(V^{\\pi_k}=T^{\\pi_k}V^{\\pi_k}\\). Policy improvement: choose \\(\\pi_{k+1}\\) greedy w.r.t. \\(V^{\\pi_k}\\): \\[ \\pi_{k+1}(s) \\in \\arg\\max_{a}\\Big[ R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;|s,a)\\,V^{\\pi_k}(s&#39;)\\Big]. \\] Then: \\(V^{\\pi_{k+1}} \\ge V^{\\pi_k}\\) componentwise, and the inequality is strict for some state unless \\(\\pi_{k+1}=\\pi_k\\). If \\(\\pi_{k+1}=\\pi_k\\), then \\(V^{\\pi_k}\\) satisfies the Bellman optimality equation; hence \\(\\pi_k\\) is optimal and \\(V^{\\pi_k}=V^*\\). Because the number of stationary policies is finite, PI terminates in finitely many iterations at an optimal policy \\(\\pi^*\\) with value \\(V^*\\). \\(\\Vert V^{\\pi_{k+1}} - V^\\star \\Vert_{\\infty} \\leq \\gamma \\Vert V^{\\pi_k} - V^\\star \\Vert_{\\infty}\\), for any \\(k\\) (i.e., contraction). Proof. By the policy improvement lemma, we have \\[ V^{\\pi_{k+1}} \\geq V^{\\pi_k}. \\] By monotonicity of the Bellman operator \\(T^{\\pi_{k+1}}\\), we have \\[ V^{\\pi_{k+1}} = T^{\\pi_{k+1}} V^{\\pi_{k+1}} \\geq T^{\\pi_{k+1}} V^{\\pi_k}. \\] By definition of the Bellman optimality operator, we have \\[ T^{\\pi_{k+1}} V^{\\pi_k} = T^\\star V^{\\pi_k}. \\] Therefore, \\[ 0 \\geq V^{\\pi_{k+1}} - V^\\star \\geq T^{\\pi_{k+1}} V^{\\pi_k} - V^\\star = T^\\star V^{\\pi_k} - T^\\star V^\\star \\] As a result, \\[ \\Vert V^{\\pi_{k+1}} - V^\\star \\Vert_{\\infty} \\leq \\Vert T^\\star V^{\\pi_k} - T^\\star V^\\star \\Vert_{\\infty} \\leq \\gamma \\Vert V^{\\pi_k} - V^\\star \\Vert_{\\infty}. \\] This proves the contraction result (d). Let us apply Policy Iteration to the inverted pendulum problem. Example 1.4 (Policy Iteration for Inverted Pendulum) The following code performs policy iteration for the inverted pendulum problem. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 101 N_thetadot = 101 N_u = 51 theta_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_theta) thetadot_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) nn_idx = nn_idx[np.argsort(dists[nn_idx])] # sort those 3 by distance d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): # reward at current (s,a) R[s, a] = reward(th, thd, u) # next continuous state th_n, thd_n = step_euler(th, thd, u) # map to 3 nearest grid states nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ======================= # POLICY ITERATION # ======================= # Represent policy as a deterministic action index per state: pi[s] in {0..A-1} # Start from uniform-random policy (deterministic tie-breaker: middle action) pi = np.full(S, A // 2, dtype=int) def policy_evaluation(pi, V_init=None, tol=1e-6, max_iters=10000): &quot;&quot;&quot;Iterative policy evaluation for deterministic pi (action index per state).&quot;&quot;&quot; V = np.zeros(S) if V_init is None else V_init.copy() for k in range(max_iters): # For each state s, use chosen action a = pi[s] a = pi # shape (S,) # Expected next value under chosen action EV_next = (NS_prob[np.arange(S), a] * V[NS_idx[np.arange(S), a]]).sum(axis=1) # (S,) V_new = R[np.arange(S), a] + gamma * EV_next if np.max(np.abs(V_new - V)) &lt; tol: # print(f&quot;Policy evaluation converged in {k+1} iterations.&quot;) return V_new V = V_new # print(&quot;Policy evaluation reached max_iters without meeting tolerance.&quot;) return V def policy_improvement(V, pi_old=None): &quot;&quot;&quot;Greedy improvement: pi&#39;(s) = argmax_a [ R(s,a) + gamma * E[V(s&#39;)] ].&quot;&quot;&quot; # Compute Q(s,a) = R + gamma * sum_j P(s,a,j) V(ns_j) EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # (S, A) Q = R + gamma * EV_next # (S, A) pi_new = np.argmax(Q, axis=1).astype(int) # greedy deterministic policy stable = (pi_old is not None) and np.array_equal(pi_new, pi_old) return pi_new, stable # Main PI loop max_pi_iters = 100 V = np.zeros(S) for it in range(max_pi_iters): # Policy evaluation V = policy_evaluation(pi, V_init=V, tol=1e-6, max_iters=10000) # Policy improvement pi_new, stable = policy_improvement(V, pi_old=pi) print(f&quot;[PI] Iter {it+1}: policy changed = {not stable}&quot;) pi = pi_new if stable: print(&quot;Policy iteration converged: policy stable.&quot;) break else: print(&quot;Reached max_pi_iters without policy stability (may still be near-optimal).&quot;) # ----- Visualization ----- V_grid = V.reshape(N_theta, N_thetadot) fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^{\\pi}(\\theta,\\dot{\\theta})$ (final PI)&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V$ after Policy Iteration&quot;) plt.tight_layout() plt.show() # Visualize the greedy action *value* (torque) pi_grid = pi.reshape(N_theta, N_thetadot) # action indices action_values = u_grid[pi_grid] # map indices -&gt; torques plt.figure(figsize=(7,5), dpi=120) im = plt.imshow(action_values, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;coolwarm&quot;) # diverging colormap good for ± torque cbar = plt.colorbar(im) cbar.set_label(&quot;Greedy action value (torque)&quot;) plt.xlabel(r&quot;$\\dot{\\theta}$&quot;) plt.ylabel(r&quot;$\\theta$&quot;) plt.title(&quot;Greedy policy (torque) after PI&quot;) plt.tight_layout() plt.show() Running the code produces the optimal value function shown in Fig. 1.5 and the optimal policy shown in Fig. 1.6. Figure 1.5: Optimal Value Function after Policy Iteration Figure 1.6: Optimal Policy after Policy Iteration We can apply the optimal policy to the pendulum with an initial state of \\((-\\pi, 0)\\) (i.e., the bottomright position). Fig. 1.7 plots the rollout trajectory of \\(\\theta, \\dot{\\theta}, u\\). We can see that the optimal policy is capable of performing “bang-bang” control to accumulate energy before swinging up. Fig. 1.8 overlays the trajectory on top of the optimal value function. You can play with the code here. Figure 1.7: Optimal Trajectory of Pendulum Swing-Up Figure 1.8: Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function 1.2.6 Value Iteration Policy iteration—as the name suggests—iterates on policies: it alternates between (1) policy evaluation (computing \\(V^{\\pi}\\) for the current policy \\(\\pi\\)) and (2) policy improvement (making \\(\\pi\\) greedy w.r.t. \\(V^{\\pi}\\)). An alternative, often very effective, method is value iteration. Unlike policy iteration, value iteration does not explicitly maintain a policy during its updates; it iterates directly on the value function toward the fixed point of the Bellman optimality* operator. Once the value function has (approximately) converged, the optimal policy is obtained by a single greedy extraction step. Note that intermediate value iterates need not correspond to the value of any actual policy. The value iteration (VI) algorithm works as follows: Initialization. Choose any \\(V_0:\\mathcal{S}\\to\\mathbb{R}\\) (e.g., \\(V_0 \\equiv 0\\)). Iteration. For \\(k=0,1,2,\\dots\\), \\[ V_{k+1}(s) \\;\\leftarrow\\; \\max_{a\\in\\mathcal{A}} \\Big[\\, R(s,a) \\;+\\; \\gamma \\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\; V_k(s&#39;) \\,\\Big], \\quad \\forall s\\in\\mathcal{S}. \\] Stopping rule. Stop when \\(\\lVert V_{k+1}-V_k\\rVert_\\infty \\le \\varepsilon\\) (or any chosen tolerance). Policy extraction (greedy): \\[ \\pi_{k+1}(s) \\in \\arg\\max_{a\\in\\mathcal{A}} \\Big[\\, R(s,a) \\;+\\; \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\; V_{k+1}(s&#39;) \\,\\Big]. \\] The following theorem states the convergence of value iteration. Theorem 1.4 (Convergence of Value Iteration) Let \\(T^\\star\\) be the Bellman optimality operator, \\[ (T^\\star V)(s) := \\max_{a}\\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V(s&#39;) \\Big]. \\] For \\(\\gamma\\in[0,1)\\) and finite \\(\\mathcal{S},\\mathcal{A}\\), \\(T^\\star\\) is a \\(\\gamma\\)-contraction in the sup-norm. Hence, for any \\(V_0\\), \\[ V_k \\;=\\; (T^\\star )^k V_0 \\;\\xrightarrow[k\\to\\infty]{}\\; V^*, \\] the unique fixed point of \\(T^\\star\\). Moreover, the greedy policy \\(\\pi_k\\) extracted from \\(V_k\\) converges to an optimal policy \\(\\pi^\\star\\). In addition, after \\(k\\) iterations, we have \\[ \\lVert V_k - V^* \\rVert_\\infty \\;\\le\\; \\gamma^k \\, \\lVert V_0 - V^* \\rVert_\\infty. \\] Finally, we apply value iteration to the inverted pendulum problem. Example 1.5 (Value Iteration for Inverted Pendulum) The following code performs value iteration for the inverted pendulum problem. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 101 N_thetadot = 101 N_u = 51 theta_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_theta) thetadot_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) nn_idx = nn_idx[np.argsort(dists[nn_idx])] # sort those 3 by distance d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): R[s, a] = reward(th, thd, u) th_n, thd_n = step_euler(th, thd, u) nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ======================= # VALUE ITERATION # ======================= # Bellman optimality update: # V_{k+1}(s) = max_a [ R(s,a) + gamma * sum_j P(s,a,j) * V_k(ns_j) ] V = np.zeros(S) tol = 1e-6 max_vi_iters = 1000 for k in range(max_vi_iters): # Expected next V for every (s,a), given current V_k EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # shape (S, A) Q = R + gamma * EV_next # shape (S, A) V_new = np.max(Q, axis=1) # greedy backup over actions delta = np.max(np.abs(V_new - V)) # Optional: a stopping rule aligned with policy loss bound could scale tol # e.g., stop when delta &lt;= tol * (1 - gamma) / (2 * gamma) if delta &lt; tol: V = V_new print(f&quot;Value Iteration converged in {k+1} iterations (sup-norm change {delta:.2e}).&quot;) break V = V_new else: print(f&quot;Reached max_vi_iters={max_vi_iters} (last sup-norm change {delta:.2e}).&quot;) # Greedy policy extraction from the final V EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # recompute with final V Q = R + gamma * EV_next pi = np.argmax(Q, axis=1) # deterministic greedy policy (indices) # ----- Visualization: Value function ----- V_grid = V.reshape(N_theta, N_thetadot) fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^*(\\theta,\\dot{\\theta})$ (Value Iteration)&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V$ after Value Iteration&quot;) plt.tight_layout() plt.show() # ----- Visualization: Greedy torque field ----- pi_grid = pi.reshape(N_theta, N_thetadot) # action indices action_values = u_grid[pi_grid] # map indices -&gt; torques plt.figure(figsize=(7,5), dpi=120) im = plt.imshow( action_values, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;coolwarm&quot; # good for ± torque ) cbar = plt.colorbar(im) cbar.set_label(&quot;Greedy action value (torque)&quot;) plt.xlabel(r&quot;$\\dot{\\theta}$&quot;) plt.ylabel(r&quot;$\\theta$&quot;) plt.title(&quot;Greedy policy (torque) extracted from Value Iteration&quot;) plt.tight_layout() plt.show() Try it for yourself here! You should obtain the same results as policy iteration. "],["value-rl.html", "Chapter 2 Value-based Reinforcement Learning 2.1 Tabular Methods 2.2 Function Approximation", " Chapter 2 Value-based Reinforcement Learning In Chapter 1, we introduced algorithms for policy evaluation, policy improvement, and computing optimal policies in the tabular setting when the model is known. These dynamic-programming methods are grounded in Bellman consistency and optimality and come with strong convergence guarantees. A key limitation of the methods in Chapter 1 is that they require the transition dynamics \\(P(s&#39; \\mid s, a)\\) to be known. While in some applications modeling the dynamics is feasible (e.g., the inverted pendulum), in many others it is costly or impractical to obtain an accurate model of the environment (e.g., a humanoid robot interacting with everyday objects). This motivates relaxing the known-dynamics assumption and asking whether we can design algorithms that learn purely from interaction—i.e., by collecting data through environment interaction. This brings us to model-free reinforcement learning. In this chapter we focus on value-based RL methods. The central idea is to learn the value functions—\\(V(s)\\) and \\(Q(s,a)\\)—from interaction with the environment and then leverage these estimates to derive (approximately) optimal policies. We begin with tabular methods and then move to function-approximation approaches (e.g., neural networks) for problems where a tabular representation is intractable. 2.1 Tabular Methods Consider an infinite-horizon Markov decision process (MDP) \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma), \\] with a discount factor \\(\\gamma \\in [0,1)\\). We focus on the tabular setting where both the state space \\(\\mathcal{S}\\) and the action space \\(\\mathcal{A}\\) are finite, with cardinalities \\(|\\mathcal{S}|\\) and \\(|\\mathcal{A}|\\), respectively. A policy is a stationary stochastic mapping \\[ \\pi: \\mathcal{S} \\to \\Delta(\\mathcal{A}), \\] where \\(\\pi(a \\mid s)\\) denotes the probability of selecting action \\(a\\) in state \\(s\\). Unlike in Chapter 1, here we do not assume knowledge of the transition dynamics \\(P\\) or the reward function \\(R\\) (other than that \\(R\\) is deterministic). Instead, we assume we can interact with the environment and obtain trajectories of the form \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots), \\] by following a policy \\(\\pi\\). 2.1.1 Policy Evaluation We first consider the problem of estimating the value function of a given policy \\(\\pi\\). Recall the definition of the state-value function associated with \\(\\pi\\) is: \\[\\begin{equation} V^{\\pi}(s) = \\mathbb{E}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right], \\tag{2.1} \\end{equation}\\] where the expectation is taken over the randomness of both the policy \\(\\pi\\) and the transition dynamics \\(P\\). 2.1.1.1 Monte Carlo Estimation The basic idea of Monte Carlo (MC) estimation is to approximate the value function \\(V^\\pi\\) by averaging empirical returns observed from sampled trajectories generated under policy \\(\\pi\\). Since the return is defined as the discounted sum of future rewards, MC methods replace the expectation in the definition of \\(V^\\pi\\) with an average over sampled trajectories. Episodic Assumption. To make Monte Carlo methods well-defined, we restrict attention to the episodic setup, where each trajectory terminates upon reaching a terminal state (and the rewards thereafter are always zero). This ensures that the return is finite and can be computed exactly for each trajectory. Concretely, if an episode terminates at time \\(T\\), the return starting from time \\(t\\) is \\[\\begin{equation} g_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k} = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t-1} r_{T-1}. \\tag{2.2} \\end{equation}\\] Algorithmic Form. Let \\(\\mathcal{D}(s)\\) denote the set of all time indices at which state \\(s\\) is visited across sampled episodes. Then the Monte Carlo estimate of the value function is \\[\\begin{equation} \\hat{V}(s) = \\frac{1}{|\\mathcal{D}(s)|} \\sum_{t \\in \\mathcal{D}(s)} g_t. \\tag{2.3} \\end{equation}\\] There are two common variants: First-visit MC: use only the first occurrence of \\(s\\) in each episode. Every-visit MC: use all occurrences of \\(s\\) within an episode. Both variants converge to the same value function in the limit of infinitely many episodes. Incremental Implementation. Monte Carlo can be written as an incremental stochastic-approximation update that uses the return \\(g_t\\) as the target and a diminishing step size. Let \\(N(s)\\) be the number of (first- or every-) visits to state \\(s\\) that have been used to update \\(\\hat V(s)\\) so far, and let \\(g_t\\) be the return computed at a particular visit time \\(t\\in\\mathcal{D}(s)\\). Then the MC update is \\[\\begin{equation} \\hat V(s) \\;\\leftarrow\\; \\hat V(s) + \\alpha_{N(s)}\\,\\big( g_t - \\hat V(s) \\big), \\qquad \\alpha_{N(s)} &gt; 0 \\text{ diminishing.} \\tag{2.4} \\end{equation}\\] A canonical choice is the sample-average step size \\(\\alpha_{N(s)} = 1/N(s)\\), which yields the recurrence \\[\\begin{align} \\hat V_{N}(s) = \\hat V_{N-1}(s) + \\tfrac{1}{N}\\big(g_t - \\hat V_{N-1}(s)\\big) &amp; = \\Big(1-\\tfrac{1}{N}\\Big)\\hat V_{N-1}(s) + \\tfrac{1}{N}\\, g_t \\\\ &amp; = \\frac{N-1}{N} \\frac{1}{N-1} \\sum_{i=1}^{N-1} g_{t,i} + \\frac{1}{N} g_t \\\\ &amp; = \\frac{1}{N} \\sum_{i=1}^N g_{t,i} \\end{align}\\] so that \\(\\hat V_{N}(s)\\) equals the average of the \\(N\\) observed returns for \\(s\\) (i.e., Eq. (2.3)). In the above equation, I have used \\(g_{t,i}\\) to denote the \\(i\\)-th return before \\(g_t\\) was collected (and \\(g_t = g_{t,N}\\)). More generally, any diminishing schedule satisfying \\[ \\sum_{n=1}^\\infty \\alpha_n = \\infty, \\qquad \\sum_{n=1}^\\infty \\alpha_n^2 &lt; \\infty \\] (e.g., \\(\\alpha_n = c/(n+t_0)^p\\) with \\(1/2 &lt; p \\le 1\\)) also ensures consistency in the tabular setting. In first-visit MC, \\(N(s)\\) increases by one per episode at most; in every-visit MC, \\(N(s)\\) increases at each occurrence of \\(s\\) within an episode. Theoretical Guarantees. Unbiasedness: For any state \\(s\\), the return \\(g_t\\) is an unbiased sample of \\(V^\\pi(s)\\). \\[ \\mathbb{E}[g_t \\mid s_t = s] = V^\\pi(s). \\] Consistency: By the law of large numbers, as the number of episodes grows, \\[ \\hat{V}(s) \\xrightarrow{\\text{a.s.}} V^\\pi(s). \\] Asymptotic Normality: The MC estimator converges at rate \\(O(1/\\sqrt{N})\\), where \\(N\\) is the number of episodes used for the estimation. Limitations. Despite its conceptual simplicity, MC estimation suffers from several drawbacks: It requires episodes to terminate, making it unsuitable for continuing tasks without artificial truncation. It can only update value estimates after an episode ends, which is data-inefficient. While unbiased, MC estimates often have high variance, leading to slow convergence. These limitations motivate the study of Temporal-Difference (TD) learning, which updates value estimates online and can handle continuing tasks. 2.1.1.2 Temporal-Difference Learning While Monte Carlo methods estimate value functions by averaging full returns from complete episodes, Temporal-Difference (TD) learning provides an alternative approach that updates value estimates incrementally after each step of interaction with the environment. The key idea is to combine the sampling of Monte Carlo with the bootstrapping of dynamic programming. High-Level Intuition. TD learning avoids waiting until the end of an episode by using the Bellman consistency equation as a basis for updates. Recall that for any policy \\(\\pi\\), the Bellman consistency equation reads: \\[\\begin{equation} V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} \\left[ R(s,a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(s&#39; \\mid s, a)} V(s&#39;) \\right]. \\tag{2.5} \\end{equation}\\] At a high level, TD learning turns the expectation in Bellman equation into sampling. At each step, it updates the current estimate of the value function toward a one-step bootstrap target: the immediate reward plus the discounted value of the next state. This makes TD methods more data-efficient and applicable to continuing tasks without terminal states. Algorithmic Form. Suppose the agent is in state \\(s_t\\), takes action \\(a_t \\sim \\pi(\\cdot \\mid s_t)\\), receives reward \\(r_t\\), and transitions to \\(s_{t+1}\\). The TD(0) update rule is \\[\\begin{equation} \\hat{V}(s_t) \\;\\leftarrow\\; \\hat{V}(s_t) + \\alpha \\big[ r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t) \\big], \\tag{2.6} \\end{equation}\\] where \\(\\alpha \\in (0,1]\\) is the learning rate. The term inside the brackets, \\[\\begin{equation} \\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t), \\tag{2.7} \\end{equation}\\] is called the TD error. It measures the discrepancy between the current value estimate and the bootstrap target. The algorithm updates \\(\\hat{V}(s_t)\\) in the direction of reducing this error. Theoretical Guarantees. Convergence in the Tabular Case: If each state is visited infinitely often and the learning rate sequence satisfies \\[ \\sum_t \\alpha_t = \\infty, \\; \\sum_t \\alpha_t^2 &lt; \\infty \\] then TD(0) converges almost surely to the true value function \\(V^\\pi\\). For example, choosing \\(\\alpha_t = 1/(t+1)\\) satisfies this condition. Section 2.1.2 provides a detailed proof of the convergence of TD learning. Bias–Variance Tradeoff: The TD target uses the current estimate \\(\\hat{V}(s_{t+1})\\) rather than the true value, which introduces bias. However, it has significantly lower variance than Monte Carlo estimates, often leading to faster convergence in practice. To see this, note that for TD(0), the target is a one-step bootstrap: \\[ y_t = r_t + \\gamma \\hat{V}(s_{t+1}). \\] This replaces the true value \\(V^\\pi(s_{t+1})\\) with the current estimate \\(\\hat{V}(s_{t+1})\\). As a result, \\(y_t\\) is biased relative to the true return. However, since it depends only on the immediate reward and the next state, the variance of \\(y_t\\) is much lower than that of the Monte Carlo target. Limitations. TD(0) relies on bootstrapping, which introduces bias relative to Monte Carlo methods. Convergence can be slow if the learning rate is not chosen carefully. In summary, Temporal-Difference learning addresses the major limitations of Monte Carlo estimation: it works in continuing tasks, updates online at each step, and is generally more sample-efficient. However, it trades away unbiasedness for bias–variance efficiency, motivating further extensions such as multi-step TD and TD(\\(\\lambda\\)). 2.1.1.3 Multi-Step TD Learning Monte Carlo methods use the full return \\(g_t\\), while TD(0) uses a one-step bootstrap. Multi-step TD learning generalizes these two extremes by using \\(n\\)-step returns as targets. In this way, multi-step TD interpolates between Monte Carlo and TD(0). High-Level Intuition. The motivation is to balance the high variance of Monte Carlo with the bias of TD(0). Instead of waiting for a full return (MC) or using only one step of bootstrapping (TD(0)), multi-step TD uses partial returns spanning \\(n\\) steps of real rewards, followed by a bootstrap. This provides a flexible tradeoff between bias and variance. Algorithmic Form. The \\(n\\)-step return starting from time \\(t\\) is defined as \\[\\begin{equation} g_t^{(n)} = r_t + \\gamma r_{t+1} + \\dots + \\gamma^{n-1} r_{t+n-1} + \\gamma^n \\hat{V}(s_{t+n}). \\tag{2.8} \\end{equation}\\] The \\(n\\)-step TD update is \\[\\begin{equation} \\hat{V}(s_t) \\;\\leftarrow\\; \\hat{V}(s_t) + \\alpha \\big[ g_t^{(n)} - \\hat{V}(s_t) \\big], \\tag{2.9} \\end{equation}\\] where \\(g_t^{(n)}\\) replaces the one-step target in TD(0) (2.6). For \\(n=1\\): the method reduces to TD(0). For \\(n=T-t\\) (the full episode length): the method reduces to Monte Carlo. Theoretical Guarantees. Convergence in the Tabular Case: With suitable learning rates and sufficient exploration, \\(n\\)-step TD converges to \\(V^\\pi\\). Bias–Variance Tradeoff: Larger \\(n\\): lower bias, higher variance (closer to Monte Carlo). Smaller \\(n\\): higher bias, lower variance (closer to TD(0)). Intermediate \\(n\\) provides a balance that often yields faster learning in practice. Limitations. Choosing the right \\(n\\) is problem-dependent: too small and bias dominates; too large and variance grows. Requires storing \\(n\\)-step reward sequences before updating, which can increase memory and computation. In summary, multi-step TD unifies Monte Carlo and TD(0) by introducing \\(n\\)-step returns. It allows practitioners to tune the bias–variance tradeoff by selecting \\(n\\). Later, we will see how TD(\\(\\lambda\\)) averages over all \\(n\\)-step returns in a principled way, further smoothing this tradeoff. 2.1.1.4 Eligibility Traces and TD(\\(\\lambda\\)) So far, we have seen that Monte Carlo methods use full returns \\(g_t\\), while TD(0) uses a one-step bootstrap. Multi-step TD methods generalize between these two extremes by using \\(n\\)-step returns. However, a natural question arises: can we combine information from all possible \\(n\\)-step returns in a principled way? This motivates TD(\\(\\lambda\\)), which blends multi-step TD methods into a single algorithm using eligibility traces. High-Level Intuition. TD(\\(\\lambda\\)) introduces a parameter \\(\\lambda \\in [0,1]\\) that controls the weighting of \\(n\\)-step returns: \\(\\lambda = 0\\): reduces to TD(0), relying only on one-step bootstrapping. \\(\\lambda = 1\\): reduces to Monte Carlo, relying on full returns. \\(0 &lt; \\lambda &lt; 1\\): interpolates smoothly between these two extremes by averaging all \\(n\\)-step returns with exponentially decaying weights. Formally, the \\(\\lambda\\)-return is \\[\\begin{equation} g_t^{(\\lambda)} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} g_t^{(n)}, \\tag{2.10} \\end{equation}\\] where \\(g_t^{(n)}\\) is the \\(n\\)-step return defined in (2.8). Remark. To make the \\(\\lambda\\)-return well defined, we consider two cases. Episodic Case: Well-posed. If an episode terminates at time \\(T\\), let \\(N=T-t\\) be the remaining steps. Then \\[\\begin{equation} \\begin{split} g_t^{(\\lambda)} &amp; = (1-\\lambda)\\sum_{n=1}^{N-1}\\lambda^{\\,n-1} \\, g_t^{(n)} \\;+\\; \\lambda^{\\,N-1}\\, g_t^{(N)}, \\\\ &amp; = (1-\\lambda)\\sum_{n=1}^{N}\\lambda^{\\,n-1} \\, g_t^{(n)} \\;+\\; \\lambda^{N}\\, g_t^{(N)}, \\end{split} \\tag{2.11} \\end{equation}\\] where \\(g_t^{(n)}\\) is the \\(n\\)-step return (Eq. (2.8)) and \\(g_t^{(N)}\\) is the full Monte Carlo return (Eq. (2.2)). This expression is well-defined for all \\(\\lambda\\in[0,1]\\). Note that the weights form a convex combination: \\[ (1-\\lambda)\\sum_{n=1}^{N-1}\\lambda^{n-1} + \\lambda^{N-1} = 1-\\lambda^{N-1}+\\lambda^{N-1} = 1. \\] Continuing Case: Limit. Taking \\(\\lambda\\uparrow 1\\) in (2.11) gives \\[ \\lim_{\\lambda\\uparrow 1} g_t^{(\\lambda)} = g_t^{(N)} = g_t, \\] so the \\(\\lambda\\)-return reduces to the Monte Carlo return at \\(\\lambda=1\\). For continuing tasks (no terminal \\(T\\)), \\(\\lambda=1\\) is conventionally defined by this same limiting argument, yielding the infinite-horizon discounted return when \\(\\gamma&lt;1\\). Eligibility Traces. Naively computing \\(g_t^{(\\lambda)}\\) would require storing and combining infinitely many \\(n\\)-step returns, which is impractical. Instead, TD(\\(\\lambda\\)) uses eligibility traces to implement this efficiently online. An eligibility trace is a temporary record that tracks how much each state is “eligible” for updates based on how recently and frequently it has been visited. Specifically, for each state \\(s\\), we maintain a trace \\(z_t(s)\\) that evolves as \\[\\begin{equation} z_t(s) = \\gamma \\lambda z_{t-1}(s) + \\mathbf{1}\\{s_t = s\\}, \\tag{2.12} \\end{equation}\\] where \\(\\mathbf{1}\\{s_t = s\\}\\) is an indicator that equals 1 if state \\(s\\) is visited at time \\(t\\), and 0 otherwise. TD(\\(\\lambda\\)) Update Rule. At each time step \\(t\\), we compute the TD error \\[ \\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t), \\] as in (2.7). Then, for each state \\(s\\), we update \\[\\begin{equation} \\hat{V}(s) \\;\\leftarrow\\; \\hat{V}(s) + \\alpha \\, \\delta_t \\, z_t(s). \\tag{2.13} \\end{equation}\\] Thus, all states with nonzero eligibility traces are updated simultaneously, with the magnitude of the update determined by both the TD error and the eligibility trace. See Proposition 2.1 below for a justification. Theoretical Guarantees. In the tabular case, TD(\\(\\lambda\\)) converges almost surely to the true value function \\(V^\\pi\\) under the usual stochastic approximation conditions (sufficient exploration, decaying step sizes). The parameter \\(\\lambda\\) directly controls the bias–variance tradeoff: Smaller \\(\\lambda\\): more bootstrapping, more bias but lower variance. Larger \\(\\lambda\\): less bootstrapping, less bias but higher variance. TD(\\(\\lambda\\)) can be shown to converge to the fixed point of the \\(\\lambda\\)-operator, which is itself a contraction mapping. In summary, eligibility traces provide an elegant mechanism to combine the advantages of Monte Carlo and TD learning. TD(\\(\\lambda\\)) introduces a spectrum of algorithms: at one end TD(0), at the other Monte Carlo, and in between a family of methods balancing bias and variance. In practice, intermediate values such as \\(\\lambda \\approx 0.9\\) often work well. Proposition 2.1 (Forward–Backward Equivalence) Consider one episode \\(s_0,a_0,r_0,\\ldots,s_T\\) with \\(\\hat V(s_T)=0\\). Let the forward view apply updates at the end of the episode: \\[ \\hat V(s_t) \\leftarrow \\hat V(s_t) + \\alpha \\big[g_t^{(\\lambda)}-\\hat V(s_t)\\big], \\quad t=0,\\ldots,T-1, \\] where \\(g_t^{(\\lambda)}\\) is the \\(\\lambda\\)-return in (2.10) with the \\(n\\)-step returns \\(g_t^{(n)}\\) from (2.8), and where \\(\\hat V\\) is kept fixed while computing all \\(g_t^{(\\lambda)}\\). Let the backward view run through the episode once, using the TD error \\(\\delta_t\\) from (2.7) and eligibility traces \\(z_t(s)\\) from (2.12), and then apply the cumulative update \\[ \\Delta_{\\text{back}} \\hat V(s) \\;=\\; \\alpha \\sum_{t=0}^{T-1} \\delta_t\\, z_t(s). \\] Then, for every state \\(s\\), \\[ \\Delta_{\\text{back}} \\hat V(s) \\;=\\; \\alpha \\sum_{t:\\, s_t=s}\\big[g_t^{(\\lambda)}-\\hat V(s_t)\\big], \\] i.e., the net parameter change produced by (2.13) equals that of the \\(\\lambda\\)-return updates. Proof. Fix a state \\(s\\). Using (2.12), \\[ z_t(s)=\\sum_{k=0}^{t}(\\gamma\\lambda)^{\\,t-k}\\,\\mathbf{1}\\{s_k=s\\}. \\] Hence \\[ \\sum_{t=0}^{T-1}\\delta_t z_t(s) =\\sum_{t=0}^{T-1}\\delta_t \\sum_{k=0}^{t}(\\gamma\\lambda)^{\\,t-k}\\mathbf{1}\\{s_k=s\\} =\\sum_{k:\\,s_k=s}\\; \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{\\,t-k}\\delta_t . \\tag{1} \\] Write \\(\\delta_t=r_t+\\gamma\\hat V(s_{t+1})-\\hat V(s_t)\\) and split the inner sum: \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = \\underbrace{\\sum_{t=k}^{T-1} \\gamma^{t-k}\\lambda^{t-k} r_t}_{\\text{(A)}} + \\underbrace{\\sum_{t=k}^{T-1}\\gamma^{t-k}\\lambda^{t-k}(\\gamma\\hat V(s_{t+1})-\\hat V(s_t))}_{\\text{(B)}}. \\] Term (B) telescopes. Shifting index in the first part of (B), \\[ \\sum_{t=k}^{T-1}\\gamma^{t-k}\\lambda^{t-k}\\gamma \\hat V(s_{t+1}) = \\sum_{t=k+1}^{T}\\gamma^{t-k}\\lambda^{t-1-k}\\hat V(s_t). \\] Therefore \\[ \\text{(B)}= -\\hat V(s_k) + \\sum_{t=k+1}^{T-1}\\gamma^{t-k}\\lambda^{t-1-k}(1-\\lambda)\\hat V(s_t) + \\underbrace{\\gamma^{T-k}\\lambda^{T-1-k}\\hat V(s_T)}_{=\\,0}. \\tag{2} \\] Combining (A) and (2), and reindexing with \\(n=t-k\\), \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = -\\hat V(s_k) + \\sum_{n=0}^{T-1-k}\\gamma^{n}\\lambda^{n} r_{k+n} + (1-\\lambda)\\sum_{n=1}^{T-1-k}\\gamma^{n}\\lambda^{n-1}\\hat V(s_{k+n}). \\tag{3} \\] On the other hand, expanding the \\(\\lambda\\)-return (2.10), \\[ \\begin{aligned} g_k^{(\\lambda)} &amp;=(1-\\lambda)\\sum_{n=1}^{T-k}\\lambda^{n-1} \\Bigg(\\sum_{m=0}^{n-1}\\gamma^{m} r_{k+m} + \\gamma^{n}\\hat V(s_{k+n})\\Bigg) + \\lambda^{T-k} g_k^{(T-k)}\\\\ &amp;= \\sum_{n=0}^{T-1-k}\\gamma^{n}\\lambda^{n} r_{k+n} + (1-\\lambda)\\sum_{n=1}^{T-1-k}\\gamma^{n}\\lambda^{n-1}\\hat V(s_{k+n}), \\end{aligned} \\tag{4} \\] where we used that \\(\\hat V(s_T)=0\\). Comparing (3) and (4) yields \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = g_k^{(\\lambda)} - \\hat V(s_k). \\tag{5} \\] Substituting (5) into (1) and multiplying by \\(\\alpha\\) completes the proof. Example 2.1 (Policy Evaluation (MC and TD Family)) We consider the classic random-walk MDP with terminal states: States: \\(\\{0,1,2,3,4,5,6\\}\\), where \\(0\\) and \\(6\\) are terminal; nonterminal states are \\(1{:}5\\). Actions: \\(\\{-1,+1\\}\\) (“Left”/“Right”). Dynamics: From a nonterminal state \\(s\\in\\{1,\\dots,5\\}\\), action \\(-1\\) moves to \\(s-1\\), and action \\(+1\\) moves to \\(s+1\\). Rewards: Transitioning into state \\(6\\) yields reward \\(+1\\); all other transitions yield \\(0\\). Discount: \\(\\gamma=1\\) (episodic task). Episodes start at state \\(s_0=3\\) and terminate upon reaching \\(\\{0,6\\}\\). We evaluate the equiprobable policy \\(\\pi\\) that chooses Left/Right with probability \\(1/2\\) each at every nonterminal state. Under this policy, the true state-value function on nonterminal states \\(s\\in\\{1,\\dots,5\\}\\) is \\[\\begin{equation} V^\\pi(s) \\;=\\; \\frac{s}{6}. \\tag{2.14} \\end{equation}\\] We compare four tabular policy-evaluation methods: Monte Carlo (MC), first-visit — using full returns as target. TD(0) — one-step bootstrap. \\(n\\)-step TD — here we use \\(n=3\\) (intermediate between MC and TD(0)). TD(\\(\\lambda\\)) — accumulating eligibility traces (we illustrate with \\(\\lambda=0.9\\)). All methods estimate \\(V^\\pi\\) from trajectories generated by \\(\\pi\\). Error Metric. We report the mean-squared error (MSE) over nonterminal states after each episode: \\[\\begin{equation} \\mathrm{MSE}_t \\;=\\; \\frac{1}{5}\\sum_{s=1}^{5}\\big(\\hat V_t(s)-V^\\pi(s)\\big)^2, \\tag{2.15} \\end{equation}\\] where \\(V^\\pi\\) is given by (2.14). Curves are averaged over multiple random seeds. Fixed Step Sizes. We first use a fixed step size \\(\\alpha=0.1\\) for all methods. Fig. 2.1 shows the trajectories of MSE versus number of episodes. We can see that, when using a constant step size, these methods do not converge to exactly the true value function, but to a small neighborhood. In addition, if the algorithm initially decays very fast, then the final variance is larger. For example, MC initially decays very fast, but has a higher variance, whereas TD(0) initially decays slower, but has a lower final variance. This agrees with the theoretical analysis in (Kearns and Singh 2000). Figure 2.1: Policy Evaluation, MC versus TD Family, Fixed Step Size Diminishing Step Sizes. We then use a diminishing step size for the TD family: \\[\\begin{equation} \\alpha_t(s) \\;=\\; \\frac{c}{\\big(N_t(s)+t_0\\big)^p}, \\qquad \\tfrac{1}{2} &lt; p \\le 1, \\tag{2.16} \\end{equation}\\] where \\(N_t(s)\\) counts how many times \\(V(s)\\) has been updated up to time \\(t\\). A common choice is \\(p=1\\) with moderate \\(c&gt;0\\) and \\(t_0&gt;0\\). Fig. 2.2 shows the MSE versus episodes for MC, TD(0), 3-step TD, and TD(\\(\\lambda\\)) under the diminishing step-size. Observe that all algorithms converge to the true value function under the diminishing step size schedule. Figure 2.2: Policy Evaluation, MC versus TD Family, Diminishing Step Size You are encouraged to play with the parameters of these algorithms in the code here. 2.1.2 Convergence Proof of TD Learning Setup. Consider a tabular MDP with finite state space \\(\\mathcal{S}\\) and action space \\(\\mathcal{A}\\), and a discount factor \\(\\gamma \\in [0,1)\\). Assume the reward function is bounded, for example, \\(R(s,a) \\in [0,1]\\) for any \\((s,a) \\in \\mathcal{S} \\times \\mathcal{A}\\). Let \\(\\pi\\) be a stochastic policy and \\(V^\\pi\\) be the true value function associated with \\(\\pi\\), the target we wish to estimate from interaction data. Denote \\[ \\mathcal{F}_t = \\sigma(s_0,a_0,r_0,\\dots,s_{t-1},a_{t-1},r_{t-1}), \\] as the \\(\\sigma\\) algebra of all state-action-reward information up to time \\(t-1\\). TD(0) Update. We maintain a tabular estimate \\(V_t\\) of the true value \\(V^\\pi\\). On visiting \\(s_t\\) and observing \\((s_t, a_t, r_t, s_{t+1})\\), the TD(0) algorithm performs \\[\\begin{equation} V_{t+1}(s_t) = V_t(s_t) + \\alpha_t(s_t) \\delta_t, \\tag{2.17} \\end{equation}\\] where \\(\\delta_t\\) is the TD error \\[ \\delta_t = r_t + \\gamma V_{t}(s_{t+1}) - V_t(s_t). \\] The update (2.17) only changes the value at \\(s_t\\), leaving the value at other states unchanged. Robbins–Monro Step Size. We assume the step size \\(\\alpha\\) satisfy the Robbins–Monro condition. That is, for any \\(s \\in \\mathcal{S}\\): \\[ \\alpha_t(s) &gt;0, \\quad \\sum_{t: s_t = s} \\alpha_t(s) = \\infty, \\quad \\sum_{t: s_t = s} \\alpha^2_t(s) &lt; \\infty. \\] Stationary Distribution. Assume the Markov chain over \\(\\mathcal{S}\\) induced by \\(\\pi\\) is ergodic, then a unique stationary state distribution \\(\\mu^\\pi\\) exists and satisfy: \\[\\begin{equation} \\mu^\\pi(s&#39;) = \\sum_{s \\in \\mathcal{S}} \\mu^\\pi(s) \\left( \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) P(s&#39; \\mid s, a) \\right), \\quad \\forall s&#39; \\in \\mathcal{S}. \\tag{2.18} \\end{equation}\\] If we denote \\[\\begin{equation} P^\\pi(s&#39; \\mid s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) P(s&#39; \\mid s,a), \\tag{2.19} \\end{equation}\\] as the \\(\\pi\\)-induced state-only transition dynamics, then condition (2.18) is equivalent to \\[\\begin{equation} \\mu^\\pi(s&#39;) = \\sum_{s \\in \\mathcal{S}} \\mu^\\pi(s) P^\\pi (s&#39; \\mid s), \\quad \\forall s&#39; \\in \\mathcal{S}. \\tag{2.20} \\end{equation}\\] See (2.42) for a generalization to continuous MDP. Bellman Operator. For any \\(V:\\mathcal S\\to\\mathbb R\\), define the Bellman operator associated with \\(\\pi\\) as \\(T^\\pi V:\\mathcal S\\to\\mathbb R\\) by \\[\\begin{equation} (T^\\pi V)(s)\\;=\\;\\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\sum_{s&#39;\\in\\mathcal S} P(s&#39;\\mid s,a)\\,\\Big( R(s,a)+\\gamma\\,V(s&#39;)\\Big). \\tag{2.21} \\end{equation}\\] We know that the operator \\(T^\\pi\\) is a \\(\\gamma\\)-contraction in \\(\\|\\cdot\\|_\\infty\\). Hence it has a unique fixed point \\(V^\\pi\\) satisfying \\(V^\\pi=T^\\pi V^\\pi\\). The following theorem states the almost sure convergence of TD learning iterates to the true value function. Theorem 2.1 (TD(0) Convergence (Tabular)) Under the tabular MDP setup and assumptions above, the TD(0) iterates \\(V_t\\) generated by (2.17) converge almost surely to \\(V^\\pi\\). To prove this theorem, we need the following two lemmas. Lemma 2.1 (Robbins-Siegmund Lemma) Let \\((X_t)_{t\\ge 0}\\) be nonnegative and adapted to \\((\\mathcal F_t)\\). Suppose there exist nonnegative \\((\\beta_t),(\\gamma_t),(\\xi_t)\\) with \\(\\sum_t \\gamma_t&lt;\\infty\\) and \\(\\sum_t \\xi_t&lt;\\infty\\) such that \\[ \\mathbb E[X_{t+1}\\mid \\mathcal F_t]\\;\\le\\;(1+\\gamma_t)X_t\\;-\\;\\beta_t\\;+\\;\\xi_t\\qquad\\text{almost surely} \\] Then \\(X_t\\) converges almost surely to a finite random variable and \\(\\sum_t \\beta_t&lt;\\infty\\) almost surely. This lemma is from (Robbins and Siegmund 1971). Lemma 2.2 Let \\(\\mu^\\pi\\) be the stationary distribution in (2.20), \\(D=\\mathrm{diag}(\\mu^\\pi)\\), and \\(w:=V-V^\\pi\\). Then \\[ \\langle w,\\,D\\,(T^\\pi V - V)\\rangle\\;\\le\\;-(1-\\gamma)\\,\\|w\\|_D^2, \\] where \\(\\langle x,y\\rangle=x^\\top y\\) and \\(\\|w\\|_D^2=\\sum_s \\mu^\\pi(s)\\,w(s)^2\\). Proof. First, for any two value functions \\(V, U \\in \\mathbb{R}^{|\\mathcal{S}|}\\), we have \\[ (T^\\pi V)(s) - (T^\\pi U)(s) = \\gamma \\sum_{a} \\pi(a \\mid s) \\sum_{s&#39;} P(s&#39; \\mid s,a) (V(s&#39;) - U(s)&#39;). \\] Therefore, \\[ T^\\pi V - T^\\pi U = \\gamma \\widetilde P (V - U), \\] with \\[\\begin{equation} (\\widetilde P u)(s):=\\sum_a\\pi(a\\mid s)\\sum_{s&#39;}P(s&#39;\\mid s,a)\\,u(s&#39;). \\tag{2.22} \\end{equation}\\] With this, we can write \\[\\begin{equation} \\begin{split} T^\\pi V - V &amp; = T^\\pi V - V^\\pi + V^\\pi - V \\\\ &amp; = T^\\pi V - T^\\pi V^\\pi - ( V - V^\\pi ) \\\\ &amp; = \\gamma \\widetilde P (V - V^\\pi) - ( V - V^\\pi ) \\\\ &amp; = (\\gamma \\widetilde P - I) ( V - V^\\pi ) \\\\ &amp; = (\\gamma \\widetilde P - I) w. \\end{split} \\end{equation}\\] Thus, \\[\\begin{equation} \\langle w, D(T^\\pi V - V)\\rangle = -\\,w^\\top D\\,(I-\\gamma \\widetilde P)\\,w = -\\|w\\|_D^2 + \\gamma\\,\\langle w, D\\,\\widetilde P w\\rangle. \\tag{2.23} \\end{equation}\\] Next, we prove \\(\\langle w, D\\widetilde P w\\rangle\\le \\|w\\|_D^2\\). First, we show \\(\\|\\widetilde P w\\|_D\\le \\|w\\|_D\\). For any state \\(s \\in \\mathcal{S}\\), from (2.22), we have \\[ (\\widetilde P w)(s) = \\sum_{s&#39;} P^\\pi (s&#39; \\mid s) w(s&#39;), \\] where \\(P^\\pi(s&#39; \\mid s)\\) is the \\(\\pi\\)-induced state-only transition in (2.19). Since \\(P^\\pi(\\cdot \\mid s)\\) is a probability distribution, and \\(x \\mapsto x^2\\) is convex, we have \\[ ((\\widetilde P w)(s))^2 = \\left( \\sum_{s&#39;} P^\\pi (s&#39; \\mid s) w(s&#39;) \\right)^2 \\leq \\sum_{s&#39;} P^\\pi (s&#39; \\mid s) w^2(s&#39;). \\] Therefore, we have \\[\\begin{equation} \\begin{split} \\Vert \\widetilde P w \\Vert_D^2 &amp; = \\sum_s \\mu^{\\pi}(s) ((\\widetilde P w)(s))^2 \\\\ &amp; \\leq \\sum_s \\mu^{\\pi}(s) \\left( \\sum_{s&#39;} P^\\pi (s&#39; \\mid s) w^2(s&#39;) \\right) \\\\ &amp; = \\sum_{s&#39;} \\left( \\sum_{s} \\mu^\\pi(s) P^\\pi(s&#39; \\mid s) \\right) w^2(s&#39;) \\\\ &amp; = \\sum_{s&#39;} \\mu^\\pi (s&#39;) w^2 (s&#39;) = \\Vert w \\Vert_D^2. \\end{split} \\end{equation}\\] where the second-from-last equality holds because \\(\\mu^\\pi\\) is the stationary distribution and satisfies (2.20). Second, we write \\[ \\langle w, D\\widetilde P w\\rangle = \\langle D^{0.5} w, D^{0.5} \\widetilde P w \\rangle \\leq \\Vert D^{0.5} w \\Vert \\cdot \\Vert D^{0.5} \\widetilde P w \\Vert = \\Vert w \\Vert_D \\cdot \\Vert \\widetilde P w \\Vert_D \\leq \\Vert w \\Vert_D^2. \\] Plugging this back to (2.23), we obtain \\[ \\langle w, D(T^\\pi V - V) \\rangle \\leq - \\Vert w \\Vert_D^2 + \\gamma \\Vert w \\Vert_D^2, \\] proving the desired result in the lemma. We are now ready to prove Theorem 2.1. Proof. Step 1 (TD as stochastic approximation). For the TD error \\[ \\delta_t=r_{t}+\\gamma V_t(s_{t+1})-V_t(s_t), \\] we have the conditional expectation \\[ \\mathbb E[\\delta_t\\mid \\mathcal F_t, s_t] =\\sum_{a}\\pi(a\\mid s_t)\\sum_{s&#39;}P(s&#39;\\mid s_t,a)\\Big(R(s_t,a)+\\gamma V_t(s&#39;)\\Big)-V_t(s_t) =\\big(T^\\pi V_t - V_t\\big)(s_t). \\] Define the “noise”: \\[ \\eta_{t+1}:=\\delta_t-\\mathbb E[\\delta_t\\mid \\mathcal F_t,s_t]. \\] Then \\(\\mathbb E[\\eta_{t+1}\\mid \\mathcal F_t,s_t]=0\\) and the TD update is equivalent to \\[\\begin{equation} V_{t+1}(s_t)=V_t(s_t)+\\alpha_t(s_t)\\Big( \\big(T^\\pi V_t - V_t\\big)(s_t) + \\eta_{t+1}\\Big), \\tag{2.24} \\end{equation}\\] while learving all other coordinates unchanged. Because rewards are uniformly bounded, we know that \\(V_t\\) remains bounded. Hence, \\(\\mathbb E[\\eta_{t+1}^2\\mid \\mathcal F_t, s_t]\\) is uniformly bounded. Equation (2.24) shows that the TD update can be seen as a stochastic approximation to the Bellman operator (2.21). Step 2 (Lyapunov drift). Let \\(D=\\mathrm{diag}(\\mu^\\pi)\\), a diagonal matrix whose diagonal entries are the probabilities in \\(\\mu^\\pi\\). Define the Lyapunov function \\[ \\mathcal L(V)=\\frac{1}{2} \\|V-V^\\pi\\|_D^2=\\frac{1}{2} \\sum_s \\mu^\\pi(s)\\,\\big(V(s)-V^\\pi(s)\\big)^2. \\] Let \\(w_t:=V_t - V^\\pi\\). Since only the \\(s_t\\)-coordinate changes at time \\(t\\), we have \\[\\begin{equation} \\begin{split} \\mathcal L(V_{t+1})-\\mathcal L(V_t) &amp;=\\frac{1}{2} \\mu^\\pi(s_t)\\Big(( \\underbrace{V_t(s_t)+\\alpha_t\\delta_t}_{V_{t+1}(s_t)} - V^\\pi(s_t) )^2 -(V_t(s_t)-V^\\pi(s_t))^2\\Big)\\\\ &amp;= \\mu^\\pi(s_t)\\,\\alpha_t\\,\\delta_t\\,w_t(s_t)\\;+\\;\\frac{1}{2} \\mu^\\pi(s_t)\\,\\alpha_t^2\\,\\delta_t^{\\,2}. \\end{split} \\tag{2.25} \\end{equation}\\] Define \\(g_t := T^\\pi V_t - V_t\\). Taking conditional expectation given \\(\\mathcal F_t\\) and i.i.d. \\(s_t\\sim \\mu^\\pi\\), \\[\\begin{equation} \\begin{split} \\mathbb E\\!\\left[\\mathcal L(V_{t+1})-\\mathcal L(V_t)\\mid \\mathcal F_t\\right] &amp;= \\alpha_t\\,\\mathbb E\\!\\left[\\mu^\\pi(s_t)\\,w_t(s_t)\\,g_t(s_t)\\mid \\mathcal F_t\\right] + \\frac{1}{2} \\alpha_t^2\\,\\mathbb E\\!\\left[\\mu^\\pi(s_t)\\,\\delta_t^{\\,2}\\mid \\mathcal F_t\\right]\\\\ &amp;= \\alpha_t\\,\\sum_s \\mu^\\pi(s)\\,w_t(s)\\,g_t(s) \\;+\\; \\frac{1}{2} \\alpha_t^2\\,C_t, \\end{split} \\tag{2.26} \\end{equation}\\] where \\(C_t:=\\mathbb E\\!\\left[\\mu^\\pi(s_t)\\,\\delta_t^{\\,2}\\mid \\mathcal F_t\\right]\\) is finite because rewards are bounded and \\(V_t\\) stays bounded. Assume \\(C_t \\leq C\\). At the same time, by Lemma 2.2 \\[ \\sum_s \\mu^\\pi(s)\\,w_t(s)\\,g_t(s) =\\langle w_t, D g_t\\rangle \\le -(1-\\gamma)\\,\\|w_t\\|_D^2. \\] Plugging into (2.26) yields \\[\\begin{equation} \\mathbb E\\!\\left[\\mathcal L(V_{t+1})\\mid \\mathcal F_t\\right] \\;\\le\\; \\mathcal L(V_t)\\;-\\;\\alpha_t\\,(1-\\gamma)\\,\\|w_t\\|_D^2\\;+\\;\\frac{1}{2} \\alpha_t^2\\,C. \\tag{2.27} \\end{equation}\\] This is in Robbins–Siegmund form with \\[ X_t:=\\mathcal L(V_t),\\qquad \\beta_t:=(1-\\gamma)\\,\\alpha_t\\,\\|w_t\\|_D^2,\\qquad \\gamma_t:=0,\\qquad \\xi_t:=\\frac{1}{2} C\\,\\alpha_t^2. \\] We have \\(\\sum_t \\xi_t&lt;\\infty\\) by \\(\\sum_t \\alpha_t^2&lt;\\infty\\). Therefore \\(X_t\\) converges a.s. and \\(\\sum_t \\beta_t&lt;\\infty\\) a.s., which implies \\(\\sum_t \\alpha_t \\|w_t\\|_D^2&lt;\\infty\\). Since \\(\\sum_t \\alpha_t=\\infty\\), it must be that \\(\\liminf_t \\|w_t\\|_D=0\\). Finally, using (2.27) again and the continuity of the drift, one shows that any subsequential limit of \\(V_t\\) must satisfy \\(T^\\pi V - V=0\\); by uniqueness of the fixed point, the only possible limit is \\(V^\\pi\\). Hence \\(V_t\\to V^\\pi\\) almost surely. 2.1.3 On-Policy Control Monte Carlo (MC) estimation and the TD family evaluate policies directly from interaction—no model required. We now turn evaluation into control via generalized policy iteration (GPI): repeatedly (i) evaluate the current policy from data and (ii) improve it by acting greedily with respect to the new estimates. We first cover on-policy control methods, which estimate and improve the same (typically \\(\\varepsilon\\)-greedy) policy, and then off-policy methods, which learn about a target policy while behaving with a different one. 2.1.3.1 Monte Carlo Control High-level Intuition. Goal. Learn an (approximately) optimal policy by alternating policy evaluation and policy improvement using only sampled episodes. Why action-values? Estimating \\(Q^\\pi(s,a)\\) lets us improve the policy without a model by choosing “\\(\\arg\\max_a Q(s,a)\\)”. Exploration. Pure greedy improvement can get stuck. MC control keeps the policy \\(\\varepsilon\\)-soft (e.g., \\(\\varepsilon\\)-greedy) so that every action has nonzero probability and all state-action pairs continue to be sampled. An \\(\\varepsilon\\)-soft policy is one that never rules out any action: in every state \\(s\\), each action \\(a\\) gets at least a small fraction of probability. Formally, in the tabular setup, we have that a policy \\(\\pi\\) is \\(\\varepsilon\\)-soft if and only if \\[\\begin{equation} \\forall s, \\forall a: \\quad \\pi(a \\mid s) \\geq \\frac{\\varepsilon}{|\\mathcal{A}(s)|}, \\quad \\varepsilon \\in (0,1], \\tag{2.28} \\end{equation}\\] where \\(\\mathcal{A}(s)\\) denotes the set of actions the agent can select at state \\(s\\). Coverage mechanisms. Classic guarantees use either: Exploring starts (ES): start each episode from a randomly chosen \\((s,a)\\) with nonzero probability; or \\(\\varepsilon\\)-soft / GLIE (Greedy in the Limit with Infinite Exploration): use \\(\\varepsilon\\)-greedy behavior with \\(\\varepsilon_t \\downarrow 0\\) so every \\((s,a)\\) is visited infinitely often while the policy becomes greedy in the limit. Algorithmic Form. We maintain tabular action-value estimates \\(Q(s,a)\\) and an \\(\\varepsilon\\)-soft policy \\(\\pi\\) (\\(\\varepsilon\\)-greedy w.r.t. \\(Q\\)). After each episode we update \\(Q\\) from empirical returns and then improve \\(\\pi\\). Return from time \\(t\\): \\[ g_t = r_t + \\gamma r_{t+1} + \\dots + \\gamma^{T-t} r_T = \\sum_{k=0}^{T-t} \\gamma^{k} r_{t+k}. \\] First-visit MC update (common choice): \\[\\begin{equation} Q(s_t,a_t) \\;\\leftarrow\\; Q(s_t,a_t) + \\alpha_{N(s_t,a_t)}\\!\\left(g_t - Q(s_t,a_t)\\right), \\tag{2.29} \\end{equation}\\] applied only on the first occurrence of \\((s_t,a_t)\\) in the episode. Sample-average learning uses \\(\\alpha_n = 1/n\\) per pair; more generally, use diminishing stepsizes. Policy improvement (\\(\\varepsilon\\)-greedy): \\[\\begin{equation} \\pi(a|s) \\;=\\; \\begin{cases} 1-\\varepsilon + \\dfrac{\\varepsilon}{|\\mathcal{A}(s)|}, &amp; a \\in \\arg\\max_{a&#39;} Q(s,a&#39;), \\\\ \\dfrac{\\varepsilon}{|\\mathcal{A}(s)|}, &amp; \\text{otherwise}. \\end{cases} \\tag{2.30} \\end{equation}\\] Theoretical Guarantees. Assume a tabular episodic MDP and \\(\\gamma \\in [0,1)\\). Convergence with Exploring Starts. If every state–action pair has nonzero probability of being the first pair of an episode (using ES), and each \\(Q(s,a)\\) is updated toward the true mean return from \\((s,a)\\) (e.g., via sample averages), then repeated policy evaluation and greedy improvement converge with probability 1 to an optimal deterministic policy. (If one uses an \\(\\varepsilon\\)-greedy improvement, then it converges to an optimal \\(\\varepsilon\\)-soft policy.) Convergence with \\(\\varepsilon\\)-soft GLIE behavior. If the behavior policy is GLIE—every \\((s,a)\\) is visited infinitely often and \\(\\epsilon_t \\to 0\\)—and the stepsizes for each \\((s,a)\\) satisfy the Robbins–Monro conditions \\(\\sum_{t} \\alpha_t(s,a) = \\infty,\\sum_{t} \\alpha_t(s,a)^2 &lt; \\infty\\), then \\(Q(s,a)\\) converges to \\(Q^\\star(s,a)\\) for all pairs visited infinitely often, and the \\(\\varepsilon\\)-greedy policy converges almost surely to an optimal policy. Remark. Unbiased but high-variance. MC targets \\(g_t\\) are unbiased estimates of action values under the current policy, but can have high variance—especially for long horizons—so convergence can be slower than TD methods. Keeping \\(\\varepsilon&gt;0\\) ensures exploration but limits asymptotic optimality to the best \\(\\varepsilon\\)-soft policy; hence \\(\\varepsilon_t \\downarrow 0\\) (GLIE) is recommended for optimality. 2.1.3.2 SARSA (On-Policy TD Control) High-level Intuition. Goal. Turn evaluation into control by updating action values online and improving the same policy that generates data. Key idea. Replace Monte Carlo returns with a bootstrapped target. After taking action \\(a_t\\) in state \\(s_t\\) and observing \\(r_{t}, s_{t+1}\\), sample the next action \\(a_{t+1}\\) from the current policy and update toward \\(r_{t} + \\gamma Q(s_{t+1}, a_{t+1})\\). On-policy nature. SARSA evaluates the behavior policy itself, typically an \\(\\varepsilon\\)-greedy policy w.r.t. \\(Q\\). Exploration. Use \\(\\varepsilon\\)-soft behavior so every action keeps nonzero probability. For optimality, let \\(\\varepsilon_t \\downarrow 0\\) to obtain GLIE (Greedy in the Limit with Infinite Exploration). Algorithmic Form. Let \\(Q\\) be a tabular action-value function and \\(\\pi_t\\) be \\(\\varepsilon_t\\)-greedy w.r.t. \\(Q_t\\). TD target and error: \\[\\begin{equation} y_t = r_{t} + \\gamma Q(s_{t+1}, a_{t+1}), \\qquad \\delta_t = y_t - Q(s_t, a_t). \\tag{2.31} \\end{equation}\\] SARSA update (one-step): \\[\\begin{equation} Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha_t(s_t,a_t)\\, \\delta_t. \\tag{2.32} \\end{equation}\\] \\(\\varepsilon\\)-greedy policy improvement: \\[\\begin{equation} \\pi_{t+1}(a\\mid s) = \\begin{cases} 1-\\varepsilon_{t+1} + \\dfrac{\\varepsilon_{t+1}}{|\\mathcal A(s)|}, &amp; a \\in \\arg\\max_{a&#39;} Q_{t+1}(s,a&#39;),\\\\ \\dfrac{\\varepsilon_{t+1}}{|\\mathcal A(s)|}, &amp; \\text{otherwise.} \\end{cases} \\tag{2.33} \\end{equation}\\] Variants. Expected SARSA replaces the sampled \\(a_{t+1}\\) by its expectation under \\(\\pi_t\\) for lower variance: \\[\\begin{equation} y_t = r_{t} + \\gamma \\sum_a \\pi_t(a\\mid s_{t+1}) Q(s_{t+1}, a). \\tag{2.34} \\end{equation}\\] \\(n\\)-step SARSA and SARSA(\\(\\lambda\\)) blend multi-step targets; these trade bias and variance similarly to MC vs TD. Convergence Guarantees. Assume a finite MDP, \\(\\gamma \\in [0,1)\\), asynchronous updates, and that each state–action pair is visited infinitely often. GLIE convergence to optimal policy. If the behavior is GLIE, i.e., \\(\\varepsilon_t \\downarrow 0\\) while ensuring infinite exploration, and stepsizes satisfy the Robbins–Monro conditions, then \\(Q_t \\to Q^\\star\\) almost surely and the \\(\\varepsilon_t\\)-greedy behavior becomes greedy in the limit, yielding an optimal policy almost surely. 2.1.4 Off-Policy Control Off-policy methods learn about a target policy \\(\\pi\\) while following a (potentially different) behavior policy \\(b\\) to gather data. This decoupling is useful when: you want to reuse logged data collected by some \\(b\\) (e.g., a rule-based controller or a past system), you need safer exploration by restricting behavior \\(b\\) while aiming to evaluate or improve a different \\(\\pi\\), you want to learn about the greedy policy without executing it, which motivates algorithms like Q-learning. In this section we first cover off-policy policy evaluation with importance sampling, then show how it can be used to construct an off-policy Monte Carlo control scheme in the tabular case. Finally, we present Q-learning. 2.1.4.1 Importance Sampling for Policy Evaluation Motivation. Suppose we have episodes generated by a behavior policy \\(b\\), but we want the value of a different target policy \\(\\pi\\). For a state value this is \\(V^\\pi(s) = \\mathbb{E}_\\pi[g_t \\mid s_t=s]\\), and for action values \\(Q^\\pi(s,a) = \\mathbb{E}_\\pi[g_t \\mid s_t=s, a_t=a]\\), where \\[ g_t = \\sum_{k=0}^{T-t} \\gamma^{k} r_{t+k}. \\] Because the data come from \\(b\\), the naive sample average is biased. Importance sampling (IS) reweights returns so that expectations under \\(b\\) equal those under \\(\\pi\\). A basic support condition is required: \\[\\begin{equation} \\text{If } \\pi(a\\mid s) &gt; 0 \\text{ then } b(a\\mid s) &gt; 0 \\quad \\text{for all visited } (s,a). \\tag{2.35} \\end{equation}\\] This ensures that \\(\\pi\\) is absolutely continuous with respect to \\(b\\) on the experienced trajectories. Importance Sampling (episode-wise). Consider a trajectory starting at time \\(t\\): \\[ \\tau_t = (s_t, a_t, r_t, s_{t+1}, a_{t+1}, \\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T). \\] The probability of observing this trajectory conditioned on \\(s_t = s\\), under policy \\(\\pi\\), is \\[ \\mathbb{P}_{\\pi}[\\tau_t \\mid s_t = s] = \\pi(a_t \\mid s_t) P(s_{t+1} \\mid s_t, a_t) \\pi(a_{t+1} \\mid s_{t+1}) \\cdots \\pi(a_{T-1} \\mid s_{T-1}) P(s_T \\mid s_{T-1}, a_{T-1}). \\] The probability of observing the same trajectory conditioned on \\(s_t = s\\), under policy \\(b\\), is \\[ \\mathbb{P}_{b}[\\tau_t \\mid s_t = s] = b(a_t \\mid s_t) P(s_{t+1} \\mid s_t, a_t) b(a_{t+1} \\mid s_{t+1}) \\cdots b(a_{T-1} \\mid s_{T-1}) P(s_T \\mid s_{T-1}, a_{T-1}). \\] Since the return \\(g_t\\) is a deterministic function of \\(\\tau_t\\), i.e., applying the reward function \\(R\\) to state-action pairs, we have that \\[\\begin{equation} \\begin{split} V^\\pi (s) &amp; = \\mathbb{E}_{\\pi}[g_t \\mid s_t = s] = \\sum_{\\tau_t} g_t \\mathbb{P}_\\pi [\\tau_t \\mid s_t = s] \\\\ &amp; = \\sum_{\\tau_t} g_t \\mathbb{P}_b[\\tau_t \\mid s_t = s] \\left(\\frac{\\mathbb{P}_\\pi [\\tau_t \\mid s_t = s]}{\\mathbb{P}_b[\\tau_t \\mid s_t = s]} \\right) \\\\ &amp; = \\sum_{\\tau_t} \\left( \\frac{\\pi(a_t \\mid s_t) \\pi(a_{t+1} \\mid s_{t+1}) \\cdots \\pi(a_{T-1} \\mid s_{T-1}) }{b(a_t \\mid s_t) b(a_{t+1} \\mid s_{t+1}) \\cdots b(a_{T-1} \\mid s_{T-1})} \\right) g_t \\mathbb{P}_b [\\tau_t \\mid s_t = s] \\end{split} \\tag{2.36} \\end{equation}\\] Therefore, define the likelihood ratio \\[\\begin{equation} \\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)}, \\tag{2.37} \\end{equation}\\] we have \\[\\begin{equation} V^\\pi(s) = \\mathbb{E}_b\\left[\\rho_{t:T-1} g_t \\mid s_t=s\\right]. \\tag{2.38} \\end{equation}\\] Similarly, we have \\[\\begin{equation} Q^\\pi(s,a) = \\mathbb{E}_b\\!\\left[\\rho_{t:T-1} g_t \\mid s_t=s, a_t=a\\right]. \\tag{2.39} \\end{equation}\\] Given \\(n\\) episodes, the ordinary IS estimator for \\(Q^\\pi\\) at the first visit of \\((s,a)\\) is \\[ \\hat Q_n^{\\text{IS}}(s,a) = \\frac{1}{N_n(s,a)} \\sum_{i=1}^n \\mathbf{1}\\{(s,a)\\text{ visited}\\}\\, \\rho_{t_i:T_i-1}^{(i)}\\, g_{t_i}^{(i)}, \\] where \\(N_n(s,a)\\) counts the number of first visits of \\((s,a)\\). In words, to estimate the \\(Q\\) value of the target policy \\(\\pi\\) using trajectories of the behavior policy \\(b\\), we need to reweight the return \\(g_t\\) by the likelihood ratio \\(\\rho_{t:T-1}\\). Note that the likelihood ratio does not require knowledge about the transition dynamics. Algorithmic Form: Off-policy Monte Carlo Policy Evaluation. Input: behavior \\(b\\), target \\(\\pi\\), episodes from \\(b\\) For each episode \\((s_0,a_0,r_0,s_1,\\dots,s_{T-1},a_{T-1},r_{T-1},s_T)\\): For \\(t=T-1,\\dots,0\\) compute episode-wise likelihood ratio \\(\\rho_{t:T-1}\\) and return \\(g_t\\), For first visits of \\((s_t,a_t)\\), update \\[ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha_{N(s_t,a_t)}\\big(\\rho_{t:T-1} g_t - Q(s_t,a_t)\\big). \\] Use sample averages \\(\\alpha_n=1/n\\) or Robbins-Monro stepsizes. Guarantees. Under the support condition and finite variance assumptions, ordinary IS is unbiased and converges almost surely to \\(Q^\\pi\\). 2.1.4.2 Off-Policy Monte Carlo Control High-level Intuition. We wish to improve a target policy \\(\\pi\\) toward optimality while behaving with a different exploratory policy \\(b\\). We evaluate \\(Q^\\pi\\) off-policy using IS on data from \\(b\\), then set \\(\\pi\\) greedy with respect to the updated \\(Q\\). Keep \\(b\\) sufficiently exploratory (for coverage), for example \\(\\varepsilon\\)-greedy with a fixed \\(\\varepsilon&gt;0\\) or a GLIE schedule. Algorithmic Form. Initialize \\(Q(s,a)\\) arbitrarily. Set target \\(\\pi\\) to be greedy w.r.t. \\(Q\\). Choose an exploratory behavior \\(b\\) that ensures coverage, e.g., \\(\\varepsilon\\)-greedy w.r.t. \\(Q\\) with \\(\\varepsilon&gt;0\\). Loop over iterations \\(i=0,1,2,\\dots\\): Data collection under \\(b\\): generate a batch of episodes using \\(b\\). Off-policy evaluation of \\(\\pi\\): for each episode, compute IS targets for first visits of \\((s_t,a_t)\\) and update \\(Q\\) using either ordinary IS Policy improvement: set for all states \\[ \\pi_{i+1}(s) \\in \\arg\\max_{a} Q(s,a). \\] Optionally update \\(b\\) to remain exploratory, for example \\(b\\) \\(\\leftarrow\\) \\(\\varepsilon\\)-greedy w.r.t. \\(Q\\) with a chosen \\(\\varepsilon\\) or a GLIE decay. Convergence Guarantees. Evaluation step: With the support condition and appropriate stepsizes, off-policy MC prediction converges almost surely to \\(Q^\\pi\\) when using ordinary IS. Control in the batch GPI limit: If each evaluation step produces estimates that converge to the exact \\(Q^{\\pi_i}\\) before improvement, then by the policy improvement theorem the sequence of greedy target policies \\(\\pi_i\\) converges to an optimal policy in finite MDPs. Remark. Choice of \\(b\\). A common and simple choice is an \\(\\varepsilon\\)-greedy behavior \\(b\\) w.r.t. current \\(Q\\) that maintains \\(\\varepsilon&gt;0\\) for coverage or uses GLIE so that \\(\\varepsilon_t \\downarrow 0\\) while all pairs are still visited infinitely often. 2.1.4.3 Q-Learning High-Level Intuition. What it learns. Q-Learning seeks the fixed point of the Bellman optimality operator \\[ (\\mathcal T^\\star Q)(s,a) = \\mathbb E\\big[ r_{t} + \\gamma \\max_{a&#39;} Q(s_{t+1}, a&#39;) \\mid s_t=s, a_t=a \\big], \\] whose unique fixed point is \\(Q^\\star\\). Because \\(\\mathcal T^\\star\\) is a \\(\\gamma\\)-contraction in \\(\\|\\cdot\\|_\\infty\\), repeatedly applying it converges to \\(Q^\\star\\) in the tabular case. Why off-policy. We can behave with any sufficiently exploratory policy \\(b\\) (e.g., \\(\\varepsilon\\)-greedy w.r.t. current \\(Q\\)) but learn from the greedy target \\(\\max_{a&#39;} Q(s&#39;,a&#39;)\\). No importance sampling is needed. Algorithmic Form. Let \\(Q\\) be a tabular action-value function. At each step observe a transition \\((s_t, a_t, r_{t}, s_{t+1})\\) generated by a behavior policy \\(b_t\\) (typically \\(\\varepsilon_t\\)-greedy w.r.t. \\(Q_t\\)). Target and TD error \\[ y_t = r_{t} + \\gamma \\max_{a&#39;} Q(s_{t+1}, a&#39;), \\qquad \\delta_t = y_t - Q(s_t, a_t). \\] Update \\[ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha_t(s_t,a_t)\\, \\delta_t. \\] Behavior (exploration) Use \\(\\varepsilon_t\\)-greedy with \\(\\varepsilon_t\\) decaying (GLIE) or any scheme that ensures each \\((s,a)\\) is updated infinitely often. Convergence. In a finite MDP with \\(\\gamma \\in [0,1)\\), if each \\((s,a)\\) is updated infinitely often (sufficient exploration) and stepsizes satisfy Robbins-Monro conditions, then Q-Learning converges to \\(Q^\\star\\) with probability 1. 2.1.4.4 Double Q-Learning Motivation. Max operators tend to be optimistically biased when action values are noisy. Consider an example where in state \\(s\\) one can take two actions \\(1\\) and \\(2\\). The estimated Q function \\(\\hat{Q}(s, \\cdot)\\) has two values \\(+1\\) and \\(-1\\) with equal probability. In this case we have \\(Q(s,1) = Q(s,2) = \\mathbb{E}[\\hat{Q}(s,\\cdot)] = 0\\). Therefore, \\(\\max Q(s,a) = 0\\). However, the noisy estimated \\(\\hat{Q}(s,\\cdot)\\) has four outcomes with equal probabilities: \\[ (+1,-1), (+1,+1), (-1, +1), (-1,-1). \\] Therefore, we have \\[ \\mathbb{E}[\\max_a \\hat{Q}(s,a)] = \\frac{1}{4} (1 + 1 + 1 -1) = 1/2 &gt; \\max_a Q(s,a), \\] which overestimates the max \\(Q\\) value. In general, we have \\[ \\mathbb{E}[\\max_a \\hat{Q}(s,a)] \\geq \\max_a \\mathbb{E} [\\hat{Q}(s,a)] = \\max_a Q(s,a), \\] where the estimates \\(\\hat{Q}\\) are noisy (try to prove this on your own). In Q-Learning the target \\[ y_t = r_{t} + \\gamma \\max_{a&#39;} Q(s_{t+1}, a&#39;) \\] can therefore overestimate action values and slow learning or push policies toward risky actions. Double Q-Learning reduces this bias by decoupling selection from evaluation: maintain two independent estimators \\(Q^A\\) and \\(Q^B\\). Use one to select the greedy action and the other to evaluate it, and alternate which table you update. This weakens the statistical coupling that creates overestimation. Algorithmic Form. Keep two tables \\(Q^A, Q^B\\). Use an \\(\\varepsilon\\)-greedy behavior policy with respect to a combined estimate, e.g., \\(Q^{\\text{avg}}= \\tfrac12(Q^A+Q^B)\\) or \\(Q^A+Q^B\\). At each step observe \\((s_t, a_t, r_{t}, s_{t+1})\\). With probability \\(1/2\\) update \\(Q^A\\), else update \\(Q^B\\). Update \\(Q^A\\): \\[ a^\\star = \\arg\\max_{a&#39;} Q^A(s_{t+1}, a&#39;),\\qquad y_t = r_{t} + \\gamma\\, Q^B(s_{t+1}, a^\\star), \\] \\[ Q^A(s_t, a_t) \\leftarrow Q^A(s_t, a_t) + \\alpha_t(s_t,a_t)\\big[y_t - Q^A(s_t, a_t)\\big]. \\] Update \\(Q^B\\): \\[ a^\\star = \\arg\\max_{a&#39;} Q^B(s_{t+1}, a&#39;),\\qquad y_t = r_{t} + \\gamma\\, Q^A(s_{t+1}, a^\\star), \\] \\[ Q^B(s_t, a_t) \\leftarrow Q^B(s_t, a_t) + \\alpha_t(s_t,a_t)\\big[y_t - Q^B(s_t, a_t)\\big]. \\] Behavior policy (\\(\\varepsilon\\)-greedy): choose \\(a_t \\sim \\varepsilon\\)-greedy with respect to \\(Q^{\\text{avg}}(s_t,\\cdot)\\). A GLIE schedule \\(\\varepsilon_t \\downarrow 0\\) is standard. Acting and planning: for greedy actions or plotting a single estimate, use \\(Q^{\\text{avg}} = \\tfrac12(Q^A+Q^B)\\). Convergence. Tabular setting. In a finite MDP with \\(\\gamma \\in [0,1)\\), bounded rewards, sufficient exploration so that every \\((s,a)\\) is updated infinitely often, and Robbins–Monro stepsizes for each pair. Double Q-Learning converges with probability 1 to \\(Q^\\star\\). Example 2.2 (Value-based RL for Grid World) Consider the following \\(5 \\times 5\\) grid with \\((0,4)\\) being the goal and the terminal state. At every state, the agent can take four actions: left, right, up, and down. There is a wall in the gray area shown in Fig. 2.3. Upon hitting the wall, the agent stays in the original cell. Every action incurs a reward of \\(-1\\). Once the agent arrives at the goal state, reward stays at 0. Figure 2.3: Grid World We run Generalized Policy Iteration (GPI) with Monte Carlo (on-policy), SARSA, Expected SARSA, Q-Learning, and Double Q-Learning on this problem with diminishing learning rates. Fig. 2.4 plots the error between the estimated Q values (of different algorithms) and the ground-truth optimal Q value (obtained from value iteration with known transition dynamics). Except Monte Carlo control which converges slowly, the other methods converge fast. Figure 2.4: Convergence of Estimated Q Values. From the final estimated Q value, we can extract a greedy policy, visualized below. You can play with the code here. MC Control: &gt; &gt; &gt; &gt; G ^ # ^ ^ ^ v # ^ ^ ^ v # &gt; ^ ^ &gt; &gt; &gt; &gt; ^ SARSA: &gt; &gt; &gt; &gt; G ^ # &gt; &gt; ^ ^ # ^ ^ ^ ^ # ^ ^ ^ &gt; &gt; ^ ^ ^ Expected SARSA: &gt; &gt; &gt; &gt; G ^ # &gt; &gt; ^ ^ # ^ ^ ^ ^ # &gt; &gt; ^ &gt; &gt; ^ ^ ^ Q-Learning: &gt; &gt; &gt; &gt; G ^ # ^ ^ ^ ^ # ^ ^ ^ ^ # ^ ^ ^ &gt; &gt; ^ ^ ^ Double Q-Learning: &gt; &gt; &gt; &gt; G ^ # &gt; ^ ^ ^ # &gt; ^ ^ ^ # ^ ^ ^ &gt; &gt; &gt; &gt; ^ 2.2 Function Approximation Many reinforcement learning problems have continuous state spaces–think of mechanical systems like robot arms, legged locomotion, drones, and autonomous vehicles. In these domains the state \\(s\\) (e.g., joint angles/velocities, poses) lives in \\(\\mathbb{R}^n\\), which makes a tabular representation of the value functions impossible. In this case, we must approximate values with parameterized functions. 2.2.1 Basics of Continuous MDP In a continuous MDP, at least one of the state space or the action space is a continuous space. Suppose \\(\\mathcal{S} \\subseteq \\mathbb{R}^n\\) and \\(\\mathcal{A} \\subseteq \\mathbb{R}^{m}\\) are both continuous spaces. The environment kernel \\(P(\\cdot \\mid s, a)\\) is a Markov kernel from \\(\\mathcal{S} \\times \\mathcal{A}\\) to \\(\\mathcal{S}\\): for each state-action pair \\((s,a)\\), \\(P(\\cdot \\mid s,a)\\) is a probability measure on \\(\\mathcal{S}\\). For each Borel set \\(B \\subseteq \\mathcal{S}\\), the map \\((s,a) \\mapsto P(B \\mid s, a)\\) is measurable. For example, \\(P(\\mathcal{S} \\mid s, a) = 1\\) for any \\((s,a)\\). The policy kernel \\(\\pi(\\cdot \\mid s)\\) is a stochastic kernel from \\(\\mathcal{S}\\) to \\(\\mathcal{A}\\): for each \\(s\\), \\(\\pi(\\cdot \\mid s)\\) is a probability measure on \\(\\mathcal{A}\\). Induced State-Transition Kernel. For notational convenience, given a policy and the environment kernel \\(P\\), we define a state-only Markov kernel \\[\\begin{equation} P^\\pi(B \\mid s) := \\int_{\\mathcal{A}} P(B \\mid s, a) \\pi(da \\mid s), \\quad B \\subseteq \\mathcal{S}. \\tag{2.40} \\end{equation}\\] In words, \\(P^\\pi(B \\mid s)\\) measures the probability of landing at a set \\(B\\) starting from state \\(s\\), under all actions possible for the policy \\(\\pi\\). If densities exist, i.e., \\(P(ds&#39; \\mid s, a) = p(s&#39; \\mid s, a) ds&#39;\\) and \\(\\pi(da \\mid s) = \\pi(a \\mid s) da\\), then, \\[\\begin{equation} p^\\pi(s&#39; \\mid s) := \\int_{\\mathcal{A}} p(s&#39; \\mid s, a) \\pi(a \\mid s) da \\quad\\text{and}\\quad P^{\\pi}(d s&#39; \\mid s) = p^\\pi(s&#39; \\mid s) ds&#39;. \\tag{2.41} \\end{equation}\\] Stationary State Distribution. A probability measure \\(\\mu^\\pi\\) on \\(\\mathcal{S}\\) is called stationary for the state-transition kernel \\(P^\\pi\\) if and only if \\[\\begin{equation} \\mu^{\\pi}(B) = \\int_{\\mathcal{S}} P^\\pi(B \\mid s) \\mu^{\\pi}(ds), \\quad \\forall B \\subseteq \\mathcal{S}. \\tag{2.42} \\end{equation}\\] If a density \\(\\mu^\\pi(s)\\) exists, then the above equation is the followng condition \\[\\begin{equation} \\mu^{\\pi}(s&#39;) = \\int_{\\mathcal{S}} p^\\pi(s&#39; \\mid s) \\mu^{\\pi}(s) ds. \\tag{2.43} \\end{equation}\\] In words, the state distribution \\(\\mu^\\pi\\) does not change under the state-transition kernel \\(P^\\pi\\) (e.g., if a state \\(A\\) has probability \\(0.1\\) of being visited at time \\(t\\), the probability of visiting \\(A\\) in the next time step remains \\(0.1\\), under policy \\(\\pi\\)). Under standard ergodicity assumptions, this stationary state distribution \\(\\mu^\\pi\\) exists and is unique (after sufficient steps, the initial state distribution does not matter and the state distribution follows \\(\\mu^\\pi\\)). Moreover, the empirical state distribution converge to \\(\\mu^\\pi\\). 2.2.2 Policy Evaluation For simplicity, let us first relax the state space to be a continuous space \\(\\mathcal{S} \\subseteq \\mathbb{R}^n\\). We assume the action space \\(\\mathcal{A}\\) is still finite with \\(|\\mathcal{A}|\\) elements. We first consider the problem of policy evaluation, i.e., estimate the value functions associated with a policy \\(\\pi\\) from interaction data with the environment. Bellman Consistency. Given a policy \\(\\pi: \\mathcal{S} \\mapsto \\Delta(\\mathcal{A})\\), its associated state-value function \\(V^\\pi\\) must satisfy the following Bellman Consistency equation \\[\\begin{equation} V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\int_{\\mathcal{S}} V(s&#39;) P(d s&#39; \\mid s, a) \\right]. \\tag{2.44} \\end{equation}\\] Notice that since \\(\\mathcal{S}\\) is a continuous space, we need to replace “\\(\\sum_{s&#39; \\in \\mathcal{S}}\\)” with “\\(\\int_{\\mathcal{S}}\\)”. If \\(P(d s&#39; \\mid s, a)\\) has a density \\(p(s&#39; \\mid s, a)\\), the above Bellman consistency equation also reads \\[\\begin{equation} V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\int_{\\mathcal{S}} V(s&#39;) p(s&#39; \\mid s, a) ds&#39; \\right]. \\tag{2.45} \\end{equation}\\] Bellman Operator. Define the Bellman operator \\(T^\\pi\\) acting on any bounded measurable function \\(V:\\mathcal{S}\\to\\mathbb{R}\\) by \\[\\begin{equation} (T^\\pi V)(s) = \\sum_{a\\in\\mathcal{A}} \\pi(a\\mid s)\\left[ R(s,a) + \\gamma \\int_{\\mathcal{S}} V(s&#39;) P(ds&#39;\\mid s,a)\\right]. \\tag{2.46} \\end{equation}\\] Then \\(V^\\pi\\) is the unique fixed point of \\(T^\\pi\\), i.e., \\(V^\\pi = T^\\pi V^\\pi\\). Moreover, when rewards are uniformly bounded and \\(\\gamma\\in[0,1)\\), \\(T^\\pi\\) is a \\(\\gamma\\)-contraction under the sup-norm and is monotone. Approximate Value Function. In large/continuous state spaces we restrict attention to a parametric family \\({V(\\cdot;\\theta): \\theta\\in\\mathbb{R}^d}\\) and learn \\(\\theta\\) from data. We use \\(\\nabla_\\theta V(s;\\theta) \\in \\mathbb{R}^d\\) to denote the gradient of \\(V\\) with respect to \\(\\theta\\) at state \\(s\\). A special and very important case is linear function approximation \\[\\begin{equation} V(s;\\theta) = \\theta^\\top \\phi(s), \\tag{2.47} \\end{equation}\\] where \\(\\phi(s) = [\\phi_1(s),\\ldots,\\phi_d(s)]^\\top\\) are fixed basis functions (e.g., neural network last-layer features). When \\(V(s;\\theta) = \\theta^{\\top} \\phi(s)\\), we have \\[ \\nabla_\\theta V(s;\\theta) = \\phi(s). \\] When we restrict the value function to a function class (e.g., linear features or a neural network), it is generally not guaranteed that the unique fixed point of the Bellman operator (2.46), namely \\(V^\\pi\\), belongs to that class. This misspecification (or realizability gap) means we typically cannot recover \\(V^\\pi\\) exactly; instead, we seek its best approximation according to a chosen criterion. 2.2.2.1 Monte Carlo Estimation Given an episode \\((s_t,a_t,r_t,\\dots,s_T)\\) collected by policy \\(\\pi\\), its discounted return \\[ g_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t} r_T \\] is an unbiased estimate of the value at \\(s_t\\), i.e., \\(V^{\\pi}(s_t)\\). Therefore, Monte Carlo estimation follows the intuitive idea to make the approximate value function \\(V(\\cdot, \\theta)\\) fit the returns from these episodes as close as possible: \\[\\begin{equation} \\min_{\\theta} \\frac{1}{\\mathcal{D}} \\sum_{t \\in \\mathcal{D}} \\frac{1}{2} (g_t - V(s_t, \\theta))^2, \\tag{2.48} \\end{equation}\\] where \\(\\mathcal{D}\\) denotes the dataset of episodes collected under policy \\(\\pi\\). The formulation (2.48) is a formulation in the sense that it waits until all episodes are collected before performing the optimization. In an online formulation, we can optimize after every episode the objective function \\[ \\min_{\\theta} \\frac{1}{2} (g_t - V(s_t, \\theta))^2, \\] which leads to one step of gradient descent: \\[\\begin{equation} \\theta \\ \\ \\leftarrow \\ \\ \\theta + \\alpha_t (g_t - V(s_t;\\theta)) \\nabla_\\theta V(s_t; \\theta). \\tag{2.49} \\end{equation}\\] To connect the above update back to the MC update (2.4) in the tabular case, we see that the term \\(g_t - V(s_t;\\theta\\) is similar as before the difference between the target and the current estimate. However, in the case of function approximation, the error is multiplied by the gradient \\(\\nabla_\\theta V (s_t; \\theta)\\). It is worth noting that when using function approximation, the update on \\(\\theta\\) caused by one episode \\((s_t,\\dots)\\) will affect the values at all other states even if the policy only visited state \\(s_t\\). Convergence Guarantees. Assume on-policy sampling under \\(\\pi\\), bounded rewards, and step sizes \\(\\alpha_t\\) satisfying Robbins–Monro conditions. For linear \\(V(s;\\theta)=\\theta^\\top\\phi(s)\\) with full-rank features, i.e., \\[ \\mathbb{E}_{s \\sim \\mu^\\pi} \\left[ \\phi(s) \\phi(s)^\\top \\right] \\succ 0, \\] and \\(\\mathbb{E}_{s \\sim \\mu^\\pi}\\|\\phi(s)\\|^2&lt;\\infty\\), the iterates converge almost surely to the unique global minimizer of the convex objective \\[\\begin{equation} \\theta_{\\text{MC}}^\\star \\in \\arg\\min_\\theta \\; \\frac{1}{2}\\mathbb{E}_{s_t \\sim \\mu^\\pi}\\!\\left[ \\big(V(s_t;\\theta)-V^\\pi(s_t)\\big)^2 \\right], \\tag{2.50} \\end{equation}\\] where the expectation is with respect to stationary state distribution \\(\\mu^\\pi\\) under \\(\\pi\\). For nonlinear differentiable function classes with bounded gradients, the iterates converge almost surely to a stationary point of the same objective. Correction: Since Monte Carlo Estimation can be seen as performing Stochastic Gradient Descent on the objective in (2.50), to guarantee convergence to a first-order stationary point, we need some technical conditions: (a) diminishing step sizes satisfying the Robbins-Monro condition; (b) bounded second-order moment of the stochastic gradient; and (c) \\(L\\)-smoothness of the objective. 2.2.2.2 Semi-Gradient TD(0) We know from previous discussion that MC uses the full return \\(g_t\\) as the target and thus can have high variance. A straightforward idea is to replace the MC target \\(g_t\\) in the update (2.49) by the one-step bootstrap target \\[ r_t + \\gamma V(s_{t+1};\\theta), \\] which yields the semi-gradient TD(0) update \\[\\begin{equation} \\theta \\ \\leftarrow\\ \\theta \\;+\\; \\alpha_t \\,\\big(r_t + \\gamma V(s_{t+1};\\theta) - V(s_t;\\theta)\\big)\\, \\nabla_\\theta V(s_t;\\theta). \\tag{2.51} \\end{equation}\\] (At terminal \\(s_{t+1}\\), use \\(V(s_{t+1};\\theta)=0\\) or equivalently set \\(\\gamma=0\\) for that step.) Why call it “semi-gradient”? Let the TD error be \\[ \\delta_t(\\theta) \\;:=\\; r_t + \\gamma V(s_{t+1};\\theta) - V(s_t;\\theta). \\] Consider the per-sample squared TD error objective \\[ \\min_{\\theta}\\; \\frac{1}{2} \\,\\delta_t(\\theta)^2. \\] Its true gradient (a.k.a. the residual gradient) is \\[ \\nabla_\\theta \\frac{1}{2} \\delta_t(\\theta)^2 \\;=\\; \\delta_t(\\theta)\\,\\big(\\gamma \\nabla_\\theta V(s_{t+1};\\theta) - \\nabla_\\theta V(s_t;\\theta)\\big). \\] Thus a true-gradient (residual-gradient) TD(0) step would be \\[\\begin{equation} \\theta \\ \\leftarrow\\ \\theta \\;-\\; \\alpha_t \\,\\delta_t(\\theta)\\,\\big(\\gamma \\nabla_\\theta V(s_{t+1};\\theta) - \\nabla_\\theta V(s_t;\\theta)\\big). \\tag{2.52} \\end{equation}\\] By contrast, the semi-gradient TD(0) step in (2.51) ignores the dependence of the target on \\(\\theta\\) (i.e., it drops the \\(\\gamma \\nabla_\\theta V(s_{t+1};\\theta)\\) term) and treats the target \\(r_t+\\gamma V(s_{t+1};\\theta)\\) as a constant when differentiating. Concretely, \\[ \\nabla_\\theta \\frac{1}{2} \\big( \\text{target} - V(s_t;\\theta)\\big)^2 \\;\\approx\\; -\\big(\\text{target} - V(s_t;\\theta)\\big)\\,\\nabla_\\theta V(s_t;\\theta). \\] This approximation yields the simpler update (2.51). Convergence Guarantees. When using linear approximation, the Monte Carlo estimator converges to \\(\\theta^\\star_{\\text{MC}}\\) in (2.50). We now study what the semi-gradient TD(0) updates (2.51) converge to. Projected Bellman Operator. Fix a weighting/visitation distribution \\(\\mu\\) on \\(\\mathcal S\\) (e.g., the stationary distribution \\(\\mu^\\pi\\)) and the associated inner product \\[ \\langle f,g\\rangle_\\mu := \\mathbb{E}_{s\\sim \\mu}[\\,f(s)g(s)\\,], \\qquad \\|f\\|_\\mu := \\sqrt{\\langle f,f\\rangle_\\mu}. \\] Let \\(\\mathcal V := \\{V(s;\\theta)=\\theta^\\top\\phi(s)\\;:\\;\\theta\\in\\mathbb{R}^d\\}\\) be the linear function class spanned by features \\(\\phi:\\mathcal S\\to\\mathbb{R}^d\\). The \\(\\mu\\)-orthogonal projection \\(\\Pi_\\mu:\\mathcal{F}\\to\\mathcal V\\) is \\[ \\Pi_\\mu f \\;:=\\; \\arg\\min_{V\\in\\mathcal V}\\, \\| V - f\\|_\\mu . \\] In words, given any function \\(f \\in \\mathcal{F}: \\mathcal{S} \\mapsto \\mathbb{R}\\), \\(\\Pi_\\mu f\\) returns the closest function \\(V\\) to \\(f\\) that belongs to the subset of linearly representable functions \\(\\mathcal{V}\\), where the “closest” is defined by the weighting distribution \\(\\mu\\). The Projected Bellman Operator is the composition \\[\\begin{equation} \\mathcal{T}^\\pi_{\\!\\text{proj}} \\;:=\\; \\Pi_\\mu \\, T^\\pi, \\qquad\\text{i.e.,}\\qquad \\big(\\mathcal{T}^\\pi_{\\!\\text{proj}} V\\big)(\\cdot) \\;=\\; \\Pi_\\mu \\!\\left[\\, T^\\pi V \\,\\right](\\cdot). \\tag{2.53} \\end{equation}\\] \\(T^\\pi\\) is the Bellman operator defined in (2.46). \\(\\Pi_\\mu\\) projects any function onto \\(\\mathcal V\\) using the \\(\\mu\\)-weighted \\(L^2\\) norm. In discrete \\(\\mathcal S\\), write \\(\\Phi\\in\\mathbb{R}^{|\\mathcal S|\\times d}\\) with rows \\(\\phi(s)^\\top\\) and \\(D=\\mathrm{diag}(\\mu(s))\\). Then \\[ \\Pi_\\mu f \\;=\\; \\Phi\\,(\\Phi^\\top D \\Phi)^{-1}\\Phi^\\top D f. \\] \\(T^\\pi\\) is a \\(\\gamma\\)-contraction under \\(\\|\\cdot\\|_\\mu\\), and \\(\\Pi_\\mu\\) is nonexpansive under \\(\\|\\cdot\\|_\\mu\\), hence \\(\\mathcal{T}^\\pi_{\\!\\text{proj}}\\) is a \\(\\gamma\\)-contraction: \\[ \\|\\Pi_\\mu T^\\pi V - \\Pi_\\mu T^\\pi U\\|_\\mu \\;\\le\\; \\|T^\\pi V - T^\\pi U\\|_\\mu \\;\\le\\; \\gamma \\|V-U\\|_\\mu. \\] Therefore, (2.53), the projected Bellman equation (PBE), has a unique fixed point \\(V_{\\text{TD}}^\\star\\in\\mathcal V\\) satisfying \\[\\begin{equation} V_{\\text{TD}}^\\star \\;=\\; \\Pi_\\mu\\, T^\\pi V_{\\text{TD}}^\\star. \\tag{2.54} \\end{equation}\\] Semi-gradient TD(0) Converges to the PBE Fixed Point (linear case). Assume on-policy sampling under an ergodic chain, bounded second moments, Robbins–Monro stepsizes, and full-rank features under \\(\\mu = \\mu^\\pi\\). In the linear case \\(V(s;\\theta)=\\theta^\\top\\phi(s)\\), define \\[ \\delta_t(\\theta) := r_t + \\gamma \\theta^\\top \\phi(s_{t+1}) - \\theta^\\top \\phi(s_t). \\] The semi-gradient TD(0) update (2.51) becomes \\[ \\theta \\leftarrow \\theta + \\alpha_t\\, \\delta_t(\\theta)\\,\\phi(s_t). \\] Taking conditional expectation w.r.t. the stationary visitation (and using the Markov property) yields the mean update: \\[ \\mathbb{E}\\!\\left[\\delta_t(\\theta)\\,\\phi(s_t)\\right] \\;=\\; b \\;-\\; A\\theta, \\] with the standard TD system \\[\\begin{equation} A \\;:=\\; \\mathbb{E}_{\\mu}\\!\\big[\\phi(s_t)\\big(\\phi(s_t)-\\gamma \\phi(s_{t+1})\\big)^\\top\\big], \\qquad b \\;:=\\; \\mathbb{E}_{\\mu}\\!\\big[r_{t}\\,\\phi(s_t)\\big]. \\tag{2.55} \\end{equation}\\] Thus, in expectation, TD(0) performs a stochastic approximation to the ODE \\[ \\dot\\theta \\;=\\; b - A\\theta, \\] whose unique globally asymptotically stable equilibrium is \\[ \\theta^\\star_{\\text{TD}} \\;=\\; A^{-1} b, \\] provided the symmetric part of \\(A\\) is positive definite (guaranteed on-policy with full-rank features). Standard stochastic approximation theory then gives \\[ \\theta_t \\xrightarrow{\\text{a.s.}} \\theta^\\star_{\\text{TD}}. \\] Finally, one can show the equivalence with the PBE: \\(V(\\cdot; \\theta) \\in\\mathcal V\\) satisfies \\(V(\\cdot; \\theta)=\\Pi_\\mu T^\\pi V(\\cdot; \\theta)\\) if and only if \\(A\\theta \\;=\\; b\\) (see a proof below). Hence the almost-sure limit \\(V(\\cdot; \\theta^\\star_{\\text{TD}})\\) is exactly the fixed point (2.54). Proof. The projected Bellman equation reads \\[ V(\\cdot;\\theta) = \\Pi_\\mu T^\\pi V(\\cdot; \\theta). \\] Since \\(V(\\cdot; \\theta) = \\theta^\\top \\phi(\\cdot)\\) is the orthogonal projection of \\(T^\\pi V(\\cdot; \\theta)\\) onto \\(\\mathcal V\\) weighted by \\(\\mu\\), we have that \\[ \\mathbb{E}_\\mu \\left[ \\phi(s_t) (T^\\pi V(s_t;\\theta) - V(s_t;\\theta)) \\right] = 0 = \\mathbb{E}_\\mu \\left[ \\phi(s_t) (r_t + \\gamma \\theta^\\top \\phi(s_{t+1}) - \\theta^\\top \\phi(s_t) )\\right], \\] which reduces to \\(A \\theta = b\\) with \\(A\\) and \\(b\\) defined in (2.55). What does convergence to the PBE fixed point imply? Best fixed point in the feature subspace (good). \\(V_{\\text{TD}}^\\star\\) is the unique function in \\(\\mathcal V\\) whose Bellman update \\(T^\\pi V\\) projects back to itself under \\(\\Pi_\\mu\\). If \\(V^\\pi\\in\\mathcal V\\) (realisable case), then \\(V_{\\text{TD}}^\\star=V^\\pi\\). Different target than least squares (mixed). TD(0) solves the Projected Bellman Equation (2.53); Monte Carlo least-squares solves \\[ \\min_{V\\in\\mathcal V} \\frac{1}{2} \\|V - V^\\pi\\|_\\mu^2. \\] When \\(V^\\pi\\notin\\mathcal V\\), these solutions generally differ. Either can have lower \\(\\mu\\)-weighted prediction error depending on features and dynamics; in practice TD often wins due to lower variance and online bootstrapping. 2.2.3 On-Policy Control 2.2.3.1 Semi-Gradient SARSA(0) High-level Intuition. Semi-gradient SARSA(0) is an on-policy value-based control method. It learns an action-value function \\(Q(s,a;\\theta)\\) by bootstrapping one step ahead and using the next action actually selected by the current behavior policy (e.g., \\(\\varepsilon\\)-greedy). Because the target uses \\(Q(s_{t+1},a_{t+1};\\theta)\\), SARSA trades some bias for substantially lower variance than Monte Carlo, updates online from each transition, and naturally couples policy evaluation (of the current policy) with policy improvement (by making the policy greedy/soft-greedy w.r.t. the current \\(Q\\)). Algorithmic Form (On-policy, Finite \\(\\mathcal A\\)). Let the behavior policy at time \\(t\\) be \\(\\pi_t(\\cdot\\mid s)\\) (e.g., \\(\\varepsilon_t\\)-greedy w.r.t. \\(Q(\\cdot,\\cdot;\\theta_t)\\)). For each step: Given \\(s_t\\), pick \\(a_t \\sim \\pi_t(\\cdot \\mid s_t)\\); observe \\(r_t\\) and \\(s_{t+1}\\). Pick the next action \\(a_{t+1} \\sim \\pi_t(\\cdot\\mid s_{t+1})\\). Form the TD error \\[\\begin{equation} \\delta_t \\;=\\; r_t \\;+\\; \\gamma\\, Q(s_{t+1},a_{t+1};\\theta) \\;-\\; Q(s_t,a_t;\\theta). \\tag{2.56} \\end{equation}\\] Update parameters with a semi-gradient step \\[\\begin{equation} \\theta \\;\\leftarrow\\; \\theta \\;+\\; \\alpha_t\\, \\delta_t \\,\\nabla_\\theta Q(s_t,a_t;\\theta). \\tag{2.57} \\end{equation}\\] For terminal \\(s_{t+1}\\), use \\(Q(s_{t+1},a_{t+1};\\theta)=0\\) (equivalently, set \\(\\gamma=0\\) on terminal transitions). Linear special case. If \\(Q(s,a;\\theta)=\\theta^\\top \\phi(s,a)\\), then \\(\\nabla_\\theta Q(s_t,a_t;\\theta)=\\phi(s_t,a_t)\\) and the update becomes \\[ \\theta \\leftarrow \\theta + \\alpha_t\\, \\delta_t\\, \\phi(s_t,a_t). \\] Expected SARSA (variance reduction). Replace the sample bootstrap by its expectation under \\(\\pi_t\\): \\[\\begin{equation} \\delta_t^{\\text{exp}} \\;=\\; r_t \\;+\\; \\gamma \\sum_{a&#39;\\in\\mathcal A} \\pi_t(a&#39;\\mid s_{t+1})\\, Q(s_{t+1},a&#39;;\\theta) \\;-\\; Q(s_t,a_t;\\theta), \\tag{2.58} \\end{equation}\\] then update \\(\\theta \\leftarrow \\theta + \\alpha_t\\, \\delta_t^{\\text{exp}}\\, \\nabla_\\theta Q(s_t,a_t;\\theta)\\). Update the next policy to be \\(\\varepsilon_{t+1}\\)-greedy w.r.t. the new Q value \\(Q(\\cdot, \\cdot; \\theta_{t+1})\\). (\\(\\varepsilon_t\\) follows GLIE.) Example 2.3 (Semi-Gradient SARSA for Mountain Car) Consider the Mountain Car problem from Gym illustrated in Fig. 2.5. The state space \\(\\mathcal{S} \\subset \\mathbb{R}^2\\) is continuous and contains the position of the car along the \\(x\\)-axis as well as the car’s velocity. The action space \\(\\mathcal{A}\\) is discrete and contains three elements: “\\(0\\): Accelerate to the left”, “\\(1\\): Don’t accelerate”, and “\\(2\\): Accelerate to the right”. The transition dynamics of the mountain car is: \\[\\begin{equation} \\begin{split} v_{t+1} &amp;= v_t + (a_t - 1) F - \\cos (3 p_t) g \\\\ p_{t+1} &amp;= p_t + v_{t+1} \\end{split} \\end{equation}\\] where \\((p_t, v_t)\\) denotes the state at time \\(t\\) with position and velocity, \\(a_t\\) denotes the action at time \\(t\\), \\(F=0.001\\) is the force and \\(g = 0.0025\\) is the gravitational constant. The goal is for the mountain car to reach the flag placed on top of the right hill as quickly as possible. Therefore, the agent is penalised with a reward of \\(-1\\) for each timestep. The position of the car is assigned a uniform random value in \\([-0.6 , -0.4]\\). The starting velocity of the car is always assigned to 0. In every episode, the agent is allowed a maximum of \\(200\\) steps (therefore, the worst per-episode return is \\(-200\\)). Figure 2.5: Mountain Car from Gym Naive Semi-Gradient SARSA. We first apply the semi-gradient SARSA algorithm introduced above to the mountain car problem. We parameterize the action value \\(Q\\) as a 2-layer multi-layer perceptron (MLP). Fig. 2.6 shows the average return per episode as training progreses. Clearly, the return stagnates at \\(-200\\) and the algorithm failed to learn. The rollout in Fig. 2.7 confirms that the final policy is not able to achieve the goal. You can find code for the naive semi-gradient SARSA algorithm here. Figure 2.6: Average return w.r.t. episode (Semi-Gradient SARSA) Figure 2.7: Example rollout (Semi-Gradient SARSA) Semi-Gradient SARSA with Experience Replay. Inspired by the technique of experience replay (ER) popularized by DQN (see 2.2.4.2), we incorporated ER into semi-gradient SARSA, which breaks its on-policy nature. Fig. 2.8 displays the learning curve, which shows steady increase of the per-episode return. Applying the final learned policy to the mountain car yields a successful trajectory to the top of the mountain. You can find code for the semi-gradient SARSA with experience replay algorithm here. Figure 2.8: Average return w.r.t. episode (Semi-Gradient SARSA + Experience Replay) Figure 2.9: Example rollout (Semi-Gradient SARSA + Experience Replay) 2.2.4 Off-Policy Control Off-policy control seeks to learn the optimal action–value function while collecting data under a different behavior policy (e.g., an \\(\\varepsilon\\)-soft policy). As in the tabular setting, Q-learning is the canonical off-policy control method. With function approximation, however, off-policy control becomes substantially harder than in the tabular case. To illustrate why, we first present off-policy semi-gradient TD(0) for policy evaluation and use Baird’s counterexample (Baird et al. 1995) to highlight the deadly triad: bootstrapping + function approximation + off-policy sampling can cause divergence. We then turn to the Deep Q-Network (DQN) (Mnih et al. 2015), which stabilizes Q-learning with two key mechanisms—experience replay and a target network—leading to landmark Atari results. Finally, we connect DQN to fitted Q-iteration (FQI) (Riedmiller 2005), a batch method with theoretical guarantees, to clarify why these stabilizations work (Fan et al. 2020). 2.2.4.1 Off-Policy Semi-Gradient TD(0) Setup. We aim to estimate the state-value function of a target policy \\(\\pi\\) using a different behavior policy \\(b\\). Since the state space is continuous, we employ function approximation to represent the value function as \\(V(s;\\theta)\\) with \\(\\theta \\in \\mathbb{R}^d\\). In the case of linear approximation, we have \\(V(s;\\theta) = \\theta^\\top \\phi(s)\\) where \\(\\phi(s)\\) is a function that featurizes the state. Semi-Gradient TD(0). Given a transition \\((s_t,a_t,r_t,s_{t+1})\\) collected under the behavior policy \\(b\\), form the TD error \\[ \\delta_t = r_t + \\gamma V(s_{t+1}; \\theta) - V(s_t;\\theta). \\] The off-policy Semi-Gradient TD(0) update reads \\[\\begin{equation} \\theta \\quad \\leftarrow \\quad \\theta + \\alpha_t \\rho_t \\delta_t \\nabla_\\theta V(s_t; \\theta), \\tag{2.59} \\end{equation}\\] where \\(\\rho_t\\) is the likelihood ratio as in (2.37): \\[ \\rho_t = \\frac{\\pi(a_t \\mid s_t)}{b(a_t \\mid s_t)}. \\] This off-policy semi-gradient TD(0) update (2.59) looks perfectly reasonable. However, the following Baird’s counterexample illustrates the instability of the algorithm. Baird’s Counterexample. Consider an MDP with 7 states containing 6 upper states and 1 lower state, as shown in Fig. 2.10 (the figure comes from (Sutton and Barto 1998)). There are two actions, one called “solid” and the other called “dashed”. If the agent picks “solid” at any state, then the system transitons to the lower state with probability 1. If the agent picks “dashed” at any state, then the system transitions to any one of the upper states with equal probability. All rewards are zero, and the discount factor is \\(\\gamma=0.99\\). The target policy \\(\\pi\\) always picks “solid”, while the behavior policy \\(b\\) picks “solid” with probability \\(1/7\\) and “dashed” with probability \\(6/7\\). Figure 2.10: Baird Counterexample Consider the case of linear function approximation where \\(V(s;w) = w^\\top \\phi(s)\\) where \\(w \\in \\mathbb{R}^8\\). For the upper states, the feature \\(\\phi(s)\\) leads to \\(V(s;w) = 2 w_1 + w_8\\), and for the lower state, the feature \\(\\phi(s)\\) leads to \\(V(s;w) = w_7 + 2w_8\\). This Python script implements the off-policy semi-gradient TD(0) algorithm with importance sampling for policy evaluation. Fig. 2.11 plots \\(\\Vert w \\Vert_2\\), the magnitude of \\(w\\), with respect to iterations. Clearly, we see the parameter \\(w\\) diverges under the off-policy semi-gradient TD(0) algorithm. Figure 2.11: Baird Counterexample: divergence The Deadly Triad. Three ingredients were used together in Baird’s example: Off-policy: a different behavior policy \\(b\\) is used to collect data for the evaluation of the target policy \\(\\pi\\); Function approximation: the value function employs function approximation; Bootstrapping: the TD error uses the bootstrapped target “\\(r_t + \\gamma V(s_{t+1}; \\theta)\\)” instead of the full return as in Monte Carlo. The “deadly triad” is used to illustrate that using all three ingredients together will lead to the potential divergence of policy evaluation. Why? Recall that, using linear approximation, the on-policy Semi-Gradient TD(0) algorithm guarantees convergence to the unique fixed point of the projected Bellman equation (PBE) (2.54), restated here \\[ V^\\star_{\\text{TD}} = \\Pi_{\\mu} T^\\pi V^\\star_{\\text{TD}}, \\] where \\(\\mu\\) is the stationary distribution induced by the policy \\(\\pi\\). A central reason for the guaranteed convergence is that the projected Bellman operator \\[ \\Pi_{\\mu} T^\\pi \\] is a \\(\\gamma\\)-contraction and has a unique fixed point. Therefore, the on-policy Semi-Gradient TD(0) algorithm—can be seen as a stochastic approximation of the projected Bellman operator—enjoys convergence guarantees. However, in the off-policy case, the orthogonal projection \\(\\Pi_\\mu\\) needs to be modified as \\(\\Pi_{\\nu}\\), where \\(\\nu\\) is the stationary distribution induced by the behavior policy \\(b\\). The new operator \\[ \\Pi_{\\nu} T^\\pi \\] is not guaranteed to be a \\(\\gamma\\)-contraction, due to the mismatch between \\(\\nu\\)—induced by \\(b\\)—and the target policy \\(\\pi\\). Therefore, divergence can potentially happen. How to Fix? Multiple algorithms have been proposed to fix the deadly triad. Notable examples include the gradient TD (GTD) family of algorithms (Sutton, Szepesvári, and Maei 2008), (Sutton et al. 2009), and the Emphatic TD (ETD) learning algorithm (Mahmood et al. 2015). They are influential and widely cited, but they are not (yet) mainstream in deep RL practice. Their main appeal is theoretical—they provide off-policy evaluation algorithms with convergence guarantees under linear function approximation. Moreover, GTD/TDC require two-time-scale step-sizes and ETD’s emphatic weights can have high variance, making them less attractive for large-scale control with neural networks. I encourage you to read the papers to understand the algorithms. However, in the next, I will explain the deep Q network (DQN) approach that is more popular in practice. 2.2.4.2 Deep Q Network We consider continuous state spaces with a finite action set, and a parametric action–value function \\(Q(s,a; \\theta)\\). Naive (semi-gradient) Q-Learning with Function Approximation. The goal is to learn \\(Q^\\star\\) and act \\(\\varepsilon\\)-greedily w.r.t. \\(Q(\\cdot,\\cdot; \\theta)\\). The update uses a bootstrapped optimality target built from the current network. \\[\\begin{equation} \\begin{split} y_t &amp; = r_t + \\gamma \\max_{a&#39;} Q(s_{t+1}, a&#39;;\\theta) \\\\ \\theta &amp; \\leftarrow \\theta + \\alpha \\,\\big(y_t - Q(s_t,a_t;\\theta)\\big)\\,\\nabla_\\theta Q(s_t,a_t; \\theta). \\end{split} \\tag{2.60} \\end{equation}\\] The transitions \\((s_t,a_t,r_t,s_{t+1})\\) are generated using a \\(\\varepsilon\\)-greedy policy with respect to \\(Q(s,a;\\theta)\\). This naive variant is off-policy + bootstrapping + function approximation (i.e., the deadly triad) and thus can be unstable. Deep Q Network (DQN) with Experience Replay (ER) and Target Network (TN). DQN augments the above naive Q learning with two stabilizers: Experience Replay (ER): store transitions in a buffer \\(\\mathcal{D}\\); train on i.i.d.-like mini-batches to decorrelate updates and reuse data. Target Network (TN): maintain a delayed copy \\(Q(\\cdot,\\cdot; \\theta^-)\\) to compute targets \\[ y_t = r_t + \\gamma \\max_{a&#39;} Q(s_{t+1},a&#39;;\\theta^-), \\] keeping the target fixed for many gradient steps. The full DQN algorithm is presented below. Initialize replay buffer \\(\\mathcal{D}\\) with capacity \\(N\\) Initialize approximate Q value \\(Q(s,a;\\theta)\\) Initialize target \\(Q_T(s,a;\\theta^-)\\) with \\(\\theta^- = \\theta\\) For episode \\(=1,\\dots,M\\) do: Initialize \\(s_0\\) For \\(t=0,\\dots,T\\) do: \\(\\varepsilon\\)-greedy policy: With probability \\(\\varepsilon\\) select a random action \\(a_t \\in \\mathcal{A}\\) Otherwise select \\(a_t = \\arg\\max_a Q(s_t,a_t;\\theta)\\) Observe transition \\(\\tau_t = (s_t, a_t, r_t, s_{t+1})\\) Put \\(\\tau_t\\) inside replay buffer \\(\\mathcal{D}\\) Sample a random minibatch of transitions \\(\\{(s_i, a_i, r_i, s_{i+1})\\}_{i \\in \\mathcal{I}}\\) from \\(\\mathcal{D}\\) For \\(i \\in \\mathcal{I}\\) do: Set target \\(y_i = r_i + \\gamma \\max_a Q_T(s_{i+1}, a; \\theta^-)\\) using the target network Update \\(\\theta \\leftarrow \\theta + \\alpha (y_i - Q(s_i,a_i;\\theta)) \\nabla_\\theta Q(s_i, a_i; \\theta)\\) Every \\(C\\) steps synchronize the target network with the \\(Q\\) net: \\(\\theta^- = \\theta\\) Although the naive Q-leanring with function approximation can be unstable, DQN has achieved great success and is also very efficient (Mnih et al. 2015). Fitted Q Iteration (FQI). To understand why DQN can achieve stabilized training compared to naive Q-learning with function approximation. It is insightful to look at the fitted Q iteration (FQI) algorithm, presented below. Initialize \\(Q^{(0)} = Q(s,a;\\theta_0)\\) For \\(k=0,1,2,\\dots,K-1\\) do: Sample i.i.d. transitions \\(\\{ (s_i,a_i,r_i,s_{i+1}) \\}_{i=1}^N\\) with \\((s_i, a_i)\\) drawn from a distribution \\(\\mu\\) Compute targets \\(y_i = r_i + \\gamma \\max_a Q^{(k)}(s_{t+1},a;\\theta_k),i=1,\\dots,N\\) Update the action-value function: \\[ Q^{(k+1)} = Q(s,a;\\theta_{k+1}), \\quad \\theta_{k+1} \\in \\arg\\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^N (y_i - Q(s_i,a_i;\\theta))^2 \\] The FQI algorithm samples trajectories from a fixed distribution \\(\\mu\\) and optimizes the parameter using targets generated from those trajectories. Under reasonable coverage and approximation assumptions, it converges and admits finite-sample error bounds (Antos, Szepesvári, and Munos 2007), (Munos and Szepesvári 2008). Connection to DQN. DQN is similar to FQI in the following aspects. Frozen targets: DQN’s target network \\(Q(\\cdot,\\cdot; \\theta^-)\\) plays the role of \\(Q^{(k-1)}\\) in FQI. Supervised fit: DQN’s mini-batch loss minimizes \\(\\sum (Q(s_i,a_i; \\theta) - y_i)^2\\), just like FQI’s regression step. Data usage: FQI trains on a fixed dataset; DQN’s replay buffer approximates training on an (ever-growing) quasi-fixed dataset by repeatedly sampling past transitions. Iteration vs. updates: FQI alternates full regressions and target recomputation; DQN alternates many SGD steps with periodic target updates. In the limit of many SGD steps per target update and a large replay buffer, DQN \\(\\approx\\) online, incremental FQI. This perspective explains why ER + TN make DQN far more stable than naive Q-learning with function approximation: they make the optimization behave like a sequence of supervised fits to fixed targets drawn from a nearly stationary dataset. Example 2.4 (DQN for Mountain Car) Consider again the Mountain car problem from Example 2.3. Naive Q-Learning with Function Approximation. As shown in Fig. 2.12 and Fig. 2.13, naive Q-leanring with function approximation fails to learn a good policy. Figure 2.12: Average return w.r.t. episode (Naive Q Learning) Figure 2.13: Example rollout (Naive Q Learning) DQN with Experience Replay and Target Network. Adding ER and TN to Q-learning leads to steady learning (Fig. 2.14) and a successful final policy (Fig. 2.15). You can find code for these experiments here. Figure 2.14: Average return w.r.t. episode (DQN) Figure 2.15: Example rollout (DQN) References Antos, András, Csaba Szepesvári, and Rémi Munos. 2007. “Fitted q-Iteration in Continuous Action-Space MDPs.” Advances in Neural Information Processing Systems 20. Baird, Leemon et al. 1995. “Residual Algorithms: Reinforcement Learning with Function Approximation.” In Proceedings of the Twelfth International Conference on Machine Learning, 30–37. Fan, Jianqing, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. 2020. “A Theoretical Analysis of Deep q-Learning.” In Learning for Dynamics and Control, 486–89. PMLR. Kearns, Michael J, and Satinder Singh. 2000. “Bias-Variance Error Bounds for Temporal Difference Updates.” In COLT, 142–47. Mahmood, A Rupam, Huizhen Yu, Martha White, and Richard S Sutton. 2015. “Emphatic Temporal-Difference Learning.” arXiv Preprint arXiv:1507.01569. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. Munos, Rémi, and Csaba Szepesvári. 2008. “Finite-Time Bounds for Fitted Value Iteration.” Journal of Machine Learning Research 9 (5). Riedmiller, Martin. 2005. “Neural Fitted q Iteration–First Experiences with a Data Efficient Neural Reinforcement Learning Method.” In European Conference on Machine Learning, 317–28. Springer. Robbins, Herbert, and David Siegmund. 1971. “A Convergence Theorem for Non Negative Almost Supermartingales and Some Applications.” In Optimizing Methods in Statistics, 233–57. Elsevier. Sutton, Richard S, and Andrew G Barto. 1998. Reinforcement Learning: An Introduction. Vol. 1. 1. MIT press Cambridge. Sutton, Richard S, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári, and Eric Wiewiora. 2009. “Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation.” In Proceedings of the 26th Annual International Conference on Machine Learning, 993–1000. Sutton, Richard S, Csaba Szepesvári, and Hamid Reza Maei. 2008. “A Convergent o(n) Algorithm for Off-Policy Temporal-Difference Learning with Linear Function Approximation.” Advances in Neural Information Processing Systems 21 (21): 1609–16. "],["policy-gradient.html", "Chapter 3 Policy Gradient Methods 3.1 Gradient-based Optimization 3.2 Policy Gradients 3.3 Actor–Critic Methods 3.4 Advanced Policy Gradients 3.5 Model-based Policy Optimization", " Chapter 3 Policy Gradient Methods In Chapter 2, we relaxed two key assumptions of the MDP introduced in Chapter 1: Unknown dynamics: the transition function \\(P\\) was no longer assumed to be known. Continuous states: the state space \\(\\mathcal{S}\\) was extended from finite to continuous. When only the dynamics are unknown but the MDP remains tabular, we introduced generalized versions of policy iteration (e.g., SARSA) and value iteration (e.g., Q-learning). These algorithms can recover near-optimal value functions with strong convergence guarantees. When both the dynamics are unknown and the state space is continuous, tabular methods become infeasible. In this setting, we employed function approximation to represent value functions, and generalized SARSA and Q-learning accordingly. We also introduced stabilization techniques such as experience replay and target networks to ensure more reliable learning. In this chapter, we relax a third assumption: the action space \\(\\mathcal{A}\\) is also continuous. This setting captures many important real-world systems, such as autonomous vehicles and robots. Handling continuous actions requires a departure from the value-based methods of Chapter 2. The key difficulty is that even if we had access to a near-optimal action-value function \\(Q(s,a)\\), selecting the control action requires solving \\[ \\max_a Q(s,a), \\] which is often computationally expensive and can lead to suboptimal solutions. To address this challenge, we introduce a new paradigm: policy gradient methods. Rather than learning value functions to derive policies indirectly, we directly optimize parameterized policies using gradient-based methods. We begin this chapter by reviewing the fundamentals of gradient-based optimization, and then build upon them to develop algorithms for searching optimal policies via policy gradients. 3.1 Gradient-based Optimization Gradient-based optimization is the workhorse behind most modern machine learning algorithms, including policy gradient methods. The central idea is to iteratively update the parameters of a model in the direction that most improves an objective function. 3.1.1 Basic Setup Suppose we have a differentiable objective function \\(J(\\theta)\\), where \\(\\theta \\in \\mathbb{R}^d\\) represents the parameter vector. The goal is to find \\[ \\theta^\\star \\in \\arg\\max_\\theta J(\\theta). \\] The gradient of the objective with respect to the parameters, \\[ \\nabla_\\theta J(\\theta) = \\begin{bmatrix} \\frac{\\partial J}{\\partial \\theta_1} &amp; \\frac{\\partial J}{\\partial \\theta_2} &amp; \\cdots &amp; \\frac{\\partial J}{\\partial \\theta_d} \\end{bmatrix}^\\top, \\] provides the local direction of steepest ascent. Gradient-based optimization uses this direction to iteratively update the parameters. Note that modern machine learning software tools such as PyTorch allow the user to conveniently query the gradient of any function \\(J\\) defined by neural networks. 3.1.2 Gradient Ascent and Descent The simplest method is gradient ascent (for maximization): \\[ \\theta_{k+1} = \\theta_k + \\alpha \\nabla_\\theta J(\\theta_k), \\] where \\(\\alpha &gt; 0\\) is the learning rate. For minimization, the update rule uses gradient descent: \\[ \\theta_{k+1} = \\theta_k - \\alpha \\nabla_\\theta J(\\theta_k). \\] The choice of learning rate \\(\\alpha\\) is critical: Too large \\(\\alpha\\) can cause divergence. Too small \\(\\alpha\\) leads to slow convergence. 3.1.2.1 Convergence Guarantees For convex functions \\(J(\\theta)\\), gradient descent (or ascent) can be shown to converge to the global optimum under appropriate conditions on the learning rate. For non-convex functions—which are common in reinforcement learning—gradient methods may only find so-called first-order stationary points, i.e., points \\(\\theta\\) at which the gradient \\(\\nabla_\\theta J(\\theta) = 0\\). Nevertheless, they remain effective in practice. TODO: graph different stationary points We now formalize the convergence speed of Gradient Descent (GD) for minimizing a smooth convex function. We switch to the minimization convention and write the objective as \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\) (to avoid sign confusions with \\(J\\) used for maximization). We assume exact gradients \\(\\nabla f(\\theta)\\) are available. Setup and Assumptions. (Convexity) For all \\(\\theta,\\vartheta\\in\\mathbb{R}^d\\), \\[\\begin{equation} f(\\vartheta) \\;\\ge\\; f(\\theta) + \\nabla f(\\theta)^\\top(\\vartheta-\\theta). \\tag{3.1} \\end{equation}\\] (\\(L\\)-smoothness) The gradient is \\(L\\)-Lipschitz: for all \\(\\theta,\\vartheta\\), \\[\\begin{equation} \\|\\nabla f(\\vartheta)-\\nabla f(\\theta)\\| \\;\\le\\; L\\|\\vartheta-\\theta\\|. \\tag{3.2} \\end{equation}\\] Equivalently (the descent lemma), for all \\(\\theta,\\Delta\\), \\[\\begin{equation} f(\\theta+\\Delta) \\;\\le\\; f(\\theta) + \\nabla f(\\theta)^\\top \\Delta + \\frac{L}{2}\\|\\Delta\\|^2. \\tag{3.3} \\end{equation}\\] Consider Gradient Descent with a constant stepsize \\(\\alpha&gt;0\\): \\[ \\theta_{k+1} \\;=\\; \\theta_k \\;-\\; \\alpha\\, \\nabla f(\\theta_k). \\] Theorem 3.1 (GD on smooth convex function) Let \\(f\\) be convex and \\(L\\)-smooth with a minimizer \\[ \\theta^\\star\\in\\arg\\min_\\theta f(\\theta). \\] and the global minimum \\(f^\\star = f(\\theta^\\star)\\). If \\(0&lt;\\alpha\\le \\frac{2}{L}\\), then the GD iterates satisfy for all \\(k\\ge 0\\): \\[\\begin{equation} f(\\theta_k) - f^\\star \\leq \\frac{2 (f(\\theta_0) - f^\\star) \\Vert \\theta_0 - \\theta^\\star \\Vert^2 }{2 \\Vert \\theta_0 - \\theta^\\star \\Vert^2 + k\\alpha ( 2 - L \\alpha) (f(\\theta_0) - f^\\star)} \\tag{3.4} \\end{equation}\\] In particular, choosing \\(\\alpha=\\frac{1}{L}\\) yields the canonical \\(O(1/k)\\) convergence rate in suboptimality: \\[\\begin{equation} f(\\theta_k) - f^\\star \\leq \\frac{2L \\Vert \\theta_0 - \\theta^\\star \\Vert^2}{k+4} \\tag{3.5} \\end{equation}\\] Proof. See Theorem 2.1.14 and Corollary 2.1.2 in (Nesterov 2018). Strongly Convex Case (Linear Rate). If, in addition, \\(f\\) is \\(\\mu\\)-strongly convex (\\(\\mu&gt;0\\)), i.e., for all \\(\\theta,\\vartheta\\in\\mathbb{R}^d\\), \\[\\begin{equation} f(\\vartheta)\\;\\ge\\; f(\\theta) + \\nabla f(\\theta)^\\top(\\vartheta-\\theta) \\;+\\; \\frac{\\mu}{2}\\,\\|\\vartheta-\\theta\\|^2. \\tag{3.6} \\end{equation}\\] Then, GD with \\(0&lt;\\alpha\\le \\frac{2}{\\mu + L}\\) enjoys a linear (geometric) rate: Theorem 3.2 (GD on smooth strongly convex function) If \\(f\\) is \\(L\\)-smooth and \\(\\mu\\)-strongly convex, then for \\(0&lt;\\alpha\\le \\frac{2}{\\mu + L}\\), \\[\\begin{equation} \\Vert \\theta_k - \\theta^\\star \\Vert^2 \\leq \\left( 1 - \\frac{2\\alpha \\mu L}{\\mu + L} \\right)^k \\Vert \\theta_0 - \\theta^\\star \\Vert^2. \\tag{3.7} \\end{equation}\\] If \\(\\alpha = \\frac{2}{\\mu + L}\\), then \\[\\begin{equation} \\begin{split} \\Vert \\theta_k - \\theta^\\star \\Vert &amp; \\leq \\left( \\frac{Q_f - 1}{Q_f + 1} \\right)^k \\Vert \\theta_0 - \\theta^\\star \\Vert \\\\ f(\\theta_k) - f^\\star &amp; \\leq \\frac{L}{2} \\left( \\frac{Q_f - 1}{Q_f + 1} \\right)^{2k} \\Vert \\theta_0 - \\theta^\\star \\Vert^2, \\end{split} \\tag{3.8} \\end{equation}\\] where \\(Q_f = L/\\mu\\). Proof. See Theorem 2.1.15 in (Nesterov 2018). Practical Notes. The step size \\(\\alpha=\\frac{1}{L}\\) is optimal among fixed stepsizes for the above worst-case bounds on smooth convex \\(f\\). In practice, backtracking line search or adaptive schedules can approach similar behavior without knowing \\(L\\). For policy gradients (which maximize \\(J\\)), apply the results to \\(f=-J\\) and flip the update sign (gradient ascent). The smooth/convex assumptions rarely hold globally in RL, but these results calibrate expectations about step sizes and motivate variance reduction and curvature-aware methods used later. 3.1.3 Stochastic Gradients In reinforcement learning and other large-scale machine learning problems, computing the exact gradient \\(\\nabla_\\theta J(\\theta)\\) is often infeasible. Instead, we use an unbiased estimator \\(\\hat{\\nabla}_\\theta J(\\theta)\\) computed from a subset of data (or trajectories in RL). The update becomes \\[ \\theta_{k+1} = \\theta_k + \\alpha \\hat{\\nabla}_\\theta J(\\theta_k). \\] This approach, known as stochastic gradient ascent/descent (SGD), trades off exactness for computational efficiency. Variance in the gradient estimates plays an important role in convergence speed and stability. 3.1.3.1 Convergence Guarantees We now turn to the convergence guarantees of stochastic gradient methods, which replace exact gradients with unbiased noisy estimates. Throughout this section we consider the minimization problem \\(\\min_\\theta f(\\theta)\\) and assume \\(\\nabla f\\) is available only through a stochastic oracle. Setup and Assumptions. Let \\(f:\\mathbb{R}^d\\!\\to\\!\\mathbb{R}\\) be differentiable. At iterate \\(\\theta_k\\), we observe a random vector \\(g_k\\) such that \\[ \\mathbb{E}[\\,g_k \\mid \\theta_k\\,] = \\nabla f(\\theta_k) \\quad\\text{and}\\quad \\mathbb{E}\\!\\left[\\|g_k-\\nabla f(\\theta_k)\\|^2 \\mid \\theta_k\\right] \\le \\sigma^2. \\] We will also use one of the following standard regularity conditions: (Convex + \\(L\\)-smooth) \\(f\\) is convex and the gradient is \\(L\\)-Lipschitz. (Strongly convex + \\(L\\)-smooth) \\(f\\) is \\(\\mu\\)-strongly convex and \\(L\\)-smooth. We consider the SGD update \\[ \\theta_{k+1} \\;=\\; \\theta_k - \\alpha_k\\, g_k, \\] and define the averaged iterate \\[ \\bar\\theta_K := \\frac{1}{K+1}\\sum_{k=0}^{K}\\theta_k. \\] Theorem 3.3 (SGD on smooth convex function) Assume \\(f\\) is convex and \\(L\\)-smooth. Suppose there exists \\(G\\!&gt;\\!0\\) with \\(\\mathbb{E}\\|g_k\\|^2 \\le G^2\\) for all \\(k\\). Choose a constant stepsize \\(\\alpha_k = \\alpha &gt; 0\\). Then for all \\(K \\ge 1\\), \\[\\begin{equation} \\mathbb{E}\\big[f(\\bar\\theta_K)\\big] - f^\\star \\leq \\frac{\\Vert \\theta_0 - \\theta^\\star \\Vert^2}{2 \\alpha (K+1)} + \\frac{\\alpha G^2}{2}. \\tag{3.9} \\end{equation}\\] Choose a diminishing step size \\(\\alpha_k = \\frac{\\Vert \\theta_0 - \\theta^\\star \\Vert}{G \\sqrt{k+1}}\\), then \\[\\begin{equation} \\mathbb{E}\\big[f(\\bar\\theta_K)\\big] - f^\\star \\leq \\frac{\\Vert \\theta_0 - \\theta^\\star \\Vert G}{\\sqrt{K+1}} = \\mathcal{O}\\left( \\frac{1}{\\sqrt{K}} \\right). \\tag{3.10} \\end{equation}\\] Proof. See this lecture note and (Garrigos and Gower 2023). Remarks. The bound is on the averaged iterate \\(\\bar\\theta_K\\) (the last iterate may be worse by constants without further assumptions). Replacing the second-moment bound by a variance bound \\(\\sigma^2\\) yields the same rate with \\(G^2\\) replaced by \\(\\sigma^2 + \\sup_k\\|\\nabla f(\\theta_k)\\|^2\\). With a constant stepsize, SGD converges \\(\\mathcal{O}(1/k)\\) up to a neighborhood set by the gradient noise. The next theorem states the convergence rate of SGD for minimizing strongly convex functions. Theorem 3.4 (SGD on smooth strongly convex function) Assume \\(f\\) is \\(\\mu\\)-strongly convex and \\(L\\)-smooth, and \\(\\mathbb{E}\\!\\left[\\|g_k\\|^2 \\right]\\le G^2\\). With stepsize \\(\\alpha_k = \\frac{1}{\\mu(k+1)}\\), the SGD iterates satisfy for all \\(K\\!\\ge\\!1\\), \\[\\begin{equation} \\begin{split} \\mathbb{E}[f(\\bar\\theta_K)] - f^\\star &amp; \\leq \\frac{G^2}{2 \\mu (K+1)} (1 + \\log(K+1)), \\\\ \\mathbb{E} \\Vert \\bar\\theta_K - \\theta^\\star \\Vert^2 &amp; \\leq \\frac{Q}{K+1}, \\ \\ Q = \\max \\left( \\frac{G^2}{\\mu^2}, \\Vert \\theta_0 - \\theta^\\star \\Vert^2 \\right). \\end{split} \\tag{3.11} \\end{equation}\\] Proof. See this lecture note and (Garrigos and Gower 2023). Practical Takeaways for Policy Gradients. Use diminishing stepsizes for theoretical convergence (\\(\\alpha_k \\propto 1/\\sqrt{k}\\) for general convex, \\(\\alpha_k \\propto 1/k\\) for strongly convex surrogates). With constant stepsizes, expect fast initial progress down to a variance-limited plateau; lowering variance (e.g., via baselines/advantage estimation) is as important as tuning \\(\\alpha\\). TODO: graph the different trajectories between minimizing a convex function using GD and SGD. 3.1.4 Beyond Vanilla Gradient Methods Several refinements to basic gradient updates are widely used: Momentum methods: incorporate past gradients to smooth updates and accelerate convergence. Adaptive learning rates (Adam, RMSProp, AdaGrad): adjust the learning rate per parameter based on historical gradient magnitudes. Second-order methods: approximate or use curvature information (the Hessian) for more informed updates, though often impractical in high dimensions. 3.2 Policy Gradients Policy gradients optimize a parameterized stochastic policy directly, without requiring an explicit action-value maximization step. They are applicable to both finite and continuous action spaces and are especially useful when actions are continuous or when “\\(\\arg\\max\\)” over \\(Q(s,a)\\) is costly or ill-posed. 3.2.1 Setup We consider a Markov decision process (MDP) with (possibly continuous) state space \\(\\mathcal{S}\\), action space \\(\\mathcal{A}\\), unknown dynamics \\(P\\), reward function \\(R(s,a)\\), and discount factor \\(\\gamma\\in[0,1)\\). Let \\(\\pi_\\theta(a\\mid s)\\) be a differentiable stochastic policy with parameters \\(\\theta\\in\\mathbb{R}^d\\). Trajectory. A state-action trajectory is \\(\\tau=(s_0,a_0,s_1,a_1,\\dots,s_{T})\\) with probability density/mass \\[\\begin{equation} p_\\theta(\\tau) = \\rho(s_0)\\prod_{t=0}^{T-1} \\pi_\\theta(a_t\\mid s_t)\\,P(s_{t+1}\\mid s_t,a_t), \\tag{3.12} \\end{equation}\\] where \\(\\rho\\) is the initial state distribution and \\(T\\) is the (random or fixed) episode length. Return. Define the (discounted) return \\[\\begin{equation} R(\\tau) \\;=\\; \\sum_{t=0}^{T-1}\\gamma^t R(s_t,a_t), \\tag{3.13} \\end{equation}\\] and the return-to-go \\[\\begin{equation} g_t \\;=\\; \\sum_{t&#39;=t}^{T-1}\\gamma^{t&#39;-t} R(s_{t&#39;},a_{t&#39;}). \\tag{3.14} \\end{equation}\\] Optimization objective. The goal is to maximize the expected return \\[\\begin{equation} J(\\theta) \\;\\equiv\\; \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[R(\\tau)\\right] \\;=\\; \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t R(s_t,a_t)\\right], \\tag{3.15} \\end{equation}\\] where the expectation is taken over the randomness in (i) the initial state \\(s_0 \\sim \\rho\\), (ii) the policy \\(\\pi_\\theta\\), and (iii) the transition dynamics \\(P\\). 3.2.1.1 Policy models Finite action spaces (\\(\\mathcal{A}\\) discrete). A common choice is a softmax (categorical) policy over a score (logit) function \\(f_\\theta(s,a)\\): \\[\\begin{equation} \\pi_\\theta(a\\mid s) \\;=\\; \\frac{\\exp\\{f_\\theta(s,a)\\}}{\\sum_{a&#39;\\in\\mathcal{A}}\\exp\\{f_\\theta(s,a&#39;)\\}}. \\tag{3.16} \\end{equation}\\] Here we use \\(\\exp\\{f_\\theta(s,a)\\} = e^{f_\\theta(s,a)}\\) for pretty formatting. Typically \\(f_\\theta\\) is a neural network or a linear function over features. Continuous action spaces (\\(\\mathcal{A}\\subseteq\\mathbb{R}^m\\)). A standard choice is a Gaussian policy: \\[\\begin{equation} \\pi_\\theta(a\\mid s) \\;=\\; \\mathcal{N}\\!\\big(a;\\;\\mu_\\theta(s),\\,\\Sigma_\\theta(s)\\big), \\tag{3.17} \\end{equation}\\] where \\(\\mu_\\theta(s)\\) and (often diagonal) covariance \\(\\Sigma_\\theta(s)\\) are differentiable functions (e.g., neural networks) parameterized by \\(\\theta\\). The policy \\(\\pi_\\theta(a \\mid s)\\) samples actions from the Gaussian parameterized by \\(\\mu_\\theta(s)\\) and \\(\\Sigma_\\theta(s)\\). Other choices include squashed Gaussians (e.g., \\(\\tanh\\)) or Beta distributions for bounded actions. 3.2.2 The Policy Gradient Lemma With the gradient-based optimization machinery from Section 3.1, a natural strategy for the policy optimization problem in (3.15) is gradient ascent on the objective \\(J(\\theta)\\). Consequently, the central task is to characterize the ascent direction, i.e., to compute \\(\\nabla_\\theta J(\\theta)\\). The policy gradient lemma, stated below, provides exactly this characterization. Crucially, it expresses \\(\\nabla_\\theta J(\\theta)\\) in terms of the policy’s score function \\(\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\) and returns, without differentiating through the environment dynamics. This likelihood-ratio form makes policy optimization feasible even when the transition model is unknown or non-differentiable. Theorem 3.5 (Policy Gradient Lemma) Let \\(J(\\theta)=\\mathbb{E}_{\\tau \\sim p_\\theta}[R(\\tau)]\\) as defined in (3.15) Then: \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}_{\\tau\\sim p_\\theta} \\Big[R(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau)\\Big] \\;=\\; \\mathbb{E}_{\\tau\\sim p_\\theta} \\Bigg[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\;R(\\tau)\\Bigg]. \\tag{3.18} \\end{equation}\\] By causality (future action does not affect past reward), the full return can be replaced by return-to-go: \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}_{\\tau\\sim p_\\theta} \\Bigg[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\;g_t\\Bigg]. \\tag{3.19} \\end{equation}\\] Equivalently, using value functions, \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\frac{1}{1-\\gamma} \\mathbb{E}_{s\\sim d_\\theta,\\;a\\sim\\pi_\\theta} \\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,Q^{\\pi_\\theta}(s,a)\\Big], \\tag{3.20} \\end{equation}\\] where \\(d_\\theta\\) is the (discounted) on-policy state visitation distribution for infinite-horizon MDPs: \\[\\begin{equation} d_\\theta(s) = (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t \\Pr_\\theta(s_t=s). \\tag{3.21} \\end{equation}\\] Proof. We prove the three equivalent forms step by step. Throughout, we assume \\(\\theta\\) parameterizes only the policy \\(\\pi_\\theta\\) (not the dynamics \\(P\\) nor the initial distribution \\(\\rho\\)), and that interchanging \\(\\nabla_\\theta\\) with the trajectory integral/sum is justified (e.g., bounded rewards and finite horizon or standard dominated-convergence conditions). Let the return-to-go \\(g_t\\) be defined as in (3.14). Step 1 (Log-derivative trick). Write the objective as an expectation over trajectories: \\[ J(\\theta) \\;=\\; \\int R(\\tau)\\, p_\\theta(\\tau)\\, d\\tau. \\] Differentiate under the integral and use \\[\\begin{equation} \\nabla_\\theta p_\\theta(\\tau)=p_\\theta(\\tau)\\nabla_\\theta\\log p_\\theta(\\tau) \\tag{3.22} \\end{equation}\\] we can write: \\[ \\nabla_\\theta J(\\theta) = \\int R(\\tau)\\,\\nabla_\\theta p_\\theta(\\tau)\\, d\\tau = \\int R(\\tau)\\, p_\\theta(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau)\\, d\\tau = \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\big[R(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau)\\big], \\] which is (3.18) up to expanding \\(\\log p_\\theta(\\tau)\\). To see why (3.22) is true, write \\[ \\nabla_\\theta \\log p_\\theta(\\tau) = \\frac{1}{p_\\theta(\\tau)} \\nabla_\\theta p_\\theta(\\tau), \\] using the chain rule. Step 2 (Policy-only dependence). Factor the trajectory likelihood/mass: \\[ p_\\theta(\\tau) = \\rho(s_0)\\,\\prod_{t=0}^{T-1}\\pi_\\theta(a_t\\mid s_t)\\,P(s_{t+1}\\mid s_t,a_t). \\] Since \\(\\rho\\) and \\(P\\) do not depend on \\(\\theta\\), \\[ \\log p_\\theta(\\tau) = \\text{const} \\;+\\; \\sum_{t=0}^{T-1}\\log \\pi_\\theta(a_t\\mid s_t) \\quad\\Rightarrow\\quad \\nabla_\\theta \\log p_\\theta(\\tau) \\;=\\; \\sum_{t=0}^{T-1}\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t). \\] Substitute into Step 1 to obtain the second equality in (3.18): \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\Bigg[\\sum_{t=0}^{T-1}\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,R(\\tau)\\Bigg]. \\] Step 3 (Causality \\(\\Rightarrow\\) return-to-go). Expand \\(R(\\tau)=\\sum_{t=0}^{T-1}\\gamma^{t} r_{t}\\) (with \\(r_{t}:=R(s_{t},a_{t})\\)) and swap sums: \\[ \\mathbb{E} \\Bigg[\\sum_{t=0}^{T-1}\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,R(\\tau)\\Bigg] = \\sum_{t=0}^{T-1}\\sum_{t&#39;=0}^{T-1}\\mathbb{E} \\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\gamma^{t&#39;} r_{t&#39;}\\big]. \\] For \\(t&#39;&lt;t\\), the factor \\(\\gamma^{t&#39;} r_{t&#39;}\\) is measurable w.r.t. the history \\(\\mathcal{F}_t=\\sigma(s_0,a_0,\\dots,s_t)\\), while \\[ \\mathbb{E} \\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big|\\,\\mathcal{F}_t\\big] = \\sum_{a} \\pi_\\theta(a\\mid s_t)\\,\\nabla_\\theta \\log \\pi_\\theta(a\\mid s_t) = \\nabla_\\theta \\sum_{a}\\pi_\\theta(a\\mid s_t) = \\nabla_\\theta 1 = 0, \\] (and analogously with integrals for continuous \\(\\mathcal{A}\\)). Hence by the tower property, \\[ \\mathbb{E} \\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\gamma^{t&#39;} r_{t&#39;}\\big]=0\\quad\\text{for all }t&#39;&lt;t. \\] Therefore only the terms with \\(t&#39;\\ge t\\) survive, and \\[ \\nabla_\\theta J(\\theta) = \\sum_{t=0}^{T-1}\\mathbb{E} \\Big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\sum_{t&#39;=t}^{T-1}\\gamma^{t&#39;} r_{t&#39;}\\Big] = \\mathbb{E} \\Bigg[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,g_t\\Bigg], \\] which is (3.19). Step 4 (Value-function form). Condition on \\((s_t,a_t)\\) and use the definition of the action-value function: \\[ Q^{\\pi_\\theta}(s_t,a_t) \\;\\equiv\\; \\mathbb{E}\\!\\left[g_t \\,\\middle|\\, s_t,a_t\\right]. \\] Taking expectations then yields \\[ \\mathbb{E} \\big[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,g_t\\big] = \\mathbb{E} \\big[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,Q^{\\pi_\\theta}(s_t,a_t)\\big]. \\] Summing over \\(t\\) and collecting terms with the (discounted) on-policy state visitation distribution \\(d_\\theta\\) (for the infinite-horizon case, e.g., \\(d_\\theta(s)=(1-\\gamma)\\sum_{t=0}^\\infty \\gamma^t\\,\\Pr_\\theta(s_t=s)\\); for finite \\(T\\), use the corresponding finite-horizon weighting), we obtain \\[ \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}_{s\\sim d_\\theta,\\;a\\sim \\pi_\\theta} \\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,Q^{\\pi_\\theta}(s,a)\\Big], \\] which is (3.20). Conclusion. Combining Steps 1–4 proves all three stated forms of the policy gradient. 3.2.3 REINFORCE The policy gradient lemma immediately gives us an algorithm. Specifically, the gradient receipe in (3.18) tells us that if we generate one trajectory \\(\\tau\\) by following the policy \\(\\pi\\), then \\[\\begin{equation} \\widehat{\\nabla_\\theta J} = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) R(\\tau) \\tag{3.23} \\end{equation}\\] is an unbiased estimator of the true gradient. With this sample gradient estimator, we obtain the classical REINFORCE algorithm. Single-Trajectory (Naive) REINFORCE Initialize \\(\\theta_0\\) for the initial policy \\(\\pi_{\\theta_0}(a \\mid s)\\) For \\(k=0,1,\\dots,\\) do: Obtain a trajectory \\(\\tau \\sim p_{\\theta_k}\\) Compute the stochastic gradient \\(g_k\\) as in (3.23) Update \\(\\theta_{k+1} = \\theta_k + \\alpha_k g_k\\) To reduce variance of the gradient estimator, we can use a minibatch of trajectories. For example, given a batch of \\(N\\) trajectories \\(\\{\\tau^{(i)}\\}_{i=1}^N\\) collected by \\(\\pi_\\theta\\), define for each timestep the return-to-go \\[ g_t^{(i)} = \\sum_{t&#39;=t}^{T^{(i)}-1} \\gamma^{t&#39;-t} R\\!\\left(s_{t&#39;}^{(i)},a_{t&#39;}^{(i)}\\right). \\] An unbiased gradient estimator, from (3.19) is \\[\\begin{equation} \\widehat{\\nabla_\\theta J} \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T^{(i)}-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta \\big(a_t^{(i)}\\mid s_t^{(i)}\\big) g_t^{(i)}. \\tag{3.24} \\end{equation}\\] This leads to the following minibatch REINFORCE algorithm. Minibatch REINFORCE Initialize \\(\\theta_0\\) for the initial policy \\(\\pi_{\\theta_0}(a \\mid s)\\) For \\(k=0,1,\\dots,\\) do: Obtain N trajectories \\(\\{ \\tau^{(i)} \\}_{i=1}^N \\sim p_{\\theta_k}\\) Compute the stochastic gradient \\(g_k\\) as in (3.24) Update \\(\\theta_{k+1} = \\theta_k + \\alpha_k g_k\\) We apply both the single-trajectory (naive) REINFORCE and a minibatch variant to the CartPole-v1 balancing task. The results show that variance reduction via minibatching is crucial for stable learning and for obtaining strong policies with policy-gradient methods. Example 3.1 (REINFORCE for Cart-Pole Balancing) Consider the cart-pole balancing task illustrated in Fig. 3.1. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. Figure 3.1: Cart Pole balance. State Space. The state of the cart-pole system is denoted by \\(s \\in \\mathcal{S} \\subset \\mathbb{R}^4\\), containing the position and velocity of the cart, as well as the angle and angular velocity of the pole. Action Space. The action space \\(\\mathcal{A}\\) is discrete and contains two elements: pushing to the left and pushing to the right. The dynamics of the MDP is provided by the Gym simulator and is described in the original paper (Barto, Sutton, and Anderson 2012). At the beginning of the episode, all state variables are randomly initialized in \\([-0.05,0.05]\\) and the goal for the agent is to apply the actions to balance the cart-pole for as long as possible—the agent gets a reward of \\(+1\\) every step if (1) the pole angle remains between \\(-12^\\circ\\) and \\(+12^\\circ\\) and (2) the cart position remains between \\(-2.4\\) and \\(2.4\\). The maximum episode length is \\(500\\). We design a policy network in the form of (3.16) since the action space is finite. REINFORCE. We first apply the naive REINFORCE algorithm where the gradient estimator is computed from a single trajectory as in (3.23). Fig. 3.2 shows the learning curve, which indicates that the REINFORCE algorithm was not able to learn a good policy after 2000 episodes. Figure 3.2: Learning curve (Naive REINFORCE). Minibatch REINFORCE. We then apply the minibatch REINFORCE algorithm where the gradient estimator is computed from multiple (\\(20\\) in our case) trajectories as in (3.24). Fig. 3.3 shows the learning curve, which shows steady increase in the per-episode return that eventually gets close to the maximum per-episode return \\(500\\). Fig. 3.4 shows a rollout video of applying the policy training from minibatch REINFORCE. We can see the policy nicely balances the cart-pole system. You can play with the code here. Figure 3.3: Learning curve (Minibatch REINFORCE). Figure 3.4: Policy rollout (Minibatch REINFORCE). 3.2.4 Baselines and Variance Reduction From the REINFORCE experiments above, we have seen firsthand that variance reduction is critical for stable policy-gradient learning. A natural question is: what framework can we use to systematically reduce the variance of the gradient estimator while preserving unbiasedness? 3.2.4.1 Baseline A key device is a baseline \\(b:\\mathcal{S}\\to\\mathbb{R}\\) added at each timestep: \\[\\begin{equation} \\widehat{g} \\;=\\; \\sum_{t=0}^{T-1} \\gamma^t\\,\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t - b(s_t)\\big). \\tag{3.25} \\end{equation}\\] The only difference between (3.25) and the original gradient estimator (3.19) is that the baseline \\(b(s_t)\\) is subtracted from the return-to-go \\(g_t\\). The next theorem states that any state-only baseline does not change the expectation of the gradient estimator. Theorem 3.6 (Baseline Invariance) Let \\(b:\\mathcal{S}\\to\\mathbb{R}\\) be any function independent of the action \\(a_t\\). Then \\[ \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right]=0, \\] and thus \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t - b(s_t)\\big)\\right]. \\tag{3.26} \\end{equation}\\] Equivalently, using action-values, \\[\\begin{equation} \\nabla_\\theta J(\\theta) = \\frac{1}{1-\\gamma}\\, \\mathbb{E}_{s\\sim d_\\theta,\\;a\\sim\\pi_\\theta} \\!\\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,\\big(Q^{\\pi_\\theta}(s,a)-b(s)\\big)\\Big]. \\tag{3.27} \\end{equation}\\] Proof. We prove (i) the baseline term has zero expectation, (ii) the baseline-subtracted estimator in (3.26) is unbiased, and (iii) the equivalent \\(Q\\)-value form (3.27). Throughout we assume standard conditions ensuring interchange of expectation and differentiation (e.g., bounded rewards with finite horizon or discounted infinite horizon, and a differentiable policy). Step 1 (Score-function expectation is zero). Fix a state \\(s\\in\\mathcal{S}\\). The score function integrates/sums to zero under the policy: \\[\\begin{equation} \\begin{split} \\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot\\mid s)}\\!\\big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\big] &amp; = \\sum_{a\\in\\mathcal{A}} \\pi_\\theta(a\\mid s)\\,\\nabla_\\theta \\log \\pi_\\theta(a\\mid s) = \\sum_{a\\in\\mathcal{A}} \\nabla_\\theta \\pi_\\theta(a\\mid s) \\\\ &amp; = \\nabla_\\theta \\sum_{a\\in\\mathcal{A}} \\pi_\\theta(a\\mid s) = \\nabla_\\theta 1 = 0, \\end{split} \\end{equation}\\] with the obvious replacement of sums by integrals for continuous \\(\\mathcal{A}\\). This identity is the standard “score has zero mean” property. Step 2 (Baseline term has zero expectation). Let \\(\\mathcal{F}_t := \\sigma(s_0,a_0,\\ldots,s_t)\\) be the history up to time \\(t\\) and recall that \\(b(s_t)\\) is independent of \\(a_t\\). Using iterated expectations: \\[ \\mathbb{E}\\!\\left[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right] = \\mathbb{E}\\!\\left[ \\gamma^t\\, b(s_t)\\, \\underbrace{\\mathbb{E}\\!\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\middle|\\, s_t\\right]}_{=\\,0~\\text{by Step 1}} \\right] = 0. \\] Summing over \\(t\\) yields \\[ \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right]=0. \\] Step 3 (Unbiasedness of the baseline-subtracted estimator). By the policy gradient lemma (likelihood-ratio form with return-to-go; see (3.19)), \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,g_t\\right]. \\] Subtract and add the baseline term inside the expectation: \\[ \\begin{split} \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,g_t\\right] &amp; = \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t-b(s_t)\\big)\\right] \\;+\\; \\\\ &amp; \\quad \\quad \\underbrace{\\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right]}_{=\\,0~\\text{by Step 2}}. \\end{split} \\] Therefore (3.26) holds, proving that any state-only baseline preserves unbiasedness. Step 4 (Equivalent \\(Q\\)-value form). Condition on \\((s_t,a_t)\\) and use the definition \\(Q^{\\pi_\\theta}(s_t,a_t):=\\mathbb{E}[g_t\\mid s_t,a_t]\\): \\[ \\mathbb{E}\\!\\big[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t-b(s_t)\\big)\\big] = \\mathbb{E}\\!\\Big[ \\gamma^t\\, \\mathbb{E}\\!\\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t-b(s_t)\\big)\\mid s_t\\big] \\Big]. \\] Inside the inner expectation (over \\(a_t\\sim \\pi_\\theta(\\cdot\\mid s_t)\\)) and using \\(b(s_t)\\)’s independence from \\(a_t\\), \\[ \\mathbb{E}\\!\\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t-b(s_t)\\big)\\mid s_t\\big] = \\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot\\mid s_t)}\\!\\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s_t)\\,\\big(Q^{\\pi_\\theta}(s_t,a)-b(s_t)\\big)\\Big]. \\] Summing over \\(t\\) with discount \\(\\gamma^t\\) and collecting terms with the (discounted) on-policy state-visitation distribution \\(d_\\theta\\) (cf. (3.21)) yields the infinite-horizon identity \\[ \\nabla_\\theta J(\\theta) = \\frac{1}{1-\\gamma}\\, \\mathbb{E}_{s\\sim d_\\theta,\\;a\\sim \\pi_\\theta}\\! \\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,\\big(Q^{\\pi_\\theta}(s,a)-b(s)\\big)\\Big], \\] which is (3.27). 3.2.4.2 Optimal Baseline and Advantage Among all state-only baselines \\(b(s)\\), which one minimizes the variance of the gradient estimator? Theorem 3.7 (Variance-Minimizing Baseline (per-state)) For the estimator \\[ g(s,a)=\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big), \\] the \\(b(s)\\) minimizing \\(\\operatorname{Var}[g\\mid s]\\) is \\[ b^\\star(s)= \\frac{\\mathbb{E}_{a\\sim \\pi_\\theta}\\!\\left[\\| \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\, Q^\\pi(s,a)\\right]} {\\mathbb{E}_{a\\sim \\pi_\\theta}\\!\\left[\\| \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\right]}. \\tag{3.28} \\] Assuming that the norm factor \\(\\Vert \\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\Vert^2\\) varies slowly with \\(a\\), then \\[ b^\\star(s) \\approx V^\\pi(s). \\] Proof. Let \\(s\\in\\mathcal{S}\\) be fixed and write \\[ u(a\\mid s) \\;\\equiv\\; \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\in\\mathbb{R}^d, \\qquad w(a\\mid s) \\;\\equiv\\; \\|u(a\\mid s)\\|^2 \\;\\ge 0. \\] Consider the vector-valued random variable \\[ g(s,a) \\;=\\; u(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big), \\] where the randomness is over \\(a\\sim \\pi_\\theta(\\cdot\\mid s)\\). We aim to choose \\(b(s)\\in\\mathbb{R}\\) to minimize the conditional variance \\[ \\operatorname{Var}[g\\mid s] \\;=\\; \\mathbb{E}\\!\\left[\\|g(s,a)-\\mathbb{E}[g\\mid s]\\|^2 \\,\\middle|\\, s\\right]. \\] Using the identity \\(\\operatorname{Var}[X]=\\mathbb{E}\\|X\\|^2-\\|\\mathbb{E}X\\|^2\\) (for vector \\(X\\) with Euclidean norm), we have \\[ \\operatorname{Var}[g\\mid s] \\;=\\; \\underbrace{\\mathbb{E}\\!\\left[\\|g(s,a)\\|^2 \\mid s\\right]}_{\\text{depends on } b(s)} \\;-\\; \\underbrace{\\big\\|\\mathbb{E}[g\\mid s]\\big\\|^2}_{\\text{independent of } b(s)}. \\] We first show that the mean term is independent of \\(b(s)\\). Indeed, \\[ \\mathbb{E}[g\\mid s] = \\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot\\mid s)}\\!\\big[u(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big)\\big] = \\mathbb{E}\\!\\big[u(a\\mid s)\\,Q^\\pi(s,a)\\big] \\;-\\; b(s)\\,\\underbrace{\\mathbb{E}\\!\\big[u(a\\mid s)\\big]}_{=\\,0}, \\] where \\(\\mathbb{E}[u(a\\mid s)]=\\sum_a \\pi_\\theta(a\\mid s)\\nabla_\\theta\\log\\pi_\\theta(a\\mid s)=\\nabla_\\theta \\sum_a \\pi_\\theta(a\\mid s)=\\nabla_\\theta 1=0\\) (replace sums by integrals in the continuous case). Therefore \\(\\mathbb{E}[g\\mid s]\\) does not depend on \\(b(s)\\). Consequently, minimizing \\(\\operatorname{Var}[g\\mid s]\\) is equivalent to minimizing the conditional second moment \\[ \\mathbb{E}\\!\\left[\\|g(s,a)\\|^2 \\mid s\\right] = \\mathbb{E}\\!\\left[\\|u(a\\mid s)\\|^2 \\,\\big(Q^\\pi(s,a)-b(s)\\big)^2 \\,\\middle|\\, s\\right] = \\mathbb{E}\\!\\left[w(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big)^2 \\,\\middle|\\, s\\right]. \\] The right-hand side is a convex quadratic in the scalar \\(b(s)\\). Differentiate w.r.t. \\(b(s)\\) and set to zero: \\[ \\frac{\\partial}{\\partial b(s)} \\mathbb{E}\\!\\left[w(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big)^2 \\,\\middle|\\, s\\right] = -2\\,\\mathbb{E}\\!\\left[w(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big)\\,\\middle|\\, s\\right] = 0. \\] Hence, \\[ \\mathbb{E}\\!\\left[w(a\\mid s)\\,Q^\\pi(s,a)\\,\\middle|\\, s\\right] = b(s)\\,\\mathbb{E}\\!\\left[w(a\\mid s)\\,\\middle|\\, s\\right], \\] and provided \\(\\mathbb{E}[w(a\\mid s)\\mid s]&gt;0\\) (i.e., the Fisher information at \\(s\\) is non-degenerate), the unique minimizer is \\[ b^\\star(s) = \\frac{\\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot\\mid s)}\\!\\left[\\| \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\, Q^\\pi(s,a)\\right]} {\\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot\\mid s)}\\!\\left[\\| \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\right]}, \\] which is (3.28). If \\(\\mathbb{E}[w(a\\mid s)\\mid s]=0\\) (e.g., a locally deterministic policy), then \\(g\\equiv 0\\) almost surely and any \\(b(s)\\) attains the minimum. Finally, when the weight \\(w(a\\mid s)=\\|\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\) varies slowly with \\(a\\) (or is approximately constant) for a fixed \\(s\\), the ratio simplifies to \\[ b^\\star(s)\\;\\approx\\;\\frac{\\mathbb{E}[c(s)\\,Q^\\pi(s,a)\\mid s]}{\\mathbb{E}[c(s)\\mid s]} \\;=\\; \\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot\\mid s)}\\!\\big[Q^\\pi(s,a)\\big] \\;=\\; V^\\pi(s), \\] so that the baseline-subtracted target becomes the advantage \\(A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)\\). When using \\(V^\\pi(s)\\) as the baseline, the baseline-subtracted target is called the advantage function \\[\\begin{equation} A^{\\pi_\\theta}(s,a)\\;=\\;Q^{\\pi_\\theta}(s,a)-V^{\\pi_\\theta}(s). \\tag{3.29} \\end{equation}\\] The corresponding minibatch gradient estimator becomes \\[\\begin{equation} \\widehat{\\nabla_\\theta J} \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T^{(i)}-1} \\gamma^t\\, \\nabla_\\theta \\log \\pi_\\theta \\big(a_t^{(i)}\\mid s_t^{(i)}\\big)\\, \\widehat{A}_t^{(i)}, \\quad \\widehat{A}_t^{(i)} \\approx g_t^{(i)} - V_\\phi \\big(s_t^{(i)}\\big), \\tag{3.30} \\end{equation}\\] where \\(V_\\phi\\) is a learned approximation to \\(V^{\\pi_\\theta}\\). 3.2.4.3 Intuition for the Advantage The advantage \\[ A^\\pi(s,a) \\;=\\; Q^\\pi(s,a) - V^\\pi(s) \\] measures how much better or worse action \\(a\\) is at state \\(s\\) relative to the policy’s average action quality \\(V^\\pi(s)=\\mathbb{E}_{a\\sim\\pi}[Q^\\pi(s,a)\\mid s]\\). Hence \\(\\mathbb{E}_{a\\sim\\pi}[A^\\pi(s,a)\\mid s]=0\\): it is a relative score. With a value baseline, the policy-gradient update is \\[ \\nabla_\\theta J(\\theta) \\;=\\; \\frac{1}{1-\\gamma}\\, \\mathbb{E}_{s\\sim d_\\pi,\\;a\\sim\\pi}\\!\\big[ \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,A^\\pi(s,a) \\big]. \\] If \\(A^\\pi(s,a) &gt; 0\\): the term \\(\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,A^\\pi(s,a)\\) increases \\(\\log \\pi_\\theta(a\\mid s)\\) (and thus \\(\\pi_\\theta(a\\mid s)\\))—the policy puts more probability mass on actions that outperformed its average at \\(s\\). If \\(A^\\pi(s,a) &lt; 0\\): it decreases \\(\\log \\pi_\\theta(a\\mid s)\\)—the policy puts less probability mass on actions that underperformed at \\(s\\). If \\(A^\\pi(s,a) \\approx 0\\): the action performed about as expected; the update at that \\((s,a)\\) is negligible. Subtracting \\(V^\\pi(s)\\) centers returns per state, so the update depends only on relative goodness. This: preserves unbiasedness (baseline invariance), reduces variance (no large, shared offset), focuses learning on which actions at \\(s\\) should get more/less probability. 3.2.4.4 REINFORCE with a Learned Value Baseline Recall that in Section 2.2, we have introduced multiple algorithms that can learn an approximate value function for policy evaluation. For example, we can use Monte Carlo estimation. We now combine REINFORCE with a learned baseline \\(V_\\phi(s)\\approx V^{\\pi_\\theta}(s)\\), yielding a lower-variance update while keeping the estimator unbiased. Minibatch REINFORCE with a Learned Value Baseline Inputs: policy \\(\\pi_\\theta(a\\mid s)\\), value \\(V_\\phi(s)\\), discount \\(\\gamma\\in[0,1)\\), stepsizes \\(\\alpha_\\theta,\\alpha_\\phi&gt;0\\), batch size \\(N\\). Convergence controls: tolerance \\(\\varepsilon&gt;0\\), maximum inner steps \\(K_{\\max}\\) (value-fit loop), optional patience \\(P\\). Collect trajectories. Roll out \\(N\\) on-policy trajectories \\(\\{\\tau^{(i)}\\}_{i=1}^N\\) using \\(\\pi_\\theta\\). For each trajectory \\(i\\) and timestep \\(t\\), record \\((s_t^{(i)},a_t^{(i)},r_t^{(i)})\\). Compute returns-to-go. For each \\(i,t\\), \\[ g_t^{(i)} \\;=\\; \\sum_{t&#39;=t}^{T^{(i)}-1} \\gamma^{\\,t&#39;-t}\\, r_{t&#39;}^{(i)}. \\] Fit the value to convergence (critic inner loop). Define the batch regression loss \\[ \\mathcal{L}_V(\\phi) \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T^{(i)}-1} \\big(g_t^{(i)} - V_\\phi(s_t^{(i)})\\big)^2. \\] Perform gradient steps on \\(\\phi\\) until convergence on this fixed batch: \\[ \\phi \\leftarrow \\phi - \\alpha_\\phi \\,\\nabla_\\phi \\mathcal{L}_V(\\phi). \\] Repeat for \\(k=1,\\dots,K_{\\max}\\) or until \\[ \\frac{\\mathcal{L}_V^{(k-1)}-\\mathcal{L}_V^{(k)}}{\\max\\{1,|\\mathcal{L}_V^{(k-1)}|\\}} &lt; \\varepsilon \\] for \\(M\\) consecutive checks. Denote the (approximately) converged parameters by \\(\\phi^\\star\\). Form (optionally standardized) advantages using the converged value. \\[ \\widehat{A}_t^{(i)} \\;=\\; g_t^{(i)} - V_{\\phi^\\star}\\!\\big(s_t^{(i)}\\big), \\qquad \\tilde{A}_t^{(i)} \\;=\\; \\frac{\\widehat{A}_t^{(i)} - \\mu_A}{\\sigma_A+\\delta}\\ \\ (\\text{optional, batch-wise}), \\] where \\(\\mu_A,\\sigma_A\\) are the mean and std of \\(\\{\\widehat{A}_t^{(i)}\\}\\) over the whole batch, and \\(\\delta&gt;0\\) is a small constant. Single policy (actor) update. Using the converged baseline, take one ascent step: \\[ \\theta \\;\\leftarrow\\; \\theta \\;+\\; \\alpha_\\theta \\cdot \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T^{(i)}-1} \\gamma^t\\,\\nabla_\\theta \\log \\pi_\\theta \\big(a_t^{(i)}\\mid s_t^{(i)}\\big)\\,\\tilde{A}_t^{(i)}. \\] (If not standardizing, use \\(\\widehat{A}_t^{(i)}\\) in place of \\(\\tilde{A}_t^{(i)}\\).) Repeat from Step 1 with the updated policy. Notes. By baseline invariance, subtracting \\(V_{\\phi^\\star}(s)\\) keeps the policy-gradient unbiased while reducing variance. Converging the critic on each fixed batch (Steps 3–4) approximates the variance-minimizing baseline for that batch before a single actor step, often stabilizing learning in high-variance settings. Example 3.2 (REINFORCE with a Learned Value Baseline for Cart-Pole) Consider the same cart-pole balancing task in Example 3.1. We use minibatch REINFORCE with a learned value baseline (batch size \\(50\\)), the algorithm described above. Fig. 3.5 shows the learning curve. The algorithm is able to steadily increase the per-episode returns. Fig. 3.6 shows a rollout of the system trajectory under the learned policy. You can play with the code here. Figure 3.5: Learning curve (Minibatch REINFORCE with a Learned Value Baseline). Figure 3.6: Policy rollout (Minibatch REINFORCE with a Learned Value Baseline). 3.3 Actor–Critic Methods Actor–critic (AC) algorithms marry policy gradients (the actor) with value function learning (the critic). The critic reduces variance by supplying low-noise estimates of action quality (values or advantages), while the actor updates the policy using these estimates. In contrast to pure Monte Carlo baselines, actor–critic bootstraps from its own predictions, enabling online, incremental, and often more sample-efficient learning. 3.3.1 Anatomy of an Actor–Critic Actor (policy): a differentiable policy \\(\\pi_\\theta(a\\mid s)\\). Critic (value): an approximator for \\(V_\\phi(s)\\), \\(Q_\\psi(s,a)\\), or directly the advantage \\(A_\\eta(s,a)\\). Update coupling: the actor ascends a baseline-subtracted log-likelihood objective using advantage-like targets supplied by the critic. 3.3.2 On-Policy Actor–Critic with TD(0) We first learn a state value function \\(V_\\phi(s)\\) with a one-step bootstrapped TD(0) target: \\[ \\delta_t \\;\\equiv\\; r_t + \\gamma\\,V_\\phi(s_{t+1}) - V_\\phi(s_t), \\qquad \\mathcal{L}_V(\\phi) \\;=\\; \\frac12\\,\\delta_t^{\\,2}. \\tag{3.31} \\] If \\(V_\\phi \\approx V^\\pi\\), then \\(\\mathbb{E}[\\delta_t\\mid s_t,a_t]\\approx A^\\pi(s_t,a_t)\\), so \\(\\delta_t\\) serves as a low-variance advantage target for the actor: \\[ \\widehat{\\nabla_\\theta J} \\;=\\; \\frac{1}{|\\mathcal{B}|} \\sum_{(s_t,a_t)\\in \\mathcal{B}} \\nabla_\\theta \\log\\pi_\\theta(a_t\\mid s_t)\\,\\underbrace{\\delta_t}_{\\text{advantage target}}. \\tag{3.32} \\] (Practical: normalize \\(\\{\\delta_t\\}_{\\mathcal{B}}\\) to mean \\(0\\) and unit variance within a batch; clip gradients for stability.) On-Policy Actor–Critic with One-Step Bootstrap (TD(0)) Inputs: policy \\(\\pi_\\theta(a\\mid s)\\), value \\(V_\\phi(s)\\), discount \\(\\gamma\\in[0,1)\\), stepsizes \\(\\alpha_\\theta,\\alpha_\\phi&gt;0\\), rollout length \\(K\\), minibatch size \\(|\\mathcal{B}|\\). For iterations \\(k=0,1,2,\\dots\\): Collect on-policy rollouts. Run \\(\\pi_\\theta\\) for \\(K\\) steps (optionally across parallel envs), storing transitions \\(\\{(s_t,a_t,r_t,s_{t+1}\\}\\). Compute TD errors. For each transition, compute the TD error \\[ \\delta_t \\leftarrow r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t). \\] Critic update (value). Minimize \\(\\sum_{t\\in\\mathcal{B}} \\frac12\\,\\delta_t^{\\,2}\\): perform multiple steps of \\[ \\phi \\leftarrow \\phi - \\alpha_\\phi \\,\\nabla_\\phi \\Big(\\frac{1}{|\\mathcal{B}|}\\sum_{t\\in\\mathcal{B}}\\frac12\\,\\delta_t^{\\,2}\\Big). \\] Actor advantages. Set \\(\\widehat{A}_t \\leftarrow \\delta_t\\) (optionally normalize over \\(\\mathcal{B}\\)). Actor update (policy gradient). \\[ \\theta \\leftarrow \\theta + \\alpha_\\theta \\,\\frac{1}{|\\mathcal{B}|}\\sum_{t\\in\\mathcal{B}} \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,\\widehat{A}_t. \\] Repeat from step 1. We apply the on-policy actor-critic algorithm to the cart-pole balancing task. Example 3.3 (Actor–Critic with One-Step Bootstrap for Cart-Pole) Consider the same cart-pole balancing control task as before, and this time apply the on-policy actor-critic with one-step bootstrap. Fig. 3.7 shows the learning curve. Fig. 3.8 shows an example rollout of the policy. You can play with the code here. Figure 3.7: Learning curve (Actor–Critic with One-Step Bootstrap). Figure 3.8: Policy rollout (Actor–Critic with One-Step Bootstrap). 3.3.3 Generalized Advantage Estimation (GAE) In REINFORCE with a learned value baseline (Section 3.2.4.4), we used the full Monte Carlo return \\(g_t\\) as the target for value function approximation; while in on-policy Actor-Critic with TD(0) (Section 3.3.2), we used the one-step bootstrap return \\(r_t + \\gamma V_\\phi (s_{t+1})\\) as the target for value function estimation. Recall in policy evaluation (Section 2.1.1), we have introduced a spectrum of methods that sit in between Monte Carlo and TD(0): they are methods that leverage the \\(n\\)-step bootstrap return that balance bias and variance. (Section 2.1.1.3 and 2.1.1.4). In particular, recall the definition of an \\(n\\)-step bootstrap return \\[\\begin{equation} g^{(n)}_t = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k} \\;+\\; \\gamma^{n}\\,V_\\phi(s_{t+n}), \\tag{3.33} \\end{equation}\\] where \\(V_\\phi\\) denotes the approximate value function. The \\(\\lambda\\)-return (with \\(\\lambda \\in [0,1]\\)) performs a convex combination of all the \\(n\\)-step returns \\[\\begin{equation} g^{(\\lambda)}_t = (1-\\lambda)\\sum_{n=1}^{\\infty} \\lambda^{n-1} \\, g^{(n)}_t. \\tag{3.34} \\end{equation}\\] The Generalized Advantage Estimation (GAE) algorithm (Schulman, Moritz, et al. 2015) is an Actor-Critic type of policy gradient method that leverages the \\(\\lambda\\)-return as the target for fitting the critic (i.e., the approximate value function). GAE-\\(\\lambda\\) Advantage. Start from the TD residual \\[ \\delta_t \\;=\\; r_t + \\gamma\\,V_\\phi(s_{t+1}) - V_\\phi(s_t), \\qquad \\tag{3.35} \\] and define the GAE-\\(\\lambda\\) advantage as the exponentially-weighted sum of future TD residuals: \\[ \\widehat{A}^{(\\lambda)}_t \\;=\\; \\sum_{\\ell=0}^{T-1-t} (\\gamma\\lambda)^\\ell \\,\\delta_{t+\\ell}. \\qquad \\tag{3.36} \\] This admits an efficient backward recursion: \\[ \\widehat{A}^{(\\lambda)}_t \\;=\\; \\delta_t \\;+\\; \\gamma\\lambda\\,\\widehat{A}^{(\\lambda)}_{t+1}, \\quad \\widehat{A}^{(\\lambda)}_{T}=0 \\text{ (at terminal).} \\qquad \\tag{3.37} \\] From Advantage to Return. A key identity (obtained by expanding the sum of TD residuals and grouping terms) is \\[ \\sum_{\\ell=0}^{\\infty} (\\gamma\\lambda)^\\ell \\,\\delta_{t+\\ell} \\;=\\; \\Big(1-\\lambda\\Big)\\sum_{n=1}^{\\infty}\\lambda^{n-1}\\Big(g^{(n)}_t - V_\\phi(s_t)\\Big). \\tag{3.38} \\] The left-hand side is the GAE-\\(\\lambda\\) advantage, and the right-hand side is \\(g^{(\\lambda)}_t - V_{\\phi}(s_t)\\). Therefore, \\[ \\widehat{A}^{(\\lambda)}_t \\;=\\; g^{(\\lambda)}_t \\;-\\; V_\\phi(s_t), \\qquad\\text{and hence}\\qquad g_t^{(\\lambda)} \\;=\\; \\widehat{A}^{(\\lambda)}_t + V_\\phi(s_t). \\] In GAE, we use \\[ \\widehat{V}^{\\,\\text{targ}}_t \\;=\\; \\widehat{A}^{(\\lambda)}_t \\;+\\; V_\\phi(s_t), \\tag{3.39} \\] as the target for fitting \\(V_\\phi\\). GAE Policy Gradient. The true on-policy policy gradient can be written as \\[ \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1}\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,A^\\pi(s_t,a_t)\\right]. \\qquad \\tag{3.40} \\] An estimator remains unbiased if we replace \\(A^\\pi\\) by any \\(\\widehat{A}\\) satisfying \\[ \\mathbb{E}\\!\\left[\\widehat{A}_t \\,\\middle|\\, s_t,a_t\\right] \\;=\\; A^\\pi(s_t,a_t). \\qquad \\tag{3.41} \\] When the critic is exact, \\(V_\\phi\\equiv V^\\pi\\), each \\(n\\)-step bootstrap return has expectation \\[ \\mathbb{E}\\!\\left[g^{(n)}_t \\,\\middle|\\, s_t,a_t\\right] \\;=\\; Q^\\pi(s_t,a_t), \\] so by linearity and (3.34), \\[ \\mathbb{E}\\!\\left[g^{(\\lambda)}_t \\,\\middle|\\, s_t,a_t\\right] \\;=\\; Q^\\pi(s_t,a_t). \\] Using \\(\\widehat{A}^{(\\lambda)}_t = g^{(\\lambda)}_t - V^\\pi(s_t)\\) gives \\[ \\mathbb{E}\\!\\left[\\widehat{A}^{(\\lambda)}_t \\,\\middle|\\, s_t,a_t\\right] \\;=\\; Q^\\pi(s_t,a_t) - V^\\pi(s_t) \\;=\\; A^\\pi(s_t,a_t), \\] which satisfies (3.41). Plugging \\(\\widehat{A}^{(\\lambda)}_t\\) into (3.40) thus yields an unbiased policy-gradient estimator. The pseudocode for GAE is presented below. On-Policy Actor–Critic with Generalized Advantage Estimation (GAE) Inputs: policy \\(\\pi_\\theta(a\\mid s)\\), value \\(V_\\phi(s)\\), discount \\(\\gamma\\in[0,1)\\), GAE parameter \\(\\lambda\\in[0,1]\\); stepsizes \\(\\alpha_\\theta,\\alpha_\\phi&gt;0\\); rollout length \\(T\\); minibatch size \\(B\\). For iterations \\(k=0,1,2,\\dots\\): Collect rollouts. Run \\(\\pi_\\theta\\) to collect \\(B\\) trajectories and each trajectory has \\(T\\) steps , storing \\((s_t,a_t,r_t,s_{t+1},\\mathrm{done}_t)\\). Values &amp; residuals. Compute \\[ v_t\\!\\leftarrow\\!V_\\phi(s_t), \\ \\ v_{t+1}\\!\\leftarrow\\!V_\\phi(s_{t+1}),\\ \\ m_t\\!\\leftarrow\\!1-\\mathrm{done}_t, \\ \\ \\delta_t \\leftarrow r_t + \\gamma m_t v_{t+1} - v_t. \\] Backward GAE. Set \\(\\widehat{A}_{T}\\!\\leftarrow\\!0\\), and for \\(t=T-1\\) to \\(0\\) do: \\[ \\widehat{A}_t \\leftarrow \\delta_t + \\gamma\\lambda m_t \\widehat{A}_{t+1}. \\] (Optionally normalize \\(\\{\\widehat{A}_t\\}\\) within the minibatch.) Critic target (\\(\\lambda\\)-return). Set critic target \\[ \\widehat{V}^{\\,\\text{targ}}_t \\leftarrow \\widehat{A}_t + v_t \\;\\;(=g^{(\\lambda)}_t). \\] Critic update. Gradient descent: \\[ \\phi \\leftarrow \\phi - \\alpha_\\phi \\nabla_\\phi \\frac{1}{ B }\\sum_{t }\\big(V_\\phi(s_t)-\\widehat{V}^{\\,\\text{targ}}_t\\big)^2. \\] (Often take several critic steps here.) Actor update. Gradient ascent \\[ \\theta \\leftarrow \\theta + \\alpha_\\theta \\frac{1}{ B }\\sum_{t } \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\widehat{A}_t. \\] The next example applies GAE to the cart-pole balancing problem. Example 3.4 (GAE for Cart-Pole Balancing) Fig. 3.9 shows the learning curve using Actor-Critic with GAE and Fig. 3.10 shows a sample rollout of the trained policy. The Python code can be found here. Figure 3.9: Learning curve (Actor–Critic with GAE). Figure 3.10: Policy rollout (Actor–Critic with GAE). 3.3.4 Off-Policy Actor–Critic On-policy actor–critic discards data after a single update. Off-policy methods decouple the behavior policy (that collects data) from the target policy (that we improve), enabling replay buffers and better sample efficiency. Off-Policy Policy Gradient. When data come from a behavior policy \\(b\\neq \\pi_\\theta\\), define the per-decision likelihood ratio \\[ \\rho_t \\;=\\; \\frac{\\pi_\\theta(a_t\\mid s_t)}{b (a_t\\mid s_t)}. \\qquad \\tag{3.42} \\] A basic off-policy policy gradient with an advantage target \\(\\widehat{A}_t\\) is \\[\\begin{equation} \\widehat{\\nabla_\\theta J} \\;=\\; \\mathbb{E}\\!\\left[\\rho_t\\,\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,\\widehat{A}_t\\right]. \\tag{3.43} \\end{equation}\\] In practice we often clip the ratio to control variance: \\[ \\bar\\rho_t \\;=\\; \\min\\{\\rho_t,\\;c\\}, \\quad c\\!\\ge\\!1, \\qquad \\widehat{\\nabla_\\theta J} \\approx \\mathbb{E}\\!\\left[\\bar\\rho_t\\,\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,\\widehat{A}_t\\right]. \\] Clipping introduces small bias but usually reduces variance. Off-policy Critic. A convenient choice is an action-value critic \\(Q_\\psi(s,a)\\) trained with an expected SARSA style target under the current \\(\\pi_\\theta\\): \\[ \\begin{split} y_t &amp; = r_t \\;+\\; \\gamma \\, \\mathbb{E}_{a&#39;\\sim\\pi_\\theta(\\cdot\\mid s_{t+1})}\\!\\left[Q_{\\bar\\psi}(s_{t+1},a&#39;)\\right], \\\\ \\psi &amp; \\leftarrow \\arg\\min_\\psi \\;\\mathbb{E}\\big[(Q_\\psi(s_t,a_t)-y_t)^2\\big], \\end{split} \\tag{3.44} \\] where \\(Q_{\\bar\\psi}\\) is a target network used to stabilize bootstrapping (i.e., mitigate the deadly triad). For discrete actions, the expectation is an exact sum \\(\\sum_{a&#39;}\\pi_\\theta(a&#39;\\mid s&#39;)Q_{\\bar\\psi}(s&#39;,a&#39;)\\); for continuous, we approximate the expectation with a few samples \\(a&#39;\\!\\sim\\!\\pi_\\theta(\\cdot\\mid s&#39;)\\). Advantage. Given \\(Q_\\psi\\), we can estimate the advantage by \\[ \\widehat{A}_t \\;=\\;Q_\\psi(s_t,a_t) \\;-\\; V_\\psi(s_t), \\quad V_\\psi(s) \\;\\equiv\\; \\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot\\mid s)}[Q_\\psi(s,a)]. \\qquad \\tag{3.45} \\] Again, for discrete actions, we can compute \\(V_\\psi\\) exactly; for continuous actions, we approximate using a few samples. Pseudocode for off-policy actor-critic is presented below. Experience-Replay Off-Policy Actor–Critic Inputs: target policy \\(\\pi_\\theta\\), Q-critic \\(Q_\\psi\\) (and target \\(Q_{\\bar\\psi}\\)), discount \\(\\gamma\\), stepsizes \\(\\alpha_\\theta,\\alpha_\\psi\\), replay buffer \\(\\mathcal{D}\\), IS clip \\(c\\ge 1\\), minibatch size \\(B\\). Initialize: \\(\\bar\\psi\\leftarrow\\psi\\). Behavior policy \\(b\\) can be \\(\\pi_\\theta\\) with exploration (e.g., \\(\\varepsilon\\)-greedy). For iterations \\(k=0,1,2,\\dots\\): Interact &amp; store. Use \\(b\\) to step the env and append to \\(\\mathcal{D}\\) tuples \\((s_t,a_t,r_t,s_{t+1},\\mathrm{done}_t,\\;p_t^b)\\), where \\(p_t^b=b(a_t\\mid s_t)\\) (store this to compute \\(\\rho_t\\)). Sample minibatch Sample transitions \\(\\{(s,a,r,s&#39;,d,p^\\mu)\\}_{i=1}^B\\) from the replay buffer \\(\\mathcal{D}\\). Critic target (expected SARSA). Compute \\(\\pi_\\theta(a&#39;\\mid s&#39;)\\) and \\(Q_{\\bar\\psi}(s&#39;,a&#39;)\\). Set \\(y \\leftarrow r + \\gamma(1-\\mathrm{done}_t)\\sum_{a&#39;} \\pi_\\theta(a&#39;\\mid s&#39;)\\,Q_{\\bar\\psi}(s&#39;,a&#39;)\\). (for continuous actions: perform sample average) Critic update. \\[ \\psi \\leftarrow \\psi - \\alpha_\\psi \\nabla_\\psi\\frac{1}{B}\\sum_{i=1}^B\\big(Q_\\psi(s_i,a_i)-y_i\\big)^2. \\] (Optionally clip gradients; perform multiple critic steps.) Actor advantage. Compute \\(V_\\psi(s)=\\sum_{a}\\pi_\\theta(a\\mid s)\\,Q_\\psi(s,a)\\) (or sample-average for continuous actions). Set \\(\\widehat{A}=Q_\\psi(s,a)-V_\\psi(s)\\); optionally normalize \\(\\widehat{A}\\) within the batch. Importance ratios (clipped). \\[ \\rho \\leftarrow \\frac{\\pi_\\theta(a\\mid s)}{p^b},\\qquad \\bar\\rho \\leftarrow \\min\\{\\rho,\\;c\\}. \\] Actor update. \\[ \\theta \\leftarrow \\theta + \\alpha_\\theta\\, \\frac{1}{B}\\sum_{i=1}^B \\bar\\rho_i \\,\\nabla_\\theta \\log \\pi_\\theta(a_i\\mid s_i)\\,\\widehat{A}_i. \\] Target network (moving average). \\[ \\bar\\psi \\leftarrow \\tau\\,\\psi + (1-\\tau)\\,\\bar\\psi. \\] Notes &amp; Variants. Unbiased vs. biased: Without clipping and with a correct critic/advantage, (3.43) is unbiased; clipping \\(\\bar\\rho\\) adds bias but improves variance. Critic options: You can learn \\(V_\\phi\\) instead of \\(Q_\\psi\\) using off-policy TD with IS; using \\(Q\\) with an expected SARSA target avoids IS in the critic while keeping evaluation under \\(\\pi_\\theta\\). Behavior refresh: Periodically update \\(b\\) toward \\(\\pi_\\theta\\) (reduce exploration) to keep ratios well-behaved. The next example applies off-policy actor-critic to cart-pole balancing. Example 3.5 (Off-Policy Actor-Critic for Cart-Pole Balancing) Fig. 3.11 shows the learning curve of applying off-policy actor-critic to cart-pole balancing. Fig. 3.12 shows a sample rollout of the learned policy. The Python code can be found here. Figure 3.11: Learning curve (Off-Policy Actor–Critic). Figure 3.12: Policy rollout (Off-Policy Actor–Critic). The next example applies off-policy actor-critic to a control problem with a continuous action space. Example 3.6 (Off-Policy Actor-Critic for Inverted Pendulum) Consider the Inverted Pendulum problem illustrated in Fig. 3.13. The state of the pendulum is \\(s = (\\theta, \\dot{\\theta})\\), or equivalently, \\(s = (x, y, \\dot{\\theta})\\) with \\(x = \\cos(\\theta), y = \\sin(\\theta)\\). The action space is continuous: \\(\\tau \\in \\mathcal{A} = [-2,2]\\). The dynamics of the pendulum is specified by Gym, and the reward is \\[ R(s,\\tau) = -(\\theta^2 + 0.1 \\dot{\\theta}^2 + 0.001 \\tau^2). \\] The episode truncates at \\(200\\) time steps. Figure 3.13: Illustration of Inverted Pendulum in Gym. Fig. 3.14 shows the learning curve of applying off-policy actor-critic to the pendulum problem. Fig. 3.15 shows a sample rollout of the learned policy. You can find the Python code here. Figure 3.14: Learning curve (Off-Policy Actor-Critic). Figure 3.15: Policy rollout (Off-Policy Actor-Critic). 3.4 Advanced Policy Gradients 3.4.1 Revisiting Generalized Policy Iteration Recall from Chapter 2 that generalized policy iteration (GPI) extends tabular policy iteration (with known dynamics) to unknown-dynamics settings. At a high level, GPI iterates over policies; at iteration \\(k\\) it performs: Policy evaluation. Use the current policy \\(\\pi_k\\) to generate \\(N\\) trajectories and estimate either the \\(Q\\)-function \\(\\hat Q^{\\pi_k}(s,a)\\) or the advantage function \\(\\hat A^{\\pi_k}(s,a)\\), using function approximation. This can be done, for example, with the GAE algorithm introduced in Section 3.3.3, and is the “critic” in the Actor–Critic family of methods. Policy improvement. Construct a new policy \\(\\pi_{k+1}\\) that (approximately) prefers actions deemed better by \\(\\hat Q^{\\pi_k}\\) or \\(\\hat A^{\\pi_k}\\): \\[ \\pi_{k+1}(s) \\approx \\arg\\max_a \\hat Q^{\\pi_k}(s,a) \\;=\\; \\arg\\max_a \\hat A^{\\pi_k}(s,a). \\] In policy gradients, we approximate \\(\\arg\\max_a \\hat A^{\\pi_k}(s,a)\\) via gradient ascent in \\(a\\), i.e., using \\[\\begin{equation} \\nabla_\\theta J(\\theta) = \\frac{1}{1-\\gamma}\\, \\mathbb{E}_{s \\sim d_{\\pi_k},\\, a \\sim \\pi_k} \\big[\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\, \\hat A^{\\pi_k}(s,a)\\big]. \\tag{3.46} \\end{equation}\\] A key observation is that we use an advantage estimate obtained from data generated by \\(\\pi_k\\) (the old policy) to produce a new policy. In the tabular case, this improvement step guarantees monotonic improvement of \\(\\pi_{k+1}\\) over \\(\\pi_k\\), because the evaluation produces a value (or advantage) estimate over the entire state space. In continuous state spaces, this no longer holds: we typically can only obtain an advantage estimate that is accurate along the state–action distribution induced by \\(\\pi_k\\) rather than globally over \\(\\mathcal{S}\\times\\mathcal{A}\\). (If, however, we use off-policy data, the expectation here can be different.) The question “how much better is \\(\\pi_{k+1}\\) than \\(\\pi_k\\)?” motivates a relation between the performances of two policies that explicitly accounts for distribution shift. 3.4.2 Performance Difference Lemma The following performance difference lemma (PDL) expresses the return gap between two policies in terms of the (old) policy’s advantage and the (new) policy’s state-action visitation: Theorem 3.8 (Performance Difference Lemma) Let \\(\\pi\\) and \\(\\pi&#39;\\) be two stationary policies in a discounted MDP with \\(\\gamma\\in[0,1)\\). Then \\[\\begin{equation} J(\\pi&#39;) - J(\\pi) \\;=\\; \\frac{1}{1-\\gamma}\\; \\mathbb{E}_{s\\sim d^{\\pi&#39;},\\,a\\sim \\pi&#39;}\\!\\left[A^{\\pi}(s,a)\\right], \\qquad \\tag{3.47} \\end{equation}\\] where \\(d^{\\pi}(s)=(1-\\gamma)\\sum_{t=0}^\\infty \\gamma^t\\,\\Pr_{\\pi}(s_t=s)\\) is the (discounted) state-visitation distribution generated by policy \\(\\pi\\) and \\(A^\\pi=Q^\\pi-V^\\pi\\) is the advantage. Interpretation. The performance difference lemma highlights distribution shift: the advantage is evaluated under policy \\(\\pi\\), while the expectation is taken over the state–action distribution induced by \\(\\pi&#39;\\). In policy gradients, when performing a step using (3.46), we are approximately maximizing the surrogate \\[ \\mathcal{L}_{\\pi}(\\pi&#39;) := \\frac{1}{1-\\gamma}\\; \\mathbb{E}_{s\\sim d^{\\pi},\\,a\\sim \\pi&#39;}[A^{\\pi}(s,a)], \\] where the state distribution is \\(d_\\pi\\), not \\(d_{\\pi&#39;}\\). To guarantee improvement, we want this surrogate to reflect the true gain \\(J(\\pi&#39;)-J(\\pi)\\). The two coincide when \\(d^{\\pi&#39;} \\approx d^{\\pi}\\). Hence, keep \\(\\pi&#39;\\) close to \\(\\pi\\) so state visitation does not change dramatically, making the surrogate reliable (to some extent, off-policy versions of actor–critic aim to achieve this). This “stay local” principle underpins TRPO, NPG, and PPO. 3.4.3 Trust Region Constraint How to enforce the new policy \\(\\pi_{\\theta_{k+1}}\\) to be close to the old policy \\(\\pi_{\\theta_{k}}\\)? KL Divergence. The Kullback–Leibler (KL) divergence is a type of statistical distance: a measure of how much an approximating probability distribution \\(Q\\) is different from a true probability distribution \\(P\\). Formally, let \\(P\\) and \\(Q\\) be two probability distributions supported on \\(\\mathcal{X}\\), the KL divergence between \\(P\\) and \\(Q\\) is \\[\\begin{equation} D_{\\mathrm{KL}}(P \\Vert Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right) = \\mathbb{E}_{x \\sim P(x)}\\left[ \\log \\left( \\frac{P(x)}{Q(x)} \\right) \\right]. \\tag{3.48} \\end{equation}\\] For example, when \\(P = Q\\), we have \\(D_{\\mathrm{KL}}(P \\Vert Q) = 0\\). Indeed, \\(D_{\\mathrm{KL}}(P \\Vert Q) \\geq 0\\) and the equality holds if and only if \\(P = Q\\). Trust Region Constraint. We now augment the usual policy optimization problem with a trust region constraint defined by the KL divergence. In particular, we wish to improve the current policy \\(\\pi_{\\theta_k}\\) locally by maximizing a surrogate advantage objective while constraining the expected KL divergence from the old policy. This keeps the new policy \\(\\pi_{\\theta}\\) close to \\(\\pi_{\\theta_k}\\), so the surrogate built under \\(d^{\\pi_{\\theta_k}}\\) remains predictive of true improvement. Formally, let \\(\\theta_k\\) denote the current policy parameters. Define the importance ratio \\[ \\rho_\\theta(s,a)=\\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_k}(a\\mid s)}. \\] We aim to maximize the on-policy surrogate \\[\\begin{equation} L_{\\theta_k}(\\theta) \\;=\\; \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}},\\,a\\sim \\pi_{\\theta_k}} \\!\\big[\\rho_\\theta(s,a)\\,\\widehat{A}^{\\,\\pi_{\\theta_k}}(s,a)\\big], \\qquad \\tag{3.49} \\end{equation}\\] subject to an expected KL constraint measured under the old state distribution: \\[\\begin{equation} \\bar D_{\\mathrm{KL}}(\\theta_k\\,\\|\\,\\theta) \\;:=\\; \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}}} \\!\\left[ D_{\\mathrm{KL}}\\!\\big(\\pi_{\\theta_k}(\\cdot\\mid s)\\,\\|\\,\\pi_{\\theta}(\\cdot\\mid s)\\big) \\right] \\;\\le\\;\\delta, \\qquad \\tag{3.50} \\end{equation}\\] with a small radius \\(\\delta&gt;0\\). In summary, we are now interested in the following constrained policy optimization problem: \\[\\begin{equation} \\begin{split} \\max_\\theta &amp; \\quad L_{\\theta_k}(\\theta) \\\\ \\text{subject to} &amp; \\quad \\bar D_{\\mathrm{KL}}(\\theta_k\\,\\|\\,\\theta) \\leq \\delta. \\end{split} \\tag{3.51} \\end{equation}\\] 3.4.4 Natural Policy Gradient The natural policy gradient method (Kakade 2001) can be seen as first performing a linear approximation to the objective of (3.51) and a quadratic approximation to the constraint of (3.51), and then solve the resulting approximate problem in closed form. Leading-Order Approximation. To maximize the surrogate \\(L_{\\theta_k}(\\theta)\\) in (3.49) subject to the KL trust-region constraint (3.50), we linearize the surrogate around \\(\\theta_k\\) and quadratically approximate the KL trust region constraint. This leads to the following convex quadratic program (QP) \\[\\begin{equation} \\max_{\\Delta\\theta}\\ \\ g^\\top \\Delta\\theta \\quad\\text{s.t.}\\quad \\frac{1}{2}\\,\\Delta\\theta^\\top F(\\theta_k)\\,\\Delta\\theta \\;\\le\\; \\delta, \\tag{3.52} \\end{equation}\\] where \\[\\begin{equation} g \\;=\\; \\nabla_\\theta L_{\\theta_k}(\\theta)\\big|_{\\theta=\\theta_k} = \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}},\\,a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)} \\big[\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)\\,\\widehat{A}(s,a)\\big] \\tag{3.53} \\end{equation}\\] is the policy gradient, and \\[\\begin{equation} F(\\theta_k) \\;=\\; \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}},\\,a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)} \\big[\\nabla_\\theta \\log \\pi_{\\theta_k}(a \\mid s)\\,\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)^\\top\\big] \\tag{3.54} \\end{equation}\\] is the (empirical) Fisher information of the policy under the old distribution. See a proof in Section 3.4.5. One can show that the QP (3.52) has a closed-form solution: \\[\\begin{equation} p_{\\text{NPG}} \\;=\\; F(\\theta_k)^{-1}\\, g, \\qquad \\Delta\\theta_{\\text{NPG}} \\;=\\; \\sqrt{\\frac{2\\delta}{g^\\top F(\\theta_k)^{-1} g}}\\;\\;p_{\\text{NPG}}, \\tag{3.55} \\end{equation}\\] where \\(p_{\\text{NPG}}\\) is called the natural policy gradient, for the reason that the usual policy gradient \\(g\\) is pre-multiplied by \\(F(\\theta_k)^{-1}\\), which contains the second-order curvature of the KL constraint. In practice, \\(p_{\\text{NPG}}\\) is computed with conjugate gradient (CG) using Fisher–vector products; no matrices are formed. In (3.55), \\[ \\alpha = \\sqrt{\\frac{2\\delta}{g^\\top F(\\theta_k)^{-1} g}} = \\sqrt{ \\frac{2\\delta}{p^\\top_{\\text{NPG}} F(\\theta_k) p_{\\text{NPG}} } } \\] is often called the trust-region step size. The following pseudocode implements NPG with GAE as the critic. Natural Policy Gradient (with GAE advantages) Inputs: initial policy \\(\\theta_0\\); value/critic \\(\\phi_0\\); discount \\(\\gamma\\); GAE parameter \\(\\lambda\\); KL radius \\(\\delta\\) (or learning rate \\(\\eta\\)); CG iterations \\(K_{\\mathrm{cg}}\\); (optional) damping \\(\\xi&gt;0\\). For iterations \\(k=0,1,2,\\dots\\): Collect rollouts (on-policy). Run \\(\\pi_{\\theta_k}\\) to obtain a batch \\(\\{(s_t,a_t,r_t,s_{t+1},\\mathrm{done}_t)\\}_{t=1}^N\\); cache \\(\\log \\pi_{\\theta_k}(a_t\\mid s_t)\\). Critic / advantages (GAE). Compute TD residuals \\(\\delta_t = r_t + \\gamma(1-\\mathrm{done}_t)V_\\phi(s_{t+1}) - V_\\phi(s_t)\\); backward recursion \\(\\widehat{A}_t = \\delta_t + \\gamma\\lambda(1-\\mathrm{done}_t)\\widehat{A}_{t+1}\\), with \\(\\widehat{A}_{T}=0\\); (optionally) standardize \\(\\widehat{A}\\); set value targets \\(\\widehat{V}^{\\,\\text{targ}}_t=\\widehat{A}_t+V_\\phi(s_t)\\). Value update. Fit \\(V_\\phi\\) by minimizing \\(\\sum_t (V_\\phi(s_t)-\\widehat{V}^{\\,\\text{targ}}_t)^2\\) (one or several epochs). Surrogate gradient. \\[ g \\;=\\; \\frac{1}{N} \\sum_{t} \\nabla_\\theta \\log \\pi_{\\theta_k}(a_t \\mid s_t )\\,\\widehat{A}_t . \\] Fisher–vector product (FvP). Define the empirical KL \\(\\bar D_{\\mathrm{KL}}(\\theta_k\\,\\|\\,\\theta)\\). Implement \\(v\\mapsto Fv\\) as the Hessian–vector product of \\(\\bar D_{\\mathrm{KL}}\\) at \\(\\theta_k\\) (optionally use damping \\(F \\leftarrow F+\\xi I\\) to make sure \\(F\\) is positive definite). Conjugate gradient (CG). Approximately solve \\((F)\\,p = g\\) to obtain \\(p_{\\text{NPG}}\\approx F^{-1}g\\). Step size. Trust-region scaling: set \\(\\alpha \\leftarrow \\sqrt{\\frac{2\\delta}{p^\\top_{\\text{NPG}} F p_{\\text{NPG}}}}\\) and update \\(\\theta_{k+1} \\leftarrow \\theta_k + \\alpha p_{\\text{NPG}}\\). Fixed-rate natural step: choose \\(\\eta&gt;0\\) and set \\(\\theta_{k+1} \\leftarrow \\theta_k + \\eta p_{\\text{NPG}}\\) (monitor empirical KL for safety). 3.4.5 Proof of Fisher Information Let the expected KL trust-region constraint (measured under the old policy’s state distribution) be \\[ \\bar D_{\\mathrm{KL}}(\\theta_k\\!\\parallel\\!\\theta) \\;:=\\; \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}}} \\Big[ D_{\\mathrm{KL}}\\!\\big(\\pi_{\\theta_k}(\\cdot\\mid s)\\,\\|\\,\\pi_\\theta(\\cdot\\mid s)\\big) \\Big]. \\] Write \\(\\theta=\\theta_k+\\Delta\\theta\\) and define, for a fixed state \\(s\\), \\[ f_s(\\theta) \\;=\\; D_{\\mathrm{KL}}\\!\\big(\\pi_{\\theta_k}(\\cdot\\mid s)\\,\\|\\,\\pi_\\theta(\\cdot\\mid s)\\big) = \\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)} \\big[\\log \\pi_{\\theta_k}(a\\mid s) - \\log \\pi_\\theta(a\\mid s)\\big]. \\] We will show that the second-order Taylor expansion of \\(\\bar D_{\\mathrm{KL}}\\) around \\(\\theta_k\\) is \\[ \\bar D_{\\mathrm{KL}}(\\theta_k\\!\\parallel\\!\\theta_k+\\Delta\\theta) \\;=\\; \\frac{1}{2}\\,\\Delta\\theta^\\top \\underbrace{ \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}},\\,a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)} \\!\\big[\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)\\,\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)^\\top\\big] }_{F(\\theta_k)\\ \\text{(Fisher information)}} \\,\\Delta\\theta \\;+\\; \\mathcal{O}(\\|\\Delta\\theta\\|^3). \\] Step 1: Zeroth- and first-order terms vanish at \\(\\theta=\\theta_k\\). For each \\(s\\), \\[ f_s(\\theta_k) = D_{\\mathrm{KL}}\\!\\big(\\pi_{\\theta_k}\\,\\|\\,\\pi_{\\theta_k}\\big)=0. \\] The gradient (holding the expectation under \\(\\pi_{\\theta_k}\\)) is \\[ \\nabla_\\theta f_s(\\theta) = -\\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)}\\big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\big]. \\] Evaluating at \\(\\theta=\\theta_k\\), \\[ \\begin{split} \\nabla_\\theta f_s(\\theta_k) &amp; = -\\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)}\\big[\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)\\big] \\\\ &amp; = -\\sum_a \\pi_{\\theta_k}(a\\mid s)\\,\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s) = -\\nabla_\\theta \\sum_a \\pi_{\\theta_k}(a\\mid s) = 0, \\end{split} \\] using the normalization \\(\\sum_a \\pi_{\\theta_k}(a\\mid s)=1\\). Hence both the value and the first-order term are zero. Step 2: The Hessian equals the (per-state) Fisher information. The Hessian of \\(f_s\\) is \\[ \\nabla_\\theta^2 f_s(\\theta) = -\\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)}\\big[\\nabla_\\theta^2 \\log \\pi_\\theta(a\\mid s)\\big]. \\] At \\(\\theta=\\theta_k\\), apply the information identity (a.k.a. Bartlett identity): \\[ -\\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)}\\big[\\nabla_\\theta^2 \\log \\pi_{\\theta_k}(a\\mid s)\\big] = \\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)} \\big[\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)\\, \\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)^\\top\\big]. \\] Proof sketch of the identity: start from \\(\\sum_a \\pi_\\theta(a\\mid s)=1\\), differentiate once to get \\(\\mathbb{E}_{a\\sim \\pi_\\theta}[\\nabla\\log\\pi_\\theta]=0\\); differentiate again and use the product rule to obtain \\(\\mathbb{E}_{a\\sim \\pi_\\theta}[\\nabla^2\\log\\pi_\\theta + (\\nabla\\log\\pi_\\theta)(\\nabla\\log\\pi_\\theta)^\\top]=0\\). Thus, \\[ \\nabla_\\theta^2 f_s(\\theta_k) = \\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot\\mid s)} \\big[\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)\\,\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)^\\top\\big] \\;=:\\; F_s(\\theta_k). \\] Step 3: Second-order Taylor expansion and averaging over states. For each \\(s\\), \\[ f_s(\\theta_k+\\Delta\\theta) = \\frac{1}{2}\\,\\Delta\\theta^\\top F_s(\\theta_k)\\,\\Delta\\theta \\;+\\; \\mathcal{O}(\\|\\Delta\\theta\\|^3). \\] Taking expectation over \\(s\\sim d^{\\pi_{\\theta_k}}\\) gives \\[ \\bar D_{\\mathrm{KL}}(\\theta_k\\!\\parallel\\!\\theta_k+\\Delta\\theta) = \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}}}[f_s(\\theta_k+\\Delta\\theta)] = \\frac{1}{2}\\,\\Delta\\theta^\\top \\underbrace{\\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}}}[F_s(\\theta_k)]}_{F(\\theta_k)} \\,\\Delta\\theta \\;+\\; \\mathcal{O}(\\|\\Delta\\theta\\|^3). \\] Conclusion. The Fisher information \\(F(\\theta_k)\\) is exactly the Hessian of the expected KL at \\(\\theta_k\\). Therefore, the KL trust-region constraint admits the quadratic local approximation \\[ \\bar D_{\\mathrm{KL}}(\\theta_k\\!\\parallel\\!\\theta_k+\\Delta\\theta)\\;\\approx\\;\\frac{1}{2}\\,\\Delta\\theta^\\top F(\\theta_k)\\,\\Delta\\theta, \\] which yields the TRPO/NPG quadratic constraint and identifies \\(F(\\theta_k)\\) as the local metric tensor of the policy manifold. 3.4.6 Trust Region Policy Optimization The NPG algorithm presented above leverages a leading-order approximation of the KL-constrained policy optimization problem (3.51). In Trust Region Policy Optimization (Schulman, Levine, et al. 2015), we still use the leading-order approximation to obtain the natural policy gradient direction, but additionally, we perform a backtracking line search to enforce the true (nonlinear) KL constraint and surrogate improvement. The following pseudocode implements TRPO with GAE as the critic. TRPO (with GAE advantages) Inputs: initial policy \\(\\theta_0\\); value/critic parameters \\(\\phi_0\\); discount \\(\\gamma\\); GAE parameter \\(\\lambda\\); KL radius \\(\\delta\\); CG iterations \\(K_{\\mathrm{cg}}\\); backtrack factor \\(\\beta\\in(0,1)\\); max backtracks \\(M\\). For iterations \\(k=0,1,2,\\dots\\): Collect rollouts (on-policy). Run \\(\\pi_{\\theta_k}\\) to obtain trajectories; build a batch \\(\\{(s_t,a_t,r_t,s_{t+1},\\mathrm{done}_t)\\}_{t=1}^N\\). Critic / advantages (GAE). Compute TD residuals \\(\\delta_t = r_t + \\gamma(1-\\mathrm{done}_t)V_\\phi(s_{t+1}) - V_\\phi(s_t)\\); backward recursion \\(\\widehat{A}_t = \\delta_t + \\gamma\\lambda(1-\\mathrm{done}_t)\\widehat{A}_{t+1}\\), with \\(\\widehat{A}_{T}=0\\); (optionally) standardize \\(\\widehat{A}\\) within the batch; set value targets \\(\\widehat{V}^{\\,\\text{targ}}_t=\\widehat{A}_t+V_\\phi(s_t)\\). Value function update. Fit \\(V_\\phi\\) by minimizing \\(\\sum_t (V_\\phi(s_t)-\\widehat{V}^{\\,\\text{targ}}_t)^2\\) (one or several epochs). Policy gradient at \\(\\theta_k\\). \\[ g \\;=\\; \\nabla_\\theta L_{\\theta_k}(\\theta)\\big|_{\\theta=\\theta_k} \\approx \\frac{1}{N} \\sum_{t} \\nabla_\\theta \\log \\pi_{\\theta_k}(a_t \\mid s_t )\\,\\widehat{A}_t . \\] Fisher–vector product (FvP). Define the Fisher information under \\(\\pi_{\\theta_k}\\): \\[ F(\\theta_k)\\;=\\;\\mathbb{E}\\Big[\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)\\,\\nabla_\\theta \\log \\pi_{\\theta_k}(a\\mid s)^\\top\\Big]. \\] Implement \\(v\\mapsto Fv\\) via the Hessian-vector product of the empirical KL. Conjugate gradient (CG) solve. Approximately solve \\(F p = g\\) with \\(K_{\\mathrm{cg}}\\) CG iterations to get the natural direction \\(p_{\\text{NPG}}\\approx F^{-1}g\\). Compute step size for the quadratic trust region. \\[ \\alpha \\;\\leftarrow\\; \\sqrt{\\frac{2\\delta}{p^\\top_{\\text{NPG}} F p_{\\text{NPG}} }}. \\] Candidate update: \\(\\theta^\\star \\leftarrow \\theta_k + \\alpha p_{\\text{NPG}}\\). Backtracking line search (feasibility + improvement). Repeatedly set \\(\\theta^\\star \\leftarrow \\theta_k + \\beta^j \\alpha p_{\\text{NPG}}\\) for \\(j=0,1,\\dots,M\\) until both hold on the batch: KL constraint: \\(\\bar D_{\\mathrm{KL}}(\\theta_k\\,\\|\\,\\theta^\\star) \\le \\delta\\). Surrogate improvement: \\(L_{\\theta_k}(\\theta^\\star) \\ge L_{\\theta_k}(\\theta_k)\\). Accept the first \\(\\theta^\\star\\) that satisfies both; set \\(\\theta_{k+1}\\leftarrow \\theta^\\star\\). 3.4.6.1 Backtracking Line Search Batch-only evaluation. During TRPO’s line search you do not collect new trajectories. All checks are computed on the same batch gathered with the old policy \\(\\pi_{\\theta_k}\\) (i.e., under \\(d^{\\pi_{\\theta_k}}\\)). Given: a candidate update \\(\\theta^\\star = \\theta_k + \\beta^j \\alpha p_{\\text{NPG}}\\). Empirical KL constraint (nonlinear, “true” KL). Compute the state-wise KL between the full action distributions of the old and candidate policies and average over the batch states: \\[ \\widehat{\\bar D}_{\\mathrm{KL}}(\\theta_k \\,\\|\\, \\theta^\\star) \\;=\\; \\frac{1}{|\\mathcal{B}|}\\sum_{s\\in\\mathcal{B}} D_{\\mathrm{KL}}\\!\\big(\\pi_{\\theta_k}(\\cdot\\mid s)\\,\\|\\,\\pi_{\\theta^\\star}(\\cdot\\mid s)\\big). \\] Categorical policy: \\[ D_{\\mathrm{KL}}(\\pi_{\\theta_k}\\,\\|\\,\\pi_{\\theta^\\star}) \\;=\\; \\sum_{a} \\pi_{\\theta_k}(a\\mid s)\\, \\Big[\\log \\pi_{\\theta_k}(a\\mid s)-\\log \\pi_{\\theta^\\star}(a\\mid s)\\Big]. \\] Gaussian policy (mean \\(\\mu(s),\\) covariance \\(\\Sigma(s)\\); use pre-squash distribution if actions are squashed): \\[ D_{\\mathrm{KL}}\\big(\\mathcal{N}(\\mu_k,\\Sigma_k)\\,\\|\\,\\mathcal{N}(\\mu_\\star,\\Sigma_\\star)\\big) = \\frac{1}{2}\\Big( \\operatorname{tr}(\\Sigma_\\star^{-1}\\Sigma_k) + (\\mu_\\star-\\mu_k)^\\top \\Sigma_\\star^{-1}(\\mu_\\star-\\mu_k) - d + \\log\\tfrac{\\det \\Sigma_\\star}{\\det \\Sigma_k} \\Big). \\] Feasibility test: accept if \\(\\widehat{\\bar D}_{\\mathrm{KL}}(\\theta_k \\,\\|\\, \\theta^\\star) \\le \\delta\\) (cf. (3.50)). Surrogate improvement. Evaluate the TRPO surrogate \\(L_{\\theta_k}(\\theta)\\) (cf. (3.49)) on the same batch using importance ratios from \\(\\theta^\\star\\): \\[ \\widehat{L}_{\\theta_k}(\\theta^\\star) \\;=\\; \\frac{1}{|\\mathcal{B}|}\\sum_{(s,a)\\in\\mathcal{B}} \\frac{\\pi_{\\theta^\\star}(a\\mid s)}{\\pi_{\\theta_k}(a\\mid s)}\\, \\widehat{A}^{\\,\\pi_{\\theta_k}}(s,a), \\qquad \\widehat{L}_{\\theta_k}(\\theta_k)=\\frac{1}{|\\mathcal{B}|}\\sum_{(s,a)}\\widehat{A}^{\\,\\pi_{\\theta_k}}(s,a). \\] Improvement test: accept if \\(\\widehat{L}_{\\theta_k}(\\theta^\\star)\\ge \\widehat{L}_{\\theta_k}(\\theta_k)\\). Backtracking loop (on-batch). Decrease the step by \\(\\beta\\in(0,1)\\) until both tests pass or a maximum of \\(M\\) backtracks is reached: \\[ \\theta^\\star \\leftarrow \\theta_k + \\beta^j \\alpha p_{\\text{NPG}},\\quad j=0,1,\\dots,M. \\] If successful, set \\(\\theta_{k+1}\\leftarrow \\theta^\\star\\); otherwise keep \\(\\theta_{k+1}\\leftarrow \\theta_k\\). 3.4.7 Proximal Policy Optimization While NPG/TRPO are stable, they may be computationally heavier due to constrained solves or natural-step systems. Proximal Policy Optimization (PPO) (Schulman et al. 2017) replaces the hard constraint with a penalized (regularized) objective and optimizes it with standard first-order SGD: \\[\\begin{equation} \\ell_k(\\theta) \\;=\\; \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_k}},a\\sim \\pi_{\\theta_k}} \\big[\\,\\rho_\\theta(s,a)\\, \\widehat{A}^{\\,\\pi_{\\theta_k}}(s,a)\\,\\big] \\;-\\; \\lambda\\; \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_k}},a\\sim \\pi_{\\theta_k}} \\!\\left[\\log\\frac{\\pi_{\\theta_k}(a\\mid s)}{\\pi_\\theta(a\\mid s)}\\right], \\tag{3.56} \\end{equation}\\] where \\(\\lambda &gt; 0\\) and the second term is the per-sample KL penalty that discourages large departures from \\(\\pi_{\\theta_k}\\). Conceptually, this is a Lagrangian relaxation of TRPO’s trust region, where the hard constraint is moved to the objective function as a soft penalty. 3.4.7.1 Gradient of the KL–Regularized Surrogate Treat \\(\\widehat{A}^{\\,\\pi_{\\theta_k}}\\) and the sampling distribution as fixed during the policy update. Using \\(\\nabla_\\theta \\rho_\\theta = \\rho_\\theta\\,\\nabla_\\theta \\log \\pi_\\theta\\) and \\(\\nabla_\\theta \\log \\frac{\\pi_{\\theta_k}}{\\pi_\\theta} = - \\nabla_\\theta \\log \\pi_\\theta\\), the gradient of the KL-regularized objective (3.56) is \\[ \\nabla_\\theta \\ell_k(\\theta) \\;=\\; \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_k}}, a\\sim \\pi_{\\theta_k}} \\!\\Big[ \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\, \\underbrace{\\big(\\rho_\\theta(s,a)\\,\\widehat{A}(s,a)-\\lambda\\big)}_{\\text{effective advantage}} \\Big]. \\qquad \\tag{3.57} \\] This shows the KL penalty shifts the effective advantage by \\(-\\lambda\\). 3.4.7.2 From the Lagrangian Relaxation to PPO Updates There are two standard PPO realizations: PPO–KL (penalty version). Directly ascend \\(\\ell_k(\\theta)\\) with minibatch SGD: \\[ \\theta \\leftarrow \\theta + \\alpha\\; \\frac{1}{B}\\sum_{(s,a)\\in\\mathcal{B}} \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\, \\big(\\rho_\\theta(s,a)\\,\\widehat{A}(s,a)-\\lambda\\big). \\] After each epoch, measure the empirical KL \\(\\widehat{\\bar D}_{\\mathrm{KL}}(\\theta_k\\!\\parallel\\!\\theta)\\) on the batch; increase \\(\\lambda\\) if KL is too high (tighten the region), decrease \\(\\lambda\\) if it is too low. PPO–Clip (clipping version). Replace the penalty with a hard trust region on the ratio \\(\\rho_\\theta\\). When \\(\\widehat{A}&gt;0\\), forbid \\(\\rho_\\theta&gt;1+\\varepsilon\\); when \\(\\widehat{A}&lt;0\\), forbid \\(\\rho_\\theta&lt;1-\\varepsilon\\). This yields the clipped objective \\[\\begin{equation} \\hspace{-16mm} \\ell^{\\text{CLIP}}_k(\\theta) \\;=\\; \\mathbb{E}_{s \\sim d^{\\pi_{\\theta_k}}, a \\sim \\pi_{\\theta_k}} \\Big[\\min\\!\\big(\\rho_\\theta(s,a)\\,\\widehat{A}(s,a),\\; \\operatorname{clip}(\\rho_\\theta(s,a),\\,1-\\varepsilon,\\,1+\\varepsilon)\\,\\widehat{A}(s,a)\\big)\\Big], \\tag{3.58} \\end{equation}\\] which is a first-order proxy to the Lagrangian/TRPO trust region: the min/clip term cancels the incentive to move \\(\\rho_\\theta\\) outside \\([1-\\varepsilon,1+\\varepsilon]\\) in directions that would further increase the objective. Both versions are typically combined with a value-function loss and an entropy bonus to encourage exploration: \\[ \\mathcal{L}^{\\text{PPO}}(\\theta,\\phi) = - \\ell^{\\text{PG}}_k(\\theta) + c_v\\,\\mathbb{E}\\big[(V_\\phi(s)-\\widehat{V}^{\\,\\text{targ}})^2\\big] - c_e\\,\\mathbb{E}\\big[\\mathcal{H}(\\pi_\\theta(\\cdot\\mid s))\\big], \\] where \\(\\ell^{\\text{PG}}_k\\) is either \\(\\ell^{\\text{CLIP}}_k\\) or \\(\\ell_k\\). Why PPO “forbids” \\(\\rho_\\theta\\) from leaving \\([1-\\varepsilon,\\,1+\\varepsilon]\\). Let \\(r \\equiv \\rho_\\theta(s,a)=\\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_k}(a\\mid s)}\\) and \\(\\widehat{A}=\\widehat{A}^{\\,\\pi_{\\theta_k}}(s,a)\\). The PPO–Clip objective for one sample is \\[ L^{\\text{CLIP}}(r,\\widehat{A}) = \\min\\!\\big(r\\,\\widehat{A},\\ \\operatorname{clip}(r,1-\\varepsilon,1+\\varepsilon)\\,\\widehat{A}\\big). \\] Let’s do a case analysis, as shown in Fig. 3.16. If \\(\\widehat{A}&gt;0\\): increasing \\(r\\) (i.e., increasing \\(\\pi_\\theta(a\\mid s)\\)) raises the unclipped term \\(r\\,\\widehat{A}\\). The clipped term equals \\((1+\\varepsilon)\\widehat{A}\\) whenever \\(r&gt;1+\\varepsilon\\). Hence \\[ L^{\\text{CLIP}}(r,\\widehat{A}) = \\begin{cases} r\\,\\widehat{A}, &amp; r\\le 1+\\varepsilon,\\\\[4pt] (1+\\varepsilon)\\widehat{A}, &amp; r&gt;1+\\varepsilon, \\end{cases} \\] so \\(\\frac{\\partial L^{\\text{CLIP}}}{\\partial r}=\\widehat{A}\\) for \\(r\\le 1+\\varepsilon\\) and \\(0\\) for \\(r&gt;1+\\varepsilon\\). There is no further gain by pushing \\(r\\) beyond \\(1+\\varepsilon\\); the gradient vanishes. Intuitively: don’t increase an action’s probability too much even if it looks good—stay proximal. If \\(\\widehat{A}&lt;0\\): decreasing \\(r\\) (i.e., reducing \\(\\pi_\\theta(a\\mid s)\\)) lowers the unclipped term \\(r\\,\\widehat{A}\\). The clipped term equals \\((1-\\varepsilon)\\widehat{A}\\) whenever \\(r&lt;1-\\varepsilon\\). Thus \\[ L^{\\text{CLIP}}(r,\\widehat{A}) = \\begin{cases} r\\,\\widehat{A}, &amp; r\\ge 1-\\varepsilon,\\\\[4pt] (1-\\varepsilon)\\widehat{A}, &amp; r&lt;1-\\varepsilon, \\end{cases} \\] so \\(\\frac{\\partial L^{\\text{CLIP}}}{\\partial r}=\\widehat{A}(&lt;0)\\) for \\(r\\ge 1-\\varepsilon\\) and \\(0\\) for \\(r&lt;1-\\varepsilon\\). There is no incentive to shrink \\(r\\) below \\(1-\\varepsilon\\); the gradient goes to zero. Figure 3.16: The clipped objective function in PPO (from the original PPO paper). Therefore, the \\(\\min\\) with a clipped ratio creates flat regions where the objective stops improving in the “profitable” outward direction. This removes the optimization incentive to move \\(r\\) outside \\([1-\\varepsilon,1+\\varepsilon]\\), implementing a per-sample trust region on the probability ratio while retaining the standard policy-gradient inside the bracket. The following pseudocode implements PPO (clipped version) with GAE. Proximal Policy Optimization (PPO–Clip) Inputs: policy \\(\\pi_\\theta\\), value \\(V_\\phi\\), discount \\(\\gamma\\), GAE \\(\\lambda\\), clip \\(\\varepsilon\\), coefficients \\(c_v,c_e\\), learning rate \\(\\alpha\\), epochs \\(K_{\\text{epoch}}\\), minibatch size \\(B\\). For iterations \\(k=0,1,2,\\dots\\): Collect on-policy data. Roll out \\(\\pi_{\\theta_k}\\) to get trajectories \\(\\{(s_t,a_t,r_t,s_{t+1},\\mathrm{done}_t)\\}\\). Cache \\(\\log \\pi_{\\theta_k}(a_t\\mid s_t)\\). Compute GAE advantages and value targets. \\(\\delta_t = r_t + \\gamma(1-\\mathrm{done}_t) V_\\phi(s_{t+1}) - V_\\phi(s_t)\\) \\(\\widehat{A}_t = \\delta_t + \\gamma\\lambda(1-\\mathrm{done}_t)\\widehat{A}_{t+1}\\), with \\(\\widehat{A}_T=0\\). \\(\\widehat{V}^{\\,\\text{targ}}_t = \\widehat{A}_t + V_\\phi(s_t)\\). (Optionally standardize \\(\\{\\widehat{A}_t\\}\\) within the batch.) Policy/Value optimization (multiple epochs). For \\(e=1,\\dots,K_{\\text{epoch}}\\): Split the batch into minibatches \\(\\mathcal{B}\\) of size \\(B\\). For each \\(\\mathcal{B}\\): \\[ \\rho_\\theta(s,a) = \\exp\\!\\big(\\log\\pi_\\theta(a\\mid s)-\\log\\pi_{\\theta_k}(a\\mid s)\\big), \\] \\[ \\ell^{\\text{CLIP}}_{\\mathcal{B}}(\\theta) = \\frac{1}{B}\\sum_{(s,a)\\in\\mathcal{B}} \\min\\!\\big(\\rho_\\theta\\,\\widehat{A},\\; \\operatorname{clip}(\\rho_\\theta,1-\\varepsilon,1+\\varepsilon)\\,\\widehat{A}\\big), \\] \\[ \\ell^{\\text{VAL}}_{\\mathcal{B}}(\\phi)= \\frac{1}{B}\\sum_{s\\in\\mathcal{B}}(V_\\phi(s)-\\widehat{V}^{\\,\\text{targ}})^2,\\quad \\mathcal{H}_{\\mathcal{B}}(\\theta)=\\frac{1}{B}\\sum_{s\\in\\mathcal{B}}\\mathcal{H}(\\pi_\\theta(\\cdot\\mid s)). \\] The total loss to be minimized is \\[ \\mathcal{J}_{\\mathcal{B}}(\\theta,\\phi) = - \\ell^{\\text{CLIP}}_{\\mathcal{B}}(\\theta) + c_v\\,\\ell^{\\text{VAL}}_{\\mathcal{B}}(\\phi) - c_e\\,\\mathcal{H}_{\\mathcal{B}}(\\theta). \\] Take an optimizer step on \\(\\mathcal{J}_{\\mathcal{B}}\\) (e.g., Adam with learning rate \\(\\alpha\\)). (Optional) Early stopping by KL. Estimate \\(\\widehat{\\bar D}_{\\mathrm{KL}}(\\theta_k\\!\\parallel\\!\\theta)\\) on the whole batch; stop inner epochs early if it exceeds a threshold. 3.4.8 Soft Actor–Critic Standard actor–critic methods maximize expected return. Soft Actor–Critic (SAC) augments the objective with an entropy bonus that explicitly encourages exploration and robustness while remaining off-policy and sample efficient (Haarnoja et al. 2018). We first introduce a minimal implementation of SAC for discrete actions, then present full SAC with additional techniques for continuous actions. 3.4.8.1 SAC for Discrete Actions Entropy of A Probability Distribution. Given a probability distribution \\(P\\) supported on the set \\(\\mathcal{X}\\), the entropy of the distribution is defined as \\[\\begin{equation} \\mathcal{H}(P) = - \\sum_{x \\in \\mathcal{X}} P(x) \\log P(x) = - \\mathbb{E}_{x \\sim P} \\log P(x). \\tag{3.59} \\end{equation}\\] Since \\(0 \\leq P(x) \\leq 1\\) for any \\(x\\), it is clear that \\(\\mathcal{H}(P) \\geq 0\\) for any distribution \\(P\\). Suppose the set \\(\\mathcal{X}\\) has \\(N\\) elements \\(x_1,\\dots,x_N\\), and suppose \\(P(x_i) = p_i \\geq 0,i=1,\\dots,N\\). We claim that the distribution \\(P^\\star\\) that maximizes \\(\\mathcal{H}(P)\\) is such that \\(p^\\star_i = \\frac{1}{N},i=1,\\dots,N\\). To show this, consider the function \\(\\log t\\) that is concave for \\(t &gt; 0\\). Using Jensen’s inequality, we have that \\[\\begin{equation} \\begin{split} \\mathcal{H}(P) &amp; = - \\sum_{x} P(x) \\log P(x) = \\sum_{x} P(x) \\log \\frac{1}{P(x)} \\\\ &amp; \\leq \\log \\left( \\sum_{x} P(x) \\frac{1}{P(x)} \\right) \\\\ &amp; = \\log N, \\end{split} \\end{equation}\\] with the equality holds if and only if \\(P(x_1)=P(x_2)=\\dots=P(x_N) = \\frac{1}{N}\\). Therefore, maximizing the entropy \\(\\mathcal{H}(P)\\) encourages the distribution \\(P\\) to have a density function that spreads out evenly over the set \\(\\mathcal{X}\\). Maximum-Entropy Objective. SAC maximizes the soft objective \\[\\begin{equation} \\begin{split} J(\\pi) &amp;= \\mathbb{E}\\!\\left[\\sum_{t}\\gamma^t\\Big(R(s_t,a_t)\\;+\\;\\alpha\\,\\mathcal{H}\\!\\left(\\pi(\\cdot\\mid s_t)\\right)\\Big)\\right], \\\\ \\mathcal{H}(\\pi(\\cdot\\mid s)) &amp;= -\\mathbb{E}_{a\\sim\\pi}[\\log \\pi(a\\mid s)], \\end{split} \\tag{3.60} \\end{equation}\\] where the entropy function \\(\\mathcal{H}(\\cdot)\\) encourages the policy to explore, and the temperature \\(\\alpha&gt;0\\) balances reward maximization against exploration. Given a trajectory \\(\\tau = (s_0, a_0, r_0, s_1, a_1,\\dots)\\), define the soft return: \\[ g_t = \\sum_{t=0} \\gamma^t \\left( R(s_t, a_t) - \\alpha \\log \\pi (a_t \\mid s_t) \\right). \\] This leads to the “soft” state value and soft action value associated with \\(\\pi\\): \\[\\begin{equation} \\begin{split} V^\\pi(s) &amp; = \\mathbb{E}_{a\\sim\\pi}\\left[Q^\\pi(s,a)\\;-\\;\\alpha\\log\\pi(a\\mid s)\\right], \\\\ Q^\\pi(s,a) &amp; = R(s,a) + \\gamma\\,\\mathbb{E}_{s&#39;} \\left[V^\\pi(s&#39;)\\right]. \\end{split} \\tag{3.61} \\end{equation}\\] Combining the two equations above, we obtain a soft Bellman Consistency equation on the \\(Q\\) value: \\[\\begin{equation} Q^{\\pi}(s,a) = R(s, a) + \\gamma \\mathbb{E}_{s&#39;} \\left[ \\mathbb{E}_{a&#39; \\sim \\pi} \\left[ Q^\\pi(s&#39;, a&#39;) - \\alpha \\log \\pi(a&#39; \\mid s&#39;) \\right] \\right]. \\tag{3.62} \\end{equation}\\] Critic Update. For a replay sample \\((s,a,r,s&#39;)\\), assuming discrete actions, we can compute the target \\(Q\\) value following the soft Bellman Consistency equation (3.62) \\[\\begin{equation} y \\;=\\; r + \\gamma \\sum_{a&#39;} \\pi_{\\theta} (a&#39; \\mid s&#39;) \\left( Q_{\\bar{\\psi}}(s&#39;, a&#39;) - \\alpha \\log \\pi_\\theta (a&#39; \\mid s&#39;) \\right) \\tag{3.63} \\end{equation}\\] where \\(Q_{\\bar{\\psi}}\\) is the target Q network inspired by DQN to mitigate the deadly triad. The critic loss is therefore \\[\\begin{equation} \\mathcal{L}_Q(\\psi) \\;=\\; \\mathbb{E}_{(s,a) \\sim \\mathcal{D}}\\big[\\big(Q_{\\psi}(s,a)-y\\big)^2\\big], \\tag{3.64} \\end{equation}\\] where the expectation is taken over a minibatch drawn from the replay buffer. Actor Update. Given the learned critic \\(Q_\\psi\\) and replay state distribution \\(s \\sim \\mathcal{D}\\), the SAC policy improvement step chooses \\(\\pi_\\theta\\) to minimize, for each state, the soft advantage–regularized objective \\[\\begin{equation} J_\\pi(\\theta) \\;=\\; \\mathbb{E}_{s\\sim\\mathcal{D}} \\left[ \\sum_{a} \\pi_\\theta(a\\mid s)\\left( \\alpha\\,\\log \\pi_\\theta(a\\mid s) - Q_\\psi(s,a)\\right) \\right]. \\tag{3.65} \\end{equation}\\] For discrete actions, the expectation over \\(a\\) is a finite sum—no action sampling is required. Differentiating (3.65) yields the policy gradient \\[\\begin{equation} \\nabla_\\theta J_\\pi(\\theta) = \\mathbb{E}_{s\\sim\\mathcal{D}} \\left[ \\sum_{a} \\nabla_\\theta \\pi_\\theta(a\\mid s)\\; \\Big(\\alpha\\,[1+\\log \\pi_\\theta(a\\mid s)] - Q_\\psi(s,a)\\Big) \\right]. \\tag{3.66} \\end{equation}\\] The following pseudocode implements a basic SAC algorithm with discrete actions. Soft Actor–Critic (Discrete Actions, Single Q + Single Target) Inputs: replay buffer \\(\\mathcal{D}\\); policy \\(\\pi_\\theta(a\\mid s)\\) over \\(K\\) actions; single critic \\(Q_{\\psi}(s,\\cdot)\\) (returns a \\(K\\)-vector); target critic parameters \\(\\bar\\psi\\); discount \\(\\gamma\\); temperature \\(\\alpha\\) (learned or fixed); Polyak \\(\\tau\\in(0,1]\\); batch size \\(B\\); stepsizes \\(\\alpha_\\theta,\\alpha_\\psi\\). Initialize: \\(\\bar\\psi \\leftarrow \\psi\\). For iterations \\(k=0,1,2,\\dots\\): Interaction. Observe \\(s_t\\). Sample \\(a_t \\sim \\pi_\\theta(\\cdot\\mid s_t)\\); step env to get \\((s_t,a_t,r_t,s_{t+1},\\mathrm{done}_t)\\); push to \\(\\mathcal{D}\\). Sample minibatch. Draw \\(B\\) transitions \\(\\{(s,a,r,s&#39;,d)\\}\\) from \\(\\mathcal{D}\\). Target computation (single target network). Compute \\(\\pi_\\theta(\\cdot\\mid s&#39;)\\) and \\(\\log \\pi_\\theta(\\cdot\\mid s&#39;)\\). Evaluate target critic \\(Q_{\\bar\\psi}(s&#39;,\\cdot)\\). Soft value target: \\[ V_{\\text{tgt}}(s&#39;)=\\Big\\langle \\pi_\\theta(\\cdot\\mid s&#39;),\\; Q_{\\bar\\psi}(s&#39;,\\cdot)-\\alpha\\,\\log \\pi_\\theta(\\cdot\\mid s&#39;) \\Big\\rangle. \\] Bellman target: \\[ y \\leftarrow r + \\gamma(1-d)\\,V_{\\text{tgt}}(s&#39;). \\] (Matches (3.63) with one target network.) Critic update. Minimize the squared error (cf. (3.64)): \\[ \\psi \\leftarrow \\psi - \\alpha_\\psi \\nabla_{\\psi}\\;\\frac{1}{B}\\sum\\nolimits_{(s,a,r,s&#39;,d)} \\big(Q_{\\psi}(s,a)-y\\big)^2. \\] Actor update. Minimize (cf. (3.65)): \\[ \\theta \\leftarrow \\theta - \\alpha_\\theta \\nabla_\\theta \\frac{1}{B}\\sum_s \\sum_{a} \\pi_\\theta(a\\mid s)\\,\\Big(\\alpha\\,\\log \\pi_\\theta(a\\mid s)-Q_{\\psi}(s,a)\\Big). \\] Target critic (Polyak). \\[ \\bar\\psi \\;\\leftarrow\\; \\tau\\,\\psi \\;+\\; (1-\\tau)\\,\\bar\\psi. \\] The next example applies the SAC algorithm above to the cart-pole problem. Example 3.7 (SAC for Cart-pole Balancing) We use a fixed temperature \\(\\alpha=0.2\\). Fig. 3.17 shows the learning curve of SAC. Fig. 3.18 shows a sample rollout of the learned policy. You can find the code here. Play with the temperature parameter. Figure 3.17: Learning curve (Soft Actor–Critic). Figure 3.18: Policy rollout (Soft Actor–Critic). 3.4.8.2 SAC for Continuous Actions In continuous action spaces we cannot sum over actions. SAC therefore: samples actions from the current policy using a reparameterization trick (low-variance gradients), and computes the soft Bellman target with those sampled actions and a twin-target minimum to reduce overestimation. Reparameterization (pathwise) Gradient. Let the stochastic policy be a Gaussian in unconstrained space, squashed by tanh to the action bounds: \\[ u \\;=\\; \\mu_\\theta(s)\\;+\\;\\sigma_\\theta(s)\\odot\\varepsilon,\\quad \\varepsilon\\sim\\mathcal N(0,I), \\qquad a \\;=\\; \\tanh(u)\\cdot a_{\\text{scale}} + a_{\\text{bias}}, \\] where \\(\\sigma_\\theta(s)\\) outputs per-dimension standard deviation. This gives a differentiable map \\(a=f_\\theta(s,\\varepsilon)\\). Expectations over \\(a\\sim \\pi_\\theta(\\cdot\\mid s)\\) are then written as expectations over \\(\\varepsilon\\), so gradients can flow through \\(f_\\theta\\) (the pathwise derivative). The correct log-density under the squashed policy uses change-of-variables: \\[ \\log\\pi_\\theta(a\\mid s) \\;=\\; \\log\\mathcal N\\!\\big(u;\\mu_\\theta(s),\\sigma_\\theta^2(s)\\big) \\;-\\;\\sum_i \\log\\!\\big(1-\\tanh^2(u_i)\\big) \\;+\\;\\text{constant}. \\] The intuition here is that the tanh function is a nonlinear transformation that distorts the original Gaussian distribution. This “tanh correction” is crucial for stable training. Critic Update. Maintain two critics \\(Q_{\\psi_1},Q_{\\psi_2}\\) and their target copies \\(Q_{\\bar\\psi_1},Q_{\\bar\\psi_2}\\). For a replay minibatch \\((s,a,r,s&#39;,d)\\), form the target by drawing a next action from the current policy: \\[\\begin{equation} a&#39; \\sim \\pi_\\theta(\\cdot\\mid s&#39;),\\qquad y \\;=\\; r \\;+\\; \\gamma(1-d)\\,\\Big( \\min\\nolimits_{j=1,2} Q_{\\bar\\psi_j}(s&#39;,a&#39;) \\;-\\; \\alpha \\log\\pi_\\theta(a&#39;\\mid s&#39;) \\Big). \\tag{3.67} \\end{equation}\\] Each critic minimizes the squared error to \\(y\\) (with stop-grad on \\(y\\)): \\[ \\mathcal L_Q(\\psi_j) \\;=\\; \\mathbb{E}\\big[(Q_{\\psi_j}(s,a)-y)^2\\big],\\quad j=1,2, \\] where the expectation is taken over the distribution in the replay buffer. Actor Update. The actor minimizes the soft objective under the replay state distribution: \\[\\begin{equation} J_\\pi(\\theta) \\;=\\; \\mathbb{E}_{s\\sim\\mathcal D,\\,\\varepsilon}\\left[ \\alpha\\,\\log\\pi_\\theta \\big(f_\\theta(s,\\varepsilon)\\mid s\\big) -\\min\\nolimits_{j=1,2} Q_{\\psi_j} \\big(s,f_\\theta(s,\\varepsilon)\\big) \\right]. \\tag{3.68} \\end{equation}\\] By reparameterization, the gradient flows through both the explicit \\(\\log\\pi_\\theta\\) term and the path \\(a=f_\\theta(s,\\varepsilon)\\). Particularly, denote \\(Q_\\psi(\\cdot, \\cdot) = \\min_{j=1,2} Q_{\\psi_j} (\\cdot, \\cdot)\\), we have that \\[ \\nabla_\\theta J_\\pi(\\theta) = \\mathbb{E}_{s,\\varepsilon} \\left[ \\alpha \\nabla_\\theta \\log \\pi_\\theta (a \\mid s) + \\left( \\alpha \\nabla_a \\log \\pi_\\theta(a \\mid s) - \\nabla_a Q_\\psi (s,a) \\right) \\nabla_\\theta f_\\theta (s, \\varepsilon) \\right]_{a=f_\\theta(s,\\varepsilon)}. \\] In code, you typically just write the loss \\[ \\mathbb{E}_{s, \\varepsilon}\\left[ \\alpha \\log \\pi_\\theta (a \\mid s) - Q_\\psi (s, a) \\right], \\quad a = f_\\theta(s, \\varepsilon), \\] and autodiff will automatically compute the correct gradient. Tuning \\(\\alpha\\) (Temperature). \\(\\alpha\\) trades off reward pursuit vs. policy entropy. A fixed \\(\\alpha\\) is problem-dependent. SAC treats \\(\\alpha\\) as a dual variable to enforce a target entropy \\(\\bar{\\mathcal H}\\) (often \\(-\\text{dim}(\\mathcal A)\\)): \\[ J(\\alpha)\\;=\\;\\mathbb{E}_{s\\sim\\mathcal D,\\,a\\sim\\pi_\\theta}\\!\\Big[-\\alpha\\big(\\log \\pi_\\theta(a\\mid s)+\\bar{\\mathcal H}\\big)\\Big], \\quad \\log\\alpha \\leftarrow \\log\\alpha - \\alpha_\\alpha \\nabla_{\\log\\alpha} J(\\alpha). \\] This adapts exploration automatically across tasks and training phases. Soft Actor–Critic (Continuous Actions, Twin Critics + Twin Targets) Inputs: replay buffer \\(\\mathcal{D}\\); policy \\(\\pi_\\theta(a\\mid s)\\) reparameterized by \\(a=f_\\theta(s,\\varepsilon)\\) with tanh-squashed Gaussian; twin critics \\(Q_{\\psi_1},Q_{\\psi_2}\\); twin target critics with params \\(\\bar\\psi_1,\\bar\\psi_2\\); discount \\(\\gamma\\); temperature \\(\\alpha\\) (learned or fixed); Polyak \\(\\tau\\in(0,1]\\); batch size \\(B\\); stepsizes \\(\\alpha_\\theta,\\alpha_\\psi,\\alpha_\\alpha\\). Initialize: \\(\\bar\\psi_j \\leftarrow \\psi_j\\) for \\(j\\in\\{1,2\\}\\). For iterations \\(k=0,1,2,\\dots\\): Interaction. Observe \\(s_t\\). Sample \\(\\varepsilon_t\\sim\\mathcal N(0,I)\\), set \\(a_t=f_\\theta(s_t,\\varepsilon_t)\\); step env to get \\((s_t,a_t,r_t,s_{t+1},\\mathrm{done}_t)\\); push to \\(\\mathcal{D}\\). Sample minibatch. Draw \\(B\\) transitions \\(\\{(s,a,r,s&#39;,d)\\}\\) from \\(\\mathcal{D}\\). Target computation (twin targets, reparameterized next action). Sample \\(\\varepsilon&#39;\\sim\\mathcal N(0,I)\\), set \\(a&#39;=f_\\theta(s&#39;,\\varepsilon&#39;)\\). Compute \\(\\log\\pi_\\theta(a&#39;\\mid s&#39;)\\) with tanh correction. Evaluate target critics \\(Q_{\\bar\\psi_1}(s&#39;,a&#39;)\\), \\(Q_{\\bar\\psi_2}(s&#39;,a&#39;)\\); let \\(Q_{\\min}(s&#39;,a&#39;)=\\min\\{Q_{\\bar\\psi_1},Q_{\\bar\\psi_2}\\}\\). Bellman target: \\[ y \\leftarrow r + \\gamma(1-d)\\,\\big(Q_{\\min}(s&#39;,a&#39;) - \\alpha\\,\\log\\pi_\\theta(a&#39;\\mid s&#39;)\\big). \\] (Stop gradient through \\(y\\).) Critic updates (both heads). \\[ \\psi_j \\leftarrow \\psi_j - \\alpha_\\psi \\nabla_{\\psi_j}\\;\\frac{1}{B}\\sum (Q_{\\psi_j}(s,a)-y)^2,\\quad j=1,2. \\] Actor update (reparameterized). For each \\(s\\) in the batch, sample \\(\\varepsilon\\), set \\(a=f_\\theta(s,\\varepsilon)\\). Actor objective: \\[ J_\\pi(\\theta)=\\frac{1}{B}\\sum_s \\Big(\\alpha\\,\\log\\pi_\\theta(a\\mid s)-\\min\\nolimits_j Q_{\\psi_j}(s,a)\\Big). \\] Update: \\[ \\theta \\leftarrow \\theta - \\alpha_\\theta \\nabla_\\theta J_\\pi(\\theta). \\] Temperature (optional). With target entropy \\(\\bar{\\mathcal H}\\) and parameter \\(\\log\\alpha\\): \\[ J(\\alpha)=\\frac{1}{B}\\sum_s \\big[-\\alpha(\\log\\pi_\\theta(a\\mid s)+\\bar{\\mathcal H})\\big],\\quad \\log\\alpha \\leftarrow \\log\\alpha - \\alpha_\\alpha \\nabla_{\\log\\alpha} J(\\alpha),\\quad \\alpha\\leftarrow e^{\\log\\alpha}. \\] Target critics (Polyak). For \\(j=1,2\\): \\[ \\bar\\psi_j \\;\\leftarrow\\; \\tau\\,\\psi_j \\;+\\; (1-\\tau)\\,\\bar\\psi_j. \\] The next example applies SAC to Inverted Pendulum. Example 3.8 (SAC for Inverted Pendulum) Fig. 3.19 plots the learning curve. Fig. 3.20 visualizes two sample rollouts of the policy. Code can be found here. Figure 3.19: Learning curve (Soft Actor–Critic). Figure 3.20: Policy rollout (Soft Actor–Critic). 3.4.9 Deterministic Policy Gradient In continuous-control tasks, sampling or integrating over actions inside policy gradients is costly and noisy. The Deterministic Policy Gradient (DPG) framework (Silver et al. 2014) replaces the stochastic policy \\(\\pi_\\theta(a\\mid s)\\) with a deterministic actor \\[ a = \\mu_\\theta(s)\\in\\mathbb{R}^m. \\] Its state–action value and discounted state visitation measure are \\[ Q^{\\mu_\\theta}(s,a) \\;=\\; \\mathbb{E}\\!\\left[\\sum_{t=0}^\\infty \\gamma^t R(s_t,a_t)\\,\\middle|\\,s_0=s,\\ a_0=a,\\ a_{t&gt;0}=\\mu_\\theta(s_t)\\right], \\] \\[ \\rho^{\\pi}(s) \\;=\\; \\sum_{t=0}^{\\infty}\\gamma^t \\Pr(s_t=s \\mid a_t\\sim \\pi(\\cdot\\mid s_t)),\\qquad \\rho^{\\mu_\\theta}\\equiv \\rho^{\\pi=\\mu_\\theta}. \\] We consider two objectives: On-policy objective: \\[\\begin{equation} J(\\theta)\\;=\\;\\mathbb{E}_{s\\sim \\rho^{\\mu_\\theta}}\\big[\\,Q^{\\mu_\\theta}(s,\\mu_\\theta(s))\\,\\big]. \\tag{3.69} \\end{equation}\\] Off-policy surrogate (with behavior policy \\(\\beta\\)): \\[\\begin{equation} J_\\beta(\\theta)\\;=\\;\\mathbb{E}_{s\\sim \\rho^{\\beta}} \\big[\\,Q^{\\mu_\\theta}(s,\\mu_\\theta(s))\\,\\big]. \\tag{3.70} \\end{equation}\\] The on-policy objective (3.69) is the usual RL objective in policy gradient methods, as \\(Q^{\\mu_\\theta}(s, \\mu_\\theta(s)) = V^{\\mu_\\theta}(s)\\) by definition. A key result in Deterministic Policy Gradient is that under mild conditions, optimizating the surrogate off-policy objective (3.70) is the same as optimizating the original on-policy objective. To see this, assume \\(R\\) and \\(P(\\cdot\\mid s,a)\\) (the transition dynamics) are bounded/measurable; \\(Q^{\\mu_\\theta}\\) exists and is continuously differentiable in \\(a\\); \\(\\mu_\\theta(s)\\) is continuously differentiable in \\(\\theta\\); Interchange of integration and differentiation is valid (e.g., dominated convergence). Then, the on-policy and off-policy deterministic policy gradients (DPG) are: On-policy DPG. \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}_{s\\sim \\rho^{\\mu_\\theta}} \\left[ \\nabla_\\theta \\mu_\\theta(s)\\; \\nabla_a Q^{\\mu_\\theta}(s,a)\\big|_{a=\\mu_\\theta(s)} \\right]. \\tag{3.71} \\end{equation}\\] Off-policy DPG. For any behavior policy \\(\\beta\\) with visitation \\(\\rho^\\beta\\), \\[\\begin{equation} \\nabla_\\theta J_\\beta(\\theta) \\;=\\; \\mathbb{E}_{s\\sim \\rho^{\\beta}} \\left[ \\nabla_\\theta \\mu_\\theta(s)\\; \\nabla_a Q^{\\mu_\\theta}(s,a)\\big|_{a=\\mu_\\theta(s)} \\right]. \\tag{3.72} \\end{equation}\\] In particular, the off-policy DPG (3.72) can be estimated from replay sampled under \\(\\beta\\) without action-importance ratios; only the state weighting changes. The following result states that the on-policy and off-policy objectives share the same stationary points. Theorem 3.9 (Common First-Order Optima) Let \\[\\begin{equation} g(s;\\theta)\\;:=\\;\\nabla_\\theta \\mu_\\theta(s)\\,\\nabla_a Q^{\\mu_\\theta}(s,a)\\big|_{a=\\mu_\\theta(s)}\\in\\mathbb{R}^{d}, \\tag{3.73} \\end{equation}\\] where \\(d\\) is the dimension of \\(\\theta\\). Suppose \\(\\rho^\\beta\\) has coverage of the on-policy support, i.e., \\[ \\text{supp}(\\rho^{\\mu_\\theta})\\ \\subseteq\\ \\text{supp}(\\rho^\\beta),\\quad \\text{and}\\quad \\rho^\\beta(s)&gt;0\\ \\text{a.e. on }\\text{supp}(\\rho^{\\mu_\\theta}). \\] If \\(g(s;\\theta^\\star)=0\\) for \\(\\rho^{\\mu_{\\theta^\\star}}\\)-almost every \\(s\\) (in particular, if \\(\\mu_{\\theta^\\star}\\) is greedy w.r.t. \\(Q^{\\mu_{\\theta^\\star}}\\), so \\(\\nabla_a Q^{\\mu_{\\theta^\\star}}(s,a)|_{a=\\mu_{\\theta^\\star}(s)}=0\\) for all \\(s\\)), then \\[ \\nabla_\\theta J(\\theta^\\star)=0 \\quad\\text{and}\\quad \\nabla_\\theta J_\\beta(\\theta^\\star)=0. \\] Thus any deterministic policy satisfying the first-order optimality condition (greedy w.r.t. its own \\(Q\\)) is a stationary point of both \\(J\\) and \\(J_\\beta\\), regardless of the (covered) state weighting. If additionally \\(\\text{supp}(\\rho^{\\mu_\\theta})=\\text{supp}(\\rho^\\beta)\\) and both are strictly positive on that support, then \\[ \\nabla_\\theta J(\\theta)=0 \\ \\Longleftrightarrow\\ \\nabla_\\theta J_\\beta(\\theta)=0. \\] Remarks. The off-policy objective \\(J_\\beta\\) changes only the weights over states; the per-state improvement direction \\(g(s;\\theta)\\) is identical. With sufficient coverage, ascent on \\(J_\\beta\\) improves \\(J\\) and shares its stationary points. In practice, DDPG uses exploration noise to expand support of \\(\\rho^\\beta\\) and target networks to stabilize \\(Q^{\\mu_\\theta}\\), making the off-policy gradient estimate reliable. From DPG to DDPG (Deep DPG). DDPG (Lillicrap et al. 2015) implements DPG with deep networks + standard stabilizers: Replay buffer \\(\\mathcal D\\) for off-policy sample efficiency. Target networks \\(\\mu_{\\bar\\theta}, Q_{\\bar\\psi}\\) with Polyak averaging to stabilize TD targets. Exploration noise added to the deterministic action: \\(a_t = \\mu_\\theta(s_t) + \\varepsilon_t\\) (original paper used Ornstein–Uhlenbeck noise; Gaussian works well too). High-Level Algorithm (DDPG). Interact off-policy. Act with exploration: \\(a_t=\\mu_\\theta(s_t)+\\varepsilon_t\\). Store \\((s_t,a_t,r_t,s_{t+1},\\text{done}_t)\\) in \\(\\mathcal D\\). Critic TD(0). For a minibatch from \\(\\mathcal D\\), \\[ y = r + \\gamma(1-\\text{done})\\,Q_{\\bar\\psi} \\big(s&#39;,\\,\\mu_{\\bar\\theta}(s&#39;)\\big),\\qquad \\min_\\psi\\ \\frac{1}{B}\\sum (Q_\\psi(s,a)-y)^2. \\] Actor DPG step. \\[ \\max_\\theta\\ \\frac{1}{B}\\sum Q_\\psi\\big(s,\\,\\mu_\\theta(s)\\big) \\quad\\Longleftrightarrow\\quad \\nabla_\\theta J \\approx \\frac{1}{B}\\sum \\nabla_\\theta \\mu_\\theta(s)\\,\\nabla_a Q_\\psi(s,a)\\big|_{a=\\mu_\\theta(s)}. \\] Targets Polyak update. \\[ \\bar\\theta \\leftarrow \\tau\\,\\theta + (1-\\tau)\\bar\\theta,\\ \\ \\bar\\psi \\leftarrow \\tau\\,\\psi + (1-\\tau)\\bar\\psi. \\] Remarks. No entropy bonus or log-probabilities (in contrast to SAC). Exploration comes from additive noise. Overestimation and sensitivity to hyperparameters can appear; target networks, small actor steps, and proper normalization help. The following pseudocode implements DDPG. Deep Deterministic Policy Gradient (DDPG) Inputs: replay buffer \\(\\mathcal{D}\\); deterministic actor \\(\\mu_\\theta(s)\\); critic \\(Q_\\psi(s,a)\\); target networks \\(\\mu_{\\bar\\theta}, Q_{\\bar\\psi}\\); discount \\(\\gamma\\in[0,1)\\); Polyak \\(\\tau\\in(0,1]\\); batch size \\(B\\); stepsizes \\(\\alpha_\\theta,\\alpha_\\psi\\); exploration noise process \\(\\varepsilon_t\\sim \\mathcal N(0,\\sigma^2 I)\\) (or Ornstein–Uhlenbeck). Initialize: \\(\\bar\\theta\\leftarrow\\theta,\\ \\bar\\psi\\leftarrow\\psi\\). Fill \\(\\mathcal D\\) with a short random warm-up. For iterations \\(k=0,1,2,\\dots\\): Interaction (off-policy). Observe \\(s_t\\). Compute action with noise \\[ a_t \\leftarrow \\text{clip}\\big(\\mu_\\theta(s_t) + \\varepsilon_t,\\ a_{\\min}, a_{\\max}\\big). \\] Step env to get \\((s_t,a_t,r_t,s_{t+1},\\mathrm{done}_t)\\). Push into \\(\\mathcal D\\). Sample minibatch. Draw \\(B\\) transitions \\(\\{(s,a,r,s&#39;,d)\\}\\) from \\(\\mathcal D\\). Critic target. \\[ a&#39; \\leftarrow \\mu_{\\bar\\theta}(s&#39;),\\qquad y \\leftarrow r + \\gamma(1-d)\\,Q_{\\bar\\psi}(s&#39;,a&#39;). \\] (Stop gradient through \\(y\\).) Critic update. \\[ \\psi \\leftarrow \\psi - \\alpha_\\psi\\,\\nabla_\\psi\\ \\frac{1}{B}\\sum (Q_\\psi(s,a)-y)^2. \\] Actor update (DPG). \\[ \\theta \\leftarrow \\theta + \\alpha_\\theta\\ \\frac{1}{B}\\sum \\Big[\\ \\nabla_\\theta \\mu_\\theta(s)\\ \\nabla_a Q_\\psi(s,a)\\big|_{a=\\mu_\\theta(s)}\\ \\Big]. \\] (Equivalently, ascend \\(\\frac{1}{B}\\sum Q_\\psi(s,\\mu_\\theta(s))\\) by backprop.) Target networks (Polyak). \\[ \\bar\\theta \\leftarrow \\tau\\,\\theta + (1-\\tau)\\,\\bar\\theta,\\qquad \\bar\\psi \\leftarrow \\tau\\,\\psi + (1-\\tau)\\,\\bar\\psi. \\] The next example applies DDPG to Inverted Pendulum. Example 3.9 (DDPG for Inverted Pendulum) Fig. 3.21 plots the learning curve of DDPG. Fig. 3.22 visualizes sample rollouts of the learned policy. Code can be found here. Figure 3.21: Learning curve (DDPG). Figure 3.22: Policy rollout (DDPG). 3.5 Model-based Policy Optimization Model-based policy optimization (MBPO) (Janner et al. 2019) sits between pure model-free methods (high variance, data hungry) and “plan-only” model-based control (sensitive to model bias, to be introduced in Chapter 4). The key idea is to learn a dynamics model and then use only short rollouts from that model to create extra training data for a strong off-policy learner (usually SAC). Consider an MDP with unknown dynamics \\(s_{t+1} \\sim P( \\cdot \\mid s_t,a_t)\\). MBPO learns an ensemble \\(\\{\\hat f_{\\psi_k}\\}_{k=1}^K\\) that predicts the next state (often the delta-state \\(\\Delta s\\)). Rather than planning far ahead inside the learned model, MBPO: Collects real transitions \\(\\mathcal{D}_{\\text{env}}\\) by interacting with the environment. Fits the dynamics ensemble on \\(\\mathcal{D}_{\\text{env}}\\). Periodically generates short rollouts (e.g., horizon \\(H=1\\ldots5\\)) starting from real states by simulating with a random member of the ensemble, producing model transitions \\(\\mathcal{D}_{\\text{model}}\\). Trains an off-policy actor-critic (e.g., SAC) on a mixture of \\(\\mathcal{D}_{\\text{env}}\\) and \\(\\mathcal{D}_{\\text{model}}\\), typically with a high fraction of model data but short \\(H\\) to limit bias. This yields the sample-efficiency benefits of model-based learning while maintaining the robustness of model-free policy optimization. The following pseudocode implements MBPO with SAC as the off-policy learner. Model-based Policy Optimization (SAC as Off-Policy Learner) Inputs: environment \\(\\mathcal{E}\\), policy \\(\\pi_\\theta(a\\mid s)\\), twin critics \\(Q_{\\phi_1},Q_{\\phi_2}\\) with targets, temperature \\(\\alpha\\) (auto-tuned), dynamics ensemble \\(\\{\\hat f_{\\psi_k}\\}_{k=1}^K\\), real buffer \\(\\mathcal{D}_{\\text{env}}\\), model buffer \\(\\mathcal{D}_{\\text{model}}\\). Rollout horizon \\(H\\), model ratio \\(p_{\\text{model}}\\in[0,1]\\), update counts \\(G_{\\text{dyn}}, G_{\\text{rl}}\\). Warm-up &amp; data collection. Interact with \\(\\mathcal{E}\\) using \\(\\pi_\\theta\\) (or random for a short warm-up). Store \\((s,a,r,s&#39;)\\) in \\(\\mathcal{D}_{\\text{env}}\\). Fit dynamics. For \\(G_{\\text{dyn}}\\) steps: Sample minibatch \\(B\\subset\\mathcal{D}_{\\text{env}}\\). Update each \\(\\psi_k\\) to predict \\(\\Delta s = s&#39;-s\\) (and optionally \\(r\\)) by minimizing, e.g, mean squared error. Short model rollouts (data generation). Sample seed states \\(S_0\\) from recent \\(\\mathcal{D}_{\\text{env}}\\). For each \\(s\\in S_0\\) for \\(h=1\\ldots H\\): Sample \\(a\\sim \\pi_\\theta(\\cdot\\mid s)\\). Pick random ensemble member \\(k\\); predict \\(\\hat\\Delta s \\leftarrow \\hat f_{\\psi_k}(s,a)\\); set \\(\\hat s&#39; = s + \\hat\\Delta s\\). Compute \\(r\\) via a learned reward model or a known formula (when available). Push \\((s,a,r,\\hat s&#39;,\\texttt{done}=0)\\) into \\(\\mathcal{D}_{\\text{model}}\\). Set \\(s\\leftarrow\\hat s&#39;\\); break if time-limit reached. Off-policy RL updates (SAC). For \\(G_{\\text{rl}}\\) steps: Form a minibatch by drawing a fraction \\(p_{\\text{model}}\\) from \\(\\mathcal{D}_{\\text{model}}\\) and \\(1-p_{\\text{model}}\\) from \\(\\mathcal{D}_{\\text{env}}\\). Critic targets: \\[ y=r+\\gamma(1-d)\\big[\\min_j Q_{\\phi_j^-}(s&#39;,a&#39;)-\\alpha\\log\\pi_\\theta(a&#39;\\mid s&#39;)\\big], \\] where \\(a&#39;\\sim\\pi_\\theta(\\cdot\\mid s&#39;)\\). Critic update: regress \\(Q_{\\phi_j}\\) to \\(y\\) (both heads). Actor update: minimize \\(J_\\pi = \\mathbb{E}_s\\!\\big[\\alpha\\log\\pi_\\theta(a\\mid s) - \\min_j Q_{\\phi_j}(s,a)\\big]\\), with \\(a\\sim\\pi_\\theta\\). Temperature update (optional): adjust \\(\\alpha\\) towards target entropy. Soft-update target critics. Repeat steps 1–4 until convergence or iteration limits. Notes. Ensembles capture epistemic uncertainty; random-member rollouts implicitly regularize toward pessimism. Keeping \\(H\\) short (e.g., \\(1\\!\\!-\\!\\!5\\)) is crucial to prevent model error explosion. Use recent real states as rollout seeds to stay on-distribution. The next example applies MBPO to Inverted Pendulum. Example 3.10 (MBPO for Inverted Pendulum) Fig. 3.23 plots the learning curve. Fig. 3.24 visualizes sample rollouts of the policy. Code can be found here. Figure 3.23: Learning curve (MBPO). Figure 3.24: Policy rollout (MBPO). References Barto, Andrew G, Richard S Sutton, and Charles W Anderson. 2012. “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.” IEEE Transactions on Systems, Man, and Cybernetics, no. 5: 834–46. Garrigos, Guillaume, and Robert M Gower. 2023. “Handbook of Convergence Theorems for (Stochastic) Gradient Methods.” arXiv Preprint arXiv:2301.11235. Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” In International Conference on Machine Learning, 1861–70. Pmlr. Janner, Michael, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. “When to Trust Your Model: Model-Based Policy Optimization.” Advances in Neural Information Processing Systems 32. Kakade, Sham M. 2001. “A Natural Policy Gradient.” Advances in Neural Information Processing Systems 14. Lillicrap, Timothy P, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. “Continuous Control with Deep Reinforcement Learning.” arXiv Preprint arXiv:1509.02971. Nesterov, Yurii. 2018. Lectures on Convex Optimization. Vol. 137. Springer. Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. “Trust Region Policy Optimization.” In International Conference on Machine Learning, 1889–97. PMLR. Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. “High-Dimensional Continuous Control Using Generalized Advantage Estimation.” arXiv Preprint arXiv:1506.02438. Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. “Proximal Policy Optimization Algorithms.” arXiv Preprint arXiv:1707.06347. Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In International Conference on Machine Learning, 387–95. Pmlr. "],["model-based-plan-optimize.html", "Chapter 4 Model-based Planning and Optimization 4.1 Linear Quadratic Regulator 4.2 LQR Trajectory Tracking 4.3 Trajectory Optimization 4.4 Model Predictive Control 4.5 Model Predictive Path Integral Control 4.6 Rapidly Exploring Random Tree", " Chapter 4 Model-based Planning and Optimization In Chapter 1, we studied tabular MDPs with known dynamics where both the state and action spaces are finite, and we saw how dynamic-programming methods—Policy Iteration (PI) and Value Iteration (VI)—recover optimal policies. Chapters 2 and 3 generalized these ideas to unknown dynamics and continuous spaces by introducing function approximation for value functions and policies. Those methods are model-free: they assume no access to the transition model and rely solely on data collected from interaction. This chapter turns to the complementary regime: known dynamics with continuous state and action spaces. Our goal is to develop model-based planning and optimization methods that exploit the known dynamics to compute high-quality decisions efficiently. We proceed in three steps: Linear-quadratic systems. For linear dynamics and quadratic stage/terminal rewards (or costs), dynamic programming yields a linear optimal policy that can be computed efficiently via Riccati recursions. This setting serves as a tractable, illuminating baseline and a recurring building block in more general algorithms. Trajectory optimization (TO) for nonlinear systems. When dynamics are nonlinear, we focus on planning an optimal state–action trajectory from a given initial condition, maximizing the cumulative reward (or minimizing cumulative cost) subject to dynamics and constraints. Unlike RL, which seeks an optimal feedback policy valid for all states, TO computes an open-loop plan (often with a time-varying local feedback around a nominal trajectory). Although less ambitious, TO naturally accommodates state/control constraints—common in motion planning under safety, actuation, and environmental limits—and is widely used in robotics and control. Model predictive control (MPC). MPC converts open-loop plans into a feedback controller by repeatedly solving a short-horizon TO problem at each time step, applying only the first action, and receding the horizon. This receding-horizon strategy brings robustness to disturbances and model mismatch while retaining the constraint-handling benefits of TO. We adopt the standard discrete-time dynamical system notation \\[\\begin{equation} x_{t+1} = f_t(x_t, u_t, w_t), \\tag{4.1} \\end{equation}\\] where \\(x_t \\in \\mathbb{R}^n\\) is the state, \\(u_t \\in \\mathbb{R}^m\\) is the control/action, \\(w_t \\in \\mathbb{R}^d\\) is a (possibly stochastic) disturbance, and \\(f_t\\) is a known transition function, potentially nonlinear or nonsmooth. The goal is to find a policy that maximizes a sum of stage rewards \\(r(x_t,u_t)\\) and optional terminal reward \\(r_T(x_T)\\). We will often use the cost-minimization form \\(c = -r\\). State and action constraints are written as \\[ x_t \\in \\mathcal{X}, \\qquad u_t \\in \\mathcal{U}. \\] 4.1 Linear Quadratic Regulator In this section, we focus on the case when \\(f_t\\) is a linear function, and the rewards/costs are quadratic in \\(x\\) and \\(u\\). This family of problems is known as linear quadratic regulator (LQR). 4.1.1 Finite-Horizon LQR Consider a linear discrete-time dynamical system \\[\\begin{equation} x_{k+1} = A_k x_k + B_k u_k + w_k, \\quad k=0,1,\\dots,N-1, \\tag{4.2} \\end{equation}\\] where \\(x_k \\in \\mathbb{R}^n\\) the state, \\(u_k \\in \\mathbb{R}^m\\) the control, \\(w_k \\in \\mathbb{R}^n\\) the independent, zero-mean disturbance with given probability distribution that does not depend on \\(x_k,u_k\\), and \\(A_k \\in \\mathbb{R}^{n \\times n}, B_k \\in \\mathbb{R}^{n \\times m}\\) are known matrices determining the transition dynamics. We want to solve the following optimal control problem \\[\\begin{equation} \\min_{\\mu_0,\\dots,\\mu_{N-1}} \\mathbb{E} \\left\\{ x_N^\\top Q_N x_N + \\sum_{k=0}^{N-1} \\left( x_k^\\top Q_k x_k + u_k^\\top R_k u_k \\right) \\right\\}, \\tag{4.3} \\end{equation}\\] where \\(\\mu_0,\\dots,\\mu_{N-1}\\) are feedback policies/controllers that map states to actions and the expectation is taken over the randomness in \\(w_0,\\dots,w_{N-1}\\). In (4.3), \\(\\{Q_k \\}_{k=0}^N\\) are positive semidefinite matrices, and \\(\\{ R_k \\}_{k=0}^{N-1}\\) are positive definite matrices. The formulation (4.3) is known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to “regulate” the system around the origin \\(x=0\\). The Bellman Optimality condition introduced in Theorem 1.1 still holds for continuous state and action spaces. Therefore, we will try to follow the dynamic programming (DP) algorithm in Section 1.1.4 to solve for the optimal policy. The DP algorithm computes the optimal cost-to-go backwards in time. The terminal cost is \\[ J_N(x_N) = x_N^\\top Q_N x_N \\] by definition. The optimal cost-to-go at time \\(N-1\\) is equal to \\[\\begin{equation} \\begin{split} J_{N-1}(x_{N-1}) = \\min_{u_{N-1}} \\mathbb{E}_{w_{N-1}} \\{ x_{N-1}^\\top Q_{N-1} x_{N-1} + u_{N-1}^\\top R_{N-1} u_{N-1} + \\\\ \\Vert \\underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \\Vert^2_{Q_N} \\} \\end{split} \\tag{4.4} \\end{equation}\\] where \\(\\Vert v \\Vert_Q^2 = v^\\top Q v\\) for \\(Q \\succeq 0\\). Now observe that the objective in (4.4) is \\[\\begin{equation} \\begin{split} x_{N-1}^\\top Q_{N-1} x_{N-1} + u_{N-1}^\\top R_{N-1} u_{N-1} + \\Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \\Vert_{Q_N}^2 + \\\\ \\mathbb{E}_{w_{N-1}} \\left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^\\top Q_{N-1} w_{N-1} \\right] + \\\\ \\mathbb{E}_{w_{N-1}} \\left[ w_{N-1}^\\top Q_N w_{N-1} \\right] \\end{split} \\end{equation}\\] where the second line is zero due to \\(\\mathbb{E}[w_{N-1}] = 0\\) and the third line is a constant with respect to \\(u_{N-1}\\). Consequently, the optimal control \\(u_{N-1}^\\star\\) can be computed by setting the derivative of the objective with respect to \\(u_{N-1}\\) equal to zero \\[\\begin{equation} u_{N-1}^\\star = - \\left[ \\left( R_{N-1} + B_{N-1}^\\top Q_N B_{N-1} \\right)^{-1} B_{N-1}^\\top Q_N A_{N-1} \\right] x_{N-1}. \\tag{4.5} \\end{equation}\\] Plugging the optimal controller \\(u^\\star_{N-1}\\) back to the objective of (4.4) leads to \\[\\begin{equation} J_{N-1}(x_{N-1}) = x_{N-1}^\\top S_{N-1} x_{N-1} + \\mathbb{E} \\left[ w_{N-1}^\\top Q_N w_{N-1} \\right], \\tag{4.6} \\end{equation}\\] with \\[ S_{N-1} = Q_{N-1} + A_{N-1}^\\top \\left[ Q_N - Q_N B_{N-1} \\left( R_{N-1} + B_{N-1}^\\top Q_N B_{N-1} \\right)^{-1} B_{N-1}^\\top Q_N \\right] A_{N-1}. \\] We note that \\(S_{N-1}\\) is positive semidefinite (this is an exercise for you to convince yourself). Now we realize that something surprising and nice has happened. The optimal controller \\(u^{\\star}_{N-1}\\) in (4.5) is a linear feedback policy of the state \\(x_{N-1}\\), and The optimal cost-to-go \\(J_{N-1}(x_{N-1})\\) in (4.6) is quadratic in \\(x_{N-1}\\), just the same as \\(J_{N}(x_N)\\). This implies that, if we continue to compute the optimal cost-to-go at time \\(N-2\\), we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is, The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time. We summarize the solution for the LQR problem (4.3) as follows. Proposition 4.1 (Solution of Discrete-Time Finite-Horizon LQR) The optimal controller for the LQR problem (4.3) is a linear state-feedback policy \\[\\begin{equation} \\mu_k^\\star(x_k) = - K_k x_k, \\quad k=0,\\dots,N-1. \\tag{4.7} \\end{equation}\\] The gain matrix \\(K_k\\) can be computed as \\[ K_k = \\left( R_k + B_k^\\top S_{k+1} B_k \\right)^{-1} B_k^\\top S_{k+1} A_k, \\] where the matrix \\(S_k\\) satisfies the following backwards recursion \\[\\begin{equation} \\hspace{-6mm} \\begin{split} S_N &amp;= Q_N \\\\ S_k &amp;= Q_k + A_k^\\top \\left[ S_{k+1} - S_{k+1}B_k \\left( R_k + B_k^\\top S_{k+1} B_k \\right)^{-1} B_k^\\top S_{k+1} \\right] A_k, k=N-1,\\dots,0. \\end{split} \\tag{4.8} \\end{equation}\\] The optimal cost-to-go is given by \\[ J_0(x_0) = x_0^\\top S_0 x_0 + \\sum_{k=0}^{N-1} \\mathbb{E} \\left[ w_k^\\top S_{k+1} w_k\\right]. \\] The recursion (4.8) is called the discrete-time Riccati equation. Proposition 4.1 states that, to evaluate the optimal policy (4.7), one can first run the backwards Riccati equation (4.8) to compute all the positive definite matrices \\(S_k\\), and then compute the gain matrices \\(K_k\\). For systems of reasonable dimensions, evalutating the matrix inversion in (4.8) should be fairly efficient. 4.1.2 Infinite-Horizon LQR We now switch to the infinite-horizon LQR problem \\[\\begin{align} \\min_{\\mu} &amp; \\quad \\sum_{k=0}^{\\infty} \\left( x_k^\\top Q x_k + u_k^\\top R u_k \\right) \\tag{4.9} \\\\ \\text{subject to} &amp; \\quad x_{k+1} = A x_k + B u_k, \\quad k=0,\\dots,\\infty, \\tag{4.10} \\end{align}\\] where \\(Q \\succeq 0\\), \\(R \\succ 0\\), \\(A,B\\) are constant matrices, and we seek a stationary policy \\(\\mu\\) that maps states to actions. Note that here we remove the disturbance \\(w_k\\) because in general adding \\(w_k\\) will make the objective function unbounded. To handle \\(w_k\\), we will have to either add a discount factor \\(\\gamma\\), or switch to an average cost objective function. For infinite-horizon problems, the Bellman Optimality condition changes from a recursion to an equation. Specifically, according to Theorem 1.2 and equation (1.28), the optimal value function should satisfy the following Bellman optimality equation, restated for the case of cost minimization instead of reward maximization: \\[\\begin{equation} J^\\star (x) = \\min_{u} \\left[ c(x,u) + \\sum_{x&#39;} P(x&#39; \\mid x, u) J^\\star (x&#39;) \\right], \\quad \\forall x, \\tag{4.11} \\end{equation}\\] where \\(c(x,u)\\) is the cost function. Guess A Solution. Based on our derivation in the finite-horizon case, we might as well guess that the optimal value function is a quadratic function: \\[ J(x) = x^\\top S x, \\quad \\forall x, \\] for some positive definite matrix \\(S\\). Then, our guessed solution must satisfy the Bellman optimality stated in (4.11): \\[\\begin{equation} x^\\top S x = J(x) = \\min_{u} \\left\\{ x^\\top Q x + u^\\top R u + \\Vert \\underbrace{A x + B u}_{x&#39;} \\Vert_S^2 \\right\\}. \\tag{4.12} \\end{equation}\\] The minimization over \\(u\\) in (4.12) can again be solved in closed-form by setting the gradient of the objective with respect to \\(u\\) to be zero \\[\\begin{equation} u^\\star = - \\underbrace{\\left[ \\left( R + B^\\top S B \\right)^{-1} B^\\top S A \\right]}_{K} x. \\tag{4.13} \\end{equation}\\] Plugging the optimal \\(u^\\star\\) back into (4.12), we see that the matrix \\(S\\) has to satisfy the following equation \\[\\begin{equation} S = Q + A^\\top \\left[ S - SB \\left( R + B^\\top S B \\right)^{-1} B^\\top S \\right] A. \\tag{4.14} \\end{equation}\\] Equation (4.14) is known as the discrete algebraic Riccati equation (DARE). So the question boils down to if the DARE has a solution \\(S\\) that is positive definite? Proposition 4.2 (Solution of Discrete-Time Infinite-Horizon LQR) Consider a linear system \\[ x_{k+1} = A x_k + B u_k, \\] with \\((A,B)\\) controllable (see Section 4.1.3). Let \\(Q \\succeq 0\\) in (4.9) be such that \\(Q\\) can be written as \\(Q = C^\\top C\\) with \\((A,C)\\) observable. Then the optimal controller for the infinite-horizon LQR problem (4.9) is a stationary linear policy \\[ \\mu^\\star (x) = - K x, \\] with \\[ K = \\left( R + B^\\top S B \\right)^{-1} B^\\top S A. \\] The matrix \\(S\\) is the unique positive definite matrix that satisfies the discrete algebraic Riccati equation \\[\\begin{equation} S = Q + A^\\top \\left[ S - SB \\left( R + B^\\top S B \\right)^{-1} B^\\top S \\right] A. \\tag{4.15} \\end{equation}\\] Moreover, the closed-loop system \\[ x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k \\] is stable, i.e., the eigenvalues of the matrix \\(A - BK\\) are strictly within the unit circle (see Appendix B.1.2). Remark. The assumptions of \\((A,B)\\) being controllable and \\((A,C)\\) being observable can be relaxted to \\((A,B)\\) being stabilizable and \\((A,C)\\) being detectable (for definitions of stabilizability and detectability, see Appendix B). We have not discussed how to solve the algebraic Riccati equation (4.15). It is clear that (4.15) is not a linear system of equations in \\(S\\). In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see (Arnold and Laub 1984). Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab dlqr and the Python scipy.linalg.solve_discrete_are function computes the \\(K\\) and \\(S\\) matrices from \\(A,B,Q,R\\). Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum. Example 4.1 (Pendulum Stabilization by LQR) Consider the simple pendulum in Fig. 4.1 with dynamics \\[\\begin{equation} x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ -\\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix} \\tag{4.16} \\end{equation}\\] where \\(m\\) is the mass of the pendulum, \\(l\\) is the length of the pole, \\(g\\) is the gravitational constant, \\(b\\) is the damping ratio, and \\(u\\) is the torque applied to the pendulum. We are interested in applying the LQR controller to balance the pendulum in the upright position \\(x_d = [\\pi,0]^\\top\\) with a zero velocity. Figure 4.1: A Simple Pendulum. Let us first shift the dynamics so that “\\(0\\)” is the upright position. This can be done by defining a new variable \\(z = x - x_d = [\\theta - \\pi, \\dot{\\theta}]^\\top\\), which leads to \\[\\begin{equation} \\dot{z} = \\dot{x} = f(x,u) = f(z + x_d,u) = \\begin{bmatrix} z_2 \\\\ \\frac{1}{ml^2} \\left( u - b z_2 + mgl \\sin z_1 \\right) \\end{bmatrix} = f&#39;(z,u). \\tag{4.17} \\end{equation}\\] We then linearize the nonlinear dynamics \\(\\dot{z} = f&#39;(z,u)\\) at the point \\(z^\\star = 0, u^\\star = 0\\): \\[\\begin{align} \\dot{z} &amp; \\approx f&#39;(z^\\star,u^\\star) + \\left( \\frac{\\partial f&#39;}{\\partial z} \\right)_{z^\\star,u^\\star} (z - z^\\star) + \\left( \\frac{\\partial f&#39;}{\\partial u} \\right)_{z^\\star,u^\\star} (u - u^\\star) \\\\ &amp; = \\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} \\cos z_1 &amp; - \\frac{b}{ml^2} \\end{bmatrix}_{z^\\star, u^\\star} z + \\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix} u \\\\ &amp; = \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} &amp; - \\frac{b}{ml^2} \\end{bmatrix}}_{A_c} z + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix}}_{B_c} u. \\end{align}\\] Finally, we convert the continuous-time dynamics to discrete time with a fixed discretization \\(h\\) \\[ z_{k+1} = \\dot{z}_k \\cdot h + z_k = \\underbrace{(h \\cdot A_c + I )}_{A} z_k + \\underbrace{(h \\cdot B_c)}_{B} u_k. \\] We are now ready to implement the LQR controller. In the formulation (4.9), we choose \\(Q = I\\), \\(R = I\\), and compute the gain matrix \\(K\\) by solving the DARE. Fig. 4.2 shows the simulation result for \\(m=1,l=1,b=0.1\\), \\(g = 9.8\\), and \\(h = 0.01\\), with an initial condition \\(z^0 = [0.1,0.1]^\\top\\). We can see that the LQR controller successfully stabilizes the pendulum at \\(z^\\star\\), the upright position. You can play with the Python code here. Alternatively, the Matlab code can be found here. Figure 4.2: LQR stabilization of a simple pendulum. 4.1.3 Linear System Basics Consider the discrete-time linear time-invariant (LTI) system \\[ x_{k+1}=Ax_k+Bu_k,\\qquad y_k=Cx_k+Du_k, \\] with \\(x_k\\in\\mathbb{R}^n,\\;u_k\\in\\mathbb{R}^m,\\;y_k\\in\\mathbb{R}^p\\). We provide a very brief review of linear system theory to understand Proposition 4.2. More details can be found in Appendix B. Stability. The autonomous system \\(x_{k+1}=Ax_k\\) is (asymptotically) stable if for every \\(x_0\\) we have \\(x_k\\to 0\\) as \\(k\\to\\infty\\). Equivalent characterizations. \\(A\\) is Schur: all eigenvalues satisfy \\(|\\lambda_i(A)|&lt;1\\). Lyapunov: \\(\\exists P\\succ 0\\) s.t. \\(A^\\top P A - P \\prec 0\\). Controllability (reachability). The pair \\((A,B)\\) is controllable if for any \\(x_0,x_f\\) there exists a finite input sequence \\(\\{u_0,\\dots,u_{N-1}\\}\\) that drives the state from \\(x_0\\) to \\(x_N=x_f\\). Kalman controllability matrix. \\[ \\mathcal C \\;=\\; [\\,B\\; AB\\; A^2B\\;\\cdots\\; A^{n-1}B\\,],\\quad \\text{\\((A,B)\\) controllable} \\iff \\operatorname{rank}(\\mathcal C)=n. \\] Popov-Belevitch-Hautus (PBH) test. \\[ \\text{\\((A,B)\\) controllable} \\iff \\operatorname{rank}\\!\\begin{bmatrix}\\lambda I - A &amp; B\\end{bmatrix} = n \\ \\text{for all}\\ \\lambda\\in\\mathbb{C}. \\] It suffices to check \\(\\lambda\\) equal to the eigenvalues of \\(A\\). Observability. The pair \\((A,C)\\) is observable if the initial state \\(x_0\\) can be uniquely determined from a finite sequence of outputs (and known inputs), e.g., from \\(\\{y_0,\\dots,y_{n-1}\\}\\). Observability matrix. \\[ \\mathcal O \\;=\\; \\begin{bmatrix} C \\\\ CA \\\\ \\vdots \\\\ CA^{n-1}\\end{bmatrix},\\quad \\text{\\((A,C)\\) observable} \\iff \\operatorname{rank}(\\mathcal O)=n. \\] PBH test. \\[ \\text{\\((A,C)\\) observable} \\iff \\operatorname{rank}\\!\\begin{bmatrix}\\lambda I - A^\\top &amp; C^\\top\\end{bmatrix} = n \\ \\text{for all}\\ \\lambda\\in\\mathbb{C}. \\] Dual to controllability: \\((A,C)\\) observable \\(\\Leftrightarrow\\) \\((A^\\top,C^\\top)\\) controllable. 4.2 LQR Trajectory Tracking Classical LQR delivers an optimal linear state-feedback when dynamics are linear and the objective is quadratic. In many planning problems, however, we do not seek a single stationary feedback for all states but rather a local stabilizer around a given (possibly time-varying) trajectory—for instance, a motion plan from a trajectory optimizer or MPC’s rolling nominal (see Section 4.3). LQR Tracking (also called time-varying LQR, TVLQR) provides exactly this: a time-varying linear feedback that stabilizes the system near a nominal state–control sequence and rejects small disturbances. Problem Setup. Let the nominal (i.e., ignoring the disturbance) discrete-time system be \\[ x_{t+1} \\;=\\; f_t(x_t,u_t), \\qquad t=0,\\dots,N-1, \\] and suppose we have a nominal trajectory \\(\\{(\\bar x_t,\\bar u_t)\\}_{t=0}^{N-1}\\) satisfying \\[ \\bar x_{t+1} \\;=\\; f_t(\\bar x_t,\\bar u_t). \\] Our goal is to design a controller that can stabilize the system with disturbance, i.e., \\(x_{t+1} = f_t(x_t, u_t, w_t)\\), around the nominal trajectory. Towards this, we define deviations from the nominal trajectory as \\[ \\delta x_t := x_t-\\bar x_t, \\qquad \\delta u_t := u_t-\\bar u_t. \\] If the true system is linear time-varying (or we linearize a nonlinear system along the nominal), we obtain the deviation dynamics \\[ \\delta x_{t+1} \\;\\approx\\; A_t\\,\\delta x_t + B_t\\,\\delta u_t, \\quad A_t := \\left.\\frac{\\partial f_t}{\\partial x}\\right|_{(\\bar x_t,\\bar u_t)},\\quad B_t := \\left.\\frac{\\partial f_t}{\\partial u}\\right|_{(\\bar x_t,\\bar u_t)}. \\tag{4.18} \\] We penalize deviations with a quadratic cost \\[ J \\;=\\; \\delta x_N^\\top Q_N \\delta x_N \\;+\\;\\sum_{t=0}^{N-1} \\Big(\\delta x_t^\\top Q_t \\delta x_t \\;+\\; \\delta u_t^\\top R_t \\delta u_t\\Big), \\quad Q_t\\succeq 0,\\; R_t\\succ 0. \\tag{4.19} \\] LQR Tracking Algorithm. The tracking controller takes the affine form \\[ u_t \\;=\\; \\bar u_t \\;-\\; K_t\\,(x_t-\\bar x_t), \\] where \\(\\{K_t\\}_{t=0}^{N-1}\\) are time-varying gains computed by a backward Riccati recursion on the deviation system (4.18) with cost (4.19). From Proposition 4.1, we know the gains can be computed as follows. Initialize at terminal time: \\[ S_N \\;=\\; Q_N. \\] For \\(t = N-1,\\,N-2,\\,\\dots,\\,0\\): \\[\\begin{equation} \\begin{split} K_t &amp;= \\Big(R_t + B_t^\\top S_{t+1} B_t\\Big)^{-1} B_t^\\top S_{t+1} A_t, \\\\[2mm] S_t &amp;= Q_t \\;+\\; A_t^\\top \\!\\Big(S_{t+1} - S_{t+1} B_t \\big(R_t + B_t^\\top S_{t+1} B_t\\big)^{-1} B_t^\\top S_{t+1}\\Big) A_t. \\end{split} \\tag{4.20} \\end{equation}\\] Given the gains \\(\\{K_t\\}\\), apply at runtime: \\[\\begin{equation} u_t \\;=\\; \\bar u_t - K_t\\,(x_t-\\bar x_t), \\qquad t=0,\\dots,N-1. \\tag{4.21} \\end{equation}\\] The following pseudocode summarizes the algorithm. Algorithm: LQR Trajectory Tracking (TVLQR) Inputs: nominal \\(\\{(\\bar x_t,\\bar u_t)\\}_{t=0}^{N-1}\\), weights \\(\\{Q_t,R_t\\}\\), terminal \\(Q_N\\). Linearize along the nominal to get \\(A_t,B_t\\) via (4.18). Backward pass: compute \\(K_t\\) and \\(S_t\\) via (4.20). Apply feedback: \\(u_t=\\bar u_t - K_t(x_t-\\bar x_t)\\) as in (4.21). Output: time-varying gains \\(\\{K_t\\}\\) giving a local stabilizer around the trajectory. We now apply TVLQR to a vehicle trajectory tracking problem. Example 4.2 (LQR Trajectory Tracking for Unicyle) We (i) define the dynamics in continuous and discrete time, (ii) specify a circular nominal trajectory, (iii) linearize the nonlinear dynamics along the nominal, (iv) state the deviation-cost weights \\(Q,R\\) (and terminal \\(Q_N\\)), and (v) list the experiment setup (discretization and horizon length). Unicycle Dynamics (Continuous and Discrete). State and input. \\[ x=\\begin{bmatrix}p_x\\\\ p_y\\\\ \\theta\\end{bmatrix}\\in\\mathbb{R}^3, \\qquad u=\\begin{bmatrix}v\\\\ \\omega\\end{bmatrix}\\in\\mathbb{R}^2, \\] where \\((p_x,p_y)\\) is planar position, \\(\\theta\\) is heading, \\(v\\) is forward speed, and \\(\\omega\\) is yaw rate. Continuous time: \\[\\begin{equation} \\dot p_x = v\\cos\\theta,\\qquad \\dot p_y = v\\sin\\theta,\\qquad \\dot\\theta = \\omega. \\tag{4.22} \\end{equation}\\] Discrete time (forward Euler with step \\(h&gt;0\\)): \\[\\begin{equation} x_{t+1} \\;=\\; f(x_t,u_t) := \\begin{bmatrix} p_{x,t} + h\\, v_t\\cos\\theta_t\\\\[2pt] p_{y,t} + h\\, v_t\\sin\\theta_t\\\\[2pt] \\theta_t + h\\,\\omega_t \\end{bmatrix}. \\tag{4.23} \\end{equation}\\] Nominal Trajectory (Circular Motion). We track a circle of radius \\(R=\\dfrac{\\bar v}{\\bar\\omega}\\) using constant nominal inputs \\[\\begin{equation} \\bar u_t \\equiv \\begin{bmatrix}\\bar v\\\\ \\bar\\omega\\end{bmatrix},\\qquad t=0,\\dots,N-1, \\tag{4.24} \\end{equation}\\] and generate the nominal state recursively under the discrete dynamics (4.23): \\[\\begin{equation} \\bar x_{t+1} \\;=\\; f(\\bar x_t,\\bar u_t),\\qquad \\bar x_0 = \\begin{bmatrix}R\\\\ 0\\\\ \\frac{\\pi}{2}\\end{bmatrix}. \\tag{4.25} \\end{equation}\\] We define deviations from the nominal: \\[ \\delta x_t := x_t-\\bar x_t,\\qquad \\delta u_t := u_t-\\bar u_t. \\] Linearization Along the Nominal. Linearize (4.23) at \\((\\bar x_t,\\bar u_t)\\) to obtain the deviation dynamics \\[\\begin{equation} \\delta x_{t+1} \\;\\approx\\; A_t\\,\\delta x_t + B_t\\,\\delta u_t, \\tag{4.26} \\end{equation}\\] with Jacobians (using \\(c_t:=\\cos\\bar\\theta_t,\\ s_t:=\\sin\\bar\\theta_t\\)) \\[\\begin{equation} A_t \\;=\\; \\frac{\\partial f}{\\partial x}\\Big|_{(\\bar x_t,\\bar u_t)} = \\begin{bmatrix} 1 &amp; 0 &amp; -h\\,\\bar v\\,s_t\\\\ 0 &amp; 1 &amp; \\ \\ h\\,\\bar v\\,c_t\\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}, \\qquad B_t \\;=\\; \\frac{\\partial f}{\\partial u}\\Big|_{(\\bar x_t,\\bar u_t)} = \\begin{bmatrix} h\\,c_t &amp; 0\\\\ h\\,s_t &amp; 0\\\\ 0 &amp; h \\end{bmatrix}. \\tag{4.27} \\end{equation}\\] Deviation Cost (Weights \\(Q,R,Q_N\\)). We penalize deviations with a quadratic stage/terminal cost \\[ J \\;=\\; \\delta x_N^\\top Q_N\\,\\delta x_N \\;+\\;\\sum_{t=0}^{N-1}\\Big(\\delta x_t^\\top Q\\,\\delta x_t+\\delta u_t^\\top R\\,\\delta u_t\\Big), \\] using the weights: \\[\\begin{equation} Q=\\mathrm{diag}(30,\\;30,\\;5),\\qquad Q_N=\\mathrm{diag}(60,\\;60,\\;8),\\qquad R=\\mathrm{diag}(0.2,\\;0.2). \\tag{4.28} \\end{equation}\\] These reflect a stronger emphasis on position tracking, a moderate penalty on heading error, and a mild penalty on control deviations from the nominal. Experiment Setup. Discretization step: \\(h = 0.02\\ \\mathrm{s}\\). Horizon length: \\(T_{\\mathrm{final}} = 12\\ \\mathrm{s}\\). Number of steps: \\(N = T_{\\mathrm{final}}/h = \\mathbf{600}\\). Nominal inputs: \\(\\bar v = 1.2\\ \\mathrm{m/s},\\ \\bar\\omega = 0.4\\ \\mathrm{rad/s}\\) (radius \\(R=\\bar v/\\bar\\omega\\)). Initialization: the nominal starts at \\(\\bar x_0 = [\\,R,\\,0,\\,\\pi/2\\,]^\\top\\); the actual system may start with a small offset (see code). With \\((A_t,B_t)\\) from (4.27) and weights (4.28), the TVLQR backward Riccati recursion (Section 4.2) yields gains \\(K_t\\). We then apply the local feedback \\[ u_t \\;=\\; \\bar u_t - K_t\\,(x_t-\\bar x_t), \\] to robustly track the circular nominal under small disturbances. Disturbances. To test robustness, we inject additive process disturbances into the discrete dynamics: \\[ x_{t+1} \\;=\\; f(x_t,u_t)\\;+\\; w_t,\\qquad t=0,\\dots,N-1, \\] where \\[ w_t \\;=\\; \\underbrace{\\eta_t}_{\\text{i.i.d. Gaussian noise}} \\;+\\; \\underbrace{g_t}_{\\text{deterministic gust}}. \\] Small i.i.d. Gaussian process noise. We draw \\(\\eta_t \\sim \\mathcal N(0,W)\\) independently at each step with \\[ W \\;=\\; \\mathrm{diag}\\!\\big(\\sigma_x^2,\\ \\sigma_y^2,\\ \\sigma_\\theta^2\\big), \\qquad \\sigma_x = \\sigma_y = 0.01\\ \\text{m},\\quad \\sigma_\\theta = \\mathrm{deg2rad}(0.2). \\] This noise perturbs the post-update state components \\((p_x,p_y,\\theta)\\). Finite-duration “gust” impulse. In addition to \\(\\eta_t\\), we apply a brief deterministic bias over a window \\[ t \\in [t_g,\\ t_g+\\Delta] \\;=\\; [\\,4.0\\,\\mathrm{s},\\ 4.8\\,\\mathrm{s}\\,), \\] implemented at the discrete indices \\(\\{t_g,\\dots,t_g+\\Delta\\}\\). During this window we set \\[ g_t \\;=\\; \\begin{bmatrix} \\delta p_x \\\\[1mm] 0 \\\\[1mm] \\delta \\theta \\end{bmatrix}, \\qquad \\delta p_x = 0.01\\ \\text{m per step},\\quad \\delta \\theta = \\mathrm{deg2rad}(1.8)\\ \\text{per step}, \\] and \\(g_t=\\mathbf{0}\\) otherwise. This models a short-lived lateral drift and a heading kick. Results. Fig. 4.3 visualizes the nominal trajectory (the dotted circle) and the TVLQR-stabilized trajectory in blue. To clearly see the impact of closed-loop LQR tracking, we also plotted the open-loop trajectory, i.e., the system’s trajectory if no feedback is applied. We can observe that the TVLQR controller effectively rejects the disturbances and stabilizes the closed-loop trajectory along the nominal path. Fig. 4.4 visualizes the state tracking error (position and heading error) as well as compares the closed-loop control with open-loop control. You can play with the code here. Figure 4.3: LQR Trajectory Tracking for Unicyle: comparison between nominal trajectory, open-loop trajectory, and closed-loop trajectory with feedback. Figure 4.4: LQR Trajectory Tracking for Unicyle: state tracking error (top) and control signal (bottom). 4.3 Trajectory Optimization In Section 4.2 we saw that TVLQR gives a powerful local stabilizer around a nominal state–control sequence \\((\\bar x_t,\\bar u_t)\\). This raises a natural question: Where do nominal trajectories come from? In many robotics tasks (maneuvering a car, landing a rocket, walking with a robot), we must compute a feasible, high-quality open-loop plan that respects the dynamics and constraints. Trajectory Optimization (TO) does exactly this: it searches over sequences \\(\\{x_t,u_t\\}\\) to minimize a cumulative cost while satisfying the system dynamics and constraints. Moreover, if we can solve TO quickly (or approximately, but reliably), then by re-solving over a short horizon at each time step and applying only the first control, we obtain Model Predictive Control (MPC)—a feedback controller that blends optimization with robustness (see Section 4.4 later). Thus, TO is both a planner and the engine behind feedback via MPC. General Nonlinear Trajectory Optimization Problem. We adopt the standard discrete-time nonlinear system \\[ x_{t+1} = f_t(x_t,u_t),\\qquad t=0,\\dots,N-1, \\] with state \\(x_t\\in\\mathbb{R}^n\\) and control \\(u_t\\in\\mathbb{R}^m\\). A generic finite-horizon TO problem is \\[\\begin{equation} \\begin{split} \\min_{\\{x_t,u_t\\}} \\quad &amp; \\Phi(x_N) + \\sum_{t=0}^{N-1} \\ell_t(x_t,u_t) \\\\[2mm] \\text{s.t.}\\quad &amp; x_{t+1} = f_t(x_t,u_t), \\qquad t=0,\\dots,N-1,\\\\ &amp; x_0 = \\hat x_0 \\ \\ \\text{(given)},\\\\ &amp; x_t \\in \\mathcal X_t,\\quad u_t \\in \\mathcal U_t \\quad \\text{(bounds)},\\\\ &amp; g_t(x_t,u_t) \\le 0,\\quad h_t(x_t,u_t)=0 \\quad \\text{(path/terminal constraints).} \\end{split} \\tag{4.29} \\end{equation}\\] Here \\(\\ell_t\\) and \\(\\Phi\\) encode performance (e.g., energy, time, tracking error), \\(\\mathcal X_t,\\mathcal U_t\\) capture box limits and safety sets, and \\(g_t,h_t\\) represent additional nonlinear constraints (obstacles, terminal goals, etc.). Solving (4.29) directly is difficult in general. A widely used strategy is to iteratively approximate it by quadratic subproblems that can be solved efficiently. This leads to iLQR and its second-order cousin DDP (see Section 4.3.2). 4.3.1 Iterative LQR High-level intuition. iLQR (iterative LQR) alternates between: Local modeling: around a current nominal trajectory \\(\\{(\\bar x_t,\\bar u_t)\\}\\), linearize the dynamics, quadratically approximate the cost. LQR step: solve the resulting time-varying LQR subproblem to obtain a time-varying affine policy \\[ \\delta u_t = k_t + K_t\\,\\delta x_t,\\quad \\delta x_t:=x_t-\\bar x_t,\\ \\delta u_t:=u_t-\\bar u_t, \\] which gives both a feedforward step \\(k_t\\) (to change the nominal control) and a feedback gain \\(K_t\\) (to stabilize the rollout). Forward rollout + line search: apply \\(u_t^{\\text{new}}=\\bar u_t+\\alpha k_t + K_t(x_t^{\\text{new}}-\\bar x_t)\\) to the true nonlinear dynamics, producing a new nominal trajectory \\(\\{(\\bar x_t,\\bar u_t)\\}\\). Here we choose \\(\\alpha\\in(0,1]\\) to reduce the cost and respect constraints. Repeat until convergence (cost decrease and dynamics residuals are small). iLQR can be viewed as a Gauss–Newton method on trajectories: it uses first-order dynamics and second-order cost, capturing the dominant curvature while remaining numerically robust and fast. 4.3.1.1 LQR Subproblem (one iLQR outer iteration) Given a nominal trajectory \\(\\{(\\bar x_t,\\bar u_t)\\}_{t=0}^{N-1}\\) with \\(\\bar x_{t+1}=f_t(\\bar x_t,\\bar u_t)\\), define deviations \\[ \\delta x_t := x_t-\\bar x_t,\\qquad \\delta u_t := u_t-\\bar u_t,\\qquad \\delta x_0\\ \\text{given.} \\] Linearized Dynamics. We linearize the dynamics along the nominal trajectory \\[ \\delta x_{t+1} \\;\\approx\\; A_t\\,\\delta x_t + B_t\\,\\delta u_t,\\quad A_t:=\\left.\\frac{\\partial f_t}{\\partial x}\\right|_{(\\bar x_t,\\bar u_t)},\\ \\ B_t:=\\left.\\frac{\\partial f_t}{\\partial u}\\right|_{(\\bar x_t,\\bar u_t)}. \\tag{4.30} \\] Quadratic Cost Approximation. We perform a quadratic approximation of the objective function about \\((\\bar x_t,\\bar u_t)\\) \\[ \\begin{aligned} \\ell_t(x_t,u_t) &amp;\\approx \\ell_t + \\ell_{x,t}^\\top \\delta x_t + \\ell_{u,t}^\\top \\delta u_t + \\frac{1}{2} \\begin{bmatrix}\\delta x_t\\\\ \\delta u_t\\end{bmatrix}^{\\!\\top} \\!\\begin{bmatrix}\\ell_{xx,t} &amp; \\ell_{xu,t}\\\\ \\ell_{ux,t} &amp; \\ell_{uu,t}\\end{bmatrix} \\!\\begin{bmatrix}\\delta x_t\\\\ \\delta u_t\\end{bmatrix},\\\\ \\Phi(x_N) &amp;\\approx \\Phi + \\Phi_x^\\top \\delta x_N + \\frac{1}{2}\\,\\delta x_N^\\top \\Phi_{xx}\\,\\delta x_N. \\end{aligned} \\tag{4.31} \\] The LQR Subproblem. With (4.30)–(4.31), the iLQR subproblem at this outer iteration is the finite-horizon linear–quadratic program in deviations: \\[ \\begin{aligned} \\min_{\\{\\delta x_t,\\delta u_t\\}} \\quad &amp; \\underbrace{\\frac{1}{2}\\,\\delta x_N^\\top \\Phi_{xx}\\,\\delta x_N + \\Phi_x^\\top \\delta x_N}_{\\text{terminal}} \\;+\\; \\\\ &amp; \\sum_{t=0}^{N-1} \\underbrace{\\Big( \\frac{1}{2} \\begin{bmatrix}\\delta x_t\\\\ \\delta u_t\\end{bmatrix}^{\\!\\top} \\!\\begin{bmatrix}\\ell_{xx,t} &amp; \\ell_{xu,t}\\\\ \\ell_{ux,t} &amp; \\ell_{uu,t}\\end{bmatrix} \\!\\begin{bmatrix}\\delta x_t\\\\ \\delta u_t\\end{bmatrix} + \\ell_{x,t}^\\top\\delta x_t + \\ell_{u,t}^\\top\\delta u_t \\Big)}_{\\text{stage}} \\\\[1mm] \\text{s.t.}\\quad &amp; \\delta x_{t+1} = A_t\\,\\delta x_t + B_t\\,\\delta u_t,\\qquad t=0,\\dots,N-1,\\\\ &amp; \\delta x_0\\ \\text{given.} \\end{aligned} \\tag{4.32} \\] Notes. The iLQR subproblem (4.32) is slightly different from the previous finite-horizon LQR formulation (4.3) in the sense that the objective function of (4.32) also contains linear terms in \\(\\delta x_t,\\delta u_t\\), and those linear terms come from the Taylor expansion of the original nonlinear objective fuctions. In this case, we will see in the following that the optimal policy is affine (feedforward \\(k_t\\) + feedback \\(K_t\\)). 4.3.1.2 Solving the Subproblem by Dynamic Programming We posit a quadratic value approximator at each time: \\[ V_{t}(\\delta x_{t}) \\;\\approx\\; V_{t} + V_{x,t}^\\top \\delta x_t + \\frac{1}{2}\\,\\delta x_t^\\top V_{xx,t}\\,\\delta x_t, \\qquad V_{x,N}=\\Phi_x,\\; V_{xx,N}=\\Phi_{xx}. \\tag{4.33} \\] Note that this quadratic value approximator also contains linear and constant terms because the objective function contains linear terms. Define the local Q-function at stage \\(t\\) by substituting the linear dynamics into the next-step value (this is our familiar Q-value in RL): \\[ Q_t(\\delta x_t,\\delta u_t) \\;=\\; \\ell_t(x_t,u_t) + V_{t+1} \\big(A_t\\delta x_t + B_t\\delta u_t\\big), \\] which, after collecting terms, yields the iLQR blocks \\[ \\begin{aligned} Q_{x,t}&amp;=\\ell_{x,t}+A_t^\\top V_{x,t+1},\\qquad Q_{u,t}=\\ell_{u,t}+B_t^\\top V_{x,t+1},\\\\ Q_{xx,t}&amp;=\\ell_{xx,t}+A_t^\\top V_{xx,t+1}A_t,\\quad Q_{ux,t}=\\ell_{ux,t}+B_t^\\top V_{xx,t+1}A_t,\\\\ Q_{uu,t}&amp;=\\ell_{uu,t}+B_t^\\top V_{xx,t+1}B_t. \\end{aligned} \\tag{4.34} \\] The iLQR blocks assemble into a big matrix such that \\[ Q_t(\\delta x_t,\\delta u_t) \\;=\\; \\frac{1}{2}\\, \\begin{bmatrix} 1 \\\\ \\delta x_t \\\\ \\delta u_t \\end{bmatrix}^\\top \\begin{bmatrix} 2c_t &amp; Q_{x,t}^\\top &amp; Q_{u,t}^\\top\\\\[2pt] Q_{x,t} &amp; Q_{xx,t} &amp; Q_{xu,t}\\\\[2pt] Q_{u,t} &amp; Q_{ux,t} &amp; Q_{uu,t} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\delta x_t \\\\ \\delta u_t \\end{bmatrix}. \\tag{4.35} \\] where \\(c_t\\) collects all stage/terminal constants e.g., \\(\\ell_t+\\!V_{t+1}\\). Solving the local Q (backward pass). Set the first-order condition w.r.t. \\(\\delta u\\): \\[ 0 \\;=\\; \\partial_{\\delta u} Q_t \\;=\\; Q_{u,t} + Q_{ux,t}\\delta x + Q_{uu,t}\\delta u. \\] Solve for the affine control law \\[ \\delta u_t^\\star \\;=\\; k_t + K_t\\,\\delta x,\\qquad k_t = -Q_{uu,t}^{-1} Q_{u,t},\\quad K_t = -Q_{uu,t}^{-1} Q_{ux,t}\\!, \\tag{4.36} \\] which is exactly the LQR solution for the quadratic \\(Q_t\\). Substitute \\(\\delta u_t^\\star\\) back into (4.35). The minimized Q becomes a quadratic in \\(\\delta x\\) with coefficients given by \\[ \\begin{aligned} V_{x,t} &amp;= Q_{x,t} + Q_{xu,t}k_t + K_t^\\top Q_{uu,t}k_t + K_t^\\top Q_{u,t},\\\\ V_{xx,t} &amp;= Q_{xx,t} + Q_{xu,t}K_t + K_t^\\top Q_{ux,t} + K_t^\\top Q_{uu,t}K_t, \\end{aligned} \\tag{4.37} \\] with terminal \\[ V_{x,N} = \\Phi_x, V_{xx,N} = \\Phi_{xx}. \\] Forward Pass (apply the computed policy). Given \\(\\{k_t,K_t\\}\\), produce a candidate trajectory on the true nonlinear dynamics using a line search \\(\\alpha\\in(0,1]\\): \\[ \\begin{aligned} u_t^{\\text{cand}} &amp;= \\bar u_t + \\alpha k_t + K_t\\big(x_t^{\\text{cand}}-\\bar x_t\\big) ,\\\\ x_{t+1}^{\\text{cand}} &amp;= f_t\\big(x_t^{\\text{cand}},u_t^{\\text{cand}}\\big),\\qquad x_0^{\\text{cand}}=\\hat x_0. \\end{aligned} \\tag{4.38} \\] Choose \\(\\alpha\\) (e.g., \\(\\{1,\\frac{1}{2},\\frac14,\\dots\\}\\)) to reduce the true cost and respect constraints, then update the nominal: \\[ (\\bar x_t,\\bar u_t)\\ \\leftarrow\\ (x_t^{\\text{cand}},u_t^{\\text{cand}}). \\] The following pseudocode summarizes iLQR. Algorithm: iLQR (Trajectory Generation) Inputs: dynamics \\(f_t\\), initial state \\(\\hat x_0\\), horizon \\(N\\), stage/terminal costs \\(\\ell_t,\\Phi\\), initial guess \\(\\{\\bar u_t\\}\\). Initialize nominal rollout \\(\\{\\bar x_t,\\bar u_t\\}\\) from \\(\\hat x_0\\). Linearize &amp; quadratize at \\(\\{(\\bar x_t,\\bar u_t)\\}\\): build \\(A_t,B_t\\) and cost derivatives. Backward pass (TVLQR): compute \\(\\{k_t,K_t\\}\\) using (4.36) and update \\(V_{x,t},V_{xx,t}\\) via (4.37). Forward rollout: apply \\(u_t^{\\text{new}}=\\bar u_t+\\alpha k_t+K_t(x_t^{\\text{new}}-\\bar x_t)\\) on the true dynamics, pick \\(\\alpha\\) by line search. Convergence check: stop if the cost decrease and dynamics residuals fall below thresholds; otherwise, set the new nominal and repeat from Step 2. The next example applies iLQR to trajectory generation for rocket landing. Example 4.3 (iLQR for Rocket Landing) We model a planar (2D) rocket with state and control \\[ x=\\begin{bmatrix}p_x &amp; p_y &amp; v_x &amp; v_y &amp; \\theta &amp; \\omega\\end{bmatrix}^\\top,\\qquad u=\\begin{bmatrix}T &amp; \\tau\\end{bmatrix}^\\top, \\] where \\((p_x,p_y)\\) is position, \\((v_x,v_y)\\) is velocity, \\(\\theta\\) is attitude (pitch) and \\(\\omega\\) its angular rate. The thrust \\(T\\ge 0\\) acts along the body axis (pointing out of the engine), and \\(\\tau\\) is a planar torque about the center of mass. Continuous-time dynamics are \\[ \\begin{aligned} \\dot p_x &amp;= v_x, &amp; \\dot p_y &amp;= v_y, \\\\ \\dot v_x &amp;= \\frac{T}{m}\\sin\\theta, &amp; \\dot v_y &amp;= \\frac{T}{m}\\cos\\theta - g, \\\\ \\dot\\theta &amp;= \\omega, &amp; \\dot\\omega &amp;= \\frac{\\tau}{I_{zz}}. \\end{aligned} \\tag{4.39} \\] In simulation we use RK4 with stepsize \\(h\\) to propagate the true dynamics (4.39). For iLQR’s local subproblems we form the continuous Jacobians \\((A_c,B_c)=\\big(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial u}\\big)\\) at the current nominal and use the standard first-order discrete map \\[ A_t \\;\\approx\\; I + h\\,A_c(\\bar x_t,\\bar u_t),\\qquad B_t \\;\\approx\\; h\\,B_c(\\bar x_t,\\bar u_t). \\tag{4.40} \\] Soft-Landing Objective. The goal is a soft, upright landing at the origin: \\[ x_{\\mathrm{goal}} = \\mathbf{0} \\quad\\Longleftrightarrow\\quad p_x=p_y=0,\\; v_x=v_y=0,\\; \\theta=0,\\; \\omega=0. \\] We penalize deviations from this goal along the entire trajectory and especially at the terminal state to encourage low touchdown velocities and an upright attitude. Cost Function. With horizon \\(N\\) and step \\(h\\), the discrete objective is \\[ J \\;=\\; \\frac{1}{2}\\,(x_N-x_g)^\\top Q_f (x_N-x_g) \\;+\\;\\sum_{t=0}^{N-1}\\Big[ \\frac{1}{2}\\,(x_t-x_g)^\\top Q (x_t-x_g) \\;+\\; \\frac{1}{2}\\,u_t^\\top R u_t \\Big], \\tag{4.41} \\] where \\(x_g=\\mathbf{0}\\). In the example: \\[ \\begin{aligned} Q&amp;=\\mathrm{diag}(1,\\ 2,\\ 0.5,\\ 0.5,\\ 2,\\ 0.5),\\\\ Q_f&amp;=\\mathrm{diag}(200,\\ 300,\\ 50,\\ 50,\\ 300,\\ 50),\\\\ R&amp;=\\mathrm{diag}(10^{-3},\\ 10^{-3}). \\end{aligned} \\tag{4.42} \\] These weights place strong emphasis on terminal altitude and attitude (\\(p_y,\\theta\\)), moderate emphasis on velocities and lateral position, and a light regularization on the controls. Experiment Setup. Physical parameters. Gravity \\(g=9.81\\,\\mathrm{m/s^2}\\), mass \\(m=1.0\\,\\mathrm{kg}\\), planar inertia \\(I_{zz}=0.2\\,\\mathrm{kg\\,m^2}\\). Discretization. Stepsize \\(h=0.05\\,\\mathrm{s}\\); horizon \\(T=6.0\\,\\mathrm{s}\\); number of steps \\(N=T/h=120\\). Initial state. \\[ x_0=\\big[\\,5.0,\\ 10.0,\\ -0.5,\\ -1.0,\\ \\mathrm{deg2rad}(10),\\ 0\\,\\big]^\\top, \\] i.e., 10 m altitude, lateral offset, small descent and slight pitch. Initial nominal controls. Constant hover thrust and zero torque: \\[ \\bar u_t = [\\,m g,\\ 0\\,]^\\top,\\qquad t=0,\\dots,N-1. \\] iLQR procedure. Each outer iteration: Linearize dynamics and quadratize the cost along the current nominal ((4.40), (4.41)); Solve the time-varying LQR subproblem to get affine updates \\(\\delta u_t = k_t + K_t\\,\\delta x_t\\); Forward rollout on the nonlinear RK4 dynamics with \\[ u_t^{\\text{new}} = \\bar u_t + \\alpha\\,k_t + K_t\\big(x_t^{\\text{new}}-\\bar x_t\\big), \\] using a backtracking line search over \\(\\alpha\\in\\{1,\\frac{1}{2},\\frac14,\\dots\\}\\) (note: \\(\\alpha\\) scales only the feedforward \\(k_t\\), not the feedback \\(K_t\\)); Update the nominal and repeat until cost reduction is small. Fig. 4.5 plots the initial, intermediate, and final trajectories, and render the rocket as oriented rectangles (boxes) using \\((p_x,p_y,\\theta)\\) to visualize attitude along the descent. We can see iLQR successfully generated a soft landing trajectory. You can play with the code here. Figure 4.5: iLQR Trajectory Generation for Rocket Landing. 4.3.2 Differential Dynamic Programming Similar to iLQR, skipped for now. 4.3.3 Quadratic Programming Trajectory optimization (TO) with nonlinear dynamics and objectives is well served by iLQR/DDP: at each outer iteration, they linearize the dynamics and quadratize the objective, then solve a time-varying LQR subproblem. This works remarkably well for unconstrained or softly constrained problems. However, many TO tasks are constrained—e.g., obstacle avoidance, actuator limits, keep-out zones, terminal envelopes. Hard constraints are awkward for iLQR/DDP (they typically enter via penalties or saturations), and feasibility can be fragile. For such cases, it is often more natural to frame TO as a nonlinear program (NLP)—an optimization problem with general nonlinear objective and constraints. This brings the full machinery of modern numerical optimization (see, e.g., (Nocedal and Wright 1999)). As a first step, we study the convex special case where the linearization already yields linear dynamics, affine constraints, and a quadratic objective (these are typically known as “constrained LQR” problems). This leads to Quadratic Programming (QP), a cornerstone problem class with mature, efficient solvers. In the next section, we will lift these ideas to Sequential Quadratic Programming (SQP) to handle general constrained TO. 4.3.3.1 From Trajectory Optimization to Quadratic Programming Start from the constrained TO template in (4.29). Suppose: Linear (time-varying) dynamics (from linearization or an intrinsically linear model): \\[ x_{t+1} = A_t x_t + B_t u_t + a_t,\\qquad t=0,\\dots,N-1, \\] with given \\(x_0=\\hat x_0\\). Quadratic objective (from quadratization or an Linear-Quadratic tracking design): \\[ \\Phi(x_N) + \\sum_{t=0}^{N-1} \\ell_t(x_t,u_t) \\;\\equiv\\; \\frac{1}{2}\\,x_N^\\top Q_N x_N + q_N^\\top x_N + \\sum_{t=0}^{N-1}\\Big(\\frac{1}{2}\\,[x_t;u_t]^\\top H_t [x_t;u_t] + h_t^\\top [x_t;u_t]\\Big), \\] with \\(Q_N\\succeq 0\\), \\(H_t=\\begin{bmatrix}Q_t &amp; S_t\\\\ S_t^\\top &amp; R_t\\end{bmatrix} \\succeq 0\\) and \\(R_t\\succ 0\\) for convexity. Affine path/terminal constraints (from linearized safety/goal constraints): \\[ G_t^x x_t + G_t^u u_t \\le g_t,\\qquad F_x x_N \\leq f. \\] Define the stacked decision vector \\[ z := \\big[x_0^\\top,\\,u_0^\\top,\\,x_1^\\top,\\,u_1^\\top,\\,\\dots,\\,x_{N-1}^\\top,\\,u_{N-1}^\\top,\\,x_N^\\top\\big]^\\top. \\] Then the horizon-wide problem is a QP: \\[ \\begin{aligned} \\min_{z}\\quad &amp; \\frac{1}{2}\\, z^\\top H\\, z \\;+\\; h^\\top z \\\\[2mm] \\text{s.t.}\\quad &amp; A_{\\text{dyn}}\\, z = b_{\\text{dyn}} \\quad \\text{(stacked dynamics and } x_0=\\hat x_0\\text{)},\\\\ &amp; G\\, z \\le g \\qquad\\quad\\ \\ \\text{(stacked affine path/terminal constraints)}\\\\ \\end{aligned} \\tag{4.43} \\] Here \\(H\\succeq 0\\) is block-sparse (banded) due to the stagewise structure; the constraint matrices are also sparse/banded because each dynamic constraint couples only \\((x_t,u_t,x_{t+1})\\). Exercise 4.1 Can you write down the blocks in \\(H\\), \\(A\\), and \\(G\\), as functions of \\(H_t,A_t,B_t,G^x_t, G^u_t\\)? Then, observe the block-sparsity patterns. Convexity. If all stage Hessians \\(H_t\\succeq 0\\) and \\(Q_N\\succeq 0\\), (4.43) is a convex QP with a unique minimizer when \\(H\\) is positive definite on the feasible subspace (e.g., via \\(R_t\\succ 0\\)). 4.3.3.2 Solving the Quadratic Program We now discuss how to solve a general convex quadratic program (QP) containing both equality and inequality constraints: \\[ \\begin{aligned} \\min_{z \\in \\mathbb{R}^n}\\quad &amp; \\frac{1}{2}\\, z^\\top H\\, z \\;+\\; h^\\top z \\\\ \\text{s.t.}\\quad &amp; A z = b \\\\ &amp; G z \\le g \\end{aligned} \\tag{4.44} \\] where \\(z \\in \\mathbb{R}^n\\) is the decision variable, \\(H \\in \\mathbb{S}^{n}, h \\in \\mathbb{R}^n, A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, G \\in \\mathbb{R}^{p \\times n}, g \\in \\mathbb{R}^p\\) are given problem data (e.g., generated as in Section 4.3.3.1). We assume \\(H \\succeq 0\\) is positive semidefinite. There are multiple popular algorithms to solve (4.44), e.g., the active set algorithm, the interior point algorithm, and the alternating direction method of multipliers. Here we only present the primal–dual interior point method (PD-IPM) due to its generality and robustness. Before presenting the PD-IPM algorithm, it is beneficial to review Newton’s method for solving a system of equations. Newton’s Method Given a function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) that is continuously differentiable, Newton’s method is designed to find a root of \\[ f(x) = 0. \\] Given an initial iterate \\(x^{(0)}\\), Newton’s method works as follows \\[ x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f&#39;(x^{(k)})}, \\] where \\(f&#39;(x^{(k)})\\) denotes the derivative of \\(f\\) at the current iterate \\(x^{(k)}\\). This simple algorithm is indeed the most important foundation of modern numerical optimization. Under mild conditions, Newton’s method has at least quadratic convergence rate, that is to say, if \\(|x^{(k)} - x^\\star| = \\epsilon\\), then \\(|x^{(k+1)} - x^\\star| = O(\\epsilon^2)\\) (it should be noted that there exist pathological cases where even linear convergence is not guaranteed, e.g., when \\(f&#39;(x^\\star) = 0\\)). Newton’s method can be generalized to find a point at which multiple functions vanish simultaneously. Given a function \\(F: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) that is continuously differentiable, and an initial iterate \\(x^{(0)}\\), Newton’s method reads \\[\\begin{equation} x^{(k+1)} = x^{(k)} - J_F(x^{(k)})^{-1} F(x^{(k)}), \\tag{4.45} \\end{equation}\\] where \\(J_F(\\cdot)\\) denotes the Jacobian of \\(F\\). Iteration (4.45) is equivalent to \\[\\begin{equation} \\begin{split} J_F(x^{(k)}) \\Delta x^{(k)} &amp; = - F(x^{(k)}) \\\\ x^{(k+1)} &amp; = x^{(k)} + \\Delta x^{(k)} \\end{split} \\tag{4.46} \\end{equation}\\] i.e., one first solves a linear system of equations to find an update direction \\(\\Delta x^{(k)}\\), and then take a step along the direction. As we will see, PD-IPMs for solving convex QPs can be seen as applying Newton’s method to the perturbed KKT system of optimality conditions. Slacks, Lagrangian, and KKT Optimality. Introduce \\(s\\in\\mathbb{R}^p\\) so that \\[ Gz + s = g,\\qquad s \\ge 0. \\tag{4.47} \\] Let \\(y\\in\\mathbb{R}^m\\) be the Lagrangian multipliers for \\(Az=b\\), \\(\\nu\\in\\mathbb{R}^p\\) for the equality \\(Gz+s=g\\), and \\(w\\in\\mathbb{R}^p\\) (with \\(w\\ge 0\\)) for the inequality \\(s\\ge 0\\). The Lagrangian for the QP (4.44) is \\[ \\mathcal L(z,s,y,\\nu,w) = \\frac{1}{2} z^\\top H z + h^\\top z + y^\\top(Az-b) + \\nu^\\top(Gz+s-g) - w^\\top s. \\tag{4.48} \\] From the Lagrangian, we can derive the KKT optimality conditions, i.e., under technical conditions (such as constraint qualification), a point \\((z,s)\\) is a local minimizer of the QP if and only if there exists dual variables \\((\\nu,w)\\) satisfying: \\[ \\begin{aligned} \\text{(stationarity)}&amp;:\\quad \\nabla_z\\mathcal L = H z + h + A^\\top y + G^\\top \\nu = 0,\\\\ &amp;\\quad \\nabla_s\\mathcal L = \\nu - w = 0 \\ \\Longrightarrow\\ \\nu = w,\\\\[2pt] \\text{(primal feasibility)}&amp;:\\quad A z - b = 0,\\qquad G z + s - g = 0, \\qquad s \\ge 0, \\\\ \\text{(dual feasibility)}&amp;:\\quad w \\ge 0,\\\\ \\text{(complementarity)}&amp;:\\quad s_i w_i = 0,\\quad i=1,\\dots,p. \\end{aligned} \\tag{4.49} \\] Using \\(\\nu=w\\) we eliminate \\(\\nu\\) and keep variables \\((z,s,y,w)\\). Since the QP is convex, we know that any local minimizer is a global minimizer. Hence, if we can solve the KKT system (4.49), we can find an optimal solution of the QP. If you are not familiar with the Lagrangian and the KKT optimality conditions, make sure to review Appendix A.1.3 and A.1.4. Central Path and Residuals. Replace complementarity by the perturbed condition \\[ S W \\mathbf{1} = \\sigma \\mu\\,\\mathbf{1}, \\qquad \\mu := \\frac{1}{p}\\, s^\\top w, \\qquad \\sigma \\in (0,1) \\tag{4.50} \\] with \\(S=\\mathrm{diag}(s)\\), \\(W=\\mathrm{diag}(w)\\). At a current iterate \\((z,s,y,w)\\) define residuals \\[ \\begin{aligned} r_{\\mathrm{dual}} &amp;= H z + h + A^\\top y + G^\\top w,\\\\ r_{\\mathrm{pe}} &amp;= A z - b,\\\\ r_{\\mathrm{pi}} &amp;= G z + s - g,\\\\ r_{\\mathrm{cent}} &amp;= S W \\mathbf{1} - \\sigma \\mu\\,\\mathbf{1}. \\end{aligned} \\tag{4.51} \\] Newton System (primal–dual step). Now that we have arrived at the perturbed KKT system of equations in (4.51) where we aim to drive all the residuals to zero. This is a system of \\((n+m+p+p)\\) nonlinear equations in \\((n + m + p + p)\\) variables. Therefore, we can apply Newton’s method to solve the system of equations. Note that we actually want to solve the original KKT system with \\(\\sigma \\mu = 0\\). However, this system is ill-conditioned and directly applying Newton’s method would lead to instability. Therefore, we solve the perturbed KKT system with \\(\\sigma \\mu &gt; 0\\) and at each iteration we move closer to the original KKT system with \\(\\sigma \\in (0,1)\\) so that in the limit we will converge (arbitrarily close) to a solution of the original KKT system. Solve for the Newton direction \\((\\Delta z,\\Delta s,\\Delta y,\\Delta w)\\): \\[ \\begin{aligned} H\\,\\Delta z + A^\\top \\Delta y + G^\\top \\Delta w &amp;= -\\,r_{\\mathrm{dual}},\\\\ A\\,\\Delta z &amp;= -\\,r_{\\mathrm{pe}},\\\\ G\\,\\Delta z + \\Delta s &amp;= -\\,r_{\\mathrm{pi}},\\\\ W\\,\\Delta s + S\\,\\Delta w &amp;= -\\,r_{\\mathrm{cent}}. \\end{aligned} \\tag{4.52} \\] Eliminate \\(\\Delta s = -r_{\\mathrm{pi}} - G\\Delta z\\) in the last equation to get \\[ S\\,\\Delta w = -\\,r_{\\mathrm{cent}} + W\\,r_{\\mathrm{pi}} + W\\,G\\,\\Delta z \\quad\\Longrightarrow\\quad \\Delta w = S^{-1}\\!\\left(-r_{\\mathrm{cent}} + W r_{\\mathrm{pi}} + W G \\Delta z\\right). \\] Substitute into the first equation to obtain the reduced symmetric system in \\((\\Delta z,\\Delta y)\\): \\[ \\begin{bmatrix} H + G^\\top D G &amp; A^\\top\\\\ A &amp; \\ \\ 0 \\end{bmatrix} \\begin{bmatrix}\\Delta z\\\\ \\Delta y\\end{bmatrix} = - \\begin{bmatrix} r_{\\mathrm{dual}} - G^\\top S^{-1} \\big(r_{\\mathrm{cent}} + W r_{\\mathrm{pi}}\\big)\\\\[2pt] r_{\\mathrm{pe}} \\end{bmatrix} \\tag{4.53} \\] with \\(D := S^{-1} W\\). Then recover \\[ \\Delta w = S^{-1}\\!\\left(-r_{\\mathrm{cent}} + W r_{\\mathrm{pi}} + W G \\Delta z\\right),\\qquad \\Delta s = -r_{\\mathrm{pi}} - G \\Delta z. \\] Step Lengths. Choose step sizes to preserve positivity of \\(s,w\\): \\[ \\alpha_{\\mathrm{pri}} = \\min\\!\\Big(1,\\ \\eta \\min_{\\Delta s_i&lt;0}\\frac{-s_i}{\\Delta s_i}\\Big),\\qquad \\alpha_{\\mathrm{du}} = \\min\\!\\Big(1,\\ \\eta \\min_{\\Delta w_i&lt;0}\\frac{-w_i}{\\Delta w_i}\\Big), \\tag{4.54} \\] with \\(\\eta\\in(0,1)\\) (e.g., \\(0.99\\)). Update both primal and dual variables \\[ \\begin{aligned} z &amp;\\leftarrow z + \\alpha_{\\mathrm{pri}}\\,\\Delta z,\\qquad s \\leftarrow s + \\alpha_{\\mathrm{pri}}\\,\\Delta s,\\\\ y &amp;\\leftarrow y + \\alpha_{\\mathrm{du}}\\,\\Delta y,\\qquad w \\leftarrow w + \\alpha_{\\mathrm{du}}\\,\\Delta w. \\end{aligned} \\] Initialization and Stopping. Initialization. Find any \\(z\\) satisfying \\(Az=b\\) (e.g., least-squares projection). Set \\[ s := \\max\\{\\mathbf{1},\\, g - Gz\\},\\quad w := \\mathbf{1}, \\] to ensure strict positivity (\\(s&gt;0,w&gt;0\\)); choose \\(y:=0\\). Stopping. Terminate when \\[ \\|r_{\\mathrm{dual}}\\|_\\infty \\le \\varepsilon,\\quad \\|r_{\\mathrm{pe}}\\|_\\infty \\le \\varepsilon,\\quad \\|r_{\\mathrm{pi}}\\|_\\infty \\le \\varepsilon,\\quad \\mu \\le \\varepsilon, \\] for a small tolerance \\(\\varepsilon\\) (e.g., \\(10^{-6}\\)). The following pseudocode implements the PD-IPM algorithm for solving convex QP. Algorithm: Primal–Dual Interior-Point for Convex QP Input: \\(H\\succeq 0, h, A,b, G,g\\); tolerance \\(\\varepsilon\\); \\(\\eta=0.99\\); \\(\\sigma \\in (0,1)\\) Initialize \\(z\\) with \\(Az=b\\); set \\(s&gt;0, w&gt;0\\) (e.g., \\(s=\\max\\{1,g-Gz\\}\\), \\(w=\\mathbf{1}\\)); set \\(y=0\\). Repeat until convergence: Compute residuals \\(r_{\\mathrm{dual}}, r_{\\mathrm{pe}}, r_{\\mathrm{pi}}\\), \\(\\mu=(s^\\top w)/p\\). Solve the reduced system (4.53) to obtain Newton direction. Compute \\(\\alpha_{\\mathrm{pri}},\\alpha_{\\mathrm{du}}\\) by (4.54). Update \\((z,s,y,w)\\). Check stopping criteria; if satisfied, return \\(z^\\star\\). Remarks. For QPs obtained from trajectory optimization problems, the matrices are typically sparse (e.g., time-banded sparsity). This sparsity can be leveraged when forming and solving the Newton direction. In practice, Mehrotra’s predictor–corrector method is used to improve the robustness and convergence of PD-IPM. Software. It is important to understand the high-level algorithmic idea for solving a convex QP. However, in practice, there are many mature QP solvers and it takes just a few lines of code to call your favorite QP solver. The following code snippet shows how to define a QP in cvxpy and then solve it using MOSEK (which implements PD-IPM). # Minimal dense QP with CVXPY # minimize (1/2) x^T P x + q^T x # subject to Ax &lt;= b, 1^T x = 1 # # pip install cvxpy import numpy as np import cvxpy as cp # ----- QP data (dense) ----- P = np.array([ [4.0, 1.0, 0.5], [1.0, 2.0, 0.3], [0.5, 0.3, 1.5] ], dtype=float) # Make sure P is symmetric positive definite P = 0.5 * (P + P.T) + 1e-9 * np.eye(3) q = np.array([-1.0, -2.0, -3.0]) A = np.array([ [1.0, -2.0, 1.0], # linear inequality: x1 - 2 x2 + x3 ≤ 2 [-1.0, 0.0, 0.0], # x1 ≥ 0 -&gt; -x1 ≤ 0 [0.0, -1.0, 0.0], # x2 ≥ 0 -&gt; -x2 ≤ 0 [0.0, 0.0, -1.0], # x3 ≥ 0 -&gt; -x3 ≤ 0 [1.0, 0.0, 0.0], # x1 ≤ 1.5 [0.0, 1.0, 0.0], # x2 ≤ 1.5 [0.0, 0.0, 1.0], # x3 ≤ 1.5 ]) b = np.array([2.0, 0.0, 0.0, 0.0, 1.5, 1.5, 1.5]) # Equality: sum(x) = 1 e = np.ones((1, 3)) d = np.array([1.0]) # ----- CVXPY problem ----- x = cp.Variable(3) objective = cp.Minimize(0.5 * cp.quad_form(x, P) + q @ x) constraints = [ A @ x &lt;= b, e @ x == d ] prob = cp.Problem(objective, constraints) # You can choose a solver; OSQP is common for QPs. ECOS/SCS also work. prob.solve(solver=cp.MOSEK, verbose=True) print(&quot;Status:&quot;, prob.status) print(&quot;Optimal value:&quot;, prob.value) print(&quot;x* =&quot;, x.value.round(6)) # (Optional) check constraints ineq_res = (A @ x.value - b) eq_res = (e @ x.value - d) print(&quot;Max inequality residual (&lt;=0):&quot;, np.max(ineq_res)) print(&quot;Equality residual (≈0):&quot;, eq_res.item()) Running the code produces the following output. You should now be able to interpret the iterations of MOSEK. ------------------------------------------------------------------------------- Numerical solver ------------------------------------------------------------------------------- (CVXPY) Nov 04 12:06:51 PM: Invoking solver MOSEK to obtain a solution. (CVXPY) Nov 04 12:06:52 PM: Problem (CVXPY) Nov 04 12:06:52 PM: Name : (CVXPY) Nov 04 12:06:52 PM: Objective sense : maximize (CVXPY) Nov 04 12:06:52 PM: Type : CONIC (conic optimization problem) (CVXPY) Nov 04 12:06:52 PM: Constraints : 4 (CVXPY) Nov 04 12:06:52 PM: Affine conic cons. : 0 (CVXPY) Nov 04 12:06:52 PM: Disjunctive cons. : 0 (CVXPY) Nov 04 12:06:52 PM: Cones : 1 (CVXPY) Nov 04 12:06:52 PM: Scalar variables : 13 (CVXPY) Nov 04 12:06:52 PM: Matrix variables : 0 (CVXPY) Nov 04 12:06:52 PM: Integer variables : 0 (CVXPY) Nov 04 12:06:52 PM: (CVXPY) Nov 04 12:06:52 PM: Optimizer started. (CVXPY) Nov 04 12:06:52 PM: Presolve started. (CVXPY) Nov 04 12:06:52 PM: Linear dependency checker started. (CVXPY) Nov 04 12:06:52 PM: Linear dependency checker terminated. (CVXPY) Nov 04 12:06:52 PM: Eliminator started. (CVXPY) Nov 04 12:06:52 PM: Freed constraints in eliminator : 1 (CVXPY) Nov 04 12:06:52 PM: Eliminator terminated. (CVXPY) Nov 04 12:06:52 PM: Eliminator started. (CVXPY) Nov 04 12:06:52 PM: Freed constraints in eliminator : 0 (CVXPY) Nov 04 12:06:52 PM: Eliminator terminated. (CVXPY) Nov 04 12:06:52 PM: Eliminator - tries : 2 time : 0.00 (CVXPY) Nov 04 12:06:52 PM: Lin. dep. - tries : 1 time : 0.00 (CVXPY) Nov 04 12:06:52 PM: Lin. dep. - primal attempts : 1 successes : 1 (CVXPY) Nov 04 12:06:52 PM: Lin. dep. - dual attempts : 0 successes : 0 (CVXPY) Nov 04 12:06:52 PM: Lin. dep. - primal deps. : 0 dual deps. : 0 (CVXPY) Nov 04 12:06:52 PM: Presolve terminated. Time: 0.00 (CVXPY) Nov 04 12:06:52 PM: Optimizer - threads : 12 (CVXPY) Nov 04 12:06:52 PM: Optimizer - solved problem : the primal (CVXPY) Nov 04 12:06:52 PM: Optimizer - Constraints : 3 (CVXPY) Nov 04 12:06:52 PM: Optimizer - Cones : 1 (CVXPY) Nov 04 12:06:52 PM: Optimizer - Scalar variables : 10 conic : 5 (CVXPY) Nov 04 12:06:52 PM: Optimizer - Semi-definite variables: 0 scalarized : 0 (CVXPY) Nov 04 12:06:52 PM: Factor - setup time : 0.00 (CVXPY) Nov 04 12:06:52 PM: Factor - dense det. time : 0.00 GP order time : 0.00 (CVXPY) Nov 04 12:06:52 PM: Factor - nonzeros before factor : 6 after factor : 6 (CVXPY) Nov 04 12:06:52 PM: Factor - dense dim. : 0 flops : 6.40e+01 (CVXPY) Nov 04 12:06:52 PM: ITE PFEAS DFEAS GFEAS PRSTATUS POBJ DOBJ MU TIME (CVXPY) Nov 04 12:06:52 PM: 0 1.3e+00 3.0e+00 2.0e+00 0.00e+00 -2.000000000e+00 -1.000000000e+00 1.0e+00 0.00 (CVXPY) Nov 04 12:06:52 PM: 1 2.2e-01 5.3e-01 2.4e-01 1.05e-01 -1.810182709e+00 -1.723136430e+00 1.8e-01 0.00 (CVXPY) Nov 04 12:06:52 PM: 2 3.7e-02 8.9e-02 1.6e-02 9.48e-01 -2.154687486e+00 -2.135415569e+00 3.0e-02 0.00 (CVXPY) Nov 04 12:06:52 PM: 3 1.0e-02 2.5e-02 2.2e-03 1.01e+00 -2.236775806e+00 -2.230836084e+00 8.2e-03 0.00 (CVXPY) Nov 04 12:06:52 PM: 4 2.8e-03 6.7e-03 3.2e-04 1.02e+00 -2.250313751e+00 -2.248643381e+00 2.2e-03 0.00 (CVXPY) Nov 04 12:06:52 PM: 5 7.7e-04 1.8e-03 4.5e-05 1.00e+00 -2.256166317e+00 -2.255708191e+00 6.1e-04 0.00 (CVXPY) Nov 04 12:06:52 PM: 6 2.1e-05 5.1e-05 2.1e-07 1.00e+00 -2.256863865e+00 -2.256850955e+00 1.7e-05 0.00 (CVXPY) Nov 04 12:06:52 PM: 7 1.0e-07 2.4e-07 6.8e-11 1.00e+00 -2.256896295e+00 -2.256896233e+00 8.1e-08 0.00 (CVXPY) Nov 04 12:06:52 PM: 8 8.5e-09 2.0e-08 1.6e-12 1.00e+00 -2.256896526e+00 -2.256896521e+00 6.7e-09 0.00 (CVXPY) Nov 04 12:06:52 PM: Optimizer terminated. Time: 0.00 (CVXPY) Nov 04 12:06:52 PM: (CVXPY) Nov 04 12:06:52 PM: (CVXPY) Nov 04 12:06:52 PM: Interior-point solution summary (CVXPY) Nov 04 12:06:52 PM: Problem status : PRIMAL_AND_DUAL_FEASIBLE (CVXPY) Nov 04 12:06:52 PM: Solution status : OPTIMAL (CVXPY) Nov 04 12:06:52 PM: Primal. obj: -2.2568965259e+00 nrm: 3e+00 Viol. con: 2e-08 var: 1e-08 cones: 0e+00 (CVXPY) Nov 04 12:06:52 PM: Dual. obj: -2.2568965208e+00 nrm: 1e+00 Viol. con: 0e+00 var: 2e-08 cones: 0e+00 ------------------------------------------------------------------------------- Summary ------------------------------------------------------------------------------- (CVXPY) Nov 04 12:06:52 PM: Problem status: optimal (CVXPY) Nov 04 12:06:52 PM: Optimal value: -2.257e+00 (CVXPY) Nov 04 12:06:52 PM: Compilation took 3.196e-03 seconds (CVXPY) Nov 04 12:06:52 PM: Solver (including time spent in interface) took 1.100e+00 seconds Status: optimal Optimal value: -2.256896525744356 x* = [0. 0.068914 0.931086] Max inequality residual (&lt;=0): -1.938693250380652e-08 Equality residual (≈0): 0.0 4.3.4 Sequential Quadratic Programming We have seen that quadratic programs (QPs) gracefully handle constrained LQR–style trajectory optimization: convex quadratic costs, linear dynamics, and affine path/terminal constraints. With time-stacked sparsity, these problems are solved efficiently (e.g., via interior-point methods), making QP a strong tool for that regime. The natural next step is the general trajectory optimization problem in (4.29), which features nonlinear dynamics, nonconvex objectives, and nonlinear constraints. To tackle this, we turn to Sequential Quadratic Programming (SQP)—a Newton-like framework that iteratively linearizes the dynamics/constraints and quadratizes the objective to form a sequence of QP subproblems. Each QP is solved to produce a step and updated multipliers; with globalization and appropriate Hessian modeling, the sequence converges to a locally optimal solution (a KKT point) of the original nonlinear TO problem. In short: QP handles the convex linearized case; SQP extends that logic to the full nonlinear setting by repeatedly building and solving the right QP at the current iterate. For an in-depth presentation of SQP for nonlinear programming, we refer to Chapter 18 of (Nocedal and Wright 1999). 4.3.4.1 Problem Statement We will consider the following general nonlinear program (NLP) \\[ \\begin{aligned} \\min_{x\\in\\mathbb{R}^n}\\quad &amp; f(x) \\\\ \\text{s.t.}\\quad &amp; c(x)=0 \\quad (c:\\mathbb{R}^n\\!\\to\\mathbb{R}^{m}),\\\\ &amp; d(x)\\le 0 \\quad (d:\\mathbb{R}^n\\!\\to\\mathbb{R}^{p}), \\end{aligned} \\tag{4.55} \\] with objective function \\(f(x)\\), equality constraints \\(c(x)=0\\), and inequality constraints \\(d(x)\\leq 0\\). To obtain the NLP formulation (4.55) from the TO template (4.29), one needs to define the set constraints \\(x_t \\in \\mathcal{X}_t\\) and \\(u_t \\in \\mathcal{U}_t\\) as general equality and inequality constraints. The decision variable \\(x\\) contains the entire sequence of states and actions. While in many cases the functions \\(f,c,d\\) are defined analytically, technically speaking, we only need zero-order and first-order oracles of these functions to implement numerical algorithms. That is, given a point \\(x\\), we need to evaluate \\(f(x), c(x), d(x)\\) and their first-order derivatives. With dual variables \\(\\lambda \\in \\mathbb{R}^m\\) and \\(\\mu \\in \\mathbb{R}^p\\), define the Lagrangian of (4.55) as \\[ \\mathcal L(x,\\lambda,\\mu) \\;=\\; f(x) + \\lambda^\\top c(x) + \\mu^\\top d(x),\\qquad \\mu\\ge 0, \\] and Jacobians \\(J_c(x):=\\nabla c(x)\\in\\mathbb{R}^{m\\times n}\\), \\(J_d(x):=\\nabla d(x)\\in\\mathbb{R}^{p\\times n}\\). 4.3.4.2 High-Level Intuition At a current iterate \\(x_k\\), we (1) Linearize the constraints and (2) quadratize the Lagrangian to build a local QP. Solving this QP yields a primal step \\(p_k\\) and updated multipliers \\((\\lambda_{k+1},\\mu_{k+1})\\) (from QP duals). A line-search (or trust-region) with a merit or filter globalization ensures convergence from remote starts. Key ingredients: A Hessian (or quasi-Newton) approximation \\(H_k \\approx \\nabla_{xx}^2\\mathcal L(x_k,\\lambda_k,\\mu_k)\\). A QP subproblem capturing first-order feasibility and second-order optimality locally. A globalization mechanism (\\(\\ell_1\\) merit or filter) + optional second-order correction (SOC) to mitigate linearization error in active inequalities. 4.3.4.3 The SQP QP Subproblem Given the current iterate \\((x_k,\\lambda_k,\\mu_k)\\), define \\[ g_k := \\nabla f(x_k),\\qquad c_k := c(x_k),\\qquad d_k := d(x_k),\\qquad A_k := J_c(x_k),\\qquad G_k := J_d(x_k). \\] Let \\(H_k\\) be a symmetric approximation to \\(\\nabla_{xx}^2 \\mathcal L(x_k,\\lambda_k,\\mu_k)\\) (see Section 4.3.4.4). The QP subproblem in this step is \\[ \\begin{aligned} \\min_{p\\in\\mathbb{R}^n}\\quad &amp; \\frac{1}{2} p^\\top H_k\\, p + g_k^\\top p \\\\[1mm] \\text{s.t.}\\quad &amp; A_k\\, p + c_k = 0, \\\\ &amp; G_k\\, p + d_k \\le 0. \\end{aligned} \\tag{4.56} \\] This defines a local quadratic model of (4.55): constraints are linearized; the objective is the second-order Taylor model of the Lagrangian (up to a constant). Solving the QP subproblem (4.56) returns: Primal step \\(p_k\\). Dual estimates \\(\\lambda_{k+1}^{\\text{QP}}, \\mu_{k+1}^{\\text{QP}}\\) (the QP multipliers), which we use as new multipliers. 4.3.4.4 Hessian Approximation The most natural choice for \\(H_k\\) in the QP subproblem (4.56) is the exact Hessian: \\[ H_k = \\nabla_{xx}^2 \\mathcal{L}(x_k, \\lambda_k, \\mu_k) = \\nabla_{xx}^2 f(x_k) + \\sum_{i} \\lambda_{k,i} \\nabla_{xx}^2 c_i(x_k) + \\sum_{i} \\mu_{k,i} \\nabla_{xx}^2 d_i(x_k). \\] However, two potential issues with the exact Hessian are (i) it can be costly to build and store the analytic Hessians \\(\\nabla_{xx}^2 f, \\nabla_{xx}^2 c_i, \\nabla_{xx}^2 d_i\\); (ii) the exact Hessian \\(H_k\\) may not be positive semidefinite, which may lead to failure of convexity in the QP subproblem. A cornerstone result in numerical optimization, due to Broyden–Fletcher–Goldfarb–Shanno (BFGS), is to build an approximate Hessian from first-order derivatives. In particular, given two consecutive primal iterates \\(x_{k+1}, x_k\\) (and their associated dual variables) and first-order gradients of the Lagrangian \\(\\nabla_x \\mathcal{L}(x_{k+1}, \\lambda_{k+1}, \\mu_{k+1})\\), \\(\\nabla_x \\mathcal{L}(x_{k}, \\lambda_{k}, \\mu_{k})\\), define \\[ s_k := x_{k+1}-x_k,\\qquad y_k := \\nabla_x \\mathcal L(x_{k+1},\\lambda_{k+1},\\mu_{k+1}) - \\nabla_x \\mathcal L(x_k,\\lambda_k,\\mu_k). \\] The BFGS quasi-Newton method updates \\(H_{k+1}\\) from \\(H_k\\) as follows \\[ H_{k+1} = H_k - \\frac{H_k s_k s_k^\\top H_k}{s_k^\\top H_k s_k} + \\frac{y_k y_k^\\top}{s_k^\\top y_k}. \\tag{4.57} \\] One can show that if \\(y_k^\\top s_k &gt; 0\\) holds, the BFGS Hessian approximation is always positive definite (provided \\(H_0 \\succ 0\\)). Therefore, the BFGS Hessian approximation ensures the QP subproblem is convex. If the curvature condition \\(y_k^\\top s_k &gt; 0\\) fails to hold, one can resort to damped BFGS, see (Nocedal and Wright 1999). There is a broad family of quasi-Newton methods with BFGS being one of the most popular instances. For example, the symmetric rank-one (SR1) method is another popular quasi-Newton variant. In addition, the “limited memory” version of quasi-Newton methods (e.g., limited memory BFGS (Liu and Nocedal 1989)) can further reduce the price of Hessian approximation by only looking at the history of a small amount of gradients. 4.3.4.5 Globalization: Merit or Filter To accept a step, we assess optimality improvement + feasibility improvement: \\(\\ell_1\\) merit (exact-penalty style): define the merit function \\[ \\phi_\\rho(x) = f(x) + \\rho \\big(\\|c(x)\\|_1 + \\|d^+(x)\\|_1\\big),\\quad d^+ := \\max(d,0), \\] with penalty \\(\\rho\\) large enough (\\(\\rho\\) can also be adaptive with respect to iterations). Use backtracking line search on \\(\\phi_\\rho(x_k+\\alpha p_k)\\) to ensure decrease of the merit function. Filter method: Maintain a set of pairs \\((\\text{feasibility},\\text{objective})\\). Accept steps that reduce either feasibility or objective sufficiently without worsening the other beyond the filter. Often paired with Second-order correction, see more details in (Nocedal and Wright 1999). The following pseudocode implements a basic line-search SQP with quasi-Newton Hessian approximation. Algorithm: Line-Search SQP Inputs: \\(x_0\\), multipliers \\((\\lambda_0,\\mu_0\\ge 0)\\), initial Hessian \\(H_0\\succ 0\\) (e.g., \\(\\gamma I\\)), globalization parameters. For \\(k=0,1,2,\\dots\\) Linearize &amp; build QP: form \\(g_k, A_k, G_k, c_k, d_k\\) and \\(H_k\\), then solve the QP (4.56) to get \\(p_k\\) and QP multipliers \\((\\hat\\lambda_{k+1},\\hat\\mu_{k+1}\\ge 0)\\). Globalization: Choose step size \\(\\alpha_k\\in(0,1]\\) by backtracking on the \\(\\ell_1\\)-merit. Update: \\[ x_{k+1} = x_k + \\alpha_k p_k,\\qquad \\lambda_{k+1} = \\hat\\lambda_{k+1},\\qquad \\mu_{k+1} = \\Pi_{\\ge 0}(\\hat\\mu_{k+1}). \\] Hessian (quasi-Newton) update: define \\[ s_k := x_{k+1}-x_k,\\qquad y_k := \\nabla_x \\mathcal L(x_{k+1},\\lambda_{k+1},\\mu_{k+1}) - \\nabla_x \\mathcal L(x_k,\\lambda_k,\\mu_k). \\] update \\[ H_{k+1} = H_k - \\frac{H_k s_k s_k^\\top H_k}{s_k^\\top H_k s_k} + \\frac{y_k y_k^\\top}{s_k^\\top y_k}. \\] Stopping: if KKT residuals (stationarity, primal feasibility, complementarity) are below tolerance, terminate. Notes. Trust-region SQP. An alternative globalization: add \\(\\|p\\|\\le \\Delta\\) or a quadratic trust region, and update \\(\\Delta\\) by comparing predicted vs. actual reduction in a composite model. Software. The scipy package in Python implements SLSQP, which is basically the line-search SQP we presented above. A more advanced version of SQP is provided by the SNOPT commercial software. The CRISP software provides a C++ implementation of an SQP algorithm. Additionally, Matlab’s fmincon provides an implementation of SQP. Example 4.4 (Trajectory Optimization with SQP) We formulate a trajectory optimization (TO) problem for a unicycle robot that must travel from a start pose \\(A\\) to a goal pose \\(B\\) while avoiding circular (ball-shaped) obstacles. System Model. We use the standard unicycle (Dubins-like) kinematics in continuous time: \\[ \\dot{x}(t)= \\begin{bmatrix} \\dot{p}_x\\\\[2pt]\\dot{p}_y\\\\[2pt]\\dot{\\theta} \\end{bmatrix} = \\begin{bmatrix} v(t)\\cos\\theta(t)\\\\ v(t)\\sin\\theta(t)\\\\ \\omega(t) \\end{bmatrix}, \\qquad x=[p_x,p_y,\\theta]^\\top,\\ \\ u=[v,\\omega]^\\top. \\tag{4.58} \\] We discretize on a uniform grid \\(t_k=k h,\\ k=0,\\dots,N\\) with step \\(h&gt;0\\) by forward Euler: \\[ x_{k+1} \\;=\\; f_h(x_k,u_k) \\;:=\\; \\begin{bmatrix} p_{x,k} + h\\, v_k \\cos \\theta_k\\\\ p_{y,k} + h\\, v_k \\sin \\theta_k\\\\ \\theta_k + h\\, \\omega_k \\end{bmatrix}. \\tag{4.59} \\] Decision Variables. We optimize over the state–control sequence \\[ \\{x_k\\}_{k=0}^N,\\quad \\{u_k\\}_{k=0}^{N-1}, \\] and collect them into a single vector \\[ z = \\big[x_0^\\top,\\ u_0^\\top,\\ x_1^\\top,\\ u_1^\\top,\\ \\dots,\\ x_{N-1}^\\top,\\ u_{N-1}^\\top,\\ x_N^\\top\\big]^\\top \\;\\in\\;\\mathbb{R}^{(3+2)N+3}. \\tag{4.60} \\] Constraints. We impose the following constraints. (i) Initial condition. \\[ x_0 = A \\in \\mathbb{R}^3. \\tag{4.61} \\] (ii) System dynamics (equality constraints). For \\(k=0,\\dots,N-1\\), \\[ x_{k+1} - f_h(x_k,u_k) = 0. \\tag{4.62} \\] (iii) Obstacle avoidance (inequalities). Let the set of circular obstacles be \\(\\mathcal{O}=\\{(c_j,r_j)\\}_{j=1}^{n_{\\text{obs}}}\\) with centers \\(c_j=[c_{x,j},c_{y,j}]^\\top\\) and radii \\(r_j&gt;0\\). We require the robot’s position to stay outside each inflated disk of radius \\(r_j+\\delta\\) (safety margin \\(\\delta\\ge 0\\)) at every knot: \\[ \\underbrace{(p_{x,k}-c_{x,j})^2 + (p_{y,k}-c_{y,j})^2}_{\\text{dist}^2(x_k,\\text{center}_j)} \\;\\;\\ge\\;\\; (r_j+\\delta)^2, \\qquad \\forall k=0,\\dots,N,\\ \\forall j. \\] In “\\(c(x)\\le 0\\)” form (e.g., for fmincon): \\[ c_{j,k}(x_k) \\;:=\\; (r_j+\\delta)^2 - \\big((p_{x,k}-c_{x,j})^2 + (p_{y,k}-c_{y,j})^2\\big) \\;\\le\\; 0. \\tag{4.63} \\] (iv) Simple bounds. Box limits on controls (and possibly states): \\[ v_{\\min} \\le v_k \\le v_{\\max},\\qquad \\omega_{\\min} \\le \\omega_k \\le \\omega_{\\max},\\qquad k=0,\\dots,N-1. \\tag{4.64} \\] Objective function. We use a smooth quadratic objective combining (a) terminal goal tracking, (b) control effort, and (c) control smoothness (temporal regularization): Terminal goal tracking to a desired pose \\(B=[p_x^\\star,p_y^\\star,\\theta^\\star]^\\top\\): \\[ J_{\\text{goal}} \\;=\\; w_{\\text{pos}} \\,\\big\\|x_N^{\\text{pos}} - B^{\\text{pos}}\\big\\|_2^2 \\;+\\; w_{\\theta}\\,(\\theta_N-\\theta^\\star)^2, \\quad x_N^{\\text{pos}}=[p_{x,N},p_{y,N}]^\\top. \\] Control effort: \\[ J_{u} \\;=\\; \\sum_{k=0}^{N-1} w_u \\,\\|u_k\\|_2^2 \\;=\\; \\sum_{k=0}^{N-1} w_u\\,(v_k^2+\\omega_k^2). \\] Control smoothness (discrete total-variation-like quadratic): \\[ J_{\\Delta u} \\;=\\; \\sum_{k=0}^{N-2} w_{\\Delta u}\\,\\|u_{k+1}-u_k\\|_2^2. \\] The complete cost is \\[ J(x_{0:N},u_{0:N-1}) = \\frac{1}{2}\\Big( J_{\\text{goal}} + J_u + J_{\\Delta u} \\Big), \\tag{4.65} \\] where the outer factor \\(\\frac{1}{2}\\) is conventional in quadratic objectives. Complete optimization problem. Given \\(A,B,\\{(c_j,r_j)\\},\\delta,h,N\\), choose \\(\\{x_k\\}_{k=0}^N,\\{u_k\\}_{k=0}^{N-1}\\) to \\[ \\begin{aligned} \\min_{\\{x_k,u_k\\}} \\quad &amp; \\frac{1}{2}\\Big( w_{\\text{pos}} \\|x_N^{\\text{pos}}-B^{\\text{pos}}\\|_2^2 + w_{\\theta}(\\theta_N-\\theta^\\star)^2 + \\sum_{k=0}^{N-1} w_u \\|u_k\\|_2^2 + \\sum_{k=0}^{N-2} w_{\\Delta u} \\|u_{k+1}-u_k\\|_2^2 \\Big) \\\\[2mm] \\text{s.t.}\\quad &amp; x_0 = A, \\\\[2pt] &amp; x_{k+1} = f_h(x_k,u_k)\\quad \\text{for } k=0,\\dots,N-1, \\\\[2pt] &amp; (r_j+\\delta)^2 - \\big((p_{x,k}-c_{x,j})^2 + (p_{y,k}-c_{y,j})^2\\big) \\le 0, \\ \\ \\forall j,\\ \\forall k, \\\\[2pt] &amp; v_{\\min} \\le v_k \\le v_{\\max},\\quad \\omega_{\\min} \\le \\omega_k \\le \\omega_{\\max},\\quad k=0,\\dots,N-1. \\end{aligned} \\] This is a smooth, sparse nonlinear program (NLP). Experimental setup. Horizon \\(N=60\\), step \\(h=0.1\\) s. Start \\(A=[0,0,0]^\\top\\), goal \\(B=[6,5,0]^\\top\\). Obstacles \\(\\{(c_j,r_j)\\}\\) with a margin \\(\\delta=0.25\\) m. Control bounds \\(v\\in[-1.5,1.5]\\) m/s, \\(\\omega\\in[-2,2]\\) rad/s. Weights \\(w_{\\text{pos}}=400,\\ w_\\theta=20,\\ w_u=0.05,\\ w_{\\Delta u}=0.2\\). Initialization: straight-line interpolation of positions from \\(A\\) to \\(B\\), heading toward the line, constant \\(v\\), \\(\\omega=0\\). Two obstacles (success). Fig. 4.6 shows the results for trajectory optimization with two obstacles. As we can see, the SQP algorithm generates a smooth trajectory that avoids the two circular obstacles, despite the fact that the initial guess crosses both obstacles. Figure 4.6: Trajectory optimization for unicyle using SQP (two obstacles). Dotted line: initial guess; solid line: optimized trajectory. Three obstacles (failure). However, when we add a third obstacle, Fig. 4.7 shows that the SQP algorithm converges to an infeasible solution that collides with the obstacles. You can play with the Matlab code here. Figure 4.7: Trajectory optimization for unicyle using SQP (three obstacles). Dotted line: initial guess; solid line: optimized trajectory. The example above highlights both the strengths and the limitations of solving TO with numerical NLP methods such as SQP. Because the TO problem is generally nonconvex, a local method’s outcome depends strongly on the initialization. With a good initial guess, local NLP solvers often converge quickly to a high-quality solution (e.g., Fig. 4.6). With a poor initialization—especially when the landscape has many local minima—the solver may settle in a suboptimal basin or even fail to find a feasible trajectory (e.g., Fig. 4.7). Global optimization methods can avoid initialization sensitivity and provide global optimality guarantees. These techniques can be substantially more expensive and require additional structure or reformulation, but when applicable they yield powerful initialization-free solutions; see, e.g., (Kang et al. 2024) and references therein. 4.3.5 Interior-Point Methods We have seen that Primal–Dual Interior-Point Methods (PD-IPM) efficiently solve convex QPs (Section 4.3.3). The key idea was to write the KKT optimality conditions as a system of nonlinear equations and apply Newton’s method. We now extend this idea to the general NLP in (4.55): \\[ \\min_{x\\in\\mathbb{R}^n} f(x)\\quad\\text{s.t.}\\quad c(x)=0,\\;\\; d(x)\\le 0, \\] where \\(f:\\mathbb{R}^n \\to \\mathbb{R}, c:\\mathbb{R}^n \\to \\mathbb{R}^m\\) and \\(d:\\mathbb{R}^n \\to\\mathbb{R}^p\\) are smooth. 4.3.5.1 Lagrangian and KKT Introduce slack variables \\(s\\in\\mathbb{R}^p\\) to convert inequalities to equalities: \\[ c(x)=0,\\qquad d(x)+s=0,\\qquad s\\ge 0. \\tag{4.66} \\] With equality multipliers \\(y\\in\\mathbb{R}^m, \\nu \\in \\mathbb{R}^p\\) and inequality multipliers \\(\\mu\\in\\mathbb{R}^p\\) (with \\(\\lambda \\ge 0\\)), the Lagrangian of the slack-form problem is \\[ \\mathcal L(x,s,y,\\nu,\\lambda) \\;=\\; f(x) + y^\\top c(x) + \\nu^\\top \\big(d(x)+s\\big) - \\lambda^\\top s. \\tag{4.67} \\] The KKT optimality conditions are (after eliminating \\(\\nu\\)) \\[ \\begin{aligned} \\text{stationarity:}&amp;\\quad \\nabla f(x) + J_c(x)^\\top y + J_d(x)^\\top \\lambda = 0,\\\\ \\text{primal feasibility:}&amp;\\quad c(x)=0,\\;\\; d(x)+s=0,\\;\\; s\\ge 0,\\\\ \\text{dual feasibility:}&amp;\\quad \\lambda\\ge 0,\\\\ \\text{complementarity:}&amp;\\quad s_i\\,\\lambda_i = 0,\\quad i=1,\\dots,p, \\end{aligned} \\tag{4.68} \\] where \\(J_c=\\nabla c(x) \\in \\mathbb{R}^{m \\times n}\\), \\(J_d=\\nabla d(x) \\in \\mathbb{R}^{p \\times n}\\). 4.3.5.2 Two Equivalent Views There are two equivalent ways to derive primal–dual IPMs. Homotopy / Perturbed KKT (primal–dual view). Replace the complementarity slackness condition in the KKT system (4.68) by a perturbed relation that defines the central path: \\[ S\\,\\lambda \\;=\\; \\mu \\,\\mathbf{1},\\qquad S:=\\operatorname{diag}(s),\\;\\; \\mu &gt;0. \\tag{4.69} \\] Driving the parameter \\(\\mu \\downarrow 0\\) yields iterates approaching a KKT point. Barrier (log-barrier view). Solve the barrier subproblem \\[ \\min_{x,s&gt;0}\\; f(x) - \\mu \\sum_{i=1}^p \\log s_i \\quad\\text{s.t.}\\quad c(x)=0,\\;\\; d(x)+s=0, \\tag{4.70} \\] then reduce \\(\\mu\\). The KKT conditions of (4.70) imply \\(S\\lambda = \\mu \\mathbf{1}\\), hence the barrier and homotopy views are equivalent (different perspectives on the same central path). 4.3.5.3 Primal–Dual Residuals and Newton System Define residuals at \\((x,s,y,\\lambda)\\): \\[ \\begin{aligned} r_{\\mathrm{dual}} &amp;= \\nabla f(x) + J_c^\\top y + J_d^\\top \\lambda,\\\\ r_{\\mathrm{pe}} &amp;= c(x),\\\\ r_{\\mathrm{pi}} &amp;= d(x)+s,\\\\ r_{\\mathrm{cent}} &amp;= S\\lambda - \\mu \\,\\mathbf{1}. \\end{aligned} \\tag{4.71} \\] Let \\[ H \\;:=\\; \\nabla^2_{xx}\\mathcal L(x,s,y,\\lambda) = \\nabla^2 f(x) + \\sum_{i=1}^m y_i \\nabla^2 c_i(x) + \\sum_{j=1}^p \\lambda_j \\nabla^2 d_j(x) \\tag{4.72} \\] be the exact Hessian of the Lagrangian with respect to \\(x\\). A primal–dual Newton step \\((\\Delta x,\\Delta s,\\Delta y,\\Delta \\lambda)\\) solves \\[ \\begin{aligned} H\\,\\Delta x + J_c^\\top \\Delta y + J_d^\\top \\Delta \\lambda &amp;= -\\,r_{\\mathrm{dual}},\\\\ J_c\\,\\Delta x &amp;= -\\,r_{\\mathrm{pe}},\\\\ J_d\\,\\Delta x + \\Delta s &amp;= -\\,r_{\\mathrm{pi}},\\\\ S\\,\\Delta \\lambda + M\\,\\Delta s &amp;= -\\,r_{\\mathrm{cent}},\\qquad M:=\\operatorname{diag}(\\lambda). \\end{aligned} \\tag{4.73} \\] Eliminating \\(\\Delta s=-r_{\\mathrm{pi}}-J_d\\Delta x\\) gives \\[ \\Delta \\lambda = S^{-1}\\!\\left(-r_{\\mathrm{cent}} + M\\,r_{\\mathrm{pi}} + M\\,J_d\\,\\Delta x\\right). \\tag{4.74} \\] Substitute into the first line to obtain the reduced symmetric system in \\((\\Delta x,\\Delta y)\\): \\[ \\begin{bmatrix} H + J_d^\\top D\\,J_d &amp; J_c^\\top\\\\[2pt] J_c &amp; 0 \\end{bmatrix} \\begin{bmatrix}\\Delta x\\\\ \\Delta y\\end{bmatrix} = - \\begin{bmatrix} r_{\\mathrm{dual}} + J_d^\\top S^{-1}\\!\\big(-r_{\\mathrm{cent}} + M r_{\\mathrm{pi}}\\big)\\\\[2pt] r_{\\mathrm{pe}} \\end{bmatrix} \\tag{4.75} \\] with \\(D:=S^{-1}M\\). Then recover \\(\\Delta \\lambda\\) via (4.74) and \\(\\Delta s=-r_{\\mathrm{pi}}-J_d\\Delta x\\). 4.3.5.4 Line-search IPM Step lengths (fraction-to-the-boundary). Choose positive step sizes that keep strict interiority: \\[ \\alpha_{\\mathrm{pri}}=\\min\\!\\Big(1,\\ \\eta \\min_{\\Delta s_i&lt;0}\\frac{-s_i}{\\Delta s_i}\\Big),\\quad \\alpha_{\\mathrm{du}} =\\min\\!\\Big(1,\\ \\eta \\min_{\\Delta \\lambda_i&lt;0}\\frac{-\\lambda_i}{\\Delta \\lambda_i}\\Big),\\quad \\eta\\in(0,1). \\tag{4.76} \\] Merit (or filter) globalization. Use a barrier merit for backtracking, \\[ \\Phi_{\\mu}(x,s) \\;=\\; f(x) - \\mu \\sum_i \\log s_i + \\frac{\\rho}{2}\\,\\|c(x)\\|_2^2 + \\frac{\\rho}{2}\\,\\|d(x)+s\\|_2^2, \\tag{4.77} \\] or a filter that accepts steps reducing either infeasibility or the barrier objective. Mehrotra predictor–corrector (recommended). Predictor (affine) step: solve (4.75) with \\(\\mu=0\\) to get \\((\\Delta x^{\\mathrm{aff}},\\Delta s^{\\mathrm{aff}},\\Delta y^{\\mathrm{aff}},\\Delta\\lambda^{\\mathrm{aff}})\\), and affine step sizes \\(\\alpha_{\\mathrm{pri}}^{\\mathrm{aff}},\\alpha_{\\mathrm{du}}^{\\mathrm{aff}}\\). Centering: set \\[ \\tau_{\\mathrm{aff}}=\\frac{(s+\\alpha_{\\mathrm{pri}}^{\\mathrm{aff}}\\Delta s^{\\mathrm{aff}})^\\top (\\lambda+\\alpha_{\\mathrm{du}}^{\\mathrm{aff}}\\Delta\\lambda^{\\mathrm{aff}})}{p},\\qquad \\sigma=\\left(\\frac{\\tau_{\\mathrm{aff}}}{\\frac{s^\\top\\mu}{p}}\\right)^{3}, \\] and replace the complementarity RHS by \\(\\mu=\\sigma\\,\\frac{s^\\top\\lambda}{p}\\). Corrector: resolve (4.75) with the corrected central residual \\[ r_{\\mathrm{cent}}^{\\mathrm{corr}} = S\\lambda - \\mu \\,\\mathbf{1} - \\Delta S^{\\mathrm{aff}}\\Delta\\lambda^{\\mathrm{aff}}\\mathbf{1}. \\tag{4.78} \\] Line search &amp; update: use (4.76) and backtrack on \\(\\Phi_{\\mu}\\). Stopping: terminate inner loop when \\(\\|r_{\\mathrm{dual}}\\|_\\infty,\\|r_{\\mathrm{pe}}\\|_\\infty,\\|r_{\\mathrm{pi}}\\|_\\infty\\) and the average complementarity \\(\\frac{s^\\top\\mu}{p}\\) are below tolerance; then reduce \\(\\mu\\) and repeat. Hessian Modeling. Use \\(H=\\nabla^2_{xx}\\mathcal L\\), or a damped (L-)BFGS / Gauss–Newton model as in Section 4.3.4.4. 4.3.5.5 Trust-Region IPM The trust-region (TR) IPM solves the barrier subproblem (4.70) inexactly within a TR globalization that is scaled to the slacks to avoid steps that approach the boundary too aggressively. The TR-IPM subproblem is the local quadratic model of the barrier problem together with a metric that respects distance to the boundary. Quadratic model of the barrier objective. Start from the barrier problem \\[ \\min_{x,s&gt;0}\\ f(x)-\\mu\\sum_i\\log s_i \\quad\\text{s.t.}\\quad c(x)=0,\\ d(x)+s=0. \\] A second-order Taylor model at \\((x,s)\\) gives (constants omitted) \\[ m(p_x,p_s)\\;\\approx\\; \\nabla f^\\top p_x+\\frac{1}{2} p_x^\\top H\\,p_x \\;-\\;\\mu\\,\\mathbf 1^\\top S^{-1}p_s \\;+\\;\\frac{1}{2}\\,\\mu\\,\\|S^{-1}p_s\\|_2^2, \\] where \\(H=\\nabla^2_{xx}\\mathcal L\\) and \\(S=\\operatorname{diag}(s)\\). The linear term \\(-\\mu\\,\\mathbf1^\\top S^{-1}p_s\\) is exactly the gradient of the log-barrier in the slack coordinates. The quadratic curvature in \\(p_s\\) is \\(\\mu S^{-2}\\). Many implementations move (part of) this curvature into the TR metric, replacing the explicit \\(+\\frac{1}{2}\\,\\mu\\|S^{-1}p_s\\|^2\\) by the scaled trust region below. This avoids double-counting and makes the subproblem simpler while keeping the right geometry. Linearized constraints for a consistent local step. \\[ J_c\\,p_x + c(x)=0,\\qquad J_d\\,p_x + p_s + d(x)=0. \\] These are the first-order feasibility conditions of the barrier constraints. Scaled trust region \\(\\| (p_x,\\;S^{-1}p_s)\\|\\le \\Delta\\). The scaling by \\(S^{-1}\\) measures \\(p_s\\) relative to the current distance to the boundary. If a slack \\(s_i\\) is tiny, even a small absolute change \\(p_{s,i}\\) is risky; the scaled norm automatically shrinks the allowable step in that direction. This yields: Boundary awareness: steps cannot run into \\(s_i\\le0\\) unless the trust region is (incorrectly) large. Scale invariance: the step is insensitive to units or simple rescalings of the inequalities. Numerical stability: the local subproblem remains well conditioned near the boundary. Fraction-to-the-boundary safeguard \\(p_s\\ge -\\tau s\\). This is a simple interiority constraint ensuring \\(s+\\alpha p_s&gt;0\\) for admissible step sizes. Putting these choices together yields the subproblem \\[ \\min_{p_x,p_s}\\ \\nabla f^\\top p_x+\\frac{1}{2} p_x^\\top H p_x - \\mu\\,\\mathbf1^\\top S^{-1}p_s \\quad\\text{s.t.}\\quad \\begin{cases} J_c p_x + c(x)=0,\\\\ J_d p_x + p_s + d(x)=0,\\\\ \\|(p_x,S^{-1}p_s)\\|\\le\\Delta,\\ p_s\\ge-\\tau s, \\end{cases} \\tag{4.79} \\] which is exactly a trust-region SQP step on the barrier problem in the barrier metric. The outer loop adapts the trust-region radius \\(\\Delta\\) and the barrier parameter \\(\\mu\\) as follows. Updating \\(\\Delta\\) (model–reality agreement). After computing a trial step \\(p=(p_x,p_s)\\) from the scaled TR subproblem (4.79), compare the predicted reduction from the quadratic model to the actual reduction in the barrier merit: \\[ \\rho \\;=\\; \\frac{\\text{ared}}{\\text{pred}} \\;=\\; \\frac{\\Phi_\\mu(x,s)-\\Phi_\\mu(x+p_x,\\,s+p_s)} {\\text{model}(0)-\\text{model}(p)}. \\] With thresholds \\(0&lt;\\eta_1&lt;\\eta_2&lt;1\\) and factors \\(\\gamma_{\\text{dec}}\\in(0,1)\\), \\(\\gamma_{\\text{inc}}&gt;1\\): If \\(\\rho\\ge \\eta_2\\): accept the step and enlarge the radius, \\(\\Delta \\leftarrow \\min(\\gamma_{\\text{inc}}\\Delta,\\Delta_{\\max})\\). If \\(\\eta_1 \\le \\rho &lt; \\eta_2\\): accept and keep \\(\\Delta\\). If \\(\\rho&lt;\\eta_1\\): reject the step and shrink the radius, \\(\\Delta \\leftarrow \\gamma_{\\text{dec}}\\Delta\\), then resolve the TR subproblem. Updating \\(\\mu\\) (centrality progress). Decrease the barrier parameter \\(\\mu\\) only when the current barrier subproblem is solved to an accuracy commensurate with \\(\\mu\\). Using KKT residuals and average complementarity \\[ \\tau \\;:=\\; \\frac{s^\\top \\lambda}{p}, \\] require (for some constants \\(\\kappa_{\\rm dual},\\kappa_{\\rm pe},\\kappa_{\\rm pi},\\kappa_{\\rm cent}&gt;0\\)) \\[ \\|r_{\\rm dual}\\|_\\infty \\le \\kappa_{\\rm dual}\\,\\mu,\\quad \\|r_{\\rm pe}\\|_\\infty \\le \\kappa_{\\rm pe}\\,\\mu,\\quad \\|r_{\\rm pi}\\|_\\infty \\le \\kappa_{\\rm pi}\\,\\mu,\\quad |\\tau-\\mu| \\le \\kappa_{\\rm cent}\\,\\mu. \\] When these hold, reduce \\(\\mu\\) (e.g., \\(\\mu\\leftarrow\\theta\\,\\mu\\) with \\(\\theta\\in(0,1)\\)). If the tests are not met, hold \\(\\mu\\) fixed and continue improving the inner TR solve (possibly with an updated \\(\\Delta\\)). When to favor TR-IPM. TR globalization is robust for nonconvex \\(H\\), allows inexact linear solves (e.g., Krylov), and integrates naturally with limited-memory updates and scaling. Software. Implementing a robust interior-point method that reliably solves general nonlinear programs is nontrivial. Fortunately, the open-source solver IPOPT (Wächter and Biegler 2006) is a mature, widely used option that exploits sparsity and supports exact or quasi-Newton Hessians. In what follows, we formulate a trajectory-optimization problem and solve it with IPOPT to illustrate end-to-end modeling and solver usage. Example 4.5 (Trajectory Optimization with IPOPT) We solve a unicycle trajectory optimization (TO) problem (same as the one in Example 4.4) from start pose \\(A\\) to goal pose \\(B\\) while avoiding circular obstacles, now using IPOPT via cyipopt in Python. Dynamics. With state \\(x=[p_x,p_y,\\theta]^\\top\\) and control \\(u=[v,\\omega]^\\top\\), \\[ \\dot{x}(t)= \\begin{bmatrix} v\\cos\\theta\\\\[2pt] v\\sin\\theta\\\\ \\omega \\end{bmatrix}. \\tag{4.80} \\] We discretize at \\(t_k=kh\\) by forward Euler: \\[ x_{k+1} = f_h(x_k,u_k) := \\begin{bmatrix} p_{x,k} + h\\, v_k \\cos \\theta_k\\\\ p_{y,k} + h\\, v_k \\sin \\theta_k\\\\ \\theta_k + h\\, \\omega_k \\end{bmatrix},\\quad k=0,\\ldots,N-1. \\tag{4.81} \\] Decision vector. We stack the state and control trajectories into a single decision vector \\[ z=\\big[x_0^\\top,u_0^\\top,x_1^\\top,u_1^\\top,\\dots,x_{N-1}^\\top,u_{N-1}^\\top,x_N^\\top\\big]^\\top \\in\\mathbb{R}^{(3+2)N+3}. \\tag{4.82} \\] Constraints. The state and control trajectories need to satisfy the following constraints: Initial condition: \\(x_0=A\\). Dynamics equalities: \\(x_{k+1}-f_h(x_k,u_k)=0\\) for \\(k=0,\\dots,N-1\\). Obstacle inequalities: for each circular obstacle \\((c_j=[c_{x,j},c_{y,j}]^\\top,r_j)\\) and all \\(k=0,\\dots,N\\), \\[ c_{j,k}(x_k):=(r_j+\\delta)^2-\\big((p_{x,k}-c_{x,j})^2+(p_{y,k}-c_{y,j})^2\\big)\\le 0, \\tag{4.83} \\] where \\(\\delta &gt; 0\\) is a safety margin. Box bounds on controls: \\[ v_{\\min}\\le v_k\\le v_{\\max},\\qquad \\omega_{\\min}\\le \\omega_k\\le \\omega_{\\max}. \\tag{4.84} \\] In total, there are \\(m = 3 + 3N\\) equality constraints and \\(p = (N+1)O\\) inequality constraints, where \\(O\\) is the number of obstacles. Objective. A smooth quadratic cost with terminal goal tracking, control effort, and control smoothness: \\[\\begin{equation} \\hspace{-16mm} J = \\frac12\\Big( w_{\\text{pos}}\\|x_N^{\\text{pos}}-B^{\\text{pos}}\\|_2^2 + w_\\theta(\\theta_N-\\theta^\\star)^2 + \\sum_{k=0}^{N-1} w_u \\|u_k\\|_2^2 + \\sum_{k=0}^{N-2} w_{\\Delta u}\\|u_{k+1}-u_k\\|_2^2 \\Big), \\tag{4.85} \\end{equation}\\] with \\(x_N^{\\text{pos}}=[p_{x,N},p_{y,N}]^\\top\\) the terminal position and \\(B=[B_x,B_y,\\theta^\\star]^\\top\\) the terminal pose. \\(w_{\\text{pos}}\\), \\(w_\\theta\\), \\(w_u\\), and \\(w_{\\Delta u}\\) are positive weights. Jacobians. We derive the gradient of the objective and the Jacobian of the constraints with respect to the stacked decision vector \\[ z \\;=\\; \\big[x_0^\\top,\\ u_0^\\top,\\ x_1^\\top,\\ u_1^\\top,\\ \\dots,\\ x_{N-1}^\\top,\\ u_{N-1}^\\top,\\ x_N^\\top\\big]^\\top \\in \\mathbb{R}^{(3+2)N+3}, \\] where \\(x_k=[p_{x,k},\\,p_{y,k},\\,\\theta_k]^\\top\\) and \\(u_k=[v_k,\\,\\omega_k]^\\top\\). Objective gradient \\(\\nabla_z J\\). Recall the objective function from (4.85). Let \\(e_{\\text{pos}} := x_N^{\\text{pos}}-B^{\\text{pos}}\\) and \\(e_\\theta := \\theta_N-\\theta^\\star\\), we have \\[ \\frac{\\partial J}{\\partial p_{x,N}} = w_{\\text{pos}}\\, e_{\\text{pos},x},\\qquad \\frac{\\partial J}{\\partial p_{y,N}} = w_{\\text{pos}}\\, e_{\\text{pos},y},\\qquad \\frac{\\partial J}{\\partial \\theta_N} = w_\\theta\\, e_\\theta. \\] All other states \\(x_k\\) for \\(k&lt;N\\) do not appear in the objective, so \\[ \\frac{\\partial J}{\\partial x_k} = 0,\\quad k=0,\\dots,N-1. \\] The control effort term in the objective has gradients w.r.t. controls: \\[ J_u \\;=\\; \\frac{1}{2}\\sum_{k=0}^{N-1} w_u \\| u_k \\|^2 \\Rightarrow\\; \\frac{\\partial J_u}{\\partial u_k} \\;=\\; w_u\\,u_k \\quad\\text{for }k=0,\\dots,N-1. \\] The control smoothness term in the objective reads: \\[ J_{\\Delta u} \\;=\\; \\frac{1}{2}\\sum_{k=0}^{N-2} w_{\\Delta u}\\,\\|u_{k+1}-u_k\\|_2^2. \\] By collecting contributions from the two adjacent differences that contain \\(u_k\\), we obtain the gradient \\[ \\frac{\\partial J_{\\Delta u}}{\\partial u_k} = \\begin{cases} w_{\\Delta u}\\,(u_0 - u_1), &amp; k=0,\\\\[4pt] w_{\\Delta u}\\,(2u_k - u_{k-1} - u_{k+1}), &amp; k=1,\\dots,N-2,\\\\[4pt] w_{\\Delta u}\\,(u_{N-1} - u_{N-2}), &amp; k=N-1. \\end{cases} \\tag{4.86} \\] Combining the above derivations, the only nonzero blocks of \\(\\nabla_z J\\) are: the terminal state block \\(x_N\\): entries shown above for \\(p_{x,N},p_{y,N},\\theta_N\\); the control blocks \\(u_k\\): \\(w_u u_k\\) plus the smoothness terms (4.86). All other entries are zero. Thus \\(\\nabla_z J\\) is extremely sparse. Constraint Jacobian \\(\\nabla_z g(z)\\). We stack constraints as \\[ g(z)=\\begin{bmatrix} g^{\\text{init}}\\\\ g^{\\text{dyn}}\\\\ g^{\\text{obs}} \\end{bmatrix} \\in \\mathbb{R}^{3+3N+(N+1)O} \\] in the order: Initial condition \\(g^{\\text{init}} = x_0 - A = 0\\). Dynamics for \\(k=0,\\dots,N-1\\) (forward Euler with step \\(h\\)): \\[ \\begin{aligned} g^{\\text{dyn}}_{x,k}&amp;:= p_{x,k+1} - \\big(p_{x,k} + h\\,v_k\\cos\\theta_k\\big) = 0,\\\\ g^{\\text{dyn}}_{y,k}&amp;:= p_{y,k+1} - \\big(p_{y,k} + h\\,v_k\\sin\\theta_k\\big) = 0,\\\\ g^{\\text{dyn}}_{\\theta,k}&amp;:= \\theta_{k+1} - \\big(\\theta_k + h\\,\\omega_k\\big) = 0. \\end{aligned} \\] Obstacle inequalities for each obstacle \\(j=1,\\dots,O\\) and each knot \\(k=0,\\dots,N\\): \\[ g^{\\text{obs}}_{j,k} \\;:=\\; (r_j+\\delta)^2 - \\Big((p_{x,k}-c_{x,j})^2 + (p_{y,k}-c_{y,j})^2\\Big) \\;\\le\\; 0. \\] Below we list nonzero partial derivatives; all missing entries are \\(0\\). (i) Initial condition \\(g^{\\text{init}}=x_0-A\\) \\[ \\frac{\\partial g^{\\text{init}}}{\\partial x_0} = I_3. \\] (ii) Dynamics rows at time \\(k\\) \\(x\\)-row \\(g^{\\text{dyn}}_{x,k}\\): \\[ \\frac{\\partial g^{\\text{dyn}}_{x,k}}{\\partial p_{x,k+1}}=1,\\quad \\frac{\\partial g^{\\text{dyn}}_{x,k}}{\\partial p_{x,k}}=-1,\\quad \\frac{\\partial g^{\\text{dyn}}_{x,k}}{\\partial \\theta_k}=h\\,v_k\\sin\\theta_k,\\quad \\frac{\\partial g^{\\text{dyn}}_{x,k}}{\\partial v_k}=-h\\cos\\theta_k. \\] \\(y\\)-row \\(g^{\\text{dyn}}_{y,k}\\): \\[ \\frac{\\partial g^{\\text{dyn}}_{y,k}}{\\partial p_{y,k+1}}=1,\\quad \\frac{\\partial g^{\\text{dyn}}_{y,k}}{\\partial p_{y,k}}=-1,\\quad \\frac{\\partial g^{\\text{dyn}}_{y,k}}{\\partial \\theta_k}=-h\\,v_k\\cos\\theta_k,\\quad \\frac{\\partial g^{\\text{dyn}}_{y,k}}{\\partial v_k}=-h\\sin\\theta_k. \\] Heading row \\(g^{\\text{dyn}}_{\\theta,k}\\): \\[ \\frac{\\partial g^{\\text{dyn}}_{\\theta,k}}{\\partial \\theta_{k+1}}=1,\\quad \\frac{\\partial g^{\\text{dyn}}_{\\theta,k}}{\\partial \\theta_k}=-1,\\quad \\frac{\\partial g^{\\text{dyn}}_{\\theta,k}}{\\partial \\omega_k}=-h. \\] All other partials in each row are zero. Each dynamics triple only touches the local block \\((x_k,u_k,x_{k+1})\\), yielding a banded, block-sparse Jacobian in time. (iii) Obstacle row for \\((j,k)\\) \\[ g^{\\text{obs}}_{j,k}=(r_j+\\delta)^2 - \\big((p_{x,k}-c_{x,j})^2+(p_{y,k}-c_{y,j})^2\\big). \\] Nonzeros: \\[ \\frac{\\partial g^{\\text{obs}}_{j,k}}{\\partial p_{x,k}}=-2\\,(p_{x,k}-c_{x,j}),\\qquad \\frac{\\partial g^{\\text{obs}}_{j,k}}{\\partial p_{y,k}}=-2\\,(p_{y,k}-c_{y,j}). \\] This row depends only on the position of \\(x_k\\), so each obstacle row touches exactly two columns \\((p_{x,k},p_{y,k})\\). Combining the derivations above, we can see the Jacobian \\(\\nabla_z g\\) is block-banded in time. Initial block at \\(x_0\\) is identity. Each dynamics row touches \\((x_k, u_k, x_{k+1})\\) with at most 4 nonzeros in the \\(x\\)- and \\(y\\)-rows and 3 nonzeros in the \\(\\theta\\)-row. Each obstacle row touches only \\((p_{x,k}, p_{y,k})\\). Fig. 4.8 illustrates the sparsity structure of the constraint Jacobian. Figure 4.8: Sparsity structure of constraint Jacobian. The following Python code snippet defines a problem class UnicycleTO with definitions of the objective, constraints, objective gradient, and constraints Jacobian. Note that the definition of the constraints Jacobian is highly involved because it defines the Jacobian as a sparse matrix with a predefined sparsity pattern. The solver can leverage this sparsity pattern to be highly efficient. class UnicycleTO: def __init__(self, P: Params): self.P = P # --- Build bounds on variables --- lb = -np.inf * np.ones(P.Z_DIM) ub = np.inf * np.ones(P.Z_DIM) for k in range(P.N): iu = idx_u(k, P) lb[iu.start + 0] = P.v_min ub[iu.start + 0] = P.v_max lb[iu.start + 1] = P.w_min ub[iu.start + 1] = P.w_max self.lb = lb self.ub = ub # --- Build bounds on constraints --- cl = np.zeros(P.M) cu = np.zeros(P.M) # equalities: exactly 0 cl[:P.n_ceq] = 0.0 cu[:P.n_ceq] = 0.0 # inequalities: c &lt;= 0 cl[P.n_ceq:] = -np.inf cu[P.n_ceq:] = 0.0 self.cl = cl self.cu = cu # --- Precompute Jacobian sparsity (row, col) --- self.jac_rows, self.jac_cols = self._build_jacobian_structure() # Objective def objective(self, z): P = self.P X, U = unpack(z, P) # Terminal goal tracking pos_err = X[-1, 0:2] - P.B[0:2] th_err = X[-1, 2] - P.B[2] J_goal = P.w_goal_pos * np.dot(pos_err, pos_err) + P.w_goal_theta * (th_err**2) # Control effort J_u = P.w_u * np.sum(U * U) # Control smoothness dU = U[1:, :] - U[:-1, :] J_du = P.w_du * np.sum(dU * dU) return 0.5 * (J_goal + J_u + J_du) # Gradient of objective def gradient(self, z): P = self.P X, U = unpack(z, P) grad = np.zeros(P.Z_DIM) # Terminal contributions (no 0.5 after derivative: cancels 2) pos_err = X[-1, 0:2] - P.B[0:2] th_err = X[-1, 2] - P.B[2] gN = np.array([P.w_goal_pos * pos_err[0], P.w_goal_pos * pos_err[1], P.w_goal_theta * th_err]) grad[idx_x(P.N, P)] += gN # Control effort: 0.5 * 2 * w_u * u = w_u * u for k in range(P.N): iu = idx_u(k, P) grad[iu] += P.w_u * U[k, :] # Control smoothness: 0.5 * w_du * sum ||u_{k+1}-u_k||^2 # d/d u_k: -w_du*(u_{k+1}-u_k) from (k,k+1) # d/d u_{k+1}: +w_du*(u_{k+1}-u_k) for k in range(P.N - 1): du = U[k + 1, :] - U[k, :] grad[idx_u(k, P)] += -P.w_du * du grad[idx_u(k + 1, P)] += P.w_du * du return grad # Constraints g(z) def constraints(self, z): P = self.P X, U = unpack(z, P) g = np.zeros(P.M) r = 0 # Initial equality: X0 - A = 0 g[r:r+3] = X[0, :] - P.A r += 3 # Dynamics equalities for k in range(P.N): xk = X[k, :] uk = U[k, :] xnext = f_disc(xk, uk, P.h) g[r:r+3] = X[k + 1, :] - xnext r += 3 # Obstacle inequalities: (r+margin)^2 - ((px-cx)^2 + (py-cy)^2) &lt;= 0 for k in range(P.N + 1): px, py = X[k, 0], X[k, 1] for j in range(P.nObs): cx, cy, r0 = P.obstacles[j] r_eff = r0 + P.safety_margin g[r] = (r_eff ** 2) - ((px - cx) ** 2 + (py - cy) ** 2) r += 1 return g # Jacobian sparsity def jacobianstructure(self): return (np.array(self.jac_rows, dtype=int), np.array(self.jac_cols, dtype=int)) # Jacobian values (in the same order as jacobianstructure) def jacobian(self, z): P = self.P X, U = unpack(z, P) vals = [] r = 0 # Initial eq: d/dX0 is identity (one per row) # rows r..r+2 with columns X0(px,py,th) for i in range(3): vals.append(1.0) r += 3 # Dynamics eqs for k in range(P.N): xk = X[k, :] uk = U[k, :] th = xk[2] v = uk[0] # Row r: g1 = x_{k+1,px} - [px_k + h v cos(th_k)] # d wrt X_{k+1,px} vals.append(1.0) # d wrt X_k,px vals.append(-1.0) # d wrt X_k,theta ( + h v sin(th) ) vals.append(P.h * v * np.sin(th)) # d wrt U_k,v ( - h cos(th) ) vals.append(-P.h * np.cos(th)) # Row r+1: g2 = x_{k+1,py} - [py_k + h v sin(th_k)] vals.append(1.0) # d wrt X_{k+1,py} vals.append(-1.0) # d wrt X_k,py vals.append(-P.h * v * np.cos(th))# d wrt X_k,theta vals.append(-P.h * np.sin(th)) # d wrt U_k,v # Row r+2: g3 = x_{k+1,th} - [th_k + h w_k] vals.append(1.0) # d wrt X_{k+1,theta} vals.append(-1.0) # d wrt X_k,theta vals.append(-P.h) # d wrt U_k,omega r += 3 # Obstacle inequalities for k in range(P.N + 1): px, py = X[k, 0], X[k, 1] for j in range(P.nObs): cx, cy, _ = P.obstacles[j] # d/d px_k: -2(px - cx) vals.append(-2.0 * (px - cx)) # d/d py_k: -2(py - cy) vals.append(-2.0 * (py - cy)) r += 1 return np.array(vals, dtype=float) # --- Internal: build Jacobian sparsity pattern once --- def _build_jacobian_structure(self): P = self.P rows = [] cols = [] r = 0 # Initial equality: g0..g2 depend on X0(px,py,th) diagonally for i in range(3): rows.append(r + i) cols.append(idx_x(0, P).start + i) r += 3 # Dynamics equalities for k in range(P.N): ixk = idx_x(k, P) ixkp1 = idx_x(k + 1, P) iuk = idx_u(k, P) # Row r (px) rows.extend([r, r, r, r]) cols.extend([ ixkp1.start + 0, # X_{k+1,px} ixk.start + 0, # X_k,px ixk.start + 2, # X_k,theta iuk.start + 0 # U_k,v ]) # Row r+1 (py) rows.extend([r + 1, r + 1, r + 1, r + 1]) cols.extend([ ixkp1.start + 1, # X_{k+1,py} ixk.start + 1, # X_k,py ixk.start + 2, # X_k,theta iuk.start + 0 # U_k,v ]) # Row r+2 (theta) rows.extend([r + 2, r + 2, r + 2]) cols.extend([ ixkp1.start + 2, # X_{k+1,theta} ixk.start + 2, # X_k,theta iuk.start + 1 # U_k,omega ]) r += 3 # Obstacle inequalities: each depends only on px_k, py_k for k in range(P.N + 1): ixk = idx_x(k, P) for _ in range(P.nObs): rows.extend([r, r]) cols.extend([ixk.start + 0, ixk.start + 1]) r += 1 assert r == P.M, &quot;Jacobian structure row count mismatch&quot; return rows, cols After defining the problem class, we pass it to the interface of IPOPT using the folowing snippet. # Initial guess z0 = initial_guess(P) # Problem + IPOPT problem = UnicycleTO(P) nlp = cyipopt.Problem( n=P.Z_DIM, m=P.M, problem_obj=problem, lb=problem.lb, ub=problem.ub, cl=problem.cl, cu=problem.cu ) # Options (tweak as desired) nlp.add_option(&quot;tol&quot;, 1e-6) nlp.add_option(&quot;dual_inf_tol&quot;, 1e-6) nlp.add_option(&quot;constr_viol_tol&quot;, 1e-6) nlp.add_option(&quot;compl_inf_tol&quot;, 1e-6) nlp.add_option(&quot;max_iter&quot;, 2000) nlp.add_option(&quot;hessian_approximation&quot;, &quot;limited-memory&quot;) nlp.add_option(&quot;print_level&quot;, 5) z_star, info = nlp.solve(z0) In the case of three obstacles (\\(\\delta=0\\)), we obtain the trajectory shown in Fig. 4.9 with an all-zero initialization; the trajectory shown in Fig. 4.10 with a straight-line initialization; and the trajectory shown in Fig. @ref(fig: unicycle-to-ipopt-random) with a random initialization. You can play with the full code here. Figure 4.9: Trajectory optimization for unicyle using IPOPT (all-zero initialization). Figure 4.10: Trajectory optimization for unicyle using IPOPT (straight-line initialization). Figure 4.11: Trajectory optimization for unicyle using IPOPT (random initialization). 4.4 Model Predictive Control Trajectory optimization (TO) computes (locally or globally) optimal trajectories that are dynamically feasible with respect to a chosen discrete-time transcription and satisfy system constraints at the discretization grid. As a consequence, the resulting plan is open loop: it neither accounts for disturbances nor provides feedback. A common remedy is LQR trajectory tracking (Section 4.2), which linearizes the dynamics along the nominal trajectory to synthesize a local feedback controller. While effective near the nominal path, this approach can struggle in the presence of obstacles or whenever the reference trajectory itself must change over time. Model predictive control (MPC) addresses these limitations by turning TO into a receding-horizon feedback policy. At each control step, MPC resolves a finite-horizon optimal control problem with the current measured state as the initial condition, applies only the first control action, and then repeats the procedure at the next step. This closes the loop, providing robustness to model mismatch and disturbances while naturally adapting the trajectory as the environment or task evolves. In this section we introduce the core ideas behind MPC; for a comprehensive treatment, see (Rawlings, Mayne, and Diehl 2020). Dynamics. Let us consider a discrete-time dynamical system \\[\\begin{equation} s_{t+1} = f(s_t, a_t, w_t), \\tag{4.87} \\end{equation}\\] where \\(s_t \\in \\mathbb{R}^n\\) represents the state, \\(a_t \\in \\mathbb{R}^m\\) represents the control/action, and \\(w_t \\in \\mathbb{R}^d\\) represents a stochastic disturbance. With slight abuse of notation, let us use \\[ s_{t+1} = f (s_t, a_t) \\equiv f (s_t, a_t, 0) \\] to denote the system dynamics without the disturbance. Here we have used \\((s_t, a_t)\\) to represent the state and control. This departs from the notation (4.1) used in the beginning. The reason for doing so will become clear soon. Trajectory Optimization. At time step \\(t\\), let the system’s state be \\(s_t\\) as given (e.g., measured from sensor data). MPC solves the following open-loop TO problem, adapted from (4.29): \\[\\begin{equation} \\begin{split} \\min_{\\{x_k,u_k\\}} \\quad &amp; \\Phi(x_N) + \\sum_{k=0}^{N-1} \\ell_k(x_k,u_k) \\\\[2mm] \\text{s.t.}\\quad &amp; x_{k+1} = f (x_k,u_k), \\qquad t=0,\\dots,N-1,\\\\ &amp; \\boxed{x_0 = s_t} \\ \\ \\text{(given)},\\\\ &amp; x_k \\in \\mathcal X_k,\\quad u_k \\in \\mathcal U_k \\quad \\text{(bounds)},\\\\ &amp; g_k(x_k,u_k) \\le 0,\\quad h_k(x_k,u_k)=0 \\quad \\text{(path/terminal constraints).} \\end{split} \\tag{4.88} \\end{equation}\\] Notice that in (4.88): We used \\(k\\) to denote the time step in the TO problem, in contrast to \\(t\\) in the system dynamics. We used \\((x_k,u_k)\\) to denote the state and control in the TO problem, as opposed to \\((s_t, a_t)\\) in the system dynamics. We enforce the TO problem starts at \\(x_0 = s_t\\), i.e., the initial state in trajectory planning aligns with the current system state at time \\(t\\). Receding Horizon Control. Let \\[\\begin{equation} (x_0^\\star, u_0^\\star, x_1^\\star, u_1^\\star, \\dots, x_{N-1}^\\star, u_{N-1}^\\star, x_N^\\star) \\tag{4.89} \\end{equation}\\] be an optimal solution of the TO problem (4.88) (e.g., obtained from IPOPT). Instead of executing the entire sequence of optimal controls \\((u_0^\\star, u_1^\\star, \\dots, u_{N-1}^\\star)\\), MPC will only execute the first element. Formally, the actual control action applied to the system, denoted as \\(a_t\\), is \\[\\begin{equation} a_t = u_0^\\star. \\tag{4.90} \\end{equation}\\] Applying \\(a_t = u_0^\\star\\) to the real dynamics (4.87), the system will step into a new state \\(s_{t+1}\\) at time \\(t+1\\): \\[ s_{t+1} = f(s_t, u_0^\\star, w_t). \\] Then, MPC solves a new TO problem that is exactly the same as the problem (4.88) at time \\(t\\), except that the initial state is changed to \\(s_{t+1}\\): \\[\\begin{equation} \\begin{split} \\min_{\\{x_k,u_k\\}} \\quad &amp; \\Phi(x_N) + \\sum_{k=0}^{N-1} \\ell_k(x_k,u_k) \\\\[2mm] \\text{s.t.}\\quad &amp; x_{k+1} = f (x_k,u_k), \\qquad t=0,\\dots,N-1,\\\\ &amp; \\boxed{x_0 = s_{t+1}} \\ \\ \\text{(given)},\\\\ &amp; x_k \\in \\mathcal X_k,\\quad u_k \\in \\mathcal U_k \\quad \\text{(bounds)},\\\\ &amp; g_k(x_k,u_k) \\le 0,\\quad h_k(x_k,u_k)=0 \\quad \\text{(path/terminal constraints).} \\end{split} \\tag{4.91} \\end{equation}\\] Problems (4.88) and (4.91) are called parametric optimization problems, in the sense that the form of the optimization problem remains the same, but the value of the parameter \\(s_t\\) that defines the equality constraint has changed. For this reason, we should restate the solution in (4.89) as \\[\\begin{equation} (x_0^\\star(s_t), u_0^\\star(s_t), x_1^\\star(s_t), u_1^\\star(s_t), \\dots, x_{N-1}^\\star(s_t), u_{N-1}^\\star(s_t), x_N^\\star(s_t)) \\tag{4.92} \\end{equation}\\] because they are all implicit functions of the parameter \\(s_t\\). Similarly, the solution to the TO problem (4.91) at time \\(t+1\\) should be denoted as \\[\\begin{equation} (x_0^\\star(s_{t+1}), u_0^\\star(s_{t+1}), x_1^\\star(s_{t+1}), u_1^\\star(s_{t+1}), \\dots, x_{N-1}^\\star(s_{t+1}), u_{N-1}^\\star(s_{t+1}), x_N^\\star(s_{t+1})) \\tag{4.93} \\end{equation}\\] to indicate that they are functions of \\(s_{t+1}\\). After solving the TO problem at time \\(t+1\\), MPC executes the first element: \\[ a_{t+1} = u_0^\\star (s_{t+1}), \\] and the system steps into a new state \\(s_{t+2}\\), from which MPC solves a new TO problem with parameter \\(s_{t+2}\\), and the process continues. Implicit Feedback. Through receding horizon control, MPC creates an implicit feedback control policy. Let the system’s state at time \\(t\\) be \\(s_t\\), the feedback policy is: \\[\\begin{equation} a_t = u^\\star_0 (s_t) := \\mu (s_t) = \\arg\\min \\text{ (the TO problem with parameter } s_t). \\tag{4.94} \\end{equation}\\] Unlike the policy in the case of RL, which is an explicit neural network, the policy of MPC is implicit and comes from the optimal solution of the parametric TO problem. Closed-Loop System. The closed-loop system under the MPC policy is therefore \\[\\begin{equation} s_{t+1} = f(s_t, \\mu(s_t), w_t) := f_{\\text{CL}}(s_t, w_t), \\tag{4.95} \\end{equation}\\] which becomes an uncontrolled system with disturbance \\(w_t\\). The diagram of the closed-loop system is shown in Fig. 4.12. Lots of research have studied properties of the closed-loop system, such as stability and robustness to disturbance. One of the most important results states that, as long as the terminal loss function \\(\\Phi(\\cdot)\\) satisfies a technical condition (being a control Lyapunov function), then the closed-loop system is stable. We do not go deep into these results for MPC analysis and refer the reader to (Rawlings, Mayne, and Diehl 2020). Figure 4.12: Model Predictive Control. Warmstart for Parametric Optimization. The requirement that an MPC feedback policy solve an optimization problem online makes it computationally expensive. In some domains, e.g., chemical process control, control updates are infrequent (for example, every 10 minutes) and MPC is practical. In other domains such as robotics, control rates are much higher (e.g., 10,Hz), so reducing MPC’s computational cost is critical. A common strategy is warm-starting: when solving the optimization for the new parameter \\(s_{t+1}\\) at time \\(t+1\\), initialize the numerical solver with the previous solution (4.92) computed for \\(s_t\\) at time \\(t\\). If the change from \\(s_t\\) to \\(s_{t+1}\\) is small, the optimal solution at \\(t+1\\) is likely close to that at \\(t\\), and warm-starting typically reduces the solver’s required iterations. We now apply MPC to the stabilization of a double integrator with control constraints. Example 4.6 (MPC for Double Integrator) This example implements Model Predictive Control (MPC) for a 1D double-integrator (position–velocity) system using a quadratic program (QP). Dynamics of the System. We model the kinematics of a point mass with (discrete-time) constant-acceleration physics. Let the state be \\[ x_k = \\begin{bmatrix} p_k \\\\ v_k \\end{bmatrix} \\in \\mathbb{R}^2, \\quad u_k \\in \\mathbb{R} \\] where \\(p_k\\) is position, \\(v_k\\) is velocity, and \\(u_k\\) is the commanded acceleration. With sampling time \\(\\Delta t\\), the discrete-time, nominal dynamics are \\[ x_{k+1} = A x_k + B u_k, \\quad A = \\begin{bmatrix} 1 &amp; \\Delta t \\\\ 0 &amp; 1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} \\tfrac{1}{2}\\Delta t^2 \\\\ \\Delta t \\end{bmatrix}. \\] Control Goal. We consider regulation to a fixed target \\(x_{\\text{goal}}\\in\\mathbb{R}^2\\), typically the origin: \\[ x_{\\text{goal}}=\\begin{bmatrix}0\\\\0\\end{bmatrix}. \\] (Tracking a moving reference is immediate by updating \\(x_{\\text{goal}}\\) online.) Finite-Horizon Trajectory Optimization (QP). At each control time \\(t\\), with measured state \\(s_t\\), MPC solves a length-\\(N\\) open-loop trajectory optimization (TO) problem with decision variables \\(\\{x_k,u_k\\}_{k=0}^{N}\\) (with \\(u_N\\) unused): \\[ \\begin{aligned} \\hspace{-16mm} \\min_{\\{x_k,u_k\\}} \\quad &amp; \\underbrace{(x_N - x_{\\text{goal}})^\\top Q_f (x_N - x_{\\text{goal}})}_{\\text{terminal cost}} \\;+\\; \\sum_{k=0}^{N-1}\\Big[ \\underbrace{(x_k - x_{\\text{goal}})^\\top Q (x_k - x_{\\text{goal}})}_{\\text{state cost}} + \\underbrace{u_k^\\top R u_k}_{\\text{effort}} \\Big] \\\\[1.0ex] \\hspace{-16mm} \\text{s.t.}\\quad &amp; x_{k+1} = A x_k + B u_k,\\quad k=0,\\dots,N-1, \\\\[0.3ex] &amp; \\boxed{x_0 = s_t}\\quad\\text{(enforces the current initial condition)}, \\\\[0.3ex] &amp; x_{\\min} \\le x_k \\le x_{\\max},\\quad k=0,\\dots,N, \\\\[0.3ex] &amp; u_{\\min} \\le u_k \\le u_{\\max},\\quad k=0,\\dots,N-1. \\end{aligned} \\tag{4.96} \\] The problem in (4.96) is a convex QP (quadratic cost; linear dynamics and box constraints). We will write a Python code that builds this QP once with CVXPY and then only updates the Parameter for the measured state \\(s_t\\) and (optionally) \\(x_{\\text{goal}}\\) at each MPC step. Receding Horizon (Implicit Feedback). Let \\(\\{x_k^\\star,u_k^\\star\\}\\) be the optimizer of (4.96). MPC applies only the first input \\[ a_t = u_0^\\star, \\] then measures the new state, shifts/warm-starts the QP, and resolves. This closes the loop and yields an implicit feedback policy \\(a_t=\\mu(s_t)=u_0^\\star(s_t)\\). True Dynamics with Disturbance. To test robustness, the closed-loop plant advances with additive disturbance: \\[ s_{t+1} = A\\,s_t + B\\,a_t + w_t, \\] where \\(w_t\\) is zero-mean noise. In the code, \\(w_t\\) is a small Gaussian perturbation added to both position and velocity updates. This models sensor/actuator errors or unmodeled effects and illustrates how MPC corrects by re-solving at every step. Experiment Setup. Initialization. Start from \\(x_0=[4,0]^\\top\\) (far from the goal). Fix \\(x_{\\text{goal}}=[0,0]^\\top\\). Horizon and sampling. \\(N=20\\), \\(\\Delta t=0.1\\) s; run for \\(T=120\\) MPC steps (12 s). QP solver and structure. We solve with OSQP via CVXPY. The problem is constructed once; at each step we update the parameter \\(x_0=s_t\\). This preserves the QP structure (matrices \\(P\\) and \\(A\\)), enabling factorization reuse and faster solves. Warm-start. Shift the previous optimal sequence \\(u^\\star\\) left by one step and hold the last value, then roll out the dynamics from the new \\(s_t\\) to seed a feasible \\(x\\)-trajectory guess. These are passed to the solver via warm_start=True. Fig. 4.13 shows the closed-loop position and velocity under the MPC controller in the case of small disturbancen (the standard deviations of the two elements of \\(w_t\\) are \\(0.002\\) and \\(0.005\\)). Fig. 4.14 shows the closed-loop position and velocity under the MPC controller in the case of large disturbancen (the standard deviations of the two elements of \\(w_t\\) are \\(0.02\\) and \\(0.05\\)). In both cases, the MPC policy successfully regulates the system around the origin, illustrating robustness of the MPC policy to disturbances. You can play with the code here. Figure 4.13: MPC for double integrator (small disturbance). Figure 4.14: MPC for double integrator (large disturbance). 4.5 Model Predictive Path Integral Control We have seen that numerical techniques–particularly gradient-based nonlinear programming methods–can be employed to solve trajectory optimization (TO) problems. Furthermore, solving TO problems repeatedly in an online fashion yields an implicit feedback policy, as in model predictive control (MPC). The numerical methods discussed so far for TO rely on access to at least first-order derivatives of the objective and constraint functions. In this section, we introduce a different class of methods, known as model predictive path integral (MPPI) control (Williams et al. 2016). These methods do not require derivative information; instead, they rely solely on evaluating the objective and constraints through forward simulations of the system dynamics. Problem Formulation. We consider a general nonlinear discrete-time dynamical system \\[ x_{t+1} = f_t(x_t, u_t), \\qquad t = 0,\\dots, N-1, \\] with state \\(x_t \\in \\mathbb{R}^n\\) and control \\(u_t \\in \\mathbb{R}^m\\). Given an initial condition \\(x_0 = \\hat x_0\\) (known), the finite-horizon trajectory optimization problem is \\[ \\begin{aligned} \\min_{\\{x_t,u_t\\}_{t=0}^{N-1}} \\quad &amp; \\Phi(x_N) \\;+\\; \\sum_{t=0}^{N-1} \\ell_t(x_t, u_t) \\\\ \\text{s.t.} \\quad &amp; x_{t+1} = f_t(x_t, u_t), \\quad t=0,\\dots,N-1, \\\\ &amp; x_0 = \\hat x_0 \\;\\; (\\text{given}). \\end{aligned} \\] Here, \\(\\ell_t\\) is the stage (running) cost and \\(\\Phi\\) is the terminal cost. A dynamically feasible trajectory \\(\\{x_t,u_t\\}_{t=0}^{N-1}\\) satisfies the dynamics constraints for all \\(t\\). High-Level Idea of MPPI. MPPI is a sampling-based, derivative-free, receding-horizon controller derived from path-integral / information-theoretic control. Instead of solving the constrained optimization problem via gradients or linearizations, MPPI: Maintains a nominal open-loop control sequence \\(U = \\{u_0,\\dots,u_{N-1}\\}\\). At the current state \\(x_0\\), it samples many noisy control sequences \\(u_t + \\epsilon_t^{(k)}\\) for rollouts \\(k=1,\\dots,K\\). For each rollout, it propagates the dynamics forward and scores the trajectory with the same task cost \\(\\Phi + \\sum \\ell_t\\). It soft-min aggregates the sampled control perturbations with weights \\[ w_k \\; \\propto \\; \\exp\\!\\Big(-\\tfrac{1}{\\lambda}\\big(S^{(k)} - \\min_j S^{(j)}\\big)\\Big), \\] where \\(S^{(k)}\\) is the total cost of the \\(k\\)-th trajectory and \\(\\lambda&gt;0\\) is a temperature parameter. It updates the nominal sequence by adding the weighted average of the sampled perturbations: \\[ u_t \\;\\leftarrow\\; u_t \\;+\\; \\sum_{k=1}^K \\bar w_k\\, \\epsilon_t^{(k)}, \\qquad \\bar w_k = \\frac{w_k}{\\sum_j w_j}. \\] Optionally, if computational budget permits, one can repeat steps 1 to 5 for multiple iterations. If the control actions need to satisfy hard bounds, we can project \\(u_t\\) to the feasible set. It applies only the first control \\(u_0\\) to the real system, shifts the sequence forward, and repeats at the next time step (receding horizon). Key properties. Works with general nonlinear dynamics and arbitrary costs; no gradients, no linearization required. Robustness arises from the soft-min over many trajectories. Parallelizable (GPU-friendly) because rollouts are independent. Computational details of the MPPI algorithm are given as follows. Rollouts and total cost. Given current state \\(x_0\\) and nominal controls \\(U=\\{u_t\\}_{t=0}^{N-1}\\), draw \\(K\\) noise sequences \\[ \\epsilon^{(k)} = \\{\\epsilon_0^{(k)},\\ldots,\\epsilon_{N-1}^{(k)}\\}, \\qquad \\epsilon_t^{(k)} \\sim \\mathcal{N}(0, \\Sigma_t) \\] (possibly time-correlated for smoothness). For each \\(k \\in [1,K]\\): Simulate \\[ \\tilde u_t^{(k)} = u_t + \\epsilon_t^{(k)}, \\qquad x_{t+1}^{(k)} = f_t \\big(x_t^{(k)}, \\tilde u_t^{(k)}\\big), \\quad x_0^{(k)} = x_0. \\] Accumulate the total trajectory cost \\[ S^{(k)} \\;=\\; \\Phi \\big(x_N^{(k)}\\big)\\;+\\; \\sum_{t=0}^{N-1} \\ell_t \\big(x_t^{(k)}, \\tilde u_t^{(k)}\\big). \\] Exponential weights (soft-min). Stabilize by subtracting the minimum cost: \\[ w_k \\;=\\; \\exp\\!\\Big(-\\tfrac{1}{\\lambda}\\big(S^{(k)} - S_{\\min}\\big)\\Big), \\quad S_{\\min} := \\min_j S^{(j)}. \\] Normalize \\(\\bar w_k = w_k / \\sum_j w_j\\). Control update. For each time \\(t=0,\\dots,N-1\\), update \\[ u_t \\;\\leftarrow\\; u_t \\;+\\; \\sum_{k=1}^K \\bar w_k\\, \\epsilon_t^{(k)}. \\] If control is bounded, project the controls onto the feasible sets. Practical knobs. \\(K\\): number of rollouts (compute–performance tradeoff). \\(\\lambda\\): temperature (smaller \\(\\Rightarrow\\) greedier toward the best rollouts). \\(\\Sigma_t\\): exploration covariance; can use AR(1) temporal correlation for smooth controls. Example 4.7 (MPPI for Pendulum Swing Up) We apply the MPPI controller, in an MPC receding-horizon control fashion to the pendulum swing-up problem. We use \\(K=16\\) rollouts and set \\(\\lambda=1\\). When \\(\\Sigma_t\\) is large enough, for example, when we set the standard deviation of the noise to be \\(2\\), the MPPI controller can successfully swing up the pendulum, demonstrating a highly sophisticated motion plan as shown in Fig. 4.15. Figure 4.15: Pendulum swing-up using MPPI (large exploration). However, when \\(\\Sigma_t\\) is too small, when we set the standard deviation of the noise to be \\(0.1\\), the MPPI controller fails at the task, as shown in Fig. 4.16. You can play with the code here. Figure 4.16: Pendulum swing-up using MPPI (small exploration). 4.5.1 Information-Theoretic Derivation of MPPI We now derive Model Predictive Path Integral (MPPI) control from an information-theoretic (KL-control / control-as-inference) viewpoint. We proceed from a variational free-energy objective to the Gibbs (Boltzmann) optimal trajectory law, then obtain the practical MPPI update via moment projection and importance sampling. Let a finite-horizon trajectory be \\(\\tau=(x_0,u_0,\\dots,u_{H-1},x_H)\\) generated by discrete-time dynamics \\[ x_{t+1} = f_t(x_t,u_t), \\quad t=0,\\dots,H-1. \\] Let \\(S(\\tau)\\) denote the total trajectory cost (e.g., terminal \\(+\\) running \\(+\\) control effort). Let \\(p(\\tau)\\) be a baseline (prior) trajectory distribution (e.g., nominal/uncontrolled dynamics plus a prior over control sequences). We seek a controlled trajectory distribution \\(q(\\tau)\\) that trades performance against deviation from the prior via: \\[ \\mathcal{F}(q)\\;=\\;\\mathbb{E}_{q}\\big[S(\\tau)\\big]\\;+\\;\\lambda\\,\\mathrm{KL} \\big(q(\\tau)\\,\\|\\,p(\\tau)\\big),\\qquad \\lambda&gt;0. \\tag{4.97} \\] This is convex in \\(q\\) (the first term is linear in \\(q\\) while the second term is convex in \\(q\\)). The scalar \\(\\lambda\\) plays the role of a temperature (risk-sensitivity / exploration level). Gibbs–Boltzmann Optimum. Our goal is now to minimize \\(\\mathcal{F}(q)\\) over all normalized densities \\(q(\\tau)\\). This is a constrained convex optimization in \\(q\\). Therefore, we can formulate the Lagrangian and derive the KKT optimality conditions. Exercise 4.2 Formulate the Lagrangian for problem (4.97) and find its KKT optimality conditions. Solving the KKT optimality conditions, one can find the optimal solution \\[ q^\\star(\\tau)\\ \\propto\\ p(\\tau)\\,\\exp\\!\\big(-S(\\tau)/\\lambda\\big). \\] Thus, with proper normalization, we have \\[ q^\\star (\\tau)\\;=\\;\\frac{1}{Z}\\;p(\\tau)\\,\\exp\\!\\big(-S(\\tau)/\\lambda\\big), \\qquad Z \\;=\\; \\int p(\\tau)\\,e^{-S(\\tau)/\\lambda}\\,d\\tau \\;=\\;\\mathbb{E}_{p}\\!\\big[e^{-S/\\lambda}\\big]. \\tag{4.98} \\] Plugging \\(q^\\star\\) back into \\(\\mathcal{F}\\) gives the free-energy identity: \\[ \\min_{q}\\Big\\{\\mathbb{E}_{q}[S]+\\lambda\\,\\mathrm{KL}(q\\|p)\\Big\\} \\;=\\; -\\lambda \\log \\mathbb{E}_{p} \\left[e^{-S/\\lambda}\\right]. \\tag{4.99} \\] From Gibbs–Boltzmann Optimum to MPPI Updates. Let a trajectory be \\(\\tau=(x_0,u_0,\\dots,u_{H-1},x_H)\\) with dynamics \\(x_{t+1}=f_t(x_t,u_t)\\). We introduce a baseline (prior) trajectory distribution \\(p(\\tau)\\) as the law induced by: \\[ \\mathbf v=(v_0,\\dots,v_{H-1}) \\sim \\mathcal N(\\mathbf U,\\Sigma), \\qquad v_t = u_t + \\epsilon_t,\\quad \\epsilon_t\\sim\\mathcal N(0,\\Sigma_t), \\] and rolling out the dynamics with controls \\(u_t=v_t\\). Here \\(\\mathbf U = (u_0,\\dots,u_{H-1})\\) is the current nominal control sequence. Given a trajectory cost \\(S(\\tau)\\) and temperature \\(\\lambda&gt;0\\), the optimal controlled trajectory law is \\[ q^\\star(\\tau)\\;=\\;\\frac{1}{Z}\\,p(\\tau)\\,\\exp\\!\\big(-S(\\tau)/\\lambda\\big), \\qquad Z=\\mathbb{E}_{p}\\!\\left[e^{-S/\\lambda}\\right]. \\] Because of nonlinear dynamics and the exponential tilt by \\(S\\), \\(q^\\star\\) is generally not Gaussian, even if \\(p\\) is. MPPI approximates \\(q^\\star\\) by maintaining a Gaussian over the control sequence with the same covariance \\(\\Sigma\\) and a tunable mean \\(\\mathbf U=(u_0,\\dots,u_{H-1})\\): \\[ \\pi_{\\mathbf U}(\\mathbf v)\\;=\\;\\mathcal N(\\mathbf v;\\mathbf U,\\Sigma). \\] MPPI projects \\(q^\\star\\) onto this family by minimizing \\(\\mathrm{KL}\\big(\\pi_{\\mathbf U}\\,\\|\\,q^\\star\\big)\\) while holding \\(\\Sigma\\) fixed. For Gaussians with fixed covariance, this yields moment matching of the mean: \\[ \\mathbf U_{\\text{new}} \\;=\\; \\mathbb{E}_{q^\\star}[\\mathbf v]. \\] To perform this moment matching, we write \\(v_t=u_t+\\epsilon_t\\). Then \\[ u_{t,\\text{new}} \\;=\\; \\mathbb{E}_{q^\\star}[v_t] = u_t + \\mathbb{E}_{q^\\star}[\\epsilon_t]. \\] We cannot sample \\(q^\\star\\) directly, but by the Gibbs–Boltzmann form and importance sampling: \\[ \\mathbb{E}_{q^\\star}[g(\\tau)] = \\frac{\\mathbb{E}_{p}[\\,g(\\tau)\\,e^{-S(\\tau)/\\lambda}\\,]}{\\mathbb{E}_{p}[\\,e^{-S(\\tau)/\\lambda}\\,]}. \\] Choosing \\(g(\\tau)=\\epsilon_t\\) and using rollouts \\(\\{\\tau^{(k)}\\}_{k=1}^K\\sim p\\) with costs \\(S^{(k)}\\) and noises \\(\\epsilon_t^{(k)}\\) gives the MPPI update: \\[ \\boxed{ u_t \\;\\leftarrow\\; u_t \\;+\\; \\sum_{k=1}^K \\bar w_k\\,\\epsilon_t^{(k)}, \\qquad \\bar w_k = \\frac{\\exp\\!\\big(-\\tfrac{1}{\\lambda}(S^{(k)}-S_{\\min})\\big)} {\\sum_j \\exp\\!\\big(-\\tfrac{1}{\\lambda}(S^{(j)}-S_{\\min})\\big)}. } \\] Here \\(S_{\\min}=\\min_k S^{(k)}\\) is a numerical stabilizer that cancels in the ratio. Takeaway. \\(p(\\tau)\\): baseline trajectory law induced by Gaussian exploratory controls and the dynamics. \\(q^\\star(\\tau)\\): Gibbs–Boltzmann tilt of \\(p\\), not Gaussian in general. MPPI approximates \\(q^\\star\\) by a Gaussian over controls with fixed covariance \\(\\Sigma\\) and updates the mean by importance-weighted averaging of sampled noise, yielding the standard MPPI updates used in practice. 4.6 Rapidly Exploring Random Tree In Section 4.5, we examined the effectiveness of one particular sampling-based planning method—MPPI—for solving trajectory-planning problems, noting that it is often less susceptible to local minima than nonlinear-programming-based approaches. More broadly, sampling-based motion planning encompasses a wide family of algorithms, extensive enough to merit an entire semester for a comprehensive treatment. For readers interested in exploring this area further, the textbooks (LaValle 2006) and (Choset et al. 2005) provide excellent overviews. In this section, we introduce one of the most widely used sampling-based algorithms: the rapidly exploring random tree (RRT). We will begin with the case of kinematic motion planning, in which the system or robot is modeled as a point mass without dynamics, and then extend the discussion to kinodynamic motion planning, where the system’s dynamics must be taken into account. 4.6.1 Kinematic Motion Planning Problem Setup. We consider a robot with configuration space \\(\\mathcal{C} \\subset \\mathbb{R}^d\\). Obstacles occupy \\(\\mathcal{C}_\\mathrm{obs}\\), and the free space is \\[ \\mathcal{C}_\\mathrm{free} = \\mathcal{C} \\setminus \\mathcal{C}_\\mathrm{obs}. \\] Given a start configuration \\(q_\\mathrm{start} \\in \\mathcal{C}_\\mathrm{free}\\) and a goal region \\(\\mathcal{C}_\\mathrm{goal} \\subset \\mathcal{C}_\\mathrm{free}\\), the objective is to find a collision-free path \\[ \\sigma : [0,1] \\to \\mathcal{C}_\\mathrm{free}, \\qquad \\sigma(0) = q_\\mathrm{start}, \\ \\sigma(1) \\in \\mathcal{C}_\\mathrm{goal}. \\] Here, \\(\\sigma(\\cdot)\\) is a continuous curve in the configuration space \\(\\mathcal{C}\\) that connects the start position to the goal region. A path can be represented in several ways (e.g., piecewise-polynomial splines); in this section we adopt a piecewise-linear representation: the path is a sequence of waypoints \\((q_0, q_1, \\dots, q_M)\\) joined by straight-line segments in \\(\\mathcal{C}\\), i.e., \\[ \\sigma(s)= (1-s)\\,q_i + s\\,q_{i+1}, \\quad s\\in[0,1], \\ \\ i=0,\\dots,M-1. \\] The RRT algorithm requires the following components. Sampler. We require a mechanism to draw random configurations in free space. The simplest choice is uniform sampling: \\[ q_{\\mathrm{rand}} \\sim \\mathcal{U}\\!\\left(\\mathcal{C}_{\\mathrm{free}}\\right), \\] optionally with goal bias: introduce \\(B \\sim \\mathrm{Bernoulli}(p_{\\mathrm{goal}})\\) and set \\[ q_{\\mathrm{rand}} = \\begin{cases} \\text{a sample from } \\mathcal{C}_{\\mathrm{goal}}, &amp; \\text{if } B=1,\\\\ \\text{a sample from } \\mathcal{C}_{\\mathrm{free}}, &amp; \\text{if } B=0. \\end{cases} \\] To perform such sampling, we require that, given any configuration \\(q \\in \\mathcal{C}\\), we can check its membership in \\(\\mathcal{C}_{\\mathrm{free}}\\), \\(\\mathcal{C}_{\\mathrm{obs}}\\), and \\(\\mathcal{C}_{\\mathrm{goal}}\\). Metric (Distance Function). We need a distance \\(d:\\mathcal{C}\\times\\mathcal{C}\\to \\mathbb{R}_{\\ge 0}\\) to compare configurations. Common choices: \\[ d(q_i,q_j) = \\|W(q_i-q_j)\\|_2, \\] where \\(W\\) is a positive-definite weighting (e.g., to account for different joint ranges). For \\(\\text{SE}(2)\\) or \\(\\text{SE}(3)\\) manifolds, handle angular wrap-around (e.g., modulo \\(2\\pi\\)), optionally combine translational and rotational parts with different weights, use a geodesic or a local chart; for rotations, a distance induced by the Lie group metric. Sometimes \\(d\\) is defined in task space via forward kinematics \\(x=\\Phi(q)\\) (here \\(q\\) represents the joint angles of a robot arm and \\(\\Phi(q)\\) returns the position of the robot’s end-effector): \\[ d(q_i,q_j)=\\|\\Phi(q_i)-\\Phi(q_j)\\|, \\] useful when end-effector placement matters more than joint motion. Nearest-Neighbor Query. Given the current tree \\(T\\) with vertex set \\(V(T)\\), select the closest existing node to the new sample: \\[ q_{\\mathrm{near}} \\in \\arg\\min_{q\\in V(T)} d\\big(q,\\,q_{\\mathrm{rand}}\\big). \\] Implementation notes. Use spatial indices (kd-trees, ball trees, cover trees) for sublinear searches. Approximate nearest neighbor methods often suffice and substantially accelerate growth in higher dimensions. Steering / Step Map. Move from \\(q_{\\mathrm{near}}\\) toward \\(q_{\\mathrm{rand}}\\) by at most a step size \\(\\eta&gt;0\\). In Euclidean \\(\\mathcal{C}\\), \\[ \\Delta = q_{\\mathrm{rand}} - q_{\\mathrm{near}}, \\qquad q_{\\mathrm{new}} = q_{\\mathrm{near}} + \\alpha\\,\\Delta,\\quad \\alpha = \\min\\!\\left\\{1,\\ \\frac{\\eta}{\\|\\Delta\\|_2}\\right\\}. \\] On manifolds (e.g., \\(SE(3)\\)), interpolate along a feasible local geodesic; for rotations, use group operations (e.g., \\(q_{\\mathrm{new}} = q_{\\mathrm{near}}\\exp(\\alpha\\,\\log(q_{\\mathrm{near}}^{-1}q_{\\mathrm{rand}}))\\)). Tuning \\(\\eta\\). Smaller \\(\\eta\\) improves collision fidelity and resolution in narrow passages but slows exploration. Larger \\(\\eta\\) accelerates coverage but can skip through feasible corridors. Collision Checking (Local Planner Validity). Validate the entire segment between \\(q_{\\mathrm{near}}\\) and \\(q_{\\mathrm{new}}\\): \\[ \\text{ensure } \\sigma(s)\\in \\mathcal{C}_{\\mathrm{free}} \\ \\ \\forall s\\in[0,1], \\quad \\sigma(s)=(1-s)q_{\\mathrm{near}}+s q_{\\mathrm{new}}. \\] Resolution and robustness. Use a discretization step \\(\\delta\\) small enough relative to obstacle feature size and robot footprint. For articulated robots, perform swept-volume or continuous collision detection where possible; otherwise sample sufficiently along the edge (in joint and time parameter \\(s\\)). Inflate obstacles or deflate the robot model slightly to build clearance margins. Stopping / Success Criteria. Let the goal be a set \\(\\mathcal{C}_{\\mathrm{goal}}\\) (e.g., a ball of radius \\(\\varepsilon\\) around a target pose). Declare success when \\[ q_{\\mathrm{new}} \\in \\mathcal{C}_{\\mathrm{goal}}. \\] Common termination rules. Maximum number of iterations or samples. Time budget exhausted (anytime behavior). Early stop after first solution (RRT) or continue to refine with rewiring (RRT*). Post-processing (optional). Shortcutting: repeatedly replace subpaths by direct segments if collision-free. Smoothing: fit splines to waypoints while maintaining clearance, or re-optimize locally. We are ready to present the RRT algorithm. Algorithm: Rapidly-Exploring Random Tree (RRT) — Kinematic Case Inputs: start configuration \\(q_{\\mathrm{start}} \\in \\mathcal{C}_{\\mathrm{free}}\\); goal region \\(\\mathcal{C}_{\\mathrm{goal}} \\subset \\mathcal{C}_{\\mathrm{free}}\\); step size \\(\\eta&gt;0\\); goal-bias \\(p_{\\mathrm{goal}} \\in [0,1]\\); iteration budget \\(N\\); metric \\(d:\\mathcal{C}\\times\\mathcal{C}\\to\\mathbb{R}_{\\ge 0}\\); collision checker for segments in \\(\\mathcal{C}\\); nearest-neighbor data structure over the current vertex set. Initialize the tree \\(T\\) with vertex set \\(V(T)\\gets\\{q_{\\mathrm{start}}\\}\\) and no edges. For \\(i=0,1,2,\\dots,N-1\\) Goal-biased sampling. Draw \\(B\\sim\\mathrm{Bernoulli}(p_{\\mathrm{goal}})\\) and set \\[ q_{\\mathrm{rand}} \\sim \\begin{cases} \\mathcal{U}(\\mathcal{C}_{\\mathrm{goal}}), &amp; \\text{if } B=1,\\\\[2pt] \\mathcal{U}(\\mathcal{C}_{\\mathrm{free}}), &amp; \\text{if } B=0. \\end{cases} \\] Nearest neighbor. Select \\[ q_{\\mathrm{near}} \\in \\arg\\min_{q\\in V(T)} d\\!\\left(q,\\,q_{\\mathrm{rand}}\\right). \\] Steering (bounded step). Let \\(\\Delta := q_{\\mathrm{rand}}-q_{\\mathrm{near}}\\) in a local chart of \\(\\mathcal{C}\\) (with appropriate handling of angles/manifold coordinates). Define \\[ \\alpha := \\min\\!\\left\\{1,\\ \\frac{\\eta}{\\|\\Delta\\|}\\right\\},\\qquad q_{\\mathrm{new}} := q_{\\mathrm{near}} + \\alpha\\,\\Delta. \\] (On manifolds, replace the affine step with interpolation along a local geodesic.) Collision checking. Verify the straight segment \\[ \\sigma(s) = (1-s)\\,q_{\\mathrm{near}} + s\\,q_{\\mathrm{new}},\\quad s\\in[0,1], \\] satisfies \\(\\sigma(s)\\in\\mathcal{C}_{\\mathrm{free}}\\) for all sampled \\(s\\) (or via continuous collision detection). Tree update. If the segment is collision-free, append the vertex and edge: \\[ V(T)\\gets V(T)\\cup\\{q_{\\mathrm{new}}\\},\\qquad E(T)\\gets E(T)\\cup\\{(q_{\\mathrm{near}}\\rightarrow q_{\\mathrm{new}})\\}. \\] Success test. If \\(q_{\\mathrm{new}}\\in \\mathcal{C}_{\\mathrm{goal}}\\), terminate and return the path formed by the unique tree route from \\(q_{\\mathrm{start}}\\) to \\(q_{\\mathrm{new}}\\). Stopping: If no success after \\(N\\) iterations (or upon time-budget exhaustion), return failure (or the best partial progress if maintained). Properties. Probabilistic completeness: If a solution exists, the probability that RRT finds one approaches 1 as \\(N \\to \\infty\\). Not optimal: The first-found solution can be far from shortest; RRT does not asymptotically improve path quality. 4.6.1.1 Make RRT Optimal: RRT* The classical RRT is probabilistically complete but not optimal. To obtain asymptotic optimality (i.e., the best-path cost converges to the global optimum as the number of samples \\(N\\to\\infty\\)), we use RRT*: a cost-aware variant that (i) selects parents by minimizing cost-to-come and (ii) rewires nearby nodes to improve their costs when possible. Key Ideas Behind RRT*: Cost functional. Let \\(c(\\cdot)\\) denote path cost (e.g., length or any additive, nonnegative, Lipschitz continuous cost). For an edge between two configurations \\(q\\) and \\(q&#39;\\), write \\(c(q\\to q&#39;)\\) and define \\[ c(q&#39;) \\;=\\; \\min_{(q \\to q&#39;) \\in E(T)} \\big\\{c(q) + c(q\\to q&#39;)\\big\\}, \\] where \\(E(T)\\) is the set of tree edges. Near set. Rather than connecting to only the nearest node, examine a ball of neighbors around the new sample \\(q_{\\mathrm{new}}\\) whose radius shrinks with \\(n\\): \\[ r_n \\;=\\; \\gamma \\left(\\frac{\\log n}{n}\\right)^{1/d}, \\] where \\(d\\) is the dimension of \\(\\mathcal{C}\\) and \\(\\gamma&gt;\\gamma^\\ast\\) is a constant depending on the measure of \\(\\mathcal{C}_{\\mathrm{free}}\\). (Alternative: \\(k\\)-nearest with \\(k_n \\ge k_0 \\log n\\).) Choose parent by cost. Among the near nodes that can connect to \\(q_{\\mathrm{new}}\\) collision-free, pick the parent that minimizes total cost-to-come. Rewire. Attempt to improve each neighbor \\(q\\) by re-connecting it through \\(q_{\\mathrm{new}}\\) if this reduces \\(c(q)\\) while preserving feasibility. Asymptotic optimality. Under standard assumptions (e.g., \\(\\delta\\)-robustly feasible problem, absolutely continuous sampling over \\(\\mathcal{C}_{\\mathrm{free}}\\), consistent local planner, and the \\(r_n\\) or \\(k_n\\) schedules above), the best path cost in the tree converges almost surely to the global optimum. The RRT* Algorithm is presented below. Algorithm: RRT* (Asymptotically Optimal RRT) Inputs: \\(q_{\\mathrm{start}}\\in\\mathcal{C}_{\\mathrm{free}}\\), goal region \\(\\mathcal{C}_{\\mathrm{goal}}\\subset\\mathcal{C}_{\\mathrm{free}}\\); metric \\(d\\); step size \\(\\eta&gt;0\\); goal-bias \\(p_{\\mathrm{goal}}\\in[0,1]\\); iteration budget \\(N\\); neighbor schedule \\(r_n=\\gamma(\\tfrac{\\log n}{n})^{1/d}\\) (or \\(k_n\\ge k_0\\log n\\)). Initialize \\(T\\) with \\(V(T)\\gets\\{q_{\\mathrm{start}}\\}\\), \\(E(T)\\gets\\emptyset\\), \\(c(q_{\\mathrm{start}})=0\\). For \\(i=0,1,2,\\dots,N-1\\) Goal-biased sampling. Draw \\(B\\sim\\mathrm{Bernoulli}(p_{\\mathrm{goal}})\\) and set \\[ q_{\\mathrm{rand}} \\sim \\begin{cases} \\mathcal{U}(\\mathcal{C}_{\\mathrm{goal}}), &amp; B=1,\\\\ \\mathcal{U}(\\mathcal{C}_{\\mathrm{free}}), &amp; B=0. \\end{cases} \\] Nearest and steer. Choose \\[ q_{\\mathrm{near}}\\in\\arg\\min_{q\\in V(T)} d\\big(q,\\,q_{\\mathrm{rand}}\\big),\\quad q_{\\mathrm{new}} := q_{\\mathrm{near}} + \\alpha\\big(q_{\\mathrm{rand}}-q_{\\mathrm{near}}\\big), \\] with \\(\\alpha=\\min\\{1,\\eta/\\|q_{\\mathrm{rand}}-q_{\\mathrm{near}}\\|\\}\\). If the segment \\([q_{\\mathrm{near}},q_{\\mathrm{new}}]\\) is not collision-free, continue. Near set. Denote \\(n = |V(T)|\\) as the number of nodes in the current tree, let \\[ \\mathcal{N}_n(q_{\\mathrm{new}}) \\;=\\; \\big\\{q\\in V(T): d(q,q_{\\mathrm{new}})\\le r_n\\big\\}. \\] Choose parent by cost. Set provisional parent \\(\\bar{q}\\gets q_{\\mathrm{near}}\\) with provisional cost \\[ \\bar{c} \\;=\\; c(q_{\\mathrm{near}}) + c\\!\\left(q_{\\mathrm{near}}\\to q_{\\mathrm{new}}\\right). \\] For each \\(q\\in \\mathcal{N}_n(q_{\\mathrm{new}})\\) such that the segment \\([q,q_{\\mathrm{new}}]\\) is collision-free, if \\[ c(q) + c(q\\to q_{\\mathrm{new}}) &lt; \\bar{c}, \\] then update \\(\\bar{q}\\gets q\\), \\(\\bar{c}\\gets c(q)+c(q\\to q_{\\mathrm{new}})\\). Insert. Add \\(q_{\\mathrm{new}}\\) to \\(V(T)\\), add edge \\((\\bar{q}\\rightarrow q_{\\mathrm{new}})\\) to \\(E(T)\\), and set \\(c(q_{\\mathrm{new}})\\gets \\bar{c}\\). Rewire. For each \\(q\\in\\mathcal{N}_n(q_{\\mathrm{new}})\\) such that segment \\([q_{\\mathrm{new}},q]\\) is collision-free, if \\[ c(q_{\\mathrm{new}}) + c(q_{\\mathrm{new}}\\to q) &lt; c(q), \\] then change \\(q\\)’s parent to \\(q_{\\mathrm{new}}\\) and update \\(c(q)\\) (and, if maintained, the costs of its descendants). Goal check. If \\(q_{\\mathrm{new}}\\in\\mathcal{C}_{\\mathrm{goal}}\\), record/update the current best path. Stopping: Terminate upon time/iteration budget; return the best path found (its cost decreases and converges to the optimal value almost surely). Intuition of why RRT* is optimal: The shrinking neighbor radius \\(r_n\\propto(\\log n/n)^{1/d}\\) ensures that, as \\(n\\) grows, the graph locally approximates the geometry of \\(\\mathcal{C}_{\\mathrm{free}}\\) with just enough connectivity to avoid fragmentation. Parent selection and rewiring implement dynamic programming over this increasingly dense graph, continually lowering costs. Under regularity and \\(\\delta\\)-clearance, any optimal path can be covered by a sequence of balls of radius \\(r_n\\); with high probability the tree includes nodes in each ball, enabling a piecewise improvement that converges to the optimal path cost. Common Enhancements (Keep Optimality, Speed It Up): Informed RRT*: after the first solution of cost \\(C^\\star\\), sample within the informed subset that can still achieve cost \\(&lt;C^\\star\\). Batch variants (BIT*): combine sampling with incremental graph search for faster convergence. Pruning: discard nodes whose best possible heuristic cannot beat the current best cost. Example 4.8 (RRT* for 2D Kinematic Motion Planning) We can apply RRT* to find collision-free paths from a start position to a goal region. You can play with the code here. The code provides an implementation of RRT* from scratch. For an off-the-shelf implementation of RRT* and other motion planning algorithms, we recommend the Open Motion Planning Library (OMPL). Figure 4.17: RRT* finding collision-free paths. 4.6.2 Kinodynamic Motion Planning References Arnold, William F, and Alan J Laub. 1984. “Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.” Proceedings of the IEEE 72 (12): 1746–54. Choset, Howie, Kevin M Lynch, Seth Hutchinson, George A Kantor, and Wolfram Burgard. 2005. Principles of Robot Motion: Theory, Algorithms, and Implementations. MIT press. Kang, Shucheng, Xiaoyang Xu, Jay Sarva, Ling Liang, and Heng Yang. 2024. “Fast and Certifiable Trajectory Optimization.” In International Workshop on the Algorithmic Foundations of Robotics. LaValle, Steven M. 2006. Planning Algorithms. Cambridge university press. Liu, Dong C, and Jorge Nocedal. 1989. “On the Limited Memory BFGS Method for Large Scale Optimization.” Mathematical Programming 45 (1): 503–28. Nocedal, Jorge, and Stephen J Wright. 1999. Numerical Optimization. Springer. Rawlings, James Blake, David Q Mayne, and Moritz Diehl. 2020. Model Predictive Control: Theory, Computation, and Design. Vol. 2. Nob Hill Publishing Madison, WI. Wächter, Andreas, and Lorenz T Biegler. 2006. “On the Implementation of an Interior-Point Filter Line-Search Algorithm for Large-Scale Nonlinear Programming.” Mathematical Programming 106 (1): 25–57. Williams, Grady, Paul Drews, Brian Goldfain, James M Rehg, and Evangelos A Theodorou. 2016. “Aggressive Driving with Model Predictive Path Integral Control.” In 2016 IEEE International Conference on Robotics and Automation (ICRA), 1433–40. IEEE. "],["advanced-materials.html", "Chapter 5 Advanced Materials", " Chapter 5 Advanced Materials "],["appconvex.html", "A Convex Analysis and Optimization A.1 Theory A.2 Practice", " A Convex Analysis and Optimization A.1 Theory A.1.1 Sets Convex set is one of the most important concepts in convex optimization. Checking convexity of sets is crucial to determining whether a problem is a convex problem. Here we will present some definitions of some set notations in convex optimization. Definition A.1 (Affine set) A set \\(C\\subset \\mathbb{R}^n\\) is affine if the line through any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in \\mathbb{R}\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition A.2 (Convex set) A set \\(C\\subset \\mathbb{R}^n\\) is convex if the line segment between any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition A.3 (Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a cone if for any \\(x\\in C\\) and any \\(\\theta\\geq 0\\), we have \\(\\theta x \\in C\\). Definition A.4 (Convex Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a convex cone if \\(C\\) is convex and a cone. Below are some important examples of convex sets: Definition A.5 (Hyperplane) A hyperplane is a set of the form \\[\\{x|a^Tx = b\\}\\] Definition A.6 (Halfspaces) A (closed) halfspace is a set of the form \\[\\{x|a^Tx \\leq b\\}\\] Definition A.7 (Balls) A ball is a set of the form \\[B(x,r) = \\{y|\\|y-x\\|_2 \\leq r\\} = \\{x+ru|\\|u\\|_2\\leq 1\\}\\] where \\(r &gt;0\\). Definition A.8 (Ellipsoids) A ellipsoid is a set of the form \\[\\mathcal{E} = \\{y|(y-x)^TP^{-1}(y-x)\\leq 1\\}\\] where \\(P\\) is symmetric and positive definite. Definition A.9 (Polyhedra) A polyhedra is defined as the solution set of a finite number of linear equalities and inequalities: \\[\\mathcal{P} = \\{x|a_j^Tx\\leq b_j, j=1,...,m, c_k^Tx=d_k,k=1,...,p\\}\\] Definition A.10 (Norm ball) A norm ball \\(B\\) of radius \\(r\\) and a center \\(x_c\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[B = \\{x|\\|x-x_c\\|\\leq r\\}\\] Definition A.11 (Norm cone) A norm cone \\(C\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[C = \\{(x,t)|\\|x\\|\\leq t\\}\\subset \\mathbb{R}^{n+1}\\] Simplexes are important family of polyhedra. Suppose the \\(k+1\\) points \\(v_0,...,v_k\\in \\mathbb{R}^n\\) are affinely independent, which means \\(v_1-v_0,...,v_k-v_0\\) are linearly independent. Definition A.12 (Simplex) A simplex \\(C\\) defined by points \\(v_0,...,v_k\\) is: \\[C = \\textbf{conv}\\{v_0,...,v_k\\} = \\{\\theta_0v_0 + ... \\theta_kv_k|\\theta \\succeq 0, \\textbf{1}^T\\theta = 1\\}\\] Extremely important examples of convex sets are positive semidefinite cones: Definition A.13 (Symmetric,positive semidefinite,positive definite matrices) Symmetric matrices: \\(\\textbf{S}^n = \\{X\\in\\mathbb{R}^{n\\times n}| X=X^T\\}\\) Symmetric Positive Semidefinite matrices: \\(\\textbf{S}_+^n = \\{X\\in\\textbf{S}^n| X\\succeq0\\}\\) Symmetric Positive definite matrices: \\(\\textbf{S}_{++}^n = \\{X\\in\\textbf{S}^n| X\\succ0\\}\\) In most scenarios, the set we encounter is more complicated. In general it is extermely hard to determine whether a set in convex or not. But if the set is ‘generated’ by some convex sets, we can easily determine its convexity. So let’s focus on operations that preserve convexity: Proposition A.1 Assume \\(S\\) is convex, \\(S_\\alpha,\\alpha\\in\\mathcal{A}\\) is a family of convex sets. Following operations on convex sets will preserve convexity: Intersection: \\(\\bigcap_{\\alpha\\in\\mathcal{A}}S_\\alpha\\) is convex. Image under affine function: A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is affine if it has the form \\(f(x) = Ax+b\\). The image of \\(S\\) under affine function \\(f\\) is convex. I.e. \\(f(S) = \\{f(x)|x\\in S\\}\\) is convex Image under perspective function: We define the perspective function \\(P:\\mathbb{R}^{n+1}\\), with domain \\(\\textbf{dom}P = \\mathbb{R}^n\\times \\mathbb{R}_{++}\\)(where \\(\\mathbb{R}_{++}=\\{x\\in \\mathbb{R}|x&gt;0\\}\\)) as \\(P(z,t) = z/t\\). The image of \\(S\\) under perspective function is convex. Image under linear-fractional function: We define linear fractional function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) as:\\(f(x) = (Ax+b)/(c^Tx+d)\\) with \\(\\textbf{dom}f = \\{x|c^Tx+d&gt;0\\|\\). The image of \\(S\\) under linear fractional functions is convex. In some cases, the restrictions of interior is too strict. For example, imagine a plane in \\(\\mathbb{R}^3\\). The interior of the plane is \\(\\emptyset\\). But intuitively many property should be extended to this kind of situation. Because the points in the plane also lies ‘inside’ the convex set. Thus, we will define relative interior. First we will define affine hull. Definition A.14 (Affine hull) The affine hull of a set \\(S\\) is the smallest affine set that contains \\(S\\), which can be written as: \\[\\text{aff}(S) = \\{\\sum_{i=1}^k\\alpha_ix_i|k&gt;0,x_i\\in S,\\alpha_i\\in\\mathbb{R},\\sum_{i=1}^k\\alpha_i=1\\}\\] Definition A.15 (Relative Interior) The relative interior of a set \\(S\\) (denoted \\(\\text{relint}(S)\\)) is defined as its interior within the affine hull of \\(S\\). I.e. \\[\\text{relint}(S):=\\{x\\in S: \\text{there exists } \\epsilon&gt;0 \\text{ such that }N_\\epsilon \\cap \\text{aff}(S)\\subset S\\}\\] where \\(N_\\epsilon(x)\\) is a ball of radius \\(\\epsilon\\) centered on \\(x\\). A.1.2 Convex function In this section, let’s define convex functions: Definition A.16 (Convex function) A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if \\(\\textbf{dom}\\ f\\) is convex and \\(\\forall x,y\\in \\textbf{dom}\\ f\\) and with \\(\\theta \\in [0,1]\\), we have:\\[f(\\theta x +(1-\\theta)y)\\leq \\theta f(x) + (1-\\theta)f(y)\\] The function is strictly convex if the inequality holds whenever \\(x\\neq y\\) and \\(\\theta\\in (0,1)\\). If a function is differentiable, it will be easier for us to check its convexity: Proposition A.2 (Conditions for Convex function) 1.(First order condition) Suppose \\(f\\) is differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x,y\\in \\textbf{dom} f\\), \\[f(y)\\geq f(x) +\\nabla f(x)^T(y-x)\\] 2.(Second order conditions) Suppose \\(f\\) is twice differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x\\in \\textbf{dom} f\\), \\[\\nabla^2 f(x) \\succeq \\textbf{0}\\] For the same purpose, some operations that preserve the convexity of the convex functions are presented here: Proposition A.3 (Operations that preserve convexity) Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be a convex function and \\(g_1,...,g_n\\) be convex functions. The following operations will preserve convexity of the function: 1.(Nonnegative weighted sum): A nonnegative weighted sum of convex functions: \\[f = \\omega_1f_1 + ... +\\omega_mf_m\\] 2.(Composition with an affine mapping) Suppose \\(A\\in \\mathbb{R}^{n\\times m}\\) and \\(b\\in \\mathbb{R}^n\\), then \\(g(x) = f(Ax+b)\\) is convex. 3.(Pointwise maximum and supremum) \\(g(x) = \\max\\{g_1(x),...,g_n(x)\\}\\) is convex. If \\(h(x,y)\\) is convex in \\(x\\) for each \\(y\\in\\mathcal{A}\\), then \\(\\sup_{y\\in\\mathcal{A}} h(x,y)\\) is also convex in \\(x\\). 4.(Minimization) If \\(h(x,y)\\) is convex in \\((x,y)\\), and \\(C\\) is a convex nonempty set, then \\(\\inf_{x\\in C} h(x,y)\\) is convex in \\(x\\). 5.(Perspective of a function) The perspective of \\(f\\) is the function \\(h:\\mathbb{R}^{n+1}\\to\\mathbb{R}\\) defined by: \\(h(x,t) = tf(x/t)\\) with domain \\(\\textbf{dom}\\ h=\\{(x,t)|x/t\\in\\textbf{dom} f,t&gt;0\\}\\). And \\(h\\) is convex. A.1.3 Lagrange dual We consider an optimization problem in the standard form (without assuming convexity of anything): \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; h_i(x) = 0\\quad i=1,...,p \\\\ \\end{aligned} \\end{equation}\\] Definition A.17 (Lagrange dual function) The Lagrangian related to the problem above is defined as: \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\] The Lagrange dual function is defined as: \\[g(\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}}L(x,\\lambda,\\nu)\\] When the Lagrangian is unbounded below in \\(x\\), the dual function takes on the value \\(-\\infty\\). Note that since the Lagrange dual function is a pointwise infimum of a family of affine functions of \\((\\lambda,\\nu)\\), so it’s concave. The Lagrange dual function will give us lower bounds of the optimal value of the original problem: \\[g(\\lambda,\\nu)\\leq p^*\\]. We can see that, the dual function can give a nontrivial lower bound only when \\(\\lambda\\succeq 0\\). Thus we can solve the following dual problem to get the best lower bound. Definition A.18 (Lagrange dual problem) The lagrangian dual problem is defined as follows: \\[\\begin{equation} \\begin{aligned} d^* = \\quad \\max_{\\lambda,\\nu} \\quad &amp; g(\\lambda,\\nu)\\\\ \\textrm{s.t.} \\quad &amp; \\lambda\\succeq 0 \\end{aligned} \\end{equation}\\] This is a convex optimization problem. We can easily see that \\[d^*\\leq p^*\\] always hold. This property is called weak duality. If \\[d^*=p^*\\], it’s called strong duality. Strong duality does not hold in general, but it usually holfs for convex problems. We can find conditions that guarantee strong duality in convex problems, which are called constrained qualifications. Slater’s constraint qualification is a useful one. Theorem A.1 (Slater's constraint qualification) Strong duality holds for a convex problem \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; Ax=b \\\\ \\end{aligned} \\end{equation}\\] if it is strictly feasible, i.e. \\[\\exists x\\in\\textbf{relint}\\mathcal{D}:\\quad f_i(x)&lt;0,\\quad i=1...m,\\quad Ax=b\\] And the linear inequalities do not need to hold with strict inequality. A.1.4 KKT condition Note that if strong duality holds, denote \\(x^*\\) to be primal optimal, and \\((\\lambda^*,\\nu^*)\\) to be dual optimal. Then: \\[\\begin{equation} \\begin{aligned} f_0(x^*) = g(\\lambda^*,\\nu^*) = &amp; \\inf_x(f_0(x)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x))\\\\ \\leq &amp; f_0(x^*)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x)\\\\ \\leq &amp; f_0(x^*)\\\\ \\end{aligned} \\end{equation}\\] from this, combining \\(\\lambda^*\\geq 0\\) and \\(f_i(x^*)\\leq 0\\), we can know that: \\(\\lambda_i^*f_i(x^*)=0\\quad i=1\\cdots m\\). This means for \\(\\lambda_i^*\\) and \\(f_i(x^*)\\), one of them must be zero, which is known as complementary slackness). Thus we arrived at the following four conditions, which are called KKT conditions. Theorem A.2 (Karush-Kuhn-Tucker(KKT) Conditions) The following four conditions are called KKT conditions (for a problem with differentiable \\(f_i,h_i\\)) Primal feasible: \\(f_i(x) \\leq 0,i,\\cdots ,m,\\ h_i(x) = 0,i=1,\\cdots ,p\\) Dual feasible: \\(\\lambda\\succeq0\\) Complementary slackness: \\(\\lambda_if_i(x)=0,i=1,\\cdots,m\\) Gradient of Lagrangian with respect to \\(x\\) vanishes:\\(\\nabla f_0(x)+\\sum_{i=1}^m\\lambda_i\\nabla f_i(x)+\\sum_{i=1}^p\\nu_i\\nabla h_i(x) = 0\\) From the discussion above, we know that if strong duality holds and \\(x,\\lambda,\\nu\\) are optimal, then they must satisfy the KKT conditions. Also if \\(x,\\lambda,\\nu\\) satisfy KKT for a convex problem, then they are optimal. However, the converse is not generally true, since KKT condition implies strong duality. If Slater’s condition is satisfied, then \\(x\\) is optimal if and only if there exist \\(\\lambda,\\nu\\) that satisfy KKT conditions. Sometimes, by solving the KKT system, we can derive the closed-form solution of a optimization directly. Also, sometimes we will use the residual of the KKT system as the termination condition. In general, \\(f_i,h_i\\) may not be differentiable. There are also KKT conditions for them, which will include knowledge of subdifferential and will not be included here. A.2 Practice A.2.1 CVX Introduction In the last section, we have learned basic concepts and theorems in convex optimization. In this section, on the other hand, we will introduce you how to model basic convex optimization problems with CVX, an easy-to-use MATLAB package. To install CVX, please refer to this page. Note that every time you what to use the CVX package, you should add it to your MATLAB path. For example, if I install CVX package in the parent directory of my current directory with default directory name cvx, the following line should be added before your CVX codes: addpath(genpath(&quot;../cvx/&quot;)); With CVX, it is incredibly easy for us to define and solve a convex optimization problem. You just need to: define the variables. define the objective function you want to minimize or maximize. define the constraints. After running your codes, the optimal objective value is stored in the variable cvx_optval, and the problem status is stored in the variable cvx_status (when your problem is well-defined, this variable’s value will be Solved). The optimal solutions will be stored in the variables you define. Throughout this section, we will study five types of convex optimization problems: linear programming (LP), quadratic programming (QP), (convex) quadratically constrained quadratic programming (QCQP), second-order cone programming (SOCP), and semidefinite programming (SDP). Given two types of optimization problems \\(A\\) and \\(B\\), we say \\(A &lt; B\\) if \\(A\\) can always be converted to \\(B\\) while the inverse is not true. Under this notation, we have \\[\\begin{equation*} \\text{LP} &lt; \\text{QP} &lt; \\text{QCQP} &lt; \\text{SOCP} &lt; \\text{SDP} \\end{equation*}\\] A.2.2 Linear Programming (LP) Definition. An LP has the following form: \\[\\begin{equation} \\tag{A.1} \\begin{aligned} \\min_{x \\in \\mathbb{R}^n} &amp; \\ c^T x \\\\ \\text{subject to } &amp; A x \\le b \\end{aligned} \\end{equation}\\] where \\(x\\) is the variable, \\(A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m\\), and \\(c \\in \\mathbb{R}^n\\) are the parameters. Note that the constraint \\(A x \\le b\\) already incorporates linear equality constraints. To see this, consider the constraint \\(A&#39; x = b&#39;\\), we can reformulate it as \\(A x \\le b\\) by \\[\\begin{equation*} \\begin{bmatrix} A&#39; \\\\ -A&#39; \\end{bmatrix} x \\le \\begin{bmatrix} b&#39; \\\\ -b&#39; \\end{bmatrix} \\end{equation*}\\] Example. Consider the problem of minimizing a linear function \\(c_1 x_1 + c_2 x_2\\) over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). We can convert it to the standard LP form in (A.1) by simply setting \\(c\\) as \\([c_1, \\ c_2]^T\\) and the linear inequality constraint as \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the LP example setting c1 = 2; c2 = -5; l1 = 3; l2 = 7; % parameters: c, A, b c = [c1; c2]; A = [1, 0; -1, 0; 0, 1; 0, -1]; b = [l1; l1; l2; l2]; %% solve LP cvx_begin variable x(2); % define variables [x1, x2] minimize(c&#39; * x); % define the objective subject to A * x &lt;= b; % define the linear constraint cvx_end A.2.3 Quadratic Programming (QP) Definition. A QP has the following form: \\[\\begin{align} \\tag{A.2} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P x + q^T x \\\\ \\text{subject to } &amp; Gx \\le h \\\\ &amp; Ax = b \\end{align}\\] where \\(P \\in \\mathcal{S}_+^n, q\\in \\mathbb{R}^n, G \\in \\mathbb{R}^{m \\times n}, h\\in \\mathbb{R}^m, A \\in \\mathbb{R}^{p \\times n}, b \\in \\mathbb{R}^p\\). Here \\(\\mathcal{S}_+^n\\) denotes the set of positive semidefinite matrices of size \\(n\\times n\\). Obviously, if we set \\(P\\) as zero, QP will degenerate to LP. Example. Consider the problem of minimizing a quadratic function \\[\\begin{equation*} f(x_1, x_2) = p_1 x_1^2 + 2p_2 x_1 x_2 + p_3 x_2^2 + q_1 x_1 + q_2 x_2 \\end{equation*}\\] over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). Since \\(P = 2 \\begin{bmatrix} p_1 &amp; p_2 \\\\ p_2 &amp; p_3 \\end{bmatrix} \\succeq 0\\), the following two conditions must hold: \\[\\begin{equation*} \\begin{cases} p_1 \\ge 0 \\\\ p_1 p_3 - 4 p_2^2 \\ge 0 \\end{cases} \\end{equation*}\\] Same as in the LP example, \\(G\\) and \\(h\\) can be expressed as: \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the QP example setting p1 = 2; p2 = 0.5; p3 = 4; q1 = -3; q2 = -6.5; l1 = 2; l2 = 2.5; % check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0); tmp2 = (p1*p3 - 4*p2^2 &gt;= 0); if ~(tmp1 &amp;&amp; tmp2) error(&quot;P is not positve semidefinite!&quot;); end % parameters: P, q, G, h P = 2 * [p1, p2; p2, p3]; q = [q1; q2]; G = [1, 0; -1, 0; 0, 1; 0, -1]; h = [l1; l1; l2; l2]; %% Solve the QP problem cvx_begin variable x(2); % define variables [x1; x2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P) + q&#39; * x; minimize(obj); subject to G * x &lt;= h; % define the linear constraint cvx_end A.2.4 Quadratically Constrained Quadratic Programming (QCQP) Definition. An (convex) QCQP has the following form: \\[\\begin{align} \\tag{A.3} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P_0 x + q_0^T x \\\\ \\text{subject to } &amp; \\frac{1}{2} x^T P_i x + q_i^T x + r_i \\le 0, \\ i = 1 \\dots m \\\\ &amp; Ax = b \\end{align}\\] where \\(P_i \\in \\mathcal{S}_+^n, i = 0 \\dots m\\), \\(q_i \\in \\mathbb{R}^n, i = 0 \\dots m\\), \\(A \\in \\mathbb{R}^{p \\times n}\\), and \\(b \\in \\mathbb{R}^p\\). Note that in other literature, you may find a more general form of QCQP: they don’t require \\(P_i\\)’s to be positive semidefinite. Yet in this case, the problem is non-convex and beyond our scope. Example. We study the problem of getting the minimum distance between two ellipses. By convention, when the ellipses overlap, we set the minimum distance as \\(0\\). This problem can be exactly solved by (convex) QCQP. Consider two ellipses of the following form: \\[\\begin{equation*} \\begin{cases} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 \\le 0 \\\\ \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 \\le 0 \\\\ \\end{cases} \\end{equation*}\\] where \\([y_1, z_1]^T\\) and \\([y_2, z_2]^T\\) are arbitrary points inside the two ellipses respectively. Also, two ensure the ellipses are well defined, we should enforce the following properties in \\((K_i, k_i, c_i), i = 1, 2\\): (1) \\(K_i \\succ 0\\); (2) Let \\(K_i = L_i L_i^T\\) be the Cholesky decomposition of \\(K_i\\). Then, ellipse \\(i\\) can be rewritten as: \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^T \\begin{bmatrix} y_i \\\\ z_i \\end{bmatrix} - L_i^{-1} k_i \\parallel^2 \\le \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i \\end{equation*}\\] Thus, \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i &gt; 0 \\end{equation*}\\] With these two assumptions, we want to minimize: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 \\end{equation*}\\] Now, we construct \\(P, q, r\\)’s in QCQP with the above parameters. Define the variable \\(x\\) as \\([y_1, z_1, y_2, z_2]\\). \\(P_0\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 = \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix}^T \\begin{bmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix} \\end{equation*}\\] \\(P_1, q_1, r_1\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 = \\frac{1}{2} x^T \\begin{bmatrix} K_1 &amp; O \\\\ O &amp; O \\end{bmatrix} + \\begin{bmatrix} k_1 \\\\ O \\end{bmatrix}^T x + c_1 \\le 0 \\end{equation*}\\] \\(P_2, q_2, r_2\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 = \\frac{1}{2} x^T \\begin{bmatrix} O &amp; O \\\\ O &amp; K_2 \\end{bmatrix} + \\begin{bmatrix} O \\\\ k_2 \\end{bmatrix}^T x + c_2 \\le 0 \\end{equation*}\\] The corresponding codes are shown below. In this example, we test the minimum distance between a circle \\(y_1^2 + z_1^2 \\le 1\\) and another circle \\((y_2 - 2)^2 + (z_2 - 2)^2 \\le 1\\). You can check whether the result from QCQP aligns with your manual calculation. %% Define the QCQP example setting K1 = eye(2); k1 = zeros(2, 1); c1 = -0.5; K2 = eye(2); k2 = [2; 2]; c2 = 3.5; if ~(if_ellipse(K1, k1, c1) &amp;&amp; if_ellipse(K2, k2, c2)) error(&quot;The example setting is not correct&quot;); end % define parameters P0, P1, P2, q1, q2, r1, r2 P0 = [1,0,-1,0; 0,1,0,-1; -1,0,1,0; 0,-1,0,1]; P1 = zeros(4, 4); P1(1:2, 1:2) = K1; P2 = zeros(4, 4); P2(3:4, 3:4) = K2; q1 = [k1; zeros(2, 1)]; q2 = [zeros(2, 1); k2]; r1 = c1; r2 = c2; %% Solve the QCQP problem cvx_begin variable x(4); % define variables [y1; z1; y2; z2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P0); minimize(obj); subject to 0.5 * quad_form(x, P1) + q1&#39; * x + r1 &lt;= 0; 0.5 * quad_form(x, P2) + q2&#39; * x + r2 &lt;= 0; cvx_end %% detect whether (K, k, c) generates a ellipse function flag = if_ellipse(K, k, c) L = chol(K); radius_square = 0.5 * norm(L \\ k)^2 - c; % L \\ k = inv(L) * k flag = (radius_square &gt; 0); end A.2.5 Second-Order Cone Programming (SOCP) Definition. An SOCP has the following form: \\[\\begin{align} \\tag{A.4} \\min_{x \\in \\mathbb{R}^n} \\ &amp; f^T x \\\\ \\text{subject to } &amp; || A_i x + b_i ||_2 \\le c_i^T x + d_i, \\ i = 1 \\dots m \\\\ &amp; Fx = g \\end{align}\\] where \\(f \\in \\mathbb{R}^n, A_i \\in \\mathbb{R}^{n_i \\times n}, b_i \\in \\mathbb{R}^{n_i}, c_i \\in \\mathbb{R}^n, d_i \\in \\mathbb{R}, F \\in \\mathbb{R}^{p \\times n}\\), and \\(g \\in \\mathbb{R}^p\\). Example. We consider the problem of stochastic linear programming: \\[\\begin{align} \\min_x \\ &amp; c^T x \\\\ \\text{subject to } &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p, \\ i = 1 \\dots m \\\\ &amp; a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i), \\ i = 1 \\dots m \\end{align}\\] Here \\(p\\) should be more than \\(0.5\\). We show that this problem can be converted to a SOCP: Since \\(a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i)\\), then \\((a_i^T x - b_i) \\sim \\mathcal{N}(\\bar{a}_i^T x - b_i, x^T \\Sigma_i x)\\). Standardize it: \\[\\begin{equation*} t := ||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1} \\left\\{ (a_i^T x - b_i) - (\\bar{a}_i^T x - b_i) \\right\\} \\sim \\mathcal{N}(0, 1) \\end{equation*}\\] Then, \\[\\begin{align} \\mathbb{P}(a_i^T x \\le b_i) &amp; = \\mathbb{P}(a_i^T x - b_i \\le 0) \\\\ &amp; = \\mathbb{P}(t \\le -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\\\ &amp; = \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\end{align}\\] Here \\(\\Phi(\\cdot)\\) is the cumulative distribution function of the standard normal distribution: \\[\\begin{equation*} \\Phi(\\xi) = \\int_{-\\infty}^{\\xi} e^{-\\frac{1}{2} t^2} \\ dt \\end{equation*}\\] Thus, \\[\\begin{align} &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p \\\\ \\Longleftrightarrow &amp; \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\ge p \\\\ \\Longleftrightarrow &amp; -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i) \\ge \\Phi^{-1}(p) \\\\ \\Longleftrightarrow &amp; \\Phi^{-1}(p) ||\\Sigma_i^{\\frac{1}{2}} x||_2 \\le b_i - \\bar{a}_i^T x \\end{align}\\] which is exactly the same as inequality constraints in SOCP formulation. (You can see why we enforce \\(p &gt; 0.5\\) here: otherwise \\(\\Phi^{-1}(p)\\) will be negative and the constraint with not be an second-order cone.) In the following code example, we set up four inequality constraints and let \\(\\bar{a}_i^T x \\le b_i, \\ i = 1 \\dots 4\\) form an square located at the origin of size \\(2\\). Then, for convenience, we set \\(\\Sigma_i \\equiv \\sigma^2 I\\). %% Define the SOCP example setting bar_a1 = [1; 0]; b1 = 1; bar_a2 = [0; 1]; b2 = 1; bar_a3 = [-1; 0]; b3 = 1; bar_a4 = [0; -1]; b4 = 1; sigma = 0.1; c = [2; 3]; p = 0.9; % p should be more than 0.5 Phi_inv = norminv(p); % get Phi^{-1}(p) %% Solve the SOCP problem cvx_begin variable x(2); % define variables [x1; x2] minimize(c&#39; * x); subject to sigma*Phi_inv * norm(x) &lt;= b1 - bar_a1&#39; * x; sigma*Phi_inv * norm(x) &lt;= b2 - bar_a2&#39; * x; sigma*Phi_inv * norm(x) &lt;= b3 - bar_a3&#39; * x; sigma*Phi_inv * norm(x) &lt;= b4 - bar_a4&#39; * x; cvx_end A.2.6 Semidefinite Programming (SDP) Definition. An SDP has the following form: \\[\\begin{align} \\tag{A.5} \\min_{X_i, x_i} \\ &amp; \\sum_{i=1}^{n_s} C_i \\cdot X_i + \\sum_{i=1}^{n_u} c_i \\cdot x_i \\\\ \\text{subject to } &amp; \\sum_{i=1}^{n_s} A_{i,j} \\cdot X_i + \\sum_{i=1}^{n_u} a_{i,j} \\cdot x_i = b_j, \\quad j = 1 \\dots m \\\\ &amp; X_i \\in \\mathcal{S}_+^{D_i}, \\quad i = 1 \\dots n_s \\\\ &amp; x_i \\in \\mathbb{R}^{d_i}, \\quad i = 1 \\dots n_u \\end{align}\\] where \\(C_i, A_{i, j} \\in \\mathbb{R}^{D_i \\times D_i}\\), \\(c_i, a_{i, j} \\in \\mathbb{R}^{d_i}\\), and \\(\\cdot\\) means element-wise product. For two square matrices \\(A, B\\), the dot product \\(A \\cdot B\\) is equal to \\(\\text{tr}(A B)\\); for two vectors \\(a, b\\), the dot product \\(a \\cdot b\\) is the same as inner product \\(a^T b\\). Note that actually there are many “standard” forms of SDP. For example, in the convex optimization theory part, you may find an SDP that looks like: \\[\\begin{align} \\min_X \\ &amp; C \\cdot X \\\\ \\text{subject to } &amp; A \\cdot X = b \\\\ &amp; X \\succeq 0 \\end{align}\\] It is convenient for us to analyze the theoretical properties of SDP with this form. Also, in SDP solvers’ User Guide, you may see more complex SDP forms which involve more general convex cones. For example, see MOSEK’s MATLAB API docs. Here we turn to use the form of (A.5) for two reasons: (1) it is general enough: our SDP example below can be converted to this form (also, SDPs from sum-of-squares programming in this book are exactly of the form (A.5)); (2) it is more readable than more complex forms. Example. We consider the problem of finding the minimum eigenvalue for a positive semidefinite matrix \\(S\\). We will show that this problem can be converted to (A.5). Since \\(S\\) is positive semidefinite, the finding procedure can be cast as \\[\\begin{align} \\max_\\lambda &amp; \\ \\lambda \\\\ \\text{subject to } &amp; S - \\lambda I \\succeq 0 \\end{align}\\] Now define an auxiliary matrix \\(X := S - \\lambda I\\). We have \\[\\begin{align} \\min_{\\lambda, X} &amp; \\ -\\lambda \\\\ \\text{subject to } &amp; X + \\lambda I = S \\\\ &amp; X \\succeq 0 \\end{align}\\] It is obvious that the linear matrix equality constraint \\(X + \\lambda I = S\\) can be divided into several linear scalar equality constraints in (A.5). For example, we consider \\(S \\in \\mathbb{S}_+^3\\). Thereby \\(X + \\lambda I = S\\) will lead to \\(6\\) linear equality constraints (We don’t consider \\(X\\) is a symmetric matrix here, since most solvers will implicitly consider this. Thus, only the upper-triangular part of \\(X\\) and \\(S\\) are actually used in the equality construction.): \\[\\begin{align} &amp; \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[0, 0], \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 2] \\\\ &amp; \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[1, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[1, 2], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\cdot X + \\lambda = S[2, 2] \\end{align}\\] Seems tedious? Fortunately, CVX provides a high-level API to handle these linear equality constraints: you just need to write down X + lam * eye(3) == S; % linear equality constraints: X + lam *I = S CVX will autometically convert this high-level constraint to (A.5) and pass them to the underlying solver. To generate a ramdom \\(S \\in \\mathcal{S}_+^3\\), you just need to assign three nonnegative eigenvalues to the program. After that, an random \\(S\\) will be generated by \\(S = Q \\ \\text{diag}(\\lambda_1, \\lambda_2, \\lambda_3) \\ Q^T\\), where \\(Q\\) is random orthonormal matrix. %% Define the SDP example setting lam_list = [0.7; 2.4; 3.7]; S = generate_random_PD_matrix(lam_list); % get a PD matrix S %% Solve the SDP problem cvx_begin variable X(3, 3) symmetric; variable lam; maximize(lam); subject to % here &quot;==&quot; should be read as &quot;is in&quot; X == semidefinite(3); X + lam * eye(3) == S; cvx_end % this function help to generate PD matrix of size 3*3 % if you provide the eigenvalues [lam_1, lam_2, lam_3] function S = generate_random_PD_matrix(lam_list) if ~all(lam_list &gt;= 0) % all eigenvalues &gt;= 0 error(&quot;All eigenvalues must be nonnegative.&quot;); end D = diag(lam_list); % use QR factorization to generate a random orthonormal matrix Q [Q, ~] = qr(rand(3, 3)); S = Q * D * Q&#39;; end A.2.7 CVXPY Introduction and Examples Apart from CVX MATLAB, we also have a Python package called CVXPY, which functions almost the same as CVX MATLAB. To define and solve a convex optimization problem CVXPY, basically, there are three steps (apart from importing necessary packages): Step 1: Define parameters and variables in a certain type of convex problem. Here variables are what you are trying to optimize or “learn”. Parameters are the “coefficients” of variables in the objective and constraints. Step 2: Define the objective function and constraints. Step 3: Solve the problem and get the results. Here we provide the CVXPY codes for the above five convex optimization examples. A.2.7.1 LP import cvxpy as cp import numpy as np ## Define the LP example setting c1 = 2 c2 = -5 l1 = 3 l2 = 7 ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: c, A, b c = np.array([c1, c2]) A = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) b = np.array([l1, l1, l2, l2]) ## Step 2: define objective and constraints obj = cp.Minimize(c.T @ x) constraints = [A @ x &lt;= b] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.2 QP import cvxpy as cp import numpy as np ## Define the LP example setting p1 = 2 p2 = 0.5 p3 = 4 q1 = -3 q2 = -6.5 l1 = 2 l2 = 2.5 # check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0) tmp2 = (p1*p3 - 4*p2**2 &gt;= 0) assert(tmp1 and tmp2, &quot;P is not positve semidefinite!&quot;) ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: P, q, G, h P = 2*np.array([[p1, p2], [p2, p3]]) q = np.array([q1, q2]) G = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) h = np.array([l1, l1, l2, l2]) ## Step 2: define the objective and constraints fx = 0.5 * cp.quad_form(x, P) + q.T @ x obj = cp.Minimize(fx) constraints = [G @ x &lt;= h] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve the problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.3 QCQP import cvxpy as cp import numpy as np from numpy.linalg import cholesky, inv, norm ## Define the QCQP example setting def if_ellipse(K, k, c): # examine whether 0.5*x^T K x + k^T x + c &lt;= 0 is a ellipse # if K is not positive semidefinite, Cholesky will raise an error L = cholesky(K) radius_square = 0.5 * norm(inv(L) @ k)**2 - c return radius_square &gt; 0 K1 = np.eye(2) k1 = np.zeros(2) c1 = -0.5 K2 = np.array([[1, 0], [0, 1]]) k2 = np.array([2, 2]) c2 = 3.5 if not (if_ellipse(K1, k1, c1) and if_ellipse(K2, k2, c2)): raise ValueError(&quot;The example setting is not correct&quot;) ## Step 1: define variables and parameters P0 = np.array([[1,0,-1,0], [0,1,0,-1], [-1,0,1,0], [0,-1,0,1]]) P1 = np.zeros((4,4)) P1[:2, :2] = K1 P2 = np.zeros((4,4)) P2[2:, 2:] = K2 q1 = np.concatenate([k1, np.zeros(2)]) q2 = np.concatenate([np.zeros(2), k2]) r1 = c1 r2 = c2 ## Step 2: define objective and constraints x = cp.Variable(4) # variable: x = [y1, z1, y2, z2]^T fx = 0.5 * cp.quad_form(x, P0) obj = cp.Minimize(fx) con1 = (0.5 * cp.quad_form(x, P1) + q1.T @ x + r1 &lt;= 0) # ellipse 1 con2 = (0.5 * cp.quad_form(x, P2) + q2.T @ x + r2 &lt;= 0) # ellipse 2 constraints = [con1, con2] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.4 SOCP import cvxpy as cp import numpy as np from scipy.stats import norm ## Define the SOCP example setting # define bar_ai, bi (i = 1, 2, 3, 4) bar_a1 = np.array([1, 0]) b1 = 1 bar_a2 = np.array([0, 1]) b2 = 1 bar_a3 = np.array([-1, 0]) b3 = 1 bar_a4 = np.array([0, -1]) b4 = 1 sigma = 0.1 c = np.array([2, 3]) p = 0.9 # p should be more than 0.5 ## Step 1: define variables and parameters Phi_inv = norm.ppf(p) # get Phi^{-1}(p) ## Step 2: define objective and constraints x = cp.Variable(2) # variable: x = [x1, x2]^T obj = cp.Minimize(c.T @ x) # use cp.SOC(t, x) to create the SOC constraint ||x||_2 &lt;= t constraints = [ cp.SOC(b1 - bar_a1.T @ x, sigma*Phi_inv*x), cp.SOC(b2 - bar_a2.T @ x, sigma*Phi_inv*x), cp.SOC(b3 - bar_a3.T @ x, sigma*Phi_inv*x), cp.SOC(b4 - bar_a4.T @ x, sigma*Phi_inv*x), ] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.5 SDP import cvxpy as cp import numpy as np from scipy.stats import ortho_group ## Define the SDP example setting # this function help to generate PD matrix of size 3*3 # if you provide the eigenvalues [lam_1, lam_2, lam_3] def generate_random_PD_matrix(lam_list): assert np.all(lam_list &gt;= 0) # all eigenvalues &gt;= 0 # S = Q @ D @ Q.T D = np.diag(lam_list) Q = ortho_group.rvs(3) return Q @ D @ Q.T lam_list = np.array([0.5, 2.4, 3.7]) S = generate_random_PD_matrix(lam_list) # get a PD matrix S ## Step 1: define variables and parameters # get coefficients for equality constraints A_00 = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_00 @ X) + lam = S_00 A_01 = np.array([[0, 1, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_01 @ X) = S_01 A_02 = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]]) # tr(A_02 @ X) = S_02 A_11 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]) # tr(A_11 @ X) + lam = S_11 A_12 = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 0]]) # tr(A_12 @ X) = S_12 A_22 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1]]) # tr(A_22 @ X) + lam = S_22 ## Step 2: define objective and constraints # define a PD matrix variable X of size 3*3 X = cp.Variable((3, 3), symmetric=True) constraints = [X &gt;&gt; 0] # the operator &gt;&gt; denotes matrix inequality lam = cp.Variable(1) constraints += [ cp.trace(A_00 @ X) + lam == S[0,0], cp.trace(A_01 @ X) == S[0,1], cp.trace(A_02 @ X) == S[0,2], cp.trace(A_11 @ X) + lam == S[1,1], cp.trace(A_12 @ X) == S[1,2], cp.trace(A_22 @ X) + lam == S[2,2], ] obj = cp.Minimize(-lam) prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, lam.value) # optimal lam "],["app-lti-system-theory.html", "B Linear System Theory B.1 Stability B.2 Controllability and Observability B.3 Stabilizability And Detectability", " B Linear System Theory Thanks to Shucheng Kang for writing this Appendix. B.1 Stability B.1.1 Continuous-Time Stability Consider the continuous-time linear time-invariant (LTI) system \\[\\begin{equation} \\dot{x} = A x. \\tag{B.1} \\end{equation}\\] the system is said to be “diagonalizable” if \\(A\\) is diagonalizable. Definition B.1 (Asymptotic and Marginal Stability) The diagonalizable, LTI system (B.1) is “asymptotically stable” if \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “marginally stable” if \\(x(t) \\nrightarrow 0\\) but remains bounded as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “stable” if it is either asymptotically or marginally stable “unstable” if it is not stable One can show that \\(A\\)’s eigenvalues determine the LTI system’s stability, as the following Theorem states: Theorem B.1 (Stability of Continuous-Time LTI System) The diagonalizable1, LTI system (B.1) is asymptotically stable if \\(\\text{Re} (\\lambda_i) &lt; 0\\) for all \\(i\\) marginally stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(\\text{Re} (\\lambda_i) = 0\\) stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) unstable if \\(\\text{Re} (\\lambda_i) &gt; 0\\) for at least one \\(i\\) Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(\\dot{x} = Ax\\) can be restated as \\(\\dot{z} = \\Lambda z\\). Consider the \\(i\\)’s component of \\(z\\): \\[\\begin{equation*} \\dot{z}_i = \\lambda_i z_i \\Longrightarrow z_i(t) = e^{\\lambda_i t} z_i(0) \\end{equation*}\\] Since \\(\\text{Re}(\\lambda_i) &lt; 0\\), \\(z_i(t)\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_i(0)\\). B.1.2 Discrete-Time Stability Now consider the diagonalizable, discrete-time linear time-invariant (LTI) system \\[\\begin{equation} x_{t+1} = A x_t. \\tag{B.2} \\end{equation}\\] Theorem B.2 (Stability of Discrete-Time LTI System) The diagonalizable, discrete-time LTI system (B.2) is asymptotically stable if \\(|\\lambda_i| &lt; 1\\) for all \\(i\\) marginally stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(|\\lambda_i| = 1\\) stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) unstable if \\(|\\lambda_i| &gt; 1\\) for at least one \\(i\\). Note that \\(|\\lambda_i| &lt; 1\\) means the eigenvalue lies strictly inside the unit circle in the complex plane. Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(x_{t+1} = Ax\\) can be restated as \\(z_{t+1} = \\Lambda z_t\\). Expanding the recursion, we have \\[\\begin{equation*} z_{t} = \\Lambda^{t-1} z_0 \\Longrightarrow z_{t,i} = \\lambda_i^{t-1} z_{0,i} \\end{equation*}\\] Since \\(|\\lambda_i| &lt; 1\\), \\(z_{t,i}\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_{0,i}\\). B.1.3 Lyapunov Analysis Theorem B.3 (Lyapunov Equation) The following is equivalent for a linear time-invariant system \\(\\dot{x} = A x\\) The system is globally asymptotically stable, i.e., \\(A\\) is Hurwitz and \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) regardless of the initial condition; For any positive definite matrix \\(Q\\), the unique solution \\(P\\) to the Lyapunov equation \\[\\begin{equation} A^T P + P A = -Q \\tag{B.3} \\end{equation}\\] is positive definite. Proof. (a): \\(2 \\Rightarrow 1\\). Suppose we are given two positive definite matrices \\(P, Q \\succ 0\\) that satisfies the Lyapunov equation (B.3). Define a scalar function \\[ V(x) = x^T P x. \\] It is clear that \\(V &gt; 0\\) for any \\(x \\neq 0\\) and \\(V(x) = 0\\) (i.e., \\(V(x)\\) is positive definite). We also see \\(V(x)\\) is radially unbounded because: \\[ V(x) \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2 \\Rightarrow \\lim_{x \\rightarrow \\infty} V(x) \\rightarrow \\infty. \\] The time derivative of \\(V\\) reads \\[ \\dot{V} = 2 x^T P \\dot{x} = x^T (A^T P + P A) x = - x^T Q x. \\] Clearly, \\(\\dot{V} &lt; 0\\) for any \\(x \\neq 0\\) and \\(\\dot{V}(0) = 0\\). According to Lyapunov’s global stability theorem ??, we conclude the linear system \\(\\dot{x} = Ax\\) is globally asymptotically stable at \\(x = 0\\). (b): \\(1 \\Rightarrow 2\\). Suppose \\(A\\) is Hurwitz, we want to show that, for any \\(Q \\succ 0\\), there exists a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (B.3). In fact, consider the matrix \\[ P = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt. \\] Because \\(A\\) is Hurwitz, the integral exists, and clearly \\(P \\succ 0\\) due to \\(Q \\succ 0\\). To show this choice of \\(P\\) satisfies the Lyapunov equation, we write \\[\\begin{align} A^T P + P A &amp;= \\int_{t=0}^{\\infty} \\left( A^T e^{A^T t} Q e^{At} + e^{A^T t} Q e^{At} A \\right) dt \\\\ &amp;=\\int_{t=0}^{\\infty} d \\left( e^{A^T t} Q e^{At} \\right) \\\\ &amp; = e^{A^T t} Q e^{At}\\vert_{t = \\infty} - e^{A^T t} Q e^{At}\\vert_{t = 0} = - Q, \\end{align}\\] where the last equality holds because \\(e^{A \\infty} = 0\\) (recall \\(A\\) is Hurwitz). To show the uniqueness of \\(P\\), we assume that there exists another matrix \\(P&#39;\\) that also satisfies the Lyapunov equation. Therefore, \\[\\begin{align} P&#39; &amp;= e^{A^T t} P&#39; e^{At} \\vert_{t=0} - e^{A^T t} P&#39; e^{At} \\vert_{t=\\infty} \\\\ &amp;= - \\int_{t=0}^{\\infty} d \\left( e^{A^T t} P&#39; e^{At} \\right) \\\\ &amp;= - \\int_{t=0}^{\\infty} e^{A^T t} \\left( A^T P&#39; + P&#39; A \\right) e^{At} dt \\\\ &amp; = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt = P, \\end{align}\\] leading to \\(P&#39; = P\\). Hence, the solution is unique. Convergence rate estimation. We now show that Theorem B.3 can allow us to quantify the convergence rate of a (stable) linear system towards zero. For a Hurwitz linear system \\(\\dot{x} = Ax\\), let us pick a positive definite matrix \\(Q\\). Theorem B.3 tells us we can find a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (B.3). In this case, we can upper bound the scalar function \\(V = x^T P x\\) as \\[ V \\leq \\lambda_{\\max}(P) \\Vert x \\Vert^2. \\] The time derivative of \\(V\\) is \\(\\dot{V} = - x^T Q x\\), which can be upper bounded by \\[\\begin{align} \\dot{V} &amp; \\leq - \\lambda_{\\min} (Q) \\Vert x \\Vert^2 \\\\ &amp; = - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} \\underbrace{ \\left( \\lambda_{\\max} (P) \\Vert x \\Vert^2 \\right)}_{\\geq V} \\\\ &amp; \\leq - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} V. \\end{align}\\] Denoting \\(\\gamma(Q) = \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max}(P)}\\), the above inequality implies \\[ V(0) e^{-\\gamma(Q) t} \\geq V(t) = x^T P x \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2. \\] As a result, \\(\\Vert x \\Vert^2\\) converges to zero exponentially with a rate at least \\(\\gamma(Q)\\), and \\(\\Vert x \\Vert\\) converges to zero exponentially with a rate at least \\(\\gamma(Q) / 2\\). Best convergence rate estimation. I have used \\(\\gamma (Q)\\) to make it explict that the rate \\(\\gamma\\) depends on the choice of \\(Q\\), because \\(P\\) is computed from the Lyapunov equation as an implicit function of \\(Q\\). Naturally, choosing different \\(Q\\) will lead to different \\(\\gamma (Q)\\). So what is the choice of \\(Q\\) that maximizes the convergence rate estimation? Corollary B.1 (Maximum Convergence Rate Estimation) \\(Q = I\\) maximizes the convergence rate estimation. Proof. let us denote \\(P_0\\) as the solution to the Lyapunov equation with \\(Q = I\\) \\[ A^T P_0 + P_0 A = - I. \\] Let \\(P\\) be the solution corresponding to a different choice of \\(Q\\) \\[ A^T P + P A = - Q. \\] Without loss of generality, we can assume \\(\\lambda_{\\min}(Q) = 1\\), because rescaling \\(Q\\) will recale \\(P\\) by the same factor, which does not affect \\(\\gamma(Q)\\). Subtracting the two Lyapunov equations above we get \\[ A^T (P - P_0) + (P - P_0) A = - (Q - I). \\] Since \\(Q - I \\succeq 0\\) (due to \\(\\lambda_{\\min}(Q) = 1\\)), we know \\(P - P_0 \\succeq 0\\) and \\(\\lambda_{\\max} (P) \\geq \\lambda_{\\max} (P_0)\\). As a result, \\[ \\gamma(Q) = \\frac{\\lambda_{\\min}(Q)}{\\lambda_{\\max}(P)} = \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P)} \\leq \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P_0)} = \\gamma(I), \\] and \\(Q = I\\) maximizes the convergence rate estimation. B.2 Controllability and Observability Consider the following linear time-invariant (LTI) system \\[\\begin{equation} \\tag{B.4} \\begin{split} \\dot{x} = A x + B u \\\\ y = C x + D u \\end{split} \\end{equation}\\] where \\(x \\in \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{R}^m\\) the control input, \\(y \\in \\mathbb{R}^p\\) the output, and \\(A,B,C,D\\) are constant matrices with proper sizes. If we know the initial state \\(x(0)\\) and the control inputs \\(u(t)\\) over a period of time \\(t \\in [0, t_1]\\), the system trajectory \\((x(t), y(t))\\) can be determined as \\[\\begin{equation} \\tag{B.5} \\begin{split} x(t) &amp; = e^{At} x(0) + \\int_{0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau \\\\ y(t) &amp; = C x(t) + D u(t) \\end{split} \\end{equation}\\] To study the internal structure of linear systems, two important properties should be considered: controllability and observability. In the following analysis, we will see that they are actually dual concepts. Their definitions (Chen 1984) are given below. Definition B.2 (Controllability) The LTI system (B.4), or the pair \\((A, B)\\), is controllable, if for any initial state \\(x(0) = x_0\\) and final state \\(x_f\\), there exists a sequence of control inputs that transfer the system from \\(x_0\\) to \\(x_f\\) in finite time. Definition B.3 (Observability) The LTI system (B.4), or the pair \\((C, A)\\), is observable, if for any unknown initial state \\(x(0)\\), there exists a finite time \\(t_1 &gt; 0\\), such that knowing \\(y\\) and \\(u\\) over \\([0, t_1]\\) suffices to determine \\(x(0)\\). Sometimes it will become more convenient for us to analyze the system (B.4) under another coordinate basis, i.e., \\(z = T x\\), where the coordinate transformation \\(T\\) is nonsingular (i.e., full-rank). Define \\(A&#39; = TAT^{-1}, B&#39; = PB, C&#39; = CT^{-1}, D&#39; = D\\), we get \\[\\begin{equation*} \\begin{split} \\dot{z} = A&#39; z + B&#39; u \\\\ y = C&#39; z + D&#39; u \\end{split} \\end{equation*}\\] Since the coordinate transformation only changes the system’s coordinate basis, physical properties like controllability and observability will not change. B.2.1 Cayley-Hamilton Theorem In the analysis of controllability and observability, Cayley Hamilton Theorem lays the foundation. The statement of the theory and its (elegant) proof are given blow. Some useful corollaries are also presented. Theorem B.4 (Cayley-Hamilton) Let \\(A \\in \\mathbb{C}^{n \\times n}\\) and denote the characteristic polynomial of \\(A\\) as \\[ \\text{det}(\\lambda I - A) = \\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n \\in \\mathbb{C}[\\lambda], \\] which is a polynomial in a single variable \\(\\lambda\\) with coefficients \\(a_1,\\dots,a_n\\). Then \\[ A^n + a_1 A^{n-1} + \\dots + a_n I = 0 \\] Proof. Define the adjugate of \\(\\lambda I - A\\) as \\[ B = \\text{adj}(\\lambda I - A) \\] From \\(B\\)’s definition, we have \\[\\begin{equation} (\\lambda I - A) B = \\text{det}(\\lambda I - A) I = (\\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n) I \\tag{B.6} \\end{equation}\\] Also, \\(B\\) is a polynomial matrix over \\(\\lambda\\), whose maximum degree is no more than \\(n - 1\\). Therefore, we write \\(B\\) as follows: \\[ B = \\sum_{i=0}^{n-1} \\lambda^i B_i \\] where \\(B_i\\)’s are constant matrices. In this way, we unfold \\((\\lambda I - A)B\\): \\[\\begin{equation} \\tag{B.7} \\begin{split} (\\lambda I - A) B &amp; = (\\lambda I - A) \\sum_{i=0}^{n-1} \\lambda^i B_i \\\\ &amp; = \\lambda^n B_{n-1} + \\sum_{i=1}^{n-1} \\lambda^i (-A B_i + B_{i-1}) - A B_0 \\end{split} \\end{equation}\\] Since \\(\\lambda\\) can be arbitrarily set, matching the coefficients of (B.6) and (B.7), we have \\[\\begin{equation*} \\begin{split} B_{n-1} &amp; = I \\\\ -A B_i + B_{i-1} &amp; = a_{n-i} I, \\quad i = 1 \\dots n - 1 \\\\ -A B_0 &amp; = a_n I \\end{split} \\end{equation*}\\] Thus, we have \\[\\begin{equation*} \\begin{split} &amp; B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I \\\\ = &amp; I \\cdot A^n + \\sum_{i=1}^{n-1} (a_{n-i} I) \\cdot A^i + (a_n I) \\cdot I \\\\ = &amp; A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I \\end{split} \\end{equation*}\\] On the other hand, one can easily check that \\[ B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I = 0 \\] since each term offsets completely. Therefore, \\[ A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I = 0, \\] concluding the proof. Here are some corollaries of the Cayley-Hamilton Theorem. Corollary B.2 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k \\ge n\\), \\(A^k B\\) is a linear combination of \\(B, AB, A^2B, \\dots, A^{n-1}B\\). Proof. Directly from Cayley Hamilton Theorem, \\(A^n\\) can be expressed as a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). By recursion, it is easy to show that for all \\(m &gt; n\\), \\(A^m\\) is also a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). Post-multiply both sides with \\(B\\), we get what we want. Corollary B.3 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k &gt; n\\), the following equality always holds: \\[ \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix}) = \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix}) \\] Proof. First prove LHS \\(\\le\\) RHS. \\(\\forall v \\in \\mathbb{C}^n\\) such that \\[ v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B &amp; \\dots A^{k-1}B \\end{bmatrix} = 0 \\] \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) must hold. Second prove LHS \\(\\ge\\) RHS. For any \\(v \\in \\mathbb{C}^n\\) such that \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) and any \\(k &gt; n\\), by Corollary B.2, there exists a sequence \\(c_i, i = 0 \\dots n-1\\) satisfy the following: \\[ v^* A^k B = v^* \\sum_{i=0}^{n-1} c_i A^i B = 0 \\] Therefore, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = 0\\). Corollary B.4 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}\\), define \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] If \\(\\text{rank}(\\mathcal{C}) = k_1 &lt; n\\), there exist a similarity transformation \\(T\\) such that \\[ T A T^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, T B = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}, \\bar{B}_c \\in \\mathbb{C}^{k_1 \\times m}\\). Moreover, the matrix \\[\\begin{equation*} \\bar{\\mathcal{C}} := \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c \\end{bmatrix} \\end{equation*}\\] has full row rank. Proof. Since \\(\\mathcal{C}\\) is not full row rank, we pick \\(k_1\\) linearly independent columns from \\(\\mathcal{C}\\). Denote them as \\(q_1\\dots q_{k_1}\\), \\(q_i \\in \\mathbb{C}^n\\). Then, we arbitrarily set other \\(n-k_1\\) vectors \\(q_{k_1+1} \\dots q_{n}\\) as long as \\[ Q = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\] is invertible. Define the similarity transformation matrix by \\(T = Q^{-1}\\). Note that \\(A q_i\\) can be seen as a column picked from \\(A^{k} B, k \\in \\left\\{1 \\dots n\\right\\}\\), which is guaranteed to be a linear combination of \\(B, AB, \\dots, A^{n-1}B\\) from Cayley Hamilton Theorem. Thus, \\(A q_i\\) is bound to be a linear transformation of columns from \\(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = \\mathcal{C}\\). Since \\(q_1\\dots q_{k_1}\\) is the largest linearly independent column vector set from \\(\\mathcal{C}\\), this implies \\(A q_i\\) can be expressed as a linear combination of \\(q_1\\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} A Q &amp; = A T^{-1} = A \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} = T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} \\end{split} \\end{equation*}\\] Similarly, \\(B\\) itself is part of \\(\\mathcal{C}\\). Therefore, each column of \\(B\\) is naturally a linear combination of \\(q_1 \\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} B = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{split} = T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] To see \\(\\bar{\\mathcal{C}}\\) has full row rank, note that \\(\\text{rank} \\mathcal{C} = k_1\\) and \\[\\begin{equation*} \\mathcal{C} = T^{-1} \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\end{equation*}\\] Thus, \\[\\text{rank}\\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\end{bmatrix} = k_1. \\] By Corollary B.3, \\(\\text{rank}\\bar{\\mathcal{C}} = k_1\\). The following Corollary is especially useful in the study of pole assignment in the single-input-multiple-output (SIMO) LTI system. Corollary B.5 For any \\(A \\in \\mathbb{C}^{n \\times n}, b \\in \\mathbb{C}^{n}\\), if \\[\\begin{equation*} \\mathcal{C} = \\begin{bmatrix} b &amp; Ab &amp; \\dots &amp; A^{n-1}b \\end{bmatrix} \\in \\mathbb{C}^{n \\times n} \\end{equation*}\\] has full rank, then there exists a similarity transformation \\(T\\) such that \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(a_1, \\dots, a_n\\) are the coefficients of \\(A\\)’s characteristic polynomial: \\[\\begin{equation*} \\det(A - \\lambda I) = \\lambda^{n} + a_1 \\lambda^{n-1} + \\dots + a_n \\lambda \\end{equation*}\\] Proof. Since \\(\\mathcal{C}\\) is invertible, define its inverse \\[\\begin{equation*} \\mathcal{C}^{-1} = \\begin{bmatrix} M_1 \\\\ M_2 \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] where \\(M_i \\in \\mathbb{C}^{1 \\times n}\\). Then, \\[\\begin{equation*} I = \\mathcal{C}^{-1} \\mathcal{C} = \\begin{bmatrix} M_1 b &amp; M_1 Ab &amp; \\dots &amp; M_1 A^{n-1}b \\\\ M_2 b &amp; M_2 Ab &amp; \\dots &amp; M_2 A^{n-1}b \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} \\Longrightarrow \\begin{cases} M_n A^{n-1}b = 1 \\\\ M_n A^ib = 0, \\ i = 0,\\dots, n-2 \\end{cases} \\end{equation*}\\] Now we claim that the transformation matrix \\(T\\) can be constructed as follows: \\[\\begin{equation*} T = \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] We first show \\(T\\) is invertible by calculating \\(T \\mathcal{C}\\): \\[\\begin{equation*} T \\mathcal{C} = \\begin{bmatrix} M_n A^{n-1}b &amp; \\star &amp; \\dots &amp; \\star \\\\ M_n A^{n-2}b &amp; M_n A^{n-1}b &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\star &amp; \\dots &amp; \\star \\\\ 0 &amp; 1 &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\end{equation*}\\] Then we calculate \\(Tb\\) and \\(TA\\): \\[\\begin{equation*} \\begin{split} Tb &amp; = \\begin{bmatrix} M_n A^{n-1}b \\\\ M_n A^{n-2}b \\\\ \\vdots \\\\ M_n b \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ T A &amp; = \\begin{bmatrix} M_n A^n \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} = \\begin{bmatrix} -M_n \\cdot \\sum_{i=0}^{n-1} a_{n-i} A^i \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\vdots \\\\ M_n A \\\\ M_n \\end{bmatrix} = A_1 T \\end{split} \\end{equation*}\\] where the penultimate equality uses Cayley Hamilton Theorem. B.2.2 Equivalent Statements for Controllability There are a few equivalent statements to express an LTI system’s controllability that one should be familiar with: Theorem B.5 (Equivalent Statements for Controllability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((A, B)\\) is controllable. The matrix \\[ W_c(t) := \\int_{0}^{t} e^{A\\tau} B B^* e^{A^* \\tau} d\\tau \\] is positive definite for any \\(t &gt; 0\\). The controllability matrix \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; A^2 B &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] has full row rank. The matrix \\([A - \\lambda I, B]\\) has full row rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(x\\) be any eigenvalue and any corresponding left eigenvector \\(A\\), i.e., \\(x^* A = x^* \\lambda\\), then \\(x^* B \\ne 0\\). The eigenvalues of \\(A+BF\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(F\\). If, in addition, all eigenvalues of \\(A\\) have negative real parts, then the unique solution of \\[ A W_c + W_c A^* = -B B^* \\] is positive definite. The solution is called the controllability Gramian and can be expressed as \\[ W_c = \\int_{0}^{\\infty} e^{A \\tau} B B^* e^{A^* \\tau} d\\tau \\] Proof. (\\(1. \\Rightarrow 2.\\)) Prove by contradiction. Assume that \\((A, B)\\) is controllable but \\(W_c(t_1)\\) is singular for some \\(t_1 &gt; 0\\). This implies there exists a real vector \\(v \\ne 0 \\in \\mathbb{R}^n\\), s.t. \\[ v^* W_c(t_1) v = v^* (\\int_{0}^{t_1} e^{At} B B^* e^{A^*t} dt) v = \\int_{0}^{t_1} v^* (e^{At} B B^* e^{A^*t}) v \\ dt = 0 \\] Since \\(e^{At} BB^* e^{A^*t} \\succeq 0\\) for all \\(t\\), we must have \\[\\begin{equation*} \\begin{split} &amp; v^* (e^{At} B B^* e^{A^*t}) v = \\parallel v^* B e^{At} \\parallel^2 = 0, \\quad \\forall t \\in [0, t_1] \\\\ \\Longrightarrow &amp; v^* B e^{At} = 0, \\quad \\forall t \\in [0, t_1] \\end{split} \\end{equation*}\\] Setting \\(x(t_1) = 0\\), from (B.5), we have \\[ 0 = e^{A t_1} x(0) + \\int_{0}^{t_1} e^{A (t_1 - \\tau)} B u(\\tau) d\\tau = 0 \\] Pre-multiply the above equation by \\(v^*\\), then \\[ 0 = v^* e^{A t_1} x(0) \\] Since \\(x(0)\\) can be chosen arbitrarily, we set \\(x(0) = v e^{-A t_1}\\), which results in \\(v = 0\\). Contradiction! (\\(2. \\Rightarrow 1.\\)) For any \\(x(0) = x_0, t_1 &gt; 0, x(t_1) = x_1\\), since \\(W_c(t_1) \\succ 0\\), we set the control inputs as \\[ u(t) = -B^* e^{A^*(t_1 - t)} W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\] We claim that the picked \\(u(t)\\) satisfies (B.5) by \\[\\begin{equation*} \\begin{split} &amp; e^{At} x_0 + \\int_{0}^{t_1} e^{A(t_1-t)} B u(t) dt \\\\ &amp; = e^{At} x_0 - \\int_{0}^{t_1} e^{A(t_1-t)} B B^* e^{A^*(t_1-t)} dt \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; \\overset{\\tau = t_1-t}{=} e^{At} x_0 - \\underbrace{\\int_{0}^{t_1} e^{A\\tau} BB^* e^{A^*\\tau} d\\tau}_{W_c(t_1)} \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; = e^{At} x_0 - [e^{At_1} x_0 - x_1] = x_1 \\end{split} \\end{equation*}\\] (\\(2. \\Rightarrow 3.\\)) Prove by contradiction. Suppose \\(W_c(t) \\succ 0, \\forall t &gt; 0\\) but \\(\\mathcal{C}\\) is not of full row rank. Then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[ v^* A^k B = 0, \\quad k = 0 \\dots n - 1 \\] By Corollary B.2, we have \\[ v^* A^k B = 0, \\ \\forall k \\in \\mathbb{N} \\Longrightarrow v^* e^{At} B = 0, \\ \\forall t &gt; 0 \\] which implies \\[ v^* W_c(t) v = v^* (\\int_{0}^{t} e^{A\\tau} B B^* e^{A^*\\tau} d\\tau) v = 0, \\quad \\forall t &gt; 0 \\] Contradiction! (\\(3. \\Rightarrow 2.\\)) Prove by contradiction. Suppose \\(\\mathcal{C}\\) has full row rank but \\(W_c(t_1)\\) is singular at some \\(t_1 &gt; 0\\). Then, similar to the proof in (\\(1. \\Rightarrow 2.\\)), there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(F(t) := v^* e^{At} B \\equiv 0, \\forall t \\in [0, t_1]\\). Since \\(F(t)\\) is infinitely differentiable, we get its \\(i\\)’s derivative at \\(t=0\\), where \\(i = 0, 1, \\dots n-1\\). This results in \\[\\begin{equation*} \\left. \\frac{d^i F}{dt^i} \\right|_{t=0} = \\left. v^* A^{i} e^{At} B \\right|_{t=0} = v^* A^i B = 0, \\quad i = 0 \\dots n-1 \\end{equation*}\\] Thus, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\). Contradiction! (\\(3. \\Rightarrow 4.\\)) Proof by contradiction. Suppose \\([A - \\lambda I, B]\\) does not have full row rank for some \\(\\lambda \\in \\mathbb{C}\\). Then, there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(v^* [A - \\lambda I, B] = 0\\). This implies \\(v^* A = v^* \\lambda\\) and \\(v^* B = 0\\). On the other hand, \\[\\begin{equation*} v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; \\lambda B &amp; \\dots &amp; \\lambda^{n-1} B \\end{bmatrix} = 0 \\end{equation*}\\] Contradiction! (\\(4. \\Rightarrow 5.\\)) Proof by contradiction. If there exists a left eigenvector and eigenvalue pair \\((x, \\lambda)\\), s.t. \\(x^* A = \\lambda x^*\\) while \\(x^*B = 0\\), then \\(x^* [A - \\lambda I, B] = 0\\). Contradiction! (\\(5. \\Rightarrow 3.\\)) Proof by contradiction. If the controllability matrix \\(\\mathcal{C}\\) does not have full row rank, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then, from Corollary B.4, there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{R}^{k \\times k}, \\bar{A}_{\\bar{c}} \\in \\mathbb{R}^{(n-k) \\times (n-k)}\\). Now arbitrarily pick one of \\(\\bar{A}_{\\bar{c}}\\)’s left eigenvector \\(x_{\\bar{c}}\\) and its corresponding eigenvalue \\(\\lambda_1\\). Define the vector \\(x = \\begin{bmatrix} 0 \\\\ x_{\\bar{c}} \\end{bmatrix}\\). Then, \\[\\begin{equation*} \\begin{split} x^* (TAT^{-1}) = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\end{bmatrix} \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\bar{A}_{\\bar{c}} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; \\lambda_1 x_{\\bar{c}}^* \\end{bmatrix} = \\lambda_1 x^* \\\\ x^* (TB) &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{x}} \\end{bmatrix} \\begin{bmatrix} B_{\\bar{c}} \\\\ 0 \\end{bmatrix} = 0 \\end{split} \\end{equation*}\\] which implies \\((TAT^{-1}, TB)\\) is not controllable. However, similarity transformation does not change controllability. Contradiction! (\\(6. \\Rightarrow 1.\\)) Prove by contradiction. If \\((A, B)\\) is not controllable, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then from Corollary B.4, there exists a similarity transformation \\(T\\) s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] Now arbitrarily pick \\(F \\in \\mathbb{R}^{m\\times n}\\) and define \\(FT^{-1} = [F_1, F_2]\\), where \\(F_1 \\in \\mathbb{R}^{m\\times k}, F_2 \\in \\mathbb{R}^{m\\times (n-k)}\\). Thus, \\[\\begin{equation*} \\begin{split} \\text{det}(A+BF-\\lambda I) &amp; = \\text{det}\\left( T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} T + T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} F - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det}\\left( T^{-1} \\left\\{ \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} FT^{-1} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right\\} T \\right) \\\\ &amp; = \\text{det}\\left( \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\begin{bmatrix} F_1 &amp; F_2 \\end{bmatrix} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1 &amp; \\bar{A}_{12} + \\bar{B}_c F_2 \\\\ 0 &amp; \\bar{A}_{\\bar{c}} - \\lambda I_2 \\end{bmatrix} \\\\ &amp; = \\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1) \\cdot \\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2) \\end{split} \\end{equation*}\\] where \\(I_1\\) is the identity matrix of size \\(k\\). Similarly, \\(I_2\\) of size \\(n-k\\). Thus, at least \\(n-k\\) eigenvalues of \\(A+BF\\) cannot be freely assigned by choosing \\(F\\). Contradiction! (\\(1. \\Rightarrow 6.\\)) Here we only represent the SIMO case. For the MIMO case, the proof is far more complex. Interesting readers can refer to (Davison and Wonham 1968) (the shortest proof I can find). Since there is only one input, the matrix \\(B\\) degenerate to vector \\(b\\). From Corollary B.5, there exist a similarity transformation matrix \\(T\\), s.t. \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] For any \\(F \\in \\mathbb{C}^{1 \\times n}\\), denote \\(FT^{-1}\\) as \\([f_1, f_2, \\dots, f_n]\\). Calculating the characteristic polynomial of \\(A + bF\\): \\[\\begin{equation*} \\begin{split} \\text{det}(\\lambda I - A - bF) &amp; = \\text{det}(\\lambda I - T^{-1}A_1 T - T^{-1} b_1 F) \\\\ &amp; = \\text{det}(\\lambda I - A_1 - b_1 F T^{-1}) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\lambda + a_1 - f_1 &amp; \\lambda + a_2 - f_2 &amp; \\dots &amp; \\lambda + a_{n-1} - f_{n-1} &amp; \\lambda + a_n - f_n \\\\ -1 &amp; \\lambda &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; -1 &amp; \\lambda \\end{bmatrix} \\\\ &amp; = \\lambda^n + (a_1 - f_1) \\lambda^{n-1} + \\dots + (a_n - f_n) \\end{split} \\end{equation*}\\] By choosing \\([f_1, f_2, \\dots, f_n]\\), \\(A+bF\\)’s eigenvalues can be arbitrarily set. (\\(7. \\Rightarrow 1.\\)) Prove by contradiction. Assume that \\((A, B)\\) is not controllable. Then from 2., there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\) and \\(t_1 &gt; 0\\), \\[\\begin{equation*} F(t) = v^* e^{At} B = 0, \\quad \\forall t \\in [0, t_1] \\end{equation*}\\] Now consider \\(F(z) = v^* e^{Az} B, z\\in \\mathcal{C}\\), which is a vector of analytic function in complex analysis. For a arbitrary \\(t_2 \\in (0, t_1)\\), we have \\(F^{(i)}(t_2) = 0, \\forall i \\in \\mathbb{N}\\). Then, by invoking the fact from complex analysis: “Let \\(G\\) a connected open set and \\(f: G \\rightarrow \\mathbb{C}\\) be analytic, then \\(f \\equiv 0\\) on \\(G\\), if and only if there is a point \\(a \\in G\\) such that \\(f^{(i)}(a) = 0, \\forall n \\in \\mathbb{N}\\)”, we have \\(f(z) \\equiv 0, \\forall z \\in \\mathbb{C}\\). On the other hand, however, \\(W_c \\succ 0\\) implies there exists \\(t_3 &gt; 0\\), such that for the above \\(v\\), we have \\(v^* e^{At_3} B \\ne 0\\). Contradiction! (\\(1. \\Rightarrow 7.\\)) Since \\((A, B)\\) is controllable, from 2., \\(W_c(t) \\succ 0, \\forall t\\). Therefore, \\(W_c \\succ 0\\). The existence and uniqueness of the solution for \\(AW_c + W_cA^* = -BB^*\\) can be obtained directly from the proof of Theorem B.3, by setting \\(Q\\) there to be positive semidefinite. B.2.3 Duality Although controllability and observability seemingly have no direct connections from their definitions B.2 and B.3, the following theorem (Chen 1984) states their tight relations. Theorem B.6 (Theorem of Duality) The pair \\((C,A)\\) is observable if and only if \\((A^*,C^*)\\) is controllable. Proof. We first show that \\((C,A)\\) is observable if and only if the \\(n \\times n\\) matrix \\(W_o(t) = \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau}\\) is positive definite (nonsingular) for any \\(t&gt;0\\): “\\(\\Longleftarrow\\)”: From (B.5), given initial state \\(x(0)\\) and the inputs \\(u(t)\\), \\(y(t)\\) can be expressed as \\[\\begin{equation*} y(t) = Ce^{At} x(0) + C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau + Du(t) \\end{equation*}\\] Define a known function \\(\\bar{y}(t)\\) as \\(y(t) - C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau - Du(t)\\) and we will get \\[\\begin{equation*} Ce^{At} x(0) = \\bar{y}(t) \\end{equation*}\\] Pre-multiply the above equation by \\(e^{A^*t}C^*\\) and integrate it over \\([0,t_1]\\) to yield \\[\\begin{equation*} (\\int_{0}^{t_1} e^{A^*t} C^*C e^{At} dt) x(0) = W_o(t_1) x(0) = \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] Since \\(W_o(t_1) \\succ 0\\), \\[\\begin{equation*} x(0) = W_o(t_1)^{-1} \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] can be observed. “\\(\\Longrightarrow\\)”: Prove by contradiction. Suppose \\((C,A)\\) is observable but there exists \\(t_1 &gt;0\\), s.t. \\(W_o(t_1)\\) is singular. This implies there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* W_o(t_1) v = 0 \\Longrightarrow Ce^{At} v \\equiv 0, \\ \\forall t \\in [0,t_1] \\end{equation*}\\] Similar to the proof of Theorem B.5 (\\(7. \\Rightarrow 1.\\)), we can use conclusions from complex analysis to claim that \\(Ce^{At} v \\equiv 0, \\forall t &gt;0\\). On the other hand, we set \\(u(t) \\equiv 0\\), which results in \\(y(t) = Ce^{At}x(0)\\). In this case \\(x(0) = 0\\) and \\(x(0) = v \\ne 0\\) will lead to the same output responses \\(y(t)\\) over \\(t&gt;0\\), which implies \\((C,A)\\) is not observable. Contradiction! Next we show the duality of controllability and observability: From (1) we know \\((C,A)\\) is controllable if and only of \\[\\begin{equation*} \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau} d\\tau = \\int_{0}^{t} e^{(A^*)\\tau} (C^*)^* (C^*) e^{(A^*)^*\\tau} d\\tau \\end{equation*}\\] is nonsingular for all \\(t &gt;0\\). The latter is exactly the definition of \\((A^*, C^*)\\)’s controllability Gramian \\(W_c(t)\\). B.2.4 Equivalent Statements for Observability With the Theorem of Duality B.6, we can directly write down the equivalent statements of observability without any additional proofs: Theorem B.7 (Equivalent Statements for Observability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((C, A)\\) is observable. The matrix \\[\\begin{equation*} W_o(t) := \\int_{0}^{t} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] is positive definite for any \\(t&gt;0\\). The observability matrix \\[\\begin{equation*} \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^2 \\\\ \\dots \\\\ CA^{n-1} \\end{bmatrix} \\end{equation*}\\] has full column rank. The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full column rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(y\\) be any eigenvalue and any corresponding right eigenvector of \\(A\\), i.e., \\(Ay = \\lambda y\\), then \\(Cy \\ne 0\\). The eigenvalues of \\(A+LC\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(L\\). \\((A^*, C^*)\\) is controllable. If, in addition, all eigenvalues of \\(A\\) have negative parts, then the unique solution of \\[\\begin{equation*} A^* W_o + W_o A = -C^* C \\end{equation*}\\] is positive definite. The solution is called the observability Gramian and can be expressed as \\[\\begin{equation*} W_o = \\int_{0}^{\\infty} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] B.3 Stabilizability And Detectability To define stabilizability and detectability of an LTI system, we first introduce the concept of system mode, which can be naturally derived from the fifth definition of controllability B.5 (observability B.7). Definition B.4 (System Mode) \\(\\lambda\\) is a mode of an LTI system, if it is an eigenvalue of \\(A\\). The mode \\(\\lambda\\) is said to be: stable, if \\(\\text{Re}\\lambda &lt; 0\\), controllable, if \\(x^* B \\ne 0\\) for all left eigenvectors of \\(A\\) associated with \\(\\lambda\\), observable, if \\(C x \\ne 0\\) for all right eigenvectors of \\(A\\) associated with \\(\\lambda\\). Otherwise, the mode is said to be uncontrollable (unobservable). With the concept of system mode, the fifth definition of controllability B.5 (observability B.7) can be restated as An LTI system is controllable (observable) if and only if all modes are controllable (observable). Stabilizability (detectability) is defined similarly via loosening part of controllability (observability) conditions. Definition B.5 (Stabilizability) An LTI system is said to be stabilizable if all of its unstable modes are controllable. Definition B.6 (Detectability) An LTI system is said to be detectable if all of its unstable modes are observable. Like in the case of controllability and observability, duality also holds in stabilizability and detectability. Moreover, similarity transformation will not influence an LTI system’s stabilizability and detectability. B.3.1 Equivalent Statements for Stabilizability Theorem B.8 (Equivalent Statements for Stabilizability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((A,B)\\) is stabilizable. For all \\(\\lambda\\) and \\(x\\) such that \\(x^* A = \\lambda x^*\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(x^* B \\ne 0\\). The matrix \\([A-\\lambda I, B]\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(F\\) such that \\(A+BF\\) are Hurwitz. Proof. (\\(1. \\Leftrightarrow 2.\\)) Directly from stabilizability’s definition. (\\(2. \\Leftrightarrow 3.\\)) If 2. holds but 3. not hold, then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* [A-\\lambda I, B] = 0 \\Leftrightarrow v^* A = \\lambda v^*, v^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Contradiction! Vice versa. (\\(4. \\Rightarrow 2.\\)) Prove by contradiction. Suppose there \\(x \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} x^* [A-\\lambda I, B] = 0 \\Leftrightarrow x^* A = \\lambda x^*, x^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Thus, for any \\(F\\), \\[\\begin{equation*} x^* (A+BF) = \\lambda x^*, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] On the other hand, suppose \\(A+BF\\) has \\(I\\) Jordon blocks, with each equipped with an eigenvalue \\(\\eta_i, i = 1\\dots I\\) (note that \\(\\eta_\\alpha\\) may be equal to \\(\\eta_\\beta\\), i.e., they are equivalent eigenvalues with different Jordon blocks). Since \\(A+BF\\)’s eigenvalues all have negative real parts, \\(\\text{Re} (\\eta_i) &lt; 0, i = 1\\dots I\\). For each \\(\\eta_i,i \\in \\left\\{1\\dots i\\right\\}\\), denote its \\(K_i\\) generalized left eigenvectors as \\(v_{i,1}, v_{i,2}, \\dots v_{i,K_i}\\). By definition, \\(\\sum_{i=1}^{I} K_i = n\\) and \\[\\begin{equation*} \\begin{split} v_{i,1}^* (A+BF) &amp; = v_{i,1}^* \\cdot \\eta_i \\\\ v_{i,2}^* (A+BF) &amp; = v_{i,1}^* + v_{i,2}^* \\cdot \\eta_i \\\\ &amp; \\vdots \\\\ v_{i,K_i}^* (A+BF) &amp; = v_{i,K_i-1}^* + v_{i,K_i}^* \\cdot \\eta_i \\end{split} \\end{equation*}\\] for all \\(i \\in \\left\\{1\\dots i\\right\\}\\). Also, \\(v_{i,k},i=1\\dots I, k=1\\dots K_i\\) are linearly independent and spans \\(\\mathbb{C}^n\\). Therefore, \\[\\begin{equation*} x^* = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\end{equation*}\\] which leads to \\[\\begin{equation*} \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* (A+BF) = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot \\lambda \\cdot v_{i,k}^* \\end{equation*}\\] Since \\(v_{i,k}\\)’s are \\(A+BF\\)’s generalized eigenvectors, we have \\[\\begin{equation*} \\begin{split} &amp; \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\cdot (A+BF) \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\xi_{i,1} \\cdot \\eta_i \\cdot v_{i,1}^* + \\sum_{k=2}^{K_i} \\xi_{i,k} (v_{i,k-1}^* + \\eta_i \\cdot v_{i,k}^* ) \\right\\} \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} (\\xi_{i,k}\\cdot \\eta_i + \\xi_{i,k+1}) v_{i,k}^* + \\xi_{i,K_i} \\cdot \\eta_i \\cdot v_{i,K_i}^* \\right\\} \\end{split} \\end{equation*}\\] Combining the above two equations: \\[\\begin{equation*} \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} \\left[ \\xi_{i,k}\\cdot (\\eta_i - \\lambda) + \\xi_{i,k+1} \\right] v_{i,k}^* + \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) \\cdot v_{i,K_i}^* = 0 \\right\\} \\end{equation*}\\] Since \\(v_{i,k}\\)’s are linearly independent, for any \\(i \\in \\left\\{i\\dots I\\right\\}\\): \\[\\begin{equation*} \\begin{split} \\xi_{i,1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,2} &amp; = 0 \\Rightarrow \\xi_{i,2} = (-1) \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda) \\\\ \\xi_{i,2} \\cdot (\\eta_i - \\lambda) + \\xi_{i,3} &amp; = 0 \\Rightarrow \\xi_{i,3} = (-1)^2 \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^2 \\\\ &amp; \\vdots \\\\ \\xi_{i,K_i-1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,K_i} &amp; = 0 \\Rightarrow \\xi_{i,K_i} = (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i-1} \\\\ \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) &amp; = 0 \\end{split} \\end{equation*}\\] Thus, \\[\\begin{equation*} (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i} = 0 \\end{equation*}\\] Denote \\(\\xi_{i,1}\\) as \\(r_1 e^{\\theta_1}\\), \\((\\eta_i - \\lambda)\\) as \\(r_2 e^{\\theta_2}\\). Since \\(\\text{Re} \\lambda \\ge 0, \\text{Re}(\\eta_i) &lt; 0\\), \\(r_2 &gt; 0\\). On the other hand, the following equation suggests \\[\\begin{equation*} r_1 r_2^{K_i-1} e^{j[\\theta_1 + \\theta_2 (K_i-1)]} = 0 \\end{equation*}\\] Thus, \\(r_1\\) has to be \\(0\\), which implies \\(\\xi_{i,1} = 0\\). By recursion, \\(\\xi_{i,k} = 0, \\forall k = 1\\dots K_i\\). Contradiction! (\\(1. \\Rightarrow 4.\\)) If \\((A,B)\\) is controllable, then from Theorem (thm:lticontrollable)’s sixth definition, we can freely assign the poles of \\(A+BF\\) via choosing \\(F\\) properly. Otherwise, if \\((A,B)\\) is uncontrollable, then from Corollary B.4 and proof of Theorem B.5 (\\(6. \\Rightarrow 1.\\)), there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] and \\[\\begin{equation*} \\text{det}(A+BF-\\lambda I) = \\underbrace{\\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1)}_{\\chi_c(\\lambda)} \\cdot \\underbrace{\\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2)}_{\\chi_{\\bar{c}}(\\lambda)} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}\\), \\(I_1\\) identity matrix of size \\(k_1\\), \\([F_1,F_2] = FT^{-1}\\), and \\(k_1 = \\text{rank} \\mathcal{C}\\). Additionally, \\((\\bar{A}_c, \\bar{B}_c)\\) is controllable. Thus, \\(\\chi_c(\\lambda)\\)’s zeros can be freely assigned by choosing proper \\(F\\), i.e., system modes with \\(\\chi_c(\\lambda)\\) is controllable, regardless of its stability. On the other hand, system modes with \\(\\chi_{\\bar{c}}(\\lambda)\\) must be stable. Otherwise, we cannot affect it by assigning \\(F\\), which is a contradiction to statement (1). Therefore, \\((TAT^{-1}, TB)\\) is stabilizable. Since similarity transformation does not change stabilizability, \\((A,B)\\) is stabilizable. B.3.2 Equivalent Statements for Detectability Thanks to duality, we can directly write down the equivalent statements of observability without any additional proofs: Theorem B.9 (Equivalent Statements for Detectability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((C,A)\\) is detectable. For all \\(\\lambda\\) and \\(x\\) such that \\(A x = \\lambda x\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(C x \\ne 0\\). The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(L\\) such that \\(A+LC\\) are Hurwitz. \\((A^*, C^*)\\) is stabilizable. References Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. when \\(A\\) is not diagonalizable, similar results can be derived via Jordan decomposition.↩︎ "],["references.html", "References", " References Antos, András, Csaba Szepesvári, and Rémi Munos. 2007. “Fitted q-Iteration in Continuous Action-Space MDPs.” Advances in Neural Information Processing Systems 20. Arnold, William F, and Alan J Laub. 1984. “Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.” Proceedings of the IEEE 72 (12): 1746–54. Baird, Leemon et al. 1995. “Residual Algorithms: Reinforcement Learning with Function Approximation.” In Proceedings of the Twelfth International Conference on Machine Learning, 30–37. Barto, Andrew G, Richard S Sutton, and Charles W Anderson. 2012. “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.” IEEE Transactions on Systems, Man, and Cybernetics, no. 5: 834–46. Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Choset, Howie, Kevin M Lynch, Seth Hutchinson, George A Kantor, and Wolfram Burgard. 2005. Principles of Robot Motion: Theory, Algorithms, and Implementations. MIT press. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Fan, Jianqing, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. 2020. “A Theoretical Analysis of Deep q-Learning.” In Learning for Dynamics and Control, 486–89. PMLR. Garrigos, Guillaume, and Robert M Gower. 2023. “Handbook of Convergence Theorems for (Stochastic) Gradient Methods.” arXiv Preprint arXiv:2301.11235. Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” In International Conference on Machine Learning, 1861–70. Pmlr. Janner, Michael, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. “When to Trust Your Model: Model-Based Policy Optimization.” Advances in Neural Information Processing Systems 32. Kakade, Sham M. 2001. “A Natural Policy Gradient.” Advances in Neural Information Processing Systems 14. Kang, Shucheng, Xiaoyang Xu, Jay Sarva, Ling Liang, and Heng Yang. 2024. “Fast and Certifiable Trajectory Optimization.” In International Workshop on the Algorithmic Foundations of Robotics. Kearns, Michael J, and Satinder Singh. 2000. “Bias-Variance Error Bounds for Temporal Difference Updates.” In COLT, 142–47. LaValle, Steven M. 2006. Planning Algorithms. Cambridge university press. Lillicrap, Timothy P, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. “Continuous Control with Deep Reinforcement Learning.” arXiv Preprint arXiv:1509.02971. Liu, Dong C, and Jorge Nocedal. 1989. “On the Limited Memory BFGS Method for Large Scale Optimization.” Mathematical Programming 45 (1): 503–28. Mahmood, A Rupam, Huizhen Yu, Martha White, and Richard S Sutton. 2015. “Emphatic Temporal-Difference Learning.” arXiv Preprint arXiv:1507.01569. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. Munos, Rémi, and Csaba Szepesvári. 2008. “Finite-Time Bounds for Fitted Value Iteration.” Journal of Machine Learning Research 9 (5). Nesterov, Yurii. 2018. Lectures on Convex Optimization. Vol. 137. Springer. Nocedal, Jorge, and Stephen J Wright. 1999. Numerical Optimization. Springer. Rawlings, James Blake, David Q Mayne, and Moritz Diehl. 2020. Model Predictive Control: Theory, Computation, and Design. Vol. 2. Nob Hill Publishing Madison, WI. Riedmiller, Martin. 2005. “Neural Fitted q Iteration–First Experiences with a Data Efficient Neural Reinforcement Learning Method.” In European Conference on Machine Learning, 317–28. Springer. Robbins, Herbert, and David Siegmund. 1971. “A Convergence Theorem for Non Negative Almost Supermartingales and Some Applications.” In Optimizing Methods in Statistics, 233–57. Elsevier. Schulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. “Trust Region Policy Optimization.” In International Conference on Machine Learning, 1889–97. PMLR. Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. “High-Dimensional Continuous Control Using Generalized Advantage Estimation.” arXiv Preprint arXiv:1506.02438. Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. “Proximal Policy Optimization Algorithms.” arXiv Preprint arXiv:1707.06347. Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In International Conference on Machine Learning, 387–95. Pmlr. Sutton, Richard S, and Andrew G Barto. 1998. Reinforcement Learning: An Introduction. Vol. 1. 1. MIT press Cambridge. Sutton, Richard S, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári, and Eric Wiewiora. 2009. “Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation.” In Proceedings of the 26th Annual International Conference on Machine Learning, 993–1000. Sutton, Richard S, Csaba Szepesvári, and Hamid Reza Maei. 2008. “A Convergent o(n) Algorithm for Off-Policy Temporal-Difference Learning with Linear Function Approximation.” Advances in Neural Information Processing Systems 21 (21): 1609–16. Wächter, Andreas, and Lorenz T Biegler. 2006. “On the Implementation of an Interior-Point Filter Line-Search Algorithm for Large-Scale Nonlinear Programming.” Mathematical Programming 106 (1): 25–57. Williams, Grady, Paul Drews, Brian Goldfain, James M Rehg, and Evangelos A Theodorou. 2016. “Aggressive Driving with Model Predictive Path Integral Control.” In 2016 IEEE International Conference on Robotics and Automation (ICRA), 1433–40. IEEE. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
