[["index.html", "Optimal Control and Reinforcement Learning Preface Feedback Offerings", " Optimal Control and Reinforcement Learning Heng Yang 2025-03-08 Preface This is the textbook for Harvard ES/AM 158: Introduction to Optimal Control and Reinforcement Learning. Feedback I would like to invite you to provide feedback to the textbook via inline comments with Hypothesis: Go to Hypothesis and create an account Install the Chrome extension of Hypothesis Provide public comments to textbook contents and I will try to address them Offerings 2025 Fall Time: Mon/Wed 2:15 - 3:30pm Location: TBD Instructor: Heng Yang Teaching Fellow: TBD Syllabus 2023 Fall The course was previously offered as Introduction to Optimal Control and Estimation. Starting Fall 2025, contents about reinforcement learning have been added to the course. "],["formulation.html", "Chapter 1 The Optimal Control Formulation 1.1 The Basic Problem 1.2 Dynamic Programming and Principle of Optimality 1.3 Infinite-horizon Formulation", " Chapter 1 The Optimal Control Formulation 1.1 The Basic Problem Consider a discrete-time dynamical system \\[\\begin{equation} x_{k+1} = f_k (x_k, u_k, w_k), \\quad k =0,1,\\dots,N-1 \\tag{1.1} \\end{equation}\\] where \\(x_k \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) is the state of the system, \\(u_k \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) is the control we wish to design, \\(w_k \\in \\mathbb{W} \\subseteq \\mathbb{R}^p\\) a random disturbance or noise (e.g., due to unmodelled dynamics) which is described by a probability distribution \\(P_k(\\cdot \\mid x_k, u_k)\\) that may depend on \\(x_k\\) and \\(u_k\\) but not on prior disturbances \\(w_0,\\dots,w_{k-1}\\), \\(k\\) indexes the discrete time, \\(N\\) denotes the horizon, \\(f_k\\) models the transition function of the system (typically \\(f_k \\equiv f\\) is time-invariant, especially for robotics systems; we use \\(f_k\\) here to keep full generality). Remark (Deterministic v.s. Stochastic). When \\(w_k \\equiv 0\\) for all \\(k\\), we say the system (1.1) is deterministic; otherwise we say the system is stochastic. In the following we will deal with the stochastic case, but most of the methodology should carry over to the deterministic setup. We consider the class of controllers (also called policies) that consist of a sequence of functions \\[ \\pi = \\{ \\mu_0,\\dots,\\mu_{N-1} \\}, \\] where \\(\\mu_k (x_k) \\in \\mathbb{U}\\) for all \\(x_k\\), i.e., \\(\\mu_k\\) is a feedback controller that maps the state to an admissible control. Given an initial state \\(x_0\\) and an admissible policy \\(\\pi\\), the state trajectory of the system is a sequence of random variables that evolve according to \\[\\begin{equation} x_{k+1} = f_k(x_k,\\mu_k(x_k),w_k), \\quad k=0,\\dots,N-1 \\tag{1.2} \\end{equation}\\] where the randomness comes from the disturbance \\(w_k\\). We assume the state-control trajectory \\(\\{u_k\\}_{k=0}^{N-1}\\) and \\(\\{x_k \\}_{k=0}^{N}\\) induce an additive cost \\[\\begin{equation} g_N(x_N) + \\sum_{k=0}^{N-1} g_k(x_k,u_k) \\tag{1.3} \\end{equation}\\] where \\(g_k,k=0,\\dots,N\\) are some user-designed functions. With (1.2) and (1.3), for any admissible policy \\(\\pi\\), we denote its induced expected cost with initial state \\(x_0\\) as \\[\\begin{equation} J_\\pi (x_0) = \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=0}^{N-1} g_k (x_k, \\mu_k(x_k)) \\right\\}, \\tag{1.4} \\end{equation}\\] where the expectation is taken over the randomness of \\(w_k\\). Definition 1.1 (Discrete-time, Finite-horizon Optimal Control) Find the best admissible controller that minimizes the expected cost in (1.4) \\[\\begin{equation} \\pi^\\star \\in \\arg\\min_{\\pi \\in \\Pi} J_\\pi(x_0), \\end{equation}\\] where \\(\\Pi\\) is the set of all admissible controllers. The cost attained by the optimal controller, i.e., \\(J^\\star = J_{\\pi^\\star}(x_0)\\) is called the optimal cost-to-go, or the optimal value function. Remark (Open-loop v.s. Closed-loop). An important feature of the basic problem in Definition 1.1 is that the problem seeks feedback policies, instead of numerical values of the controls, i.e., \\(u_k = \\mu_k(x_k)\\) is in general a function of the state \\(x_k\\). In other words, the controls are executed sequentially, one at a time after observing the state at each time. This is called closed-loop control, and is in general better than open-loop control \\[ \\min_{u_0,\\dots,u_{N-1}} \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=0}^{N-1} g_k (x_k, u_k) \\right\\} \\] where all the controls are planned at \\(k=0\\). Intuitively, a closed-loop policy is able to utilize the extra information received at each timestep (i.e., it observes \\(x_{k+1}\\) and hence also observes the disturbance \\(w_k\\)) to obtain a lower cost than an open-loop controller. Example 1.2.1 in (Bertsekas 2012) gives a concrete application where a closed-loop policy attains a lower cost than an open-loop policy. In deterministic control (i.e., when \\(w_k \\equiv 0,\\forall k\\)), however, a closed-loop policy has no advantage over an open-loop controller. This is obvious because at \\(k=0\\), even the open-loop controller predicts perfectly the consequences of all its actions and there is no extra information to be observed at later time steps. In fact, even in stochastic problems, a closed-loop policy may not be advantageous, see Exercise 1.27 in (Bertsekas 2012). 1.2 Dynamic Programming and Principle of Optimality We now introduce a general and powerful algorithm, namely dynamic programming (DP), for solving the optimal control problem 1.1. The DP algorithm builds upon a quite simple intuition called the Bellman principle of optimality. Theorem 1.1 (Bellman Principle of Optimality) Let \\(\\pi^\\star = \\{ \\mu_0^\\star,\\mu_1^\\star,\\dots,\\mu_{N-1}^\\star \\}\\) be an optimal policy for the optimal control problem 1.1. Assume that when using \\(\\pi^\\star\\), a given state \\(x_i\\) occurs at timestep \\(i\\) with positive probability (i.e., \\(x_i\\) is reachable at time \\(i\\)). Now consider the following subproblem where we are at \\(x_i\\) at time \\(i\\) and wish to minimize the cost-to-go from time \\(i\\) to time \\(N\\) \\[ \\min_{\\mu_i,\\dots,\\mu_{N-1}} \\mathbb{E} \\left\\{ g_N(x_N) + \\sum_{k=i}^{N-1} g_k (x_k, \\mu_k(x_k)) \\right\\}. \\] Then the truncated policy \\(\\{\\mu^\\star_i,\\mu^\\star_{i+1},\\dots, \\mu^\\star_{N-1}\\}\\) must be optimal for the subproblem. Theorem 1.1 can be proved intuitively by contradiction: if the truncated policy \\(\\{\\mu^\\star_i,\\mu^\\star_{i+1},\\dots, \\mu^\\star_{N-1}\\}\\) is not optimal for the subproblem, say there exists a different policy \\(\\{\\mu_i&#39;,\\mu_{i+1}&#39;,\\dots, \\mu_{N-1}&#39;\\}\\) that attains a lower cost for the subproblem starting at \\(x_i\\) at time \\(i\\). Then the combined policy \\(\\{\\mu_0^\\star,\\dots,\\mu^\\star_{i-1},\\mu_i&#39;,\\dots,\\mu_{N-1}&#39;\\}\\) must attain a lower cost for the original optimal control problem 1.1 due to the additive cost structure, contradicting the optimality of \\(\\pi^\\star\\). The Bellman principle of optimality is more than just a principle, it is also an algorithm. It suggests that, to build an optimal policy, one can start by solving the last-stage subproblem to obtain \\(\\{\\mu^\\star_{N-1} \\}\\), and then proceed to solve the subproblem containing the last two stages to obtain \\(\\{ \\mu^\\star_{N-2},\\mu^\\star_{N-1} \\}\\). The recursion continues until optimal policies at all stages are computed. The following theorem formalizes this concept. Theorem 1.2 (Dynamic Programming) The optimal value function \\(J^\\star(x_0)\\) of the optimal control problem 1.1 (starting from any given initial condition \\(x_0\\)) is equal to \\(J_0(x_0)\\), which can be computed backwards and recursively as \\[\\begin{align} J_N(x_N) &amp;= g_N(x_N) \\\\ J_k(x_k) &amp;= \\min_{u_k \\in \\mathbb{U}} \\displaystyle \\mathbb{E}_{w_k \\sim P_k(\\cdot \\mid x_k, u_k)} \\displaystyle \\left\\{ g_k(x_k,u_k) + J_{k+1}(f_k(x_k,u_k,w_k) ) \\right\\}, \\ k=N-1,\\dots,1,0. \\tag{1.5} \\end{align}\\] Moreover, if \\(u_k^\\star = \\mu_k^\\star(x_k)\\) is a minimizer of (1.5) for every \\(x_k\\), then the policy \\(\\pi^\\star = \\{\\mu_0^\\star,\\dots,\\mu_{N-1}^\\star \\}\\) is optimal. Proof. For any admissible policy \\(\\pi = \\{ \\mu_0,\\dots,\\mu_{N-1} \\}\\), denote \\(\\pi^k = \\{ \\mu_k,\\dots,\\mu_{N-1} \\}\\) the last-\\((N-k)\\)-stage truncated policy. Consider the subproblem consisting of the last \\(N-k\\) stages starting from \\(x_k\\), and let \\(J^\\star_k(x_k)\\) be its optimal cost-to-go. Mathematically, this is \\[\\begin{equation} J^\\star_{k}(x_k) = \\min_{\\pi^k} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k}^{N-1} g_i (x_i,\\mu_i(x_i)) \\right\\}, \\quad k=0,1,\\dots,N-1. \\tag{1.6} \\end{equation}\\] We define \\(J^\\star_N(x_N) = g(x_N)\\) for \\(k=N\\). Our goal is to prove the \\(J_k(x_k)\\) computed by dynamic programming from (1.5) is equal to \\(J^\\star_k (x_k)\\) for all \\(k=0,\\dots,N\\). We will prove this by induction. Firstly, we already have \\(J^\\star_N(x_N) = J_N(x_N) = g(x_N)\\), so \\(k=N\\) holds automatically. Now we assume \\(J^\\star_{k+1}(x_{k+1}) = J_{k+1}(x_{k+1})\\) for all \\(x_{k+1}\\), and we wish to induce \\(J^\\star_{k}(x_{k}) = J_{k}(x_{k})\\). To show this, we write \\[\\begin{align} \\hspace{-16mm} J^\\star_{k}(x_k) &amp;= \\min_{\\pi^k} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k}^{N-1} g_i (x_i,\\mu_i(x_i)) \\right\\} \\tag{1.7}\\\\ &amp;= \\min_{\\mu_k,\\pi^{k+1}} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_k(x_k,\\mu_k(x_k)) + g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\} \\tag{1.8}\\\\ &amp;= \\min_{\\mu_k} \\left[ \\min_{\\pi^{k+1}} \\mathbb{E}_{w_k,\\dots,w_{N-1}} \\left\\{ g_k(x_k,\\mu_k(x_k)) + g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\}\\right] \\tag{1.9}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + \\min_{\\pi^{k+1}} \\left[ \\mathbb{E}_{w_{k+1},\\dots,w_{N-1}} \\left\\{ g_N(x_N) + \\sum_{i=k+1}^{N-1} g_i(x_i,\\mu_i(x_i)) \\right\\} \\right] \\right\\} \\tag{1.10}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J^\\star_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.11}\\\\ &amp;= \\min_{\\mu_k} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.12}\\\\ &amp;= \\min_{u_k \\in \\mathbb{U}} \\mathbb{E}_{w_k} \\left\\{ g_k(x_k,\\mu_k(x_k)) + J_{k+1}(f_k(x_k,\\mu_k(x_k),w_k)) \\right\\} \\tag{1.13}\\\\ &amp;= J_k(x_k), \\tag{1.14} \\end{align}\\] where (1.7) follows from definition (1.6); (1.8) expands \\(\\pi^k = \\{ \\mu_k, \\pi^{k+1}\\}\\) and \\(\\sum_{i=k}^{N-1} g_i = g_k + \\sum_{i=k+1}^{N-1}\\); (1.9) writes the joint minimization over \\(\\mu_k\\) and \\(\\pi^{k+1}\\) as equivalently first minimizing over \\(\\pi^{k+1}\\) and then minimizing over \\(\\mu_k\\); (1.10) is the key step and holds because \\(g_k\\) and \\(w_k\\) depend only on \\(\\mu_k\\) but not on \\(\\pi^{k+1}\\); (1.11) follows again from definition (1.6) with \\(k\\) replaced by \\(k+1\\); (1.12) results from the induction assumption; (1.13) clearly holds because any \\(\\mu_k(x_k)\\) belongs to \\(\\mathbb{U}\\) and any element in \\(\\mathbb{U}\\) can be chosen by a feedback controller \\(\\mu_k\\); and lastly (1.14) follows from the dynamic programming algorithm (1.5). By induction, this shows that \\(J^\\star_k(x_k) = J_k(x_k)\\) for all \\(k=0,\\dots,N\\). The careful reader, especially from a robotics background, may soon become disappointed when seeing the DP algorithm (1.5) because it is rather conceptual than practical. To see this, we only need to run DP for \\(k=N-1\\): \\[\\begin{equation} J_{N-1}(x_{N-1}) = \\min_{u_{N-1} \\in \\mathbb{U}} \\mathbb{E}_{w_{N-1}} \\left\\{ g_{N-1}(x_{N-1},u_{N-1}) + J_N(f_{N-1}(x_{N-1},u_{N-1},w_{N-1})) \\right\\}. \\tag{1.15} \\end{equation}\\] Two challenges immediately show up: How to perform the minimization over \\(u_{N-1}\\) when \\(\\mathbb{U}\\) is a continuous constraint set? Even if we assume \\(g_{N-1}\\) is convex1 in \\(u_{N-1}\\), \\(J_N\\) is convex in \\(x_{N}\\), and the dynamics \\(f_{N-1}\\) is also convex in \\(u_{N-1}\\) (so that the optimization (1.15) is convex), we may be able to solve the minimization numerically for each \\(x_{N-1}\\) using a convex optimization solver, but rarely will we be able to find an analytical policy \\(\\mu_{N-1}^\\star\\) such that \\(u_{N-1}^\\star = \\mu_{N-1}^\\star (x_{N-1})\\) for every \\(x_{N-1}\\) (i.e., the optimal policy \\(\\mu_{N-1}^\\star\\) is implict but not explict). Suppose we can find an anlytical optimal policy \\(\\mu_{N-1}^\\star\\), say \\(\\mu_{N-1}^\\star = K x_{N-1}\\) a linear policy, how will plugging \\(\\mu_{N-1}^\\star\\) into (1.15) affect the complexity of \\(J_{N-1}(x_{N-1})\\)? One can see that even if \\(\\mu_{N-1}^\\star\\) is linear in \\(x_{N-1}\\), \\(J_{N-1}\\) may be highly nonlinear in \\(x_{N-1}\\) due to the composition with \\(g_{N-1}\\), \\(f_{N-1}\\) and \\(J_N\\). If \\(J_{N-1}(x_{N-1})\\) becomes too complex, then clearly it becomes more challenging to perform (1.15) for the next step \\(k=N-2\\). Due to these challenges, only in a very limited amount of cases will we be able to perform exact dynamic programming. For example, when the state space \\(\\mathbb{X}\\) and control space \\(\\mathbb{U}\\) are discrete, we can design efficient algorithms for exact DP. For another example, when the dynamics \\(f_k\\) is linear and the cost \\(g_k\\) is quadratic, we will also be able to compute \\(J_k(x_k)\\) in closed form (though this sounds a bit surprising!). We will study these problems in more details in Chapter 2. For general optimal control problems with continuous state space and control space (and most problems we care about in robotics), unfortunately, we will have to resort to approximate dynamic programming, basically variations of the DP algorithm (1.5) where approximate value functions \\(J_k(x_k)\\) and/or control policies \\(\\mu_k(x_k)\\) are used (e.g., with neural networks and machine learning).2 We will introduce several popular approximation schemes in Chapter 3. We will see that, although exact DP is not possible anymore, the Bellman principle of optimality still remains one of the most important guidelines for designing approximation algorithms. Efficient algorithms for approximate dynamic programming, preferrably with performance guarantees, still remain an active area of research. 1.3 Infinite-horizon Formulation So far we are focusing on problems with a finite horizon \\(N\\), what if the horizon \\(N\\) tends to infinity? In particular, consider the controller \\(\\pi\\) now contains an infinite sequence of functions \\[ \\pi = \\{ \\mu_0,\\dots \\} \\] and let us try to find the best policy that minimizes the cost-to-go starting from \\(x_0\\) subject to the same dynamics as in (1.1) (with \\(N\\) tends to infinity and \\(f_k \\equiv f\\)) \\[\\begin{equation} J_{\\pi}(x_0) = \\mathbb{E} \\left\\{ \\sum_{k=0}^{\\infty} g(x_k, \\mu_k(x_k)) \\right\\} \\tag{1.16}, \\end{equation}\\] where the expectation is taken over the (infinite number of) disturbances \\(\\{w_0,\\dots \\}\\). We can write (1.16) equivalently as \\[ J_{\\pi}(x_0) = \\lim_{N \\rightarrow \\infty} J_\\pi^N(x_0), \\] where, with a slight abuse of notation, \\(J_\\pi^N(x_0)\\) is (1.4) with \\(g_N(x_N)\\) set to zero. Now we invoke the dynamic programming algorithm in Theorem 1.2. We will first set \\(J_N(x_N) = g_N(x_N)=0\\), and then compute backwards in time \\[ J_k(x_k) = \\min_{u_k \\in \\mathbb{U}} \\mathbb{E}_{w_k} \\left\\{ g(x_k,u_k) + J_{k+1}(f(x_k,u_k,w_k)) \\right\\}, \\quad k=N-1,\\dots,0. \\] To make our presentation easier later, the above DP iterations are equivalent to \\[\\begin{align} J_0(x_0) &amp;= 0 \\\\ J_{k+1}(x_{k+1}) &amp;= \\min_{u_k \\in \\mathbb{U}} \\mathbb{E}_{w_k} \\left\\{ g(x_k,u_k) + J_k(f(x_k,u_k,w_k)) \\right\\}, \\quad k=0,\\dots,N, \\tag{1.17} \\end{align}\\] where I have done nothing but reversed the time indexing. Observe that when \\(N \\rightarrow \\infty\\), (1.17) performs the recursion an infinite number of times. We may want to conjecture three natural consequences of the infinite-horizon solution: The optimal infinite-horizon cost is the limit of the corresponding \\(N\\)-stage optimal cost as \\(N \\rightarrow \\infty\\), i.e., \\[ J^\\star(x) = \\lim_{N \\rightarrow \\infty} J_N(x_N), \\] where \\(J_N(x_N)\\) is computed from DP (1.17). Bacause \\(J^\\star\\) is the result of DP (1.17) when \\(N\\) tends to infinity, if the DP algorithm converges to \\(J^\\star\\), then \\(J^\\star\\) should satisfy \\[\\begin{equation} J^\\star(x) = \\min_{u \\in \\mathbb{U}} \\mathbb{E}_w \\left\\{ g(x,u) + J^\\star(f(x,u,w)) \\right\\}, \\quad \\forall x \\tag{1.18} \\end{equation}\\] Note that (1.18) is an equation that \\(J^\\star(x)\\) should satisfy for all \\(x\\). In fact, this is called the Bellman Optimality Equation. If \\(\\mu(x)\\) satisfies the Bellman equation (1.18), i.e., \\(u = \\mu(x)\\) minimizes the right-hand side of (1.18) for any \\(x\\), then the policy \\(\\pi = \\{\\mu,\\mu,\\dots \\}\\) should be optimal. This is saying, the optimal policy is time-invariant. In fact, all of our conjectures above are true, for most infinite-horizon problems. For example, in Chapter 2.2, we will investigate the Markov Decision Process (MDP) formulation, under which the above conjectures all hold. However, one should know that there also exist many infinite-horizon problems where our conjectures will fail, and there are many mathematical subtleties in rigorously proving the conjectures. The reader should see why it can be more convenient to study the infinite-horizon formulation: (i) the optimal cost-to-go is only a function of the state \\(x\\), but not a function of timestep \\(k\\); (ii) the optimal policy is time-invariant and easier to implement. Value Iteration. The Bellman optimality equation (1.18) also suggests a natural algorithm for computing \\(J^\\star(x)\\). We start with \\(J(x)\\) being all zero, and then iteratively update \\(J(x)\\) by performing the right-hand side of (1.18). This is the famous value iteration algorithm. We will study it in Chapter 2.2. As practitioners, we may simply execute the dynamic programming (value iteration) algoithm without carefully checking if our problem satisfies the assumptions. If the algorithm converges, oftentimes the problem indeed satisfies the assumptions. Otherwise, the algorithm may fail to converge, as we will see in Example 2.3. References ———. 2012. Dynamic Programming and Optimal Control: Volume i. Vol. 1. Athena scientific. You may want to read Appendix B if this is your first time seeing “convex” things.↩︎ Another possible solution is to discretize continuous states and controls. However, when the dimension of state and control is high, discretization becomes too expensive in terms of memory and computational complexity.↩︎ "],["exactdp.html", "Chapter 2 Exact Dynamic Programming 2.1 Linear Quadratic Regulator 2.2 Markov Decision Process", " Chapter 2 Exact Dynamic Programming In Chapter 1, we introduced the basic formulation of the finite-horizon and discrete-time optimal control problem, presented the Bellman principle of optimality, and derived the dynamic programming (DP) algorithm. We mentioned that, despite being a general-purpose algorithm, it can be difficult to implement DP exactly in practical applications. In this Chapter, we will introduce two problem setups where DP can in fact be implemented exactly. 2.1 Linear Quadratic Regulator Consider a linear discrete-time dynamical system \\[\\begin{equation} x_{k+1} = A_k x_k + B_k u_k + w_k, \\quad k=0,1,\\dots,N-1, \\tag{2.1} \\end{equation}\\] where \\(x_k \\in \\mathbb{R}^n\\) the state, \\(u_k \\in \\mathbb{R}^m\\) the control, \\(w_k \\in \\mathbb{R}^n\\) the independent, zero-mean disturbance with given probability distribution that does not depend on \\(x_k,u_k\\), and \\(A_k \\in \\mathbb{R}^{n \\times n}, B_k \\in \\mathbb{R}^{n \\times m}\\) are known matrices determining the transition dynamics. We want to solve the following optimal control problem \\[\\begin{equation} \\min_{\\mu_0,\\dots,\\mu_{N-1}} \\mathbb{E} \\left\\{ x_N^T Q_N x_N + \\sum_{k=0}^{N-1} \\left( x_k^T Q_k x_k + u_k^T R_k u_k \\right) \\right\\}, \\tag{2.2} \\end{equation}\\] where the expectation is taken over the randomness in \\(w_0,\\dots,w_{N-1}\\). In (2.2), \\(\\{Q_k \\}_{k=0}^N\\) are positive semidefinite matrices, and \\(\\{ R_k \\}_{k=0}^{N-1}\\) are positive definite matrices. The formulation (2.2) is typically known as the linear quadratic regulator (LQR) problem because the dynamics is linear, the cost is quadratic, and the formulation can be considered to “regulate” the system around the origin \\(x=0\\). We will now show that the DP algorithm in Theorem 1.2 can be exactly implemented for LQR. The DP algorithm computes the optimal cost-to-go backwards in time. The terminal cost is \\[ J_N(x_N) = x_N^T Q_N x_N \\] by definition. The optimal cost-to-go at time \\(N-1\\) is equal to \\[\\begin{equation} \\begin{split} J_{N-1}(x_{N-1}) = \\min_{u_{N-1}} \\mathbb{E}_{w_{N-1}} \\{ x_{N-1}^T Q_{N-1} x_{N-1} + u_{N-1}^T R_{N-1} u_{N-1} + \\\\ \\Vert \\underbrace{A_{N-1} x_{N-1} + B_{N-1} u_{N-1} + w_{N-1} }_{x_N} \\Vert^2_{Q_N} \\} \\end{split} \\tag{2.3} \\end{equation}\\] where \\(\\Vert v \\Vert_Q^2 = v^T Q v\\) for \\(Q \\succeq 0\\). Now observe that the objective in (2.3) is \\[\\begin{equation} \\begin{split} x_{N-1}^T Q_{N-1} x_{N-1} + u_{N-1}^T R_{N-1} u_{N-1} + \\Vert A_{N-1} x_{N-1} + B_{N-1} u_{N-1} \\Vert_{Q_N}^2 + \\\\ \\mathbb{E}_{w_{N-1}} \\left[ 2(A_{N-1} x_{N-1} + B_{N-1} u_{N-1} )^T Q_{N-1} w_{N-1} \\right] + \\\\ \\mathbb{E}_{w_{N-1}} \\left[ w_{N-1}^T Q_N w_{N-1} \\right] \\end{split} \\end{equation}\\] where the second line is zero due to \\(\\mathbb{E}(w_{N-1}) = 0\\) and the third line is a constant with respect to \\(u_{N-1}\\). Consequently, the optimal control \\(u_{N-1}^\\star\\) can be computed by setting the derivative of the objective with respect to \\(u_{N-1}\\) equal to zero \\[\\begin{equation} u_{N-1}^\\star = - \\left[ \\left( R_{N-1} + B_{N-1}^T Q_N B_{N-1} \\right)^{-1} B_{N-1}^T Q_N A_{N-1} \\right] x_{N-1}. \\tag{2.4} \\end{equation}\\] Plugging the optimal controller \\(u^\\star_{N-1}\\) back to the objective of (2.3) leads to \\[\\begin{equation} J_{N-1}(x_{N-1}) = x_{N-1}^T S_{N-1} x_{N-1} + \\mathbb{E} \\left[ w_{N-1}^T Q_N w_{N-1} \\right], \\tag{2.5} \\end{equation}\\] with \\[ S_{N-1} = Q_{N-1} + A_{N-1}^T \\left[ Q_N - Q_N B_{N-1} \\left( R_{N-1} + B_{N-1}^T Q_N B_{N-1} \\right)^{-1} B_{N-1}^T Q_N \\right] A_{N-1}. \\] We note that \\(S_{N-1}\\) is positive semidefinite (this is an exercise for you to convince yourself). Now we realize that something surprising and nice has happened. The optimal controller \\(u^{\\star}_{N-1}\\) in (2.4) is a linear feedback policy of the state \\(x_{N-1}\\), and The optimal cost-to-go \\(J_{N-1}(x_{N-1})\\) in (2.5) is quadratic in \\(x_{N-1}\\), just the same as \\(J_{N}(x_N)\\). This implies that, if we continue to compute the optimal cost-to-go at time \\(N-2\\), we will again compute a linear optimal controller and a quadratic optimal cost-to-go. This is the rare nice property for the LQR problem, that is, The (representation) complexity of the optimal controller and cost-to-go does not grow as we run the DP recursion backwards in time. We summarize the solution for the LQR problem (2.2) as follows. Proposition 2.1 (Solution of Discrete-Time Finite-Horizon LQR) The optimal controller for the LQR problem (2.2) is a linear state-feedback policy \\[\\begin{equation} \\mu_k^\\star(x_k) = - K_k x_k, \\quad k=0,\\dots,N-1. \\tag{2.6} \\end{equation}\\] The gain matrix \\(K_k\\) can be computed as \\[ K_k = \\left( R_k + B_k^T S_{k+1} B_k \\right)^{-1} B_k^T S_{k+1} A_k, \\] where the matrix \\(S_k\\) satisfies the following backwards recursion \\[\\begin{equation} \\hspace{-6mm} \\begin{split} S_N &amp;= Q_N \\\\ S_k &amp;= Q_k + A_k^T \\left[ S_{k+1} - S_{k+1}B_k \\left( R_k + B_k^T S_{k+1} B_k \\right)^{-1} B_k^T S_{k+1} \\right] A_k, k=N-1,\\dots,0. \\end{split} \\tag{2.7} \\end{equation}\\] The optimal cost-to-go is given by \\[ J_0(x_0) = x_0^T S_0 x_0 + \\sum_{k=0}^{N-1} \\mathbb{E} \\left[ w_k^T S_{k+1} w_k\\right]. \\] The recursion (2.7) is called the discrete-time Riccati equation. Proposition 2.1 states that, to evaluate the optimal policy (2.6), one can first run the backwards Riccati equation (2.7) to compute all the positive definite matrices \\(S_k\\), and then compute the gain matrices \\(K_k\\). For systems of reasonable dimensions, evalutating the matrix inversion in (2.7) should be fairly efficient. 2.1.1 Infinite-Horizon LQR In many robotics applications, it is often more useful to study the infinite-horizon LQR problem \\[\\begin{align} \\min_{u_k} &amp; \\quad \\sum_{k=0}^{\\infty} \\left( x_k^T Q x_k + u_k^T R u_k \\right) \\tag{2.8} \\\\ \\text{subject to} &amp; \\quad x_{k+1} = A x_k + B u_k, \\quad k=0,\\dots,\\infty, \\tag{2.9} \\end{align}\\] where \\(Q \\succeq 0\\), \\(R \\succ 0\\), and \\(A,B\\) are constant matrices. The reason for studying the formulation (2.8) is twofold. First, for nonlinear systems, we often linearize the nonlinear dynamics around an (equilibrium) point we care about, leading to constant \\(A\\) and \\(B\\) matrices. Second, we care more about the asymptotic effect of our controller than its behavior in a fixed number of steps. We will soon see an example of this formulation for balancing a simple pendulum. The infinite-horizon formulation is essentially the finite-horizon formulation (2.2) with \\(N \\rightarrow \\infty\\). Based on our intuition in deriving the finite-horizon LQR solution, we may want to hypothesize that the optimal cost-to-go is a quadratic function \\[\\begin{equation} J_{k}(x_{k}) = x_{k}^T S x_{k}, k=0,\\dots,\\infty \\tag{2.10} \\end{equation}\\] for some positive definite matrix \\(S\\), and proceed to invoke the DP algorithm. Notice that we hypothesize the matrix \\(S\\) is in fact stationary, i.e., it does not change with respect to time. This hypothesis makes sense because the \\(A,B,Q,R\\) matrices are stationary in the formulation (2.8). Invoking the DP algorithm we have \\[\\begin{equation} x_k^T S x_k = J_k(x_k) = \\min_{u_k} \\left\\{ x_k^T Q x_k + u_k^T R u_k + \\Vert \\underbrace{A x_k + B u_k}_{x_{k+1}} \\Vert_S^2 \\right\\}. \\tag{2.11} \\end{equation}\\] The minimization over \\(u_k\\) in (2.11) can again be solved in closed-form by setting the gradient of the objective with respect to \\(u_k\\) to be zero \\[\\begin{equation} u_k^\\star = - \\underbrace{\\left[ \\left( R + B^T S B \\right)^{-1} B^T S A \\right]}_{K} x_k. \\tag{2.12} \\end{equation}\\] Plugging the optimal \\(u_k^\\star\\) back into (2.11), we see that the matrix \\(S\\) has to satisfy the following equation \\[\\begin{equation} S = Q + A^T \\left[ S - SB \\left( R + B^T S B \\right)^{-1} B^T S \\right] A. \\tag{2.13} \\end{equation}\\] Equation (2.13) is the famous algebraic Riccati equation. Let’s zoom out to see what we have done. We started with a hypothetical optimal cost-to-go (2.10) that is stationary, and invoked the DP algorithm in (2.11), which led us to the algebraic Riccati equation (2.13). Therefore, if there actually exists a solution to the algebraic Riccati equation (2.13), then the linear controller (2.12) is indeed optimal (by the optimality of DP)! So the question boils down to if the algebraic Riccati equation has a solution \\(S\\) that is positive definite? The following proposition gives an answer. Proposition 2.2 (Solution of Discrete-Time Infinite-Horizon LQR) Consider a linear system \\[ x_{k+1} = A x_k + B u_k, \\] with \\((A,B)\\) controllable (see Appendix C.2). Let \\(Q \\succeq 0\\) in (2.8) be such that \\(Q\\) can be written as \\(Q = C^T C\\) with \\((A,C)\\) observable. Then the optimal controller for the infinite-horizon LQR problem (2.8) is a stationary linear policy \\[ \\mu^\\star (x) = - K x, \\] with \\[ K = \\left( R + B^T S B \\right)^{-1} B^T S A. \\] The matrix \\(S\\) is the unique positive definite matrix that satisfies the algebraic Riccati equation \\[ S = Q + A^T \\left[ S - SB \\left( R + B^T S B \\right)^{-1} B^T S \\right] A. \\] Moreover, the closed-loop system \\[ x_{k+1} = A x_k + B (-K x_k) = (A - BK) x_k \\] is stable, i.e., the eigenvalues of the matrix \\(A - BK\\) are strictly within the unit circle (see Appendix C.1.2). A rigorous proof of Proposition 2.2 is available in Proposition 3.1.1 of (Bertsekas 2012). The proof basically studies the limit of the discrete-time Riccati equation (2.7) when \\(N \\rightarrow \\infty\\). Indeed, the algebraic Riccati equation (2.13) is the limit of the discrete-time Riccati equation (2.7) when \\(N \\rightarrow \\infty\\). The assumptions of \\((A,B)\\) being controllable and \\((A,C)\\) being observable can be relaxted to \\((A,B)\\) being stabilizable and \\((A,C)\\) being detectable (for definitions of stabilizability and detectability, see Appendix C). We have not discussed how to solve the algebraic Riccati equation (2.7). It is clear that (2.7) is not a linear system of equations in \\(S\\). In fact, the numerical algorithms for solving the algebraic Riccati equation can be highly nontrivial, for example see (Arnold and Laub 1984). Fortunately, such algorithms are often readily available, and as practitioners we do not need to worry about solving the algebraic Riccati equation by ourselves. For example, the Matlab dlqr function computes the \\(K\\) and \\(S\\) matrices from \\(A,B,Q,R\\). Let us now apply the infinite-horizon LQR solution to stabilizing a simple pendulum. Example 2.1 (Pendulum Stabilization by LQR) Consider the simple pendulum in Fig. 2.1 with dynamics \\[\\begin{equation} x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ -\\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix} \\tag{2.14} \\end{equation}\\] where \\(m\\) is the mass of the pendulum, \\(l\\) is the length of the pole, \\(g\\) is the gravitational constant, \\(b\\) is the damping ratio, and \\(u\\) is the torque applied to the pendulum. We are interested in applying the LQR controller to balance the pendulum in the upright position \\(x_d = [\\pi,0]^T\\) with a zero velocity. Figure 2.1: A Simple Pendulum. Let us first shift the dynamics so that “\\(0\\)” is the upright position. This can be done by defining a new variable \\(z = x - x_d = [\\theta - \\pi, \\dot{\\theta}]^T\\), which leads to \\[\\begin{equation} \\dot{z} = \\dot{x} = f(x,u) = f(z + x_d,u) = \\begin{bmatrix} z_2 \\\\ \\frac{1}{ml^2} \\left( u - b z_2 + mgl \\sin z_1 \\right) \\end{bmatrix} = f&#39;(z,u). \\tag{2.15} \\end{equation}\\] We then linearize the nonlinear dynamics \\(\\dot{z} = f&#39;(z,u)\\) at the point \\(z^\\star = 0, u^\\star = 0\\): \\[\\begin{align} \\dot{z} &amp; \\approx f&#39;(z^\\star,u^\\star) + \\left( \\frac{\\partial f&#39;}{\\partial z} \\right)_{z^\\star,u^\\star} (z - z^\\star) + \\left( \\frac{\\partial f&#39;}{\\partial u} \\right)_{z^\\star,u^\\star} (u - u^\\star) \\\\ &amp; = \\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} \\cos z_1 &amp; - \\frac{b}{ml^2} \\end{bmatrix}_{z^\\star, u^\\star} z + \\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix} u \\\\ &amp; = \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} &amp; - \\frac{b}{ml^2} \\end{bmatrix}}_{A_c} z + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix}}_{B_c} u. \\end{align}\\] Finally, we convert the continuous-time dynamics to discrete time with a fixed discretization \\(h\\) \\[ z_{k+1} = \\dot{z}_k \\cdot h + z_k = \\underbrace{(h \\cdot A_c + I )}_{A} z_k + \\underbrace{(h \\cdot B_c)}_{B} u_k. \\] We are now ready to implement the LQR controller. In the formulation (2.8), we choose \\(Q = I\\), \\(R = I\\), and solve the gain matrix \\(K\\) using the Matlab dlqr function. Fig. 2.2 shows the simulation result for \\(m=1,l=1,b=0.1\\), \\(g = 9.8\\), and \\(h = 0.01\\), with an initial condition \\(z^0 = [0.1,0.1]^T\\). We can see that the LQR controller successfully stabilizes the pendulum at \\(z^\\star\\), the upright position. You can play with the Matlab code here. Figure 2.2: LQR stabilization of a simple pendulum. 2.1.2 LQR with Constraints Let’s explore LQR with constraints in Exercise 5.2 2.2 Markov Decision Process In Section 2.1, we see that linear dynamics and quadratic costs leads to exact dynamic programming. We now introduce another setup where the number of states and controls is finite (as opposed to the LQR case where \\(x_k\\) and \\(u_k\\) live in continuous spaces). We will see that we can execute DP exactly in this setup as well. Optimal control in the case of finite states and controls is typically introduced in the framework of a Markov Decision Process (MDP, which is common in Reinforcement Learning). There are many variations of a MDP, and here we only focus on the discounted infinite-horizon MDP. For a more complete treatment of MDPs, I suggest checking out this course at Harvard. Formally, a discounted infinite-horizon MDP \\(\\mathcal{M} = (\\mathbb{X},\\mathbb{U},P,g,\\gamma,\\sigma)\\) is specified by a state space \\(\\mathbb{X}\\) that is finite with size \\(|\\mathbb{X}|\\) a control space \\(\\mathbb{U}\\) that is finite with size \\(|\\mathbb{U}|\\) a transition function \\(P: \\mathbb{X} \\times \\mathbb{U} \\rightarrow \\Delta(\\mathbb{X})\\), where \\(\\Delta(\\mathbb{X})\\) is the space of probability distributions over \\(\\mathbb{X}\\); specifically, \\(P(x&#39; \\mid x, u)\\) is the probability of transitioning into state \\(x&#39;\\) from state \\(x\\) using control \\(u\\). If the system is deterministic, then \\(P(x&#39; \\mid x, u)\\) is nonzero only for a single next state \\(x&#39;\\) a cost function \\(g: \\mathbb{X} \\times \\mathbb{U} \\rightarrow [0,1]\\); \\(g(x,u)\\) is the cost of taking the control \\(u\\) at state \\(x\\) a discount factor \\(\\gamma \\in [0,1)\\) an initial state distribution \\(\\sigma \\in \\Delta(\\mathbb{X})\\) that specifies how the initial state \\(x_0\\) is generated; in many cases we will assume \\(x_0\\) is fixed and \\(\\sigma\\) is a distribution supported only on \\(x_0\\). In an MDP, the system starts at some state \\(x_0 \\sim \\sigma\\). At each step \\(k = 0,1,2,\\dots\\), the system decides a control \\(u_k \\in \\mathbb{U}\\) and incurs a cost \\(g(s_k,u_k)\\). The control \\(u_k\\) brings the system into a new state \\(x_{k+1} \\sim P(\\cdot \\mid x_k, u_k)\\), at which the controller decides a new control \\(u_{k+1}\\). This process continues forever. Controller (policy). In general, a time-varying controller \\(\\pi = (\\pi_0,\\dots,\\pi_k,\\dots)\\) is a mapping from all previous states and controls to a distribution over current controls. The mapping \\(\\pi_k\\) at timestep \\(k\\) is \\[ \\pi_k: (x_0,u_0,x_1,u_1,\\dots,x_k) \\mapsto u_k \\sim q_k \\in \\Delta(\\mathbb{U}). \\] Note that \\(u_k\\) can be randomized and it is drawn from a distribution \\(q_k\\) supported on the set of controls \\(\\mathbb{U}\\). A stationary controller (policy) \\(\\pi: \\mathbb{X} \\rightarrow \\Delta(\\mathbb{U})\\) specifies a decison-making strategy that is purely based on the current state \\(x_k\\). A deterministic and stationary controller \\(\\pi: \\mathbb{X} \\rightarrow \\mathbb{U}\\) excutes a deterministic control \\(u_k\\) at each step. Cost-to-go and \\(Q\\)-value. Given a controller \\(\\pi\\) and an initial state \\(x_0\\), we associate with it the following discounted infinite-horizon cost \\[\\begin{equation} J_\\pi(x_0) = \\mathbb{E} \\left\\{ \\sum_{k=0}^{\\infty} \\gamma^k g(x_k, u_k^{\\pi}) \\right\\}, \\tag{2.16} \\end{equation}\\] where the expectation is taken over the randomness of the transition \\(P\\) and the controller \\(\\pi\\). Note that we have used \\(u^{\\pi}_k\\) to denote the control at step \\(k\\) by following the controller \\(\\pi\\). Similarly, we define the \\(Q\\)-value function as \\[\\begin{equation} Q_\\pi(x_0,u_0) = \\mathbb{E} \\left\\{ g(x_0,u_0) + \\sum_{k=1}^{\\infty} \\gamma^k g(x_k,u_k^{\\pi}) \\right\\}. \\tag{2.17} \\end{equation}\\] The difference between \\(Q_\\pi(x_0,u_0)\\) and \\(J_\\pi(x_0)\\) is that at step zero, \\(J_\\pi(x_0)\\) follows the controller \\(\\pi\\) while \\(Q_\\pi(x_0,u_0)\\) assumes the control \\(u_0\\) is given. By the assumption that \\(g(x_k,u_k) \\in [0,1]\\), we have \\[ 0 \\leq J_\\pi(x_0), Q_\\pi(x_0,u_0) \\leq \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}, \\quad \\forall \\pi. \\] Our goal is to find the best controller that minimizes the cost function \\[\\begin{equation} \\pi^\\star \\in \\arg\\min_{\\pi \\in \\Pi} J_\\pi(x_0) \\tag{2.18} \\end{equation}\\] for a given initial state \\(x_0\\), where \\(\\Pi\\) is the space of all non-stationary and randomized controllers. A remarkable property of MDPs is that there exists an optimal controller that is stationary and deterministic. Theorem 2.1 (Deterministic and Stationary Optimal Policy) Let \\(\\Pi\\) be the space of all non-stationary and randomized policies. Define \\[ J^\\star_\\pi(x) = \\min_{\\pi \\in \\Pi} J_\\pi(x), \\quad Q^\\star_\\pi(x,u) = \\min_{\\pi \\in \\Pi } Q_\\pi(x,u). \\] There exists a deterministic and stationary policy \\(\\pi^\\star\\) such that for all \\(x \\in \\mathbb{X}\\) and \\(u \\in \\mathbb{U}\\), \\[ J_{\\pi^\\star}(x) = J^\\star(x), \\quad Q_{\\pi^\\star}(x,u) = Q^\\star(x,u). \\] We call such a policy \\(\\pi\\) an optimal policy. Proof. See Theorem 1.7 in (Agarwal et al. 2022). This Theorem shows that we can restrict ourselves to stationary and deterministic policies without losing performance. In the next, we show how to characterize the optimal policy and value function. 2.2.1 Bellman Optimality Equations We now restrict ourselves to stationary policies. We first introduce the Bellman Consistency Equations for stationary policies. Lemma 2.1 (Bellman Consistency Equations) Let \\(\\pi\\) be a stationary policy. Then \\(J_\\pi\\) and \\(Q_\\pi\\) satisfy the following Bellman consistency equations \\[\\begin{align} J_\\pi(x) &amp;= \\mathbb{E}_{u \\sim \\pi(\\cdot \\mid x)} Q_\\pi(x,u), \\tag{2.19} \\\\ Q_\\pi(x,u) &amp;=g(x,u) + \\gamma \\mathbb{E}_{x&#39; \\sim P(\\cdot \\mid x,u)} J_\\pi(x&#39;). \\tag{2.20} \\end{align}\\] Proof. By the definition of the cost-to-go function in (2.16), we have \\[ J_\\pi(x_0) = \\mathbb{E}\\left\\{ \\sum_{k=0}^{\\infty} \\gamma^k g(x_k, \\pi(x_k)) \\right\\} = \\mathbb{E}_{u_0 \\sim \\pi(\\cdot \\mid x_0)} \\underbrace{\\mathbb{E} \\left\\{ g(x_0,u_0) + \\sum_{k=1}^{\\infty} \\gamma^k g(x_k,\\pi(x_k)) \\right\\}}_{Q_\\pi(x_0,u_0)}. \\] The above equation holds for any \\(x_0\\), proving (2.19). To show (2.20), we recall the definition of the \\(Q\\)-value function (2.17) \\[\\begin{align} Q_\\pi(x_0,u_0) &amp; = \\mathbb{E} \\left\\{ g(x_0,u_0) + \\sum_{k=1}^{\\infty} \\gamma^k g(x_k,\\pi(x_k)) \\right\\} \\\\ &amp;= g(x_0,u_0) + \\gamma \\mathbb{E} \\left\\{ \\sum_{k=1}^{\\infty} \\gamma^{k-1} g(x_k,\\pi(x_k)) \\right\\} \\tag{2.21} \\end{align}\\] Now observe that the expectation of the second term in (2.21) is taken over both the randomness of \\(x_1\\) and the randomness of the policy after \\(x_1\\) is reached. Therefore, \\[ \\mathbb{E} \\left\\{ \\sum_{k=1}^{\\infty} \\gamma^{k-1} g(x_k,\\pi(x_k)) \\right\\} = \\mathbb{E}_{x_1 \\sim P(\\cdot \\mid x_0,u_0)} \\underbrace{\\left\\{ \\mathbb{E} \\left\\{ \\sum_{k=1}^{\\infty} \\gamma^{k-1} g(x_1,\\pi(x_1)) \\right\\} \\right\\}}_{J_\\pi(x_1)}. \\] Plugging the above equation back to (2.21), we obtain the desired result in (2.20). Matrix Representation. It is useful to think of \\(P,g,J_\\pi,Q_\\pi\\) as matrices. In particular, the transition function \\(P\\) can be considered as a matrix of dimension \\(|\\mathbb{X}||\\mathbb{U}| \\times \\mathbb{X}\\), where \\[ P_{(x,u),x&#39;} = P(x&#39; \\mid x,u) \\] is the entry of \\(P\\) at the row \\((x,u)\\) (there are \\(|\\mathbb{X}||\\mathbb{U}|\\) such rows) and column \\(x&#39;\\) (there are \\(|\\mathbb{X}|\\) such columns). The running cost \\(g\\) is vector of \\(|\\mathbb{X}||\\mathbb{U}|\\) entries. The cost-to-go \\(J_\\pi(x)\\) is a vector of \\(|\\mathbb{X}|\\) entries. The \\(Q\\)-value function \\(Q_\\pi(x,u)\\) is a vector of \\(|\\mathbb{X}||\\mathbb{U}|\\) entries. We also introduce \\(P^{\\pi}\\) with dimension \\(|\\mathbb{X}||\\mathbb{U}| \\times |\\mathbb{X}||\\mathbb{U}|\\) as the transition matrix induced by a stationary policy \\(\\pi\\). In particular, \\[ P^\\pi_{(x,u),(x&#39;,u&#39;)} = P(x&#39; \\mid x,u) \\pi(u&#39;\\mid x&#39;). \\] In words, \\(P^\\pi_{(x,u),(x&#39;,u&#39;)}\\) is the probability that \\((x&#39;,u&#39;)\\) follows \\((x,u)\\). With the matrix representation, we can compactly write the Bellman consistency equation (2.20) as \\[\\begin{equation} Q_\\pi = g + \\gamma P J_\\pi. \\tag{2.22} \\end{equation}\\] We can also combine (2.19) and (2.20) together and write \\[ Q_\\pi(x,u) = g(x,u) + \\gamma \\mathbb{E}_{x&#39; \\sim P(\\cdot \\mid x,u)} \\left\\{ \\mathbb{E}_{u&#39; \\sim \\pi(\\cdot \\mid x&#39;)} Q_\\pi(x&#39;,u&#39;) \\right\\} = g(x,u) + \\gamma \\mathbb{E}_{(x&#39;,u&#39;) \\sim P^\\pi(\\cdot \\mid (x,u))} Q_\\pi(x&#39;,u&#39;), \\] which, using matrix representation, becomes \\[\\begin{equation} Q_\\pi = g + \\gamma P^\\pi Q_\\pi. \\tag{2.23} \\end{equation}\\] Equation (2.23) immediately yields \\[\\begin{equation} Q_\\pi = (I - \\gamma P^\\pi)^{-1} g, \\tag{2.24} \\end{equation}\\] that is, the \\(Q\\)-value function associated with a stationary policy \\(\\pi\\) can be simply computed from solving a linear system as in (2.24).3 Lemma 2.1, together with the equivalent matrix equations (2.22) and (2.23), provide the conditions that \\(J_\\pi\\) and \\(Q_\\pi\\), induced by any stationary policy \\(\\pi\\), need to satisfy. In the next, we describe the conditions that characterize the optimal policy. Theorem 2.2 (Bellman Optimality Equations) A vector \\(Q \\in \\mathbb{R}^{|\\mathbb{X}||\\mathbb{U}|}\\) is said to satisfy the Bellman optimality equation if \\[\\begin{equation} Q(x,u) = g(x,u) + \\gamma \\mathbb{E}_{x&#39; \\sim P(\\cdot \\mid x,u)} \\left\\{ \\min_{u&#39; \\in \\mathbb{U}} Q(x&#39;,u&#39;) \\right\\}, \\quad \\forall (x,u) \\in \\mathbb{X} \\times \\mathbb{U}. \\tag{2.25} \\end{equation}\\] A vector \\(Q^\\star\\) is the optimal \\(Q\\)-value function if and only if it satisfies (2.25). Moreover, the deterministic policy defined by \\[ \\pi^\\star(x) \\in \\arg\\min_{u \\in \\mathbb{U}} Q^\\star(x,u) \\] with ties broken arbitrarily is an optimal policy. Proof. See Theorem 1.8 in (Agarwal et al. 2022). We now make a few definitions to interpret Theorem 2.2. For any vector \\(Q \\in \\mathbb{R}^{|\\mathbb{X}||\\mathbb{U}|}\\), define the greedy policy as \\[\\begin{equation} \\pi_{Q}(x) \\in \\arg\\min_{u \\in \\mathbb{U}} Q(x,u) \\tag{2.26} \\end{equation}\\] with ties broken arbitrarily. With this notation, by Theorem 2.2, the optimal policy is \\[ \\pi^\\star = \\pi_{Q^\\star}, \\] where \\(Q^\\star\\) is the optimal \\(Q\\)-value function. Similarly, let us define \\[ J_Q(x) = \\min_{u \\in \\mathbb{U}} Q(x,u). \\] Note that \\(J_Q\\) has dimension \\(|\\mathbb{X}|\\). With these notations, the Bellman optimality operator is defined as \\[\\begin{equation} \\mathcal{T}Q = g + \\gamma P J_Q, \\tag{2.27} \\end{equation}\\] which is nothing but a matrix representation of the right-hand side of (2.25). This allows us to concisely write the Bellman optimality equation (2.25) as \\[\\begin{equation} Q = \\mathcal{T}Q. \\tag{2.28} \\end{equation}\\] Therefore, an equivalent way to interpret Theorem 2.2 is that \\(Q = Q^\\star\\) if and only if \\(Q\\) is a fixed point to the Bellman optimality operator \\(\\mathcal{T}\\). 2.2.2 Value Iteration Intepreting the optimal \\(Q\\)-value function as the fixed point to the Bellman optimality operator (2.28) leads us to a natural algorithm for solving the optimal control problem. We start with \\(Q^{(0)}\\) being an all-zero vector and then at iteration \\(t\\), we perform \\[ Q^{(t+1)} \\leftarrow \\mathcal{T} Q^{(t)}, \\] with \\(\\mathcal{T}\\) defined in (2.27). Let us observe the simplicity of this algorithm: at each iteration, one only needs to perform \\(\\min_{u \\in \\mathbb{U}} Q^{(t)}(x,u)\\), which is very efficient when \\(|\\mathbb{U}|\\) is not too large. The next theorem states this simple algorithm converges to the optimal value function. Theorem 2.3 (Value Iteration) Set \\(Q^{(0)} = 0\\). For \\(t=0,\\dots\\), perform \\[ Q^{(t+1)} \\leftarrow \\mathcal{T} Q^{(t)}. \\] Let \\(\\pi^{(k)} = \\pi_{Q^{(k)}}\\) (see the definition in (2.26)). For \\(t \\geq \\frac{\\log \\frac{2}{(1-\\gamma)^2 \\epsilon}}{1-\\gamma}\\), we have \\[ J^{\\pi^{(t)}} \\leq J^\\star + \\epsilon \\mathbb{1}, \\] where \\(\\mathbb{1}\\) is the all-ones vector. Essentially, the value function obtained from value iteration converges to the optimal cost-to-go. Let us use an example to appreciate this algorithm. Example 2.2 (Shortest Path in Grid World) Consider the following \\(10 \\times 10\\) grid world, where the top-right cell is the goal location, and the dark blue colored cells are obstacles. Figure 2.3: Grid World with Obstacles. We want to find the shortest path from a given cell to the target cell, while not hitting obstacles. To do so, we set the state space of the system as \\[ \\mathbb{X} = \\left\\{ \\begin{bmatrix} r \\\\ c \\end{bmatrix} \\middle\\vert r,c \\in \\{ 1,\\dots,10 \\} \\right\\} \\] where \\(r\\) is the row index (from top to bottom) and \\(c\\) is the column index (from left to right). The control space is moving left, right, up, down, or do nothing: \\[ \\mathbb{U} = \\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\right\\}. \\] The system dynamics is deterministic \\[ x&#39; = \\begin{cases} x + u &amp; \\text{if } x + u \\text{ is inside the grid} \\\\ x &amp; \\text{otherwise} \\end{cases}. \\] We then design the following running cost function \\(g\\) \\[ g(x,u) = \\begin{cases} 0 &amp; \\text{if } x = [1,10]^T \\text{ is the target} \\\\ 20 &amp; \\text{if } x \\text{ is an obstacle} \\\\ 1 &amp; \\text{otherwise} \\end{cases}. \\] Note that \\(g(x,u)\\) defined above does not even satisfy \\(g \\in [0,1]\\). We then use value iteration to solve the optimal control problem with \\(\\gamma = 1\\) \\[ J(x_0) = \\min_{\\pi} \\sum_{k=0}^{\\infty} g(x_k,\\pi(x_k)). \\] The Matlab script of value iteration converges in \\(27\\) iterations, and we obtain the optimal cost-to-go in Fig. 2.4. Starting from the cell \\([8,5]^T\\), the red line in Fig. 2.4 plots the optimal trajectory that clearly avoids the obstacles. Feel free to play with the size of the grid and the number of obstacles. Figure 2.4: Optimal cost-to-go and an optimal trajectory. Example 2.2 shows the simplicity and power of value iteration. However, the states and controls in the grid world are naturally discrete and finite. Is it possible to apply value iteration to optimal control problems where the states and controls live in continuous spaces? 2.2.3 Value Iteration with Barycentric Interpolation Let us consider the discrete-time dynamics \\[ x_{k+1} = f(x_k, u_k) \\] where both \\(x_k\\) and \\(u_k\\) live in a continuous space, say \\(\\mathbb{R}^{n}\\) and \\(\\mathbb{R}^m\\), respectively. A natural idea to apply value iteration is to discretize the state space and control space. For example, suppose \\(x \\in \\mathbb{R}^2\\) and we have discretized \\(\\mathbb{R}^2\\) using \\(N\\) points \\[ \\mathcal{S} = \\{s_1,\\dots,s_N\\} \\] that lie on a 2D grid, as shown in Fig. 2.5. Assume \\(x_k \\in \\mathcal{S}\\) lies on the mesh grid, the next state \\(x_{k+1} = f(x_k,u_k)\\) will, however, most likely not lie exactly on one of the grid points. Figure 2.5: Barycentric Interpolation. Nevertheless, \\(x_{k+1}\\) will lie inside a triangle with vertices \\(s_p, s_q, s_m\\). We will now try to write \\(x_{k+1}\\) using the vertices, that is, to find three numbers \\(\\lambda_p, \\lambda_q, \\lambda_m\\) such that \\[ \\lambda_p, \\lambda_q, \\lambda_m \\geq 0, \\quad \\lambda_p + \\lambda_q + \\lambda_m = 1, \\quad x_{k+1} = \\lambda_p s_p + \\lambda_q s_q + \\lambda_m s_m. \\] \\(\\lambda_p,\\lambda_q,\\lambda_m\\) are called the barycentric coordinates of \\(x_{k+1}\\) in the triangle formed by \\(s_p,s_q,s_m\\). With the barycentric coordinates, we will assign the transition matrix \\[ P(x_{k+1}=s_p \\mid x_k,u_k) = \\lambda_p, \\quad P(x_{k+1}=s_q \\mid x_k,u_k) = \\lambda_q, \\quad P(x_{k+1}=s_m \\mid x_k,u_k) = \\lambda_m. \\] Let us apply value iteration with barycentric interpolation to the simple pendulum. Example 2.3 (Value Iteration with Barycentric Interpolation on A Simple Pendulum) Consider the continuous-time pendulum dynamics in (2.15) that is already shifted such that \\(z=0\\) corresponds to the upright position. With time discretization \\(h\\), we can write the discrete-time dynamics as \\[ z_{k+1} = \\dot{z}_k \\cdot h + z_k = f&#39;(z_k,u_k) \\cdot h + z_k. \\] We are interested in solving the optimal control problem \\[ J(z_0) = \\min_{u_k} \\left\\{ \\sum_{k=0}^{\\infty} \\gamma^k g(x_k,u_k) \\right\\}, \\] where the running cost is simply \\[ g(x_k,u_k) = x_k^T x_k + u_k^2. \\] We will use the parameters \\(m=1,g=9.8,l=1,b=0.1\\), and assume the control is bounded in \\([-4.9,4.9]\\). We want to compute the optimal cost-to-go in the range \\(z_1 \\in [-\\pi,\\pi]\\) and \\(z_2 \\in [-\\pi,\\pi]\\). We discretize both \\(z_1\\) and \\(z_2\\) using \\(N\\) points, leading to \\(N^2\\) points in the state space. We also discretize \\(u\\) using \\(N\\) points. Applying value iteration with \\(\\gamma=0.9\\) and \\(N=50\\), we obtain the optimal cost-to-go in Fig. 2.6. The value iteration converges in \\(277\\) iterations. Figure 2.6: Optimal cost-to-go with discount factor 0.9. Applying value iteration with \\(\\gamma=0.99\\) and \\(N=50\\), we obtain the optimal cost-to-go in Fig. 2.7. The value iteration converges in \\(2910\\) iterations. Figure 2.7: Optimal cost-to-go with discount factor 0.99. Applying value iteration with \\(\\gamma=0.999\\) and \\(N=50\\), we obtain the optimal cost-to-go in Fig. 2.8. The value iteration converges in \\(28850\\) iterations. Figure 2.8: Optimal cost-to-go with discount factor 0.999. You can find the Matlab code here. References Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. “Reinforcement Learning: Theory and Algorithms.” CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 32. Arnold, William F, and Alan J Laub. 1984. “Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.” Proceedings of the IEEE 72 (12): 1746–54. ———. 2012. Dynamic Programming and Optimal Control: Volume i. Vol. 1. Athena scientific. One can show that the matrix \\(I - \\gamma P^\\pi\\) is indeed invertible, see Corollary 1.5 in (Agarwal et al. 2022).↩︎ "],["approximatedp.html", "Chapter 3 Approximate Optimal Control 3.1 Fitted Value Iteration 3.2 Trajectory Optimization 3.3 Model Predictive Control 3.4 Policy Gradient", " Chapter 3 Approximate Optimal Control Thanks to Jiarui Li for contributing to this Chapter. In Chapter 2, we have studied two cases where dynamic programming (DP) can be executed exactly. Both cases are interesting yet limiting. In the linear quadratic regulator (LQR) case, the optimal controller is a linear controller and it does not require discretization of the state and control space. However, LQR only handles systems with linear dynamics. In the Markov Decision Process (MDP) case, value iteration can obtain the optimal cost-to-go (or the optimal \\(Q\\)-value function) usually in a finite number of iterations. With barycentric interpolation, value iteration also leads to a practical controller for swinging up the pendulum (see Example 2.3), at least starting from some initial states. However, value iteration suffers from the curse of dimensionality, i.e., the amount of memory and computation needed to compute the optimal cost-to-go grows exponentially with the number of grids used to discretize the state space and control space. In this Chapter, we will study approximate dynamic programming, which aims to not find the optimal controller (simply because it is too demanding), but only a suboptimal controller that is perhaps good enough. It is worth noting that there exists a large amount of algorithms and literature for approximate DP, most of which are closely related to reinforcement learning, and studying all of them is beyond the scope of this course. In the following we will only go through several algorithmic frameworks that are representative. 3.1 Fitted Value Iteration Let us consider the infinite-horizon optimal control problem introduced in Chapter 1.3 \\[ \\min_{\\pi} \\mathbb{E} \\left\\{ \\sum_{k=0}^\\infty g(x_k, \\pi(x_k)) \\right\\}. \\] Under some technical conditions, we know that the optimal policy is a deterministic and stationary policy and the optimal cost-to-go satisfies the following Bellman Optimality Equation \\[\\begin{equation} J^\\star(x) = \\min_{u \\in \\mathbb{U}} \\mathbb{E}_w \\left\\{ g(x,u) + J^\\star(f(x,u,w)) \\right\\}, \\quad \\forall x \\in \\mathbb{X}. \\tag{3.1} \\end{equation}\\] From the plots in Example 2.3, we observe that even for a “simple” problem like pendulum swing-up, the optimal cost-to-go \\(J^\\star(x)\\) does not look simple at all. So here comes a very natural idea. What if we parametrize a cost-to-go function \\(\\tilde{J}(x,r)\\) by a vector of unknown coefficients \\(r \\in \\mathbb{R}^{d}\\) and ask \\(\\tilde{J}(x,r)\\) to satisfy (3.1) as closely as possible? This indeed leads to a valid algoithm called fitted value iteration (FVI). In FVI, we initialize the parameters \\(r\\) as \\(r_0\\) in the first step. Then, at the \\(k\\)-th iteration, we perform two subroutines. Value update. Firstly, we sample a large amount of points in the state space \\(\\{x_k^s \\}_{s=1}^q\\), and for each \\(x_k^s\\), we solve the minimization problem on the right-hand side of (3.1) using \\(J(x,r^{(k)})\\) as the cost-to-go. Formally, this is \\[\\begin{equation} \\beta_k^s \\leftarrow \\min_{u \\in \\mathbb{U}} \\mathbb{E}_w \\left\\{ g(x_k^s, u) + \\tilde{J}(f(x_k^s,u,w),r^{(k)}) \\right\\}, \\quad \\forall x_k^s, s= 1,\\dots,q. \\tag{3.2} \\end{equation}\\] This step gives us a list of scalars \\(\\{ \\beta_k^s \\}_{s=1}^q\\). If \\(\\tilde{J}(x,r)\\) indeed satisfies the Bellman optimality equation, then we should have \\[ \\tilde{J}(x_k^s,r^{(k)}) = \\beta_k^s, \\quad \\forall s = 1,\\dots,q. \\] This is certainly not true when the parameter \\(r\\) is imperfect. Parameter update. Therefore, we will see the best parameter that minimizes the violation of the above equation \\[\\begin{equation} r^{(k+1)} \\leftarrow \\arg\\min_{r \\in \\mathbb{R}^d} \\sum_{s=1}^q \\left(\\tilde{J}(x_k^s,r) - \\beta_k^s\\right)^2. \\tag{3.3} \\end{equation}\\] FVI essentially carries out (3.2) and (3.3) until some convergence metric is met. Two challenges immediately show up: How to perform the minimization over \\(u\\) in the first step (3.2)? How to find the best parameter update in the second step (3.3)? To solve the first challenge, we will assume that \\(\\mathbb{U}\\) is a finite set so that minimization over \\(u\\) can be solved exactly. Note that this assumption is not strictly necessary. In fact, as long as \\(\\mathbb{U}\\) is a convex set and the objective in (3.2) is also convex, then the minimization can also be solved exactly (A. Yang and Boyd 2023). However, we assume \\(\\mathbb{U}\\) is finite to simplify our presentation. To solve the second challenge, we will assume \\(\\tilde{J}(x,r)\\) is linear in the parameters \\(r\\), as we will study further in the following. 3.1.1 Linear Features We parameterize \\(\\tilde{J}(x,r)\\) as follows: \\[\\begin{equation} \\tilde{J}(x,r) = \\sum_{i=1}^d r_i \\cdot \\phi_i(x) = \\begin{bmatrix} \\phi_1(x) &amp; \\cdots &amp; \\phi_d(x) \\end{bmatrix} r = \\phi(x)^T r, \\end{equation}\\] where \\(\\phi_1(x),\\dots,\\phi_d(x)\\) are a set of known basis functions, or features. Common examples of features include polynomials and radial basis functions. With this parametrization, the optimization in (3.3) becomes \\[\\begin{equation} \\min_{r \\in \\mathbb{R}^d} \\sum_{s=1}^q \\left( \\phi(x_k^s)^T r - \\beta_k^s \\right)^2, \\tag{3.4} \\end{equation}\\] which is a least-squares problem that can be solved in closed form. In particular, we can compute the gradient of its objective with respect to \\(r\\) and set it to zero, leading to \\[\\begin{align} 0 = \\sum_{s=1}^q 2(\\phi(x_k^s)^T r - \\beta_k^s ) \\phi(x_k^s) \\Longrightarrow \\\\ r = \\left( \\sum_{s=1}^q \\phi(x_k^s) \\phi(x_k^s)^T \\right)^{-1} \\left( \\sum_{s=1}^q \\beta_k^s \\phi(x_k^s) \\right). \\end{align}\\] Let us apply FVI with linear features to a linear system for which we know the optimal cost-to-go. Example 3.1 (Fitted Value Iteration for Double Integrator) Consider the double integrator \\[ \\ddot{q} = u. \\] Take \\(x = [q;\\dot{q}]\\), we can write the dynamics in state-space form \\[\\begin{equation} \\dot{x} = \\begin{bmatrix} \\dot{q} \\\\ u \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} x + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u \\tag{3.5} \\end{equation}\\] We use a constant time \\(h=0.01\\) differentiation to convert the continuous-time dynamics into discrete-time: \\[ x_{k+1} = h \\cdot \\dot{x}_k + x_k = \\begin{bmatrix} 1 &amp; h \\\\ 0 &amp; 1 \\end{bmatrix} x_k + \\begin{bmatrix} 0 \\\\ h \\end{bmatrix} u_k. \\] The goal is to regulate the system at \\((0,0)\\). To do so, we use a quadratic cost \\[ J(x) = \\min \\sum_{k=0}^{\\infty} x_k^T Q x_k + u_k^T R u_k, \\quad x_0 = x. \\] with \\(Q = 0.1I_2\\) and \\(R = 1\\). In Chapter 2, we learned how to use LQR to precisely calculate the cost-to-go function of these systems using the Algebraic Recatti Equation. Solving the ARE, we obtain \\[ S_{\\mathrm{LQR}} = \\begin{bmatrix} 27.1640 &amp; 31.7584 \\\\ 31.7584 &amp; 85.9510 \\end{bmatrix}. \\] Now we want to investigate if FVI can find the same matrix \\(S\\). To do so, we parametrize \\[ \\tilde{J}(x) = x^T S x = x^T \\begin{bmatrix} S_1 &amp; S_2 \\\\ S_2 &amp; S_3 \\end{bmatrix} x = \\begin{bmatrix} x_1^2 &amp; 2 x_1 x_2 &amp; x_2^2 \\end{bmatrix} \\begin{bmatrix} S_1 \\\\ S_2 \\\\ S_3 \\end{bmatrix}, \\] where \\(S\\) only has three independent variables due to being symmetric. Do note that \\(\\tilde{J}(x)\\) is linear in \\(S\\), so the parameter update step can be solved in closed form. We now show that the value update step can also be solved in closed form. Suppose we choose \\(x_k\\) as a sample, then the right-hand side of (3.2) reads: \\[ \\min_{u} x_k^T Q x_k + u^2 + (A x_k + B u)^T S^{(k)} (A x_k + B u), \\] where \\(S^{(k)}\\) is the value of \\(S\\) at the \\(k\\)-th iteration. Clearly, the optimization problem above is a convex quadratic optimization and can be solved in closed form: \\[ u = - (1 + B^T S^{(k)} B) ^{-1} B^T S^{(k)} A x_k. \\] Applying fitted value iteration on randomly sampled points mentioned above, we obtain the fitted \\(S\\) \\[ S_{\\mathrm{FVI}} = \\begin{bmatrix} 27.1640 &amp; 31.7583 \\\\ 31.7584 &amp; 85.9509 \\end{bmatrix}. \\] We can see that \\(S_{\\mathrm{FVI}}\\) almost exactly matches the groundtruth LQR solution \\(S_{\\mathrm{LQR}}\\). You can play with the code here. 3.1.2 Neural Network Features We have seen that simple quadratic features work for the LQR problem. For more complicated problems, we will need more powerful function approximators like neural networks. When we parameterize \\(\\tilde{J}(x,r)\\) as a neural network, \\(r\\) will be the weights of the neural network. We may still be able to solve the value update step if \\(u\\) lives in a finite space. However, the parameter update step usually cannot be solved exactly and will need to rely on numerical algoithms such as gradient descent. Let us try neural FVI on the same double integrator problem. Example 3.2 (Neural Fitted Value Iteration for the Double Integrator) In this example, we will use neural network as the approximation of the cost-to-go function and conduct neural FVI on a double integrator. The dynamics of the double integrator has been introduced in example 3.1. We use a positive definite network to model the cost-to-go function \\[\\begin{equation} J(x) = N(x)^T N(x), \\end{equation}\\] where \\(N(x)\\) is a 3-layer Multi-Layer-Perceptron with ReLU activation. Using mini-batch learning plus Adam optimizer, we obtain the cost-to-go in Fig. 3.1 after 300 epochs of training. Figure 3.1: Comparison between the result of NN-based FVI and ground truth The figure shows that the approximation performance of NN is pretty good. Simulation experiments also shows that the corresponding controller could successfully regulate the system at \\((0,0)\\). You can see the code here. 3.1.3 Fitted Q-value Iteration From the MDP Chapter we know there is an equivalent representation of the Bellman Optimality Equation by replacing the \\(J\\) value function in (3.1) with the \\(Q\\)-value function \\(Q(x,u)\\). In particular, with \\[ J^\\star(x) = \\min_{u \\in \\mathbb{U}} Q^\\star(x,u) \\] substituted into (3.1), we obtain the Bellman Optimality Equation in \\(Q^\\star(x,u)\\): \\[\\begin{equation} Q^\\star(x,u) = g(x,u) + \\mathbb{E}_w \\left\\{ \\min_{u&#39; \\in \\mathbb{U}} Q^\\star(f(x,u,w),u&#39;) \\right\\}. \\tag{3.6} \\end{equation}\\] We can then use the same idea to approximate \\(Q^\\star(x,u)\\) as \\[ \\tilde{Q}(x,u,r) \\] with \\(r \\in \\mathbb{R}^d\\) a parameter vector. By iteratively evaluating the right-hand side of (3.6), we obtain the algoithm known as fitted \\(Q\\)-value iteration (FQI). For example, we can similarly adopt the linear feature parameterization and set \\[ \\tilde{Q}(x,u,r) = \\phi(x,u)^T r, \\] where \\(\\phi(x,u)\\) is a known pre-selected feature vector. Then at the \\(k\\)-th iteration of FQI, we perform two subroutines. Value update. We sample a number of state-control pairs \\(\\{(x_k^s,u_k^s) \\}_{s=1}^q\\) and evaluate the right-hand side of the Bellman optimality equation (3.6): \\[\\begin{equation} \\beta_k^s \\leftarrow g(x_k^s,u_k^s) + \\mathbb{E}_w \\left\\{ \\min_{u&#39; \\in \\mathbb{U}} \\tilde{Q}(f(x_k^s,u_k^s,w),u&#39;,r^{(k)}) \\right\\}, \\quad \\forall s = 1,\\dots,q. \\tag{3.7} \\end{equation}\\] Again, if \\(u\\) lives in a finite space, or the minimization is convex, the above value update can be solved exactly. Parameter update. We update the parameter vector using the updated values \\(\\{ \\beta_k^s \\}_{s=1}^q\\): \\[\\begin{equation} r^{(k+1)} \\leftarrow \\arg\\min_{r \\in \\mathbb{R}^d} \\sum_{s=1}^q \\left( \\tilde{Q}(x_k^s,u_k^s,r) - \\beta_k^s \\right)^2, \\tag{3.8} \\end{equation}\\] which is a least squares problem that can be solved in closed form. Let us apply FQI to the same double integrator example. Example 3.3 (Fitted Q-value Iteration for Double Integrator) Consider the same double integrator dynamics in Example 3.1. From the LQR solution we know the optimal \\(Q\\)-value function is \\[\\begin{align} Q^\\star(x,u) &amp;= x^T Q x + u^T R u + (Ax + Bu)^T S (Ax + Bu) \\\\ &amp;= \\begin{bmatrix} x \\\\ u \\end{bmatrix}^T \\underbrace{\\begin{bmatrix} A^T S A + Q &amp; A^T S B \\\\ B^T S A &amp; B^T S B + R \\end{bmatrix}}_{M_{\\mathrm{LQR}}} \\begin{bmatrix} x \\\\ u \\end{bmatrix}, \\end{align}\\] where \\(M_{\\mathrm{LQR}}\\) is \\[ M_{\\mathrm{LQR}} = \\begin{bmatrix} 27.2640 &amp; 32.0300 &amp; 0.3176 \\\\ 32.0300 &amp; 86.6889 &amp; 0.8627 \\\\ 0.3176 &amp; 0.8627 &amp; 1.0086 \\end{bmatrix}. \\] Let us apply FQI to see if we get the same solution. We parametrize \\[\\begin{align} \\tilde{Q}(x,u) &amp;= \\begin{bmatrix} x \\\\ u \\end{bmatrix}^T \\begin{bmatrix} M_1 &amp; M_2 &amp; M_3 \\\\ M_2 &amp; M_4 &amp; M_5 \\\\ M_3 &amp; M_5 &amp; M_6 \\end{bmatrix} \\begin{bmatrix} x \\\\ u \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} x_1^2 &amp; 2 x_1 x_2 &amp; 2x_1 u &amp; x_2^2 &amp; 2x_2 u &amp; u^2 \\end{bmatrix} \\begin{bmatrix} M_1 \\\\ M_2 \\\\ M_3 \\\\ M_4 \\\\ M_5 \\\\ M_6 \\end{bmatrix}. \\end{align}\\] At the \\(k\\)-th FQI iteration, we are given \\(M^{(k)}\\). Suppose we sample \\((x_k, u_k)\\), then the value update step needs to solve (3.7), which reads: \\[ \\beta_k = g(x_k, u_k) + \\min_{u&#39;} \\underbrace{\\begin{bmatrix} A x_k + B u_k \\\\ u&#39; \\end{bmatrix}^T M^{(k)} \\begin{bmatrix} A x_k + B u_k \\\\ u&#39; \\end{bmatrix}}_{\\psi(u&#39;)}. \\] The objective function \\(\\psi(u&#39;)\\) can be shown to be quadratic: \\[ \\psi(u&#39;) = (Lu&#39; + a_k)^T M^{(k)} (Lu&#39; + a_k), \\quad L = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, a_k = \\begin{bmatrix} A x_k + B u_k \\\\ 0 \\end{bmatrix}. \\] Therefore, we can solve \\(u&#39;\\) in closed-form \\[ u&#39; = - (L^T M^{(k)} L)^{-1} L^T M^{(k)} a_k. \\] Applying FQI with the closed-form update above, we get \\[ M_{\\mathrm{FQI}} = \\begin{bmatrix} 27.2640 &amp; 32.0300 &amp; 0.3176 \\\\ 32.0300 &amp; 86.6889 &amp; 0.8627 \\\\ 0.3176 &amp; 0.8627 &amp; 1.0086 \\end{bmatrix}, \\] which is exactly the same as the solution obtained from LQR. You can play with the code here. 3.1.4 Deep Q Network Although we have only tested FVI and FQI on the simple double integrator linear system, these algoithms are really not so different from the state-of-the-art reinforcement learning algoithms. For example, the core of the seminal work Deep \\(Q\\)-Network (DQN) (Mnih et al. 2015) is to use a deep neural network to parameterize the \\(Q\\)-value function. Of course the DQN work has used other clever ideas to make it work in practice, such as experience replay, but the essence is fitted \\(Q\\)-value iteration. You can find a good explanation of DQN in this tutorial, and a practical step-by-step Pytorch implementation that applies DQN on the cart-pole system. 3.1.5 Deep + Shallow Combine the rich features of DQN with the stable learning of FQI (Levine et al. 2017). 3.2 Trajectory Optimization Consider a continuous-time optimal control problem (OCP) in full generality \\[\\begin{equation} \\begin{split} \\min_{u(t), t \\in [0,T]} &amp; \\quad g_T(x(T)) + \\int_{t = 0}^T g(x(t),u(t)) dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(t),u(t)) \\\\ &amp; \\quad x(0) = x_0 \\\\ &amp; \\quad (x(t),u(t)) \\in \\mathcal{X} \\times \\mathcal{U} \\\\ &amp; \\quad \\phi_i (x(t),u(t)) \\geq 0, i=1,\\dots,q. \\end{split} \\tag{3.9} \\end{equation}\\] where \\(g_T\\) the terminal cost, \\(g\\) the running cost, \\(x_0\\) the initial condition, \\(\\mathcal{X}\\) the state constraint set, \\(\\mathcal{U}\\) the control constraint set, and \\(\\{ \\phi_i \\}_{i=1}^q\\) are general state-control constraints. We assume that the state constraint set \\(\\mathcal{X}\\) and control constraint set \\(\\mathcal{U}\\) can be described by a finite set of inequalities, i.e., \\[ \\mathcal{X} = \\{x \\in \\mathbb{R}^n \\mid c^x_i(x) \\geq 0, i =1,\\dots,q_x\\}, \\quad \\mathcal{U} = \\{u \\in \\mathbb{R}^m \\mid c^u_i(u) \\geq 0, i = 1,\\dots,q_u \\}. \\] Assume that all functions are differentiable. Our goal is to “transcribe” the infinite-dimensional continuous-time optimization (3.9) into a nonlinear programming problem (NLP) of the following form \\[\\begin{equation} \\begin{split} \\min_{v \\in \\mathbb{R}^{n_v}} &amp; \\quad c(v) \\\\ \\text{subject to} &amp; \\quad c_{\\text{eq},i}(v) = 0, i =1,\\dots, n_{\\text{eq}} \\\\ &amp; \\quad c_{\\text{ineq},i}(v) \\geq 0, i=1,\\dots,n_{\\text{ineq}}, \\end{split} \\tag{3.10} \\end{equation}\\] where \\(v\\) is a finite-dimensional vector variable to be optimized, \\(c, c_{\\text{eq},i}, c_{\\text{ineq},i}\\) are (continuously differentiable) objective and constraint functions. Once we have done the transcription from (3.9) to (3.10), then we have a good number of numerical optimization algorithms at our disposal. For example, the Matlab fmincon provides a nice interface to many such algorithms, e.g., interior-point method and sequential quadratic programming. You have already played with a simple example of fmincon in Exercise 5.1. Other well-implemented NLP algorithms include IPOPT and SNOPT. However, usually the Matlab fmincon gives a decent start point for trying out different algorithms before moving to commercial solvers such as SNOPT. For a more comprehensive understanding about how NLP algoithms work under the hood, I suggest reading (Nocedal and Wright 1999). Before we talk about how to transcribe the OCP into an NLP, let us use a simple example to get a taste of fmincon. Example 3.4 (Fire a Canon with fmincon) On a 2D plane, suppose we have a canon at the origin \\((0,0)\\), and we want to fire the canon, with control over the initial velocity \\((v_1, v_2)\\), so that the canon ball hits the target location \\((10,10)\\). From our basic physics knowledge, we know the trajectory of the canon ball is described by \\[ \\begin{cases} x_1(t) = v_1 t \\\\ x_2(t) = v_2 t - \\frac{1}{2}g t^2, \\end{cases} \\] where \\(g\\) is the gravity constant. With the trajectory of the canon ball written above, we can formulate our nonlinear programming problem (NLP) as: \\[\\begin{equation} \\begin{split} \\min_{v_1, v_2, T} &amp; \\quad \\frac{1}{2} (v_1^2 + v_2^2) \\\\ \\text{subject to} &amp; \\quad v_1, v_2, T \\geq 0 \\\\ &amp; \\quad x_1(T) = v_1 T = 10 \\\\ &amp; \\quad x_2(T) = v_2 T - \\frac{1}{2}g T^2 = 10 \\end{split} \\tag{3.11} \\end{equation}\\] where our unknown variables are the initial velocities and the time \\(T\\) at which the canon ball hits the target. In the NLP (3.11), the first constraint asks all our varaibles to be nonnegative, the second and third constraints enforce the canon ball to hit the target. The following script formulates the NLP (3.11) in matlab. clc; clear; close all; g = 9.8; x0 = [0;0;1]; % initial guess of the solution % define objective function obj = @(x) objective(x); % define nonlinear constraints nonlincon = @(x) nonlinear_con(x,g); % define options for fmincon options = optimoptions(&#39;fmincon&#39;,&#39;Algorithm&#39;,&#39;interior-point&#39;,... &#39;SpecifyObjectiveGradient&#39;,true,&#39;SpecifyConstraintGradient&#39;,true,... &#39;checkGradients&#39;,false); % call fmincon [xopt,fopt,~,out] = fmincon(obj,x0,... % objective and initial guess -eye(3),zeros(3,1),... % linear inequality constraints [],[],... % no linear equality constraints [],[],... % no upper and lower bounds nonlincon,... % nonlinear constraints options); fprintf(&quot;Maximum constraint violation: %3.2f.\\n&quot;,out.constrviolation); fprintf(&quot;Objective: %3.2f.\\n&quot;,fopt); % plot the solution T = xopt(3); t = 0:0.01:T; xt = xopt(1)*t; yt = xopt(2)*t - 0.5*g*(t.^2); figure; plot(xt,yt,&#39;LineWidth&#39;,2); hold on scatter(10,10,100,&quot;red&quot;,&#39;filled&#39;,&#39;diamond&#39;); axis equal; grid on; xlabel(&#39;$x_1(t)$&#39;,&#39;FontSize&#39;,24,&#39;Interpreter&#39;,&#39;latex&#39;); ylabel(&#39;$x_2(t)$&#39;,&#39;FontSize&#39;,24,&#39;Interpreter&#39;,&#39;latex&#39;); ax = gca; ax.FontSize = 20; %% helper functions function [f,df] = objective(x) % x is our decision variable: [v1, v2, T] % objective function f = 0.5 * (x(1)^2 + x(2)^2); % gradient of the objective function (optional) df = [x(1); x(2); 0]; % column vector end function [c,ceq,dc,dceq] = nonlinear_con(x,g) % no inequality constraints c = []; dc = []; % two equality constraints ceq = [x(1)*x(3)-10; x(2)*x(3)-0.5*g*x(3)^2-10]; % explicit gradient of ceq dceq = [x(3), 0; 0, x(3); x(1), x(2)-g*x(3)]; end Running the code above, we get the following trajectory Figure 3.2: Trajectory of the canon ball obtained from fmincon. What if you change the initial guess to fmincon, or add more constraints? Feel free to play with the code here. It turns out there are multiple different ways to transcribe the optimal control problem (3.9) as the nonlinear programming problem (3.10). In the following, we will introduce three different formulations, direct single shooting, direct multiple shooting, and direct collocation. 3.2.1 Direct Single Shooting In direct single shooting, we transcribe the OCP to the NLP by only optimizing a sequence of control values, with the intuition being that the state values are functions of the controls by invoking the system dynamics. In particular, we follow the procedure below. Time discretization. We first discretize the total time window \\([0,T]\\) into a set of \\(N\\) intervals: \\[ 0 = t_0 \\leq t_1 \\leq \\dots \\leq t_k \\leq t_{k+1} \\leq \\dots \\leq t_N = T. \\] We denote \\[ h_k = t_{k+1} - t_k \\] as the length of the \\(k\\)-th time interval. Piece-wise constant control. We will approximate the continuous-time control signal \\(u(t)\\) as a piece-wise constant function, i.e., \\[ u(t) = u_k, \\forall t \\in [t_k, t_{k+1}). \\] This is shown in Fig. 3.3. Let us collect all the constant control values as our decision variable to be optimized \\[\\begin{equation} v = \\begin{bmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{N-1} \\end{bmatrix} \\in \\mathbb{R}^{N m}. \\tag{3.12} \\end{equation}\\] This \\(v\\) will be our variable in the NLP (3.10). Note that \\(v\\) has dimension \\(N m\\). Figure 3.3: Direct single shooting. Dynamics integration. It is clear that once \\(v\\) in (3.12), i.e., the set of controls, is determined, then the state trajectory \\(x(t)\\) is also uniquely determined by the initial condition \\(x(0) = x_0\\) and the dynamics \\[ \\dot{x}(t) = f(x(t),u(t)). \\] In order to enforce the state constraint \\(x(t) \\in \\mathcal{X}\\), we will enforce the values of \\(x\\) at \\(t_0,t_1,\\dots,t_N\\), i.e., \\(x_k = x(t_k)\\), to lie in the constraint set \\(\\mathcal{X}\\). To do so, we need to integrate the dynamics. When the dynamics is linear, this integration can be done in closed form: \\[ x_{k+1} = x_k + \\int_{\\tau=t_k}^{t_k + h_k} A x(\\tau) + B u(\\tau) d\\tau = e^{Ah_k} x_k + A^{-1}(e^{Ah_k} - I) B u_k. \\] By running the above equation from \\(k=0\\) to \\(k=N-1\\), we obtain the values of \\(x_k,k=1,\\dots,N\\) as functions of the control vectors \\(v\\), shown in Fig. 3.3. When the dynamics is nonlinear, we will need to perform the dynamics integration using numerical integration. One of the most well known family of integrators is called “Runge-Kutta methods”. Among the family of methods, RK45 is perhaps the most popular one. We now briefly describe how a fourth-order RK integrator, i.e., RK4, works. Suppose we have already computed \\(x_k\\), and we want to integrate the dynamics to obtain \\(x_{k+1}\\). The RK4 integrator first computes the time derivatives at a sequence of four points: \\[\\begin{align} \\alpha_1 &amp;= f(x_k, u_k) \\\\ \\alpha_2 &amp;= f\\left( x_k + \\frac{1}{2} h_k \\alpha_1, u_k \\right) \\\\ \\alpha_3 &amp;= f\\left( x_k + \\frac{1}{2} h_k \\alpha_2, u_k \\right) \\\\ \\alpha_4 &amp;= f\\left( x_k + h_k \\alpha_3, u_k \\right). \\end{align}\\] Then we can obtain \\(x_{k+1}\\) via a weighted sum of the \\(\\alpha_i\\)’s \\[\\begin{equation} x_{k+1} = x_k + \\frac{1}{6} h_k \\left( \\alpha_1 + 2 \\alpha_2 + 2 \\alpha_3 + \\alpha_4 \\right) = \\text{RK4}(x_k, u_k). \\tag{3.13} \\end{equation}\\] Notice that a naive integrator would just perform \\(x_{k+1} = x_k + h_k \\alpha_1\\) without querying the gradients at three other points. The nice property of RK4 is that it leads to better accuracy than the naive integration. Writing the RK4 integrator recursively, we obtain \\[ x_{k+1} = \\text{RK4}(x_k, u_k) = \\text{RK4}(\\text{RK4}(x_{k-1},u_{k-1}),u_k) = \\text{RK4}(\\text{RK4}(...(x_0,u_0))...u_k), \\] where RK4 is invoked for \\(k+1\\) times. Clearly, each \\(x_k\\) is a complicated function of the sequence of controls \\(v\\) and the initial state \\(x_0\\). We will write \\(x_k(v)\\) to make this explict, as shown in Fig. 3.3. Objective approximation. We can approximate the objective of the OCP (3.9) using trapezoidal integration: \\[ g_T(x(T)) + \\int_{t=0}^T g(x(t),u(t)) dt \\approx g_T(x_N(v)) + \\sum_{k=0}^{N-1} \\frac{h_k}{2}(g(x_k(v),u_k) + g(x_{k+1}(v),u_{k+1})). \\] Summary. In summary, the transcribed NLP for the optimal control problem using direct single shooting would be \\[\\begin{equation} \\begin{split} \\min_{v = (u_0,\\dots,u_{N-1})} &amp; \\quad g_T(x_N(v)) + \\sum_{k=0}^{N-1} \\frac{g(x_k(v),u_k) + g(x_{k+1}(v), u_{k+1})}{2} h_k \\\\ \\text{subject to} &amp; \\quad c^u_i(u_k) \\geq 0, i=1,\\dots,q_u, \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad c^x_i(x_k(v)) \\geq 0, i=1,\\dots,q_x, \\quad k=1,\\dots,N \\\\ &amp; \\quad \\phi_i(x_k(v),u_k) \\geq 0, i=1,\\dots, q, \\quad k=0,\\dots,N. \\end{split} \\tag{3.14} \\end{equation}\\] Pros and Cons. The advantage of using direct single shooting to transcribe the OCP is clear: it leads to fewer variables because the only decision variables in (3.14) are the sequence of controls. In other transcription methods, the decision variable of the NLP typically involves the states as well. The disadvantage of direct single shooting is the complication of the function \\(x_k(v)\\) caused by numerical integrators such as RK4. Even though the original continuous-time dynamics \\(f(x,u)\\) may be simple, the RK4 function can be highly complicated due to evaluating \\(\\dot{x}\\) at multiple other locations. Moreover, the recursion of the RK4 operator makes this complication even worse. Let us try the RK4 simulator for a simple example. Example 3.5 (Propagation of Nonlinearities) Consider the following dynamics \\[ x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}, \\quad \\dot{x} = \\begin{bmatrix} 10(x_2 - x_1) \\\\ x_1(u - x_3) - x_2 \\\\ x_1 x_2 - 3 x_3 \\end{bmatrix} \\] where \\(u\\) is a single scalar control. We are interested in running the RK4 simulator for \\(100\\) seconds (with \\(h_k = 0.01\\) for all \\(k\\)) at \\(x_0 = [1,0,0]^T\\) with \\(u\\) varying from \\(0\\) to \\(100\\), and see how the nonlinearities propagate. Fig. 3.4 plots \\(x\\) as a function of \\(u\\). We can clearly see the nonlinearities getting worse due to RK4. You can play with the code here. Figure 3.4: RK4 simulation. 3.2.2 Direct Multiple Shooting Figure 3.5: Direct multiple shooting. Propagation of the nonlinearities through numerical integrators motivates using direct multiple shooting to transcribe the OCP problem into a nonlinear programming problem. Instead of only optimizing the controls, direct multiple shooting optimizes both the controls and states. The decision variable in the NLP becomes \\[ v = \\begin{bmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{N-1} \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} \\in \\mathbb{R}^{N(n+m)}. \\] With the state variables also introduced in the optimization, we no longer need to recursively run the RK4 integrators. Instead, we just need to run it once and enforce \\[ x_{k+1} = \\text{RK4}(x_k, u_k), \\] i.e., the current state and the next state satisfy the dynamics constraint. This is shown in Fig. 3.5. In summary, the transcribed NLP for the OCP problem using direct multiple shooting becomes \\[\\begin{equation} \\begin{split} \\min_{v = (u_0,\\dots,u_{N-1},x_1,\\dots,x_N)} &amp; \\quad g_T(x_N) + \\sum_{k=0}^{N-1} \\frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k \\\\ \\text{subject to} &amp; \\quad x_{k+1} = \\text{RK4}(x_k, u_k), \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad c^u_i (u_k) \\geq 0, i=1,\\dots,q_u, \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad c^x_i(x_k) \\geq 0, i=1,\\dots,q_x, \\quad k=1,\\dots,N \\\\ &amp; \\quad \\phi_i(x_k, u_k) \\geq 0, i=1,\\dots,q, \\quad k=0,\\dots,N. \\end{split} \\tag{3.15} \\end{equation}\\] Direct multiple shooting avoids the recursion of numerical integrators such as RK4, but at the expense of introducing additional state variables in the NLP. Let us try direct multiple shooting on the double integrator. Example 3.6 (Direct Multiple Shooting for Minimum-Time Double Integrator) The double integrator has continuous-time dynamics (which you have already seen in Exercise 5.2): \\[ \\ddot{q} = u. \\] In standard state-space form, the dynamics is linear \\[ x = \\begin{bmatrix} q \\\\ \\dot{q} \\end{bmatrix}, \\quad \\dot{x} = \\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} x + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u. \\] Let us consider the minimum-time optimal control problem \\[\\begin{equation} \\begin{split} \\min_{u(t),t \\in [0,T]} &amp; \\quad T \\\\ \\text{subject to} &amp; \\quad \\dot{x} = A x + Bu, \\quad x(0) = x_0 \\\\ &amp; \\quad x(T) = 0 \\\\ &amp; \\quad u(t) \\in [-1,1], \\forall t \\in [0,T]. \\end{split} \\end{equation}\\] which seeks to get from the initial condition \\(x_0\\) to the origin as fast as possible. You should know from our physics intuition that the optimal controller is bang-bang, i.e., the optimal control first accelerates (deaccelerates) using the maximum control and then reverses using the opposite maximum control. Let’s see if we can obtain the optimal controller using direct multiple shooting. In direct multiple shooting, we will optimize the control trajectory, the state trajectory, and the final time \\(T\\). We fix the number of intervals \\(N\\), and discretize \\([0,T]\\) evenly into \\(N\\) intervals, which leads to variables for the NLP as \\[ v=(T,x_0,\\dots,x_{N},u_0,\\dots,u_N). \\] We can easily enforce the control constraints: \\[ u_k \\in [-1,1],k=0,\\dots,N, \\] and the initial / terminal constraints \\[ x_0 = x_0, \\quad x_N = 0. \\] The only nontrivial constraint is the dynamics constraint: \\[ x_{k+1} = \\text{RK4}(x_k,u_k),k=0,\\dots,N-1. \\] The following script shows how to use the Matlab integrator ode45 to enforce the dynamics constraint. function dx = double_integrator(t,states,v) % return xdot at the selected times t and states, using information from v % assume the controls in v define piece-wise constant control signal T = v(1); % final time N = (length(v) - 1) / 3; % number of knot points u_grid = v(2+2*N:3*N+1); % N controls t_grid = linspace(0,1,N)*T; u_t = interp1(t_grid,u_grid,t,&#39;previous&#39;); % piece-wise constant A = [0 1; 0 0]; B = [0; 1]; dx = A * states + B * u_t; end function [c,ceq] = double_integrator_nonlincon(v,initial_state) % enforce x_k+1 = RK45(x_k, u_k); integration done using ode45 T = v(1); % final time N = (length(v) - 1) / 3; % number of knot points t_grid = linspace(0,1,N)*T; x1 = v(2:N+1); % position x2 = v(2+N:2*N+1); % velocity % u = v(2+2*N:3*N+1); % controls % no inequality constraints c = []; % equality constraints ceq = []; for i = 1:(N-1) ti = t_grid(i); tip1 = t_grid(i+1); xi = [x1(i);x2(i)]; xip1 = [x1(i+1);x2(i+1)]; % integrate system dynamics starting from xi in [ti,tip1] tt = ti:(tip1-ti)/20:tip1; % fine-grained time discretization [~,sol_int] = ode45(@(t,y) double_integrator(t,y,v),tt,xi); xip1_int = sol_int(end,:); % enforce them to be the same ceq = [ceq; xip1_int(1) - xip1(1); xip1_int(2) - xip1(2)]; end % add initial state constraint ceq = [ceq; x1(1) - initial_state(1); x2(1) - initial_state(2)]; % add terminal state constraint: land at origin ceq = [ceq; x1(end); x2(end)]; end I first define a function double_integrator that returns the continuous-time dynaimcs, and then in the nonlinear constraints function double_integrator_nonlincon I use ode45 to simulate the dynamics starting at \\(x_k\\) from \\(t_k\\) to \\(t_{k+1}\\): [~,sol_int] = ode45(@(t,y) double_integrator(t,y,v),tt,xi); The solution I got from ode45 should be equal to my decision state variable. Running the complete code with \\(x_0 = [-10;0]\\) and \\(N=51\\), I obtain the control signal in Fig. 3.6, which is bang-bang. Figure 3.6: Optimal control signal by direct multiple shooting. Using ode45 to integrate the double integrator dynamics from \\(x_0\\) with the controller in Fig. 3.6, we obtain the following state trajectory. Notice that the final state does not exactly land at the origin. This is expected due to our time discretization and imperfect dynamics integration using ode45. Make sure you play with the code, e.g., by changing the initial state \\(x_0\\) and see what happens. Figure 3.7: ODE45 integration with the optimal control signal found by direct multiple shooting. 3.2.3 Direct Collocation In direct multiple shooting, we still need to rely on the numerical integrator RK4, which complicates the original nonlinear dynamics. In direct collocation, we will remove our dependency of RK4. The key idea of direct collocation is to approximate the state trajectory \\(x(t)\\) and the control trajectory \\(u(t)\\) as piece-wise polynomial functions. In the following, we will describe the Hermite-Simpson collocation method. Figure 3.8: Direct collocation. Time discretization. We first discretize the total time window \\([0,T]\\) into a set of \\(N\\) intervals: \\[ 0 = t_0 \\leq t_1 \\leq \\dots \\leq t_k \\leq t_{k+1} \\leq \\dots \\leq t_N = T. \\] We denote \\[ h_k = t_{k+1} - t_k \\] as the length of the \\(k\\)-th time interval. As we will see, the length of the time interval does not need to be fixed, and instead they can themselves be unknown variables to be optimized (in which case the final time \\(T\\) also becomes flexible). Knot variables. At each of the timestamps \\(t_0,\\dots,t_N\\), we assign knot variables, which are unknown state and control varaibles that need to be optimized. In particular, we have state knot variables \\[ x_k = x(t_k), k=1,\\dots,N, \\] and control knot variables \\[ u_k = u(t_k),k=0,\\dots,N-1. \\] As a result, the entire set of knot varaibles to be optimized is \\[\\begin{equation} v = \\begin{bmatrix} u_0 \\\\ u_1 \\\\ \\vdots \\\\ u_{N-1} \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{N} \\end{bmatrix}. \\tag{3.16} \\end{equation}\\] If the time-discretization is also optimized, then \\(v\\) includes the time intervals as well. Transcribe dynamics. The most important step is to transcribe the nonlinear dynamics \\(\\dot{x}=f(x(t),u(t))\\) as constraints on the knot varaibles. In direct collocation, the way this is done is to enforce the dynamics equation at the set of collocation points that are the mid-points between each consecutive pair of knot variables \\((x_k, x_{k+1})\\). Specifically, in each subinterval \\([t_k, t_{k+1}]\\), we approximate the state trajectory as a cubic polynomial \\[\\begin{equation} x(t) = p_{k,0} + p_{k,1}(t-t_k) + p_{k,2}(t-t_k)^2 + p_{k,3}(t - t_k)^3, \\quad t \\in [t_k, t_{k+1}], \\tag{3.17} \\end{equation}\\] where \\(p_{k,0},p_{k,1},p_{k,2},p_{k,3} \\in \\mathbb{R}^n\\) are the coefficients of the polynomial. You would think that we would need to optimize the coefficients as well, but actually we won’t need to, as will be shown soon. With this parameterization, we can obtain the time derivative of \\(x(t)\\) as \\[\\begin{equation} \\dot{x}(t) = p_{k,1} + 2 p_{k,2}(t-t_k) + 3p_{k,3}(t - t_k)^2, \\quad t \\in [t_k, t_{k+1}]. \\tag{3.18} \\end{equation}\\] Now the key step is to write the coefficients \\(p_{k,0},p_{k,1},p_{k,2},p_{k,3}\\) using our knot variables (3.16). To do so, we can invoke (3.17) and (3.18) to obtain \\[ \\begin{bmatrix} x_k \\\\ \\dot{x}_k = f(x_k,u_k) \\\\ x_{k+1} \\\\ \\dot{x}_{k+1} = f(x_{k+1}, u_{k+1}) \\end{bmatrix} = \\begin{bmatrix} I &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I &amp; 0 &amp; 0 \\\\ I &amp; h_k I &amp; h_k^2 I &amp; h_k^3 I \\\\ 0 &amp; I &amp; 2 h_k I &amp; 3 h_k^2 I \\end{bmatrix} \\begin{bmatrix} p_{k,0}\\\\ p_{k,1}\\\\ p_{k,2}\\\\ p_{k,3} \\end{bmatrix}. \\] Solving the above equation, we get \\[\\begin{equation} \\begin{bmatrix} p_{k,0}\\\\ p_{k,1}\\\\ p_{k,2}\\\\ p_{k,3} \\end{bmatrix} = \\begin{bmatrix} I &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; I &amp; 0 &amp; 0 \\\\ - \\frac{3}{h_k^2} I &amp; - \\frac{2}{h_k} I &amp; \\frac{3}{h_k^2} I &amp; -\\frac{1}{h_k} I \\\\ \\frac{2}{h_k^3} I &amp; \\frac{1}{h_k^2} I &amp; -\\frac{2}{h_k^3} I &amp; \\frac{1}{h_k^2} I \\end{bmatrix} \\begin{bmatrix} x_k \\\\ f(x_k,u_k) \\\\ x_{k+1} \\\\ f(x_{k+1}, u_{k+1}) \\end{bmatrix}. \\tag{3.19} \\end{equation}\\] Equation (3.19) implies, using the knot variables \\(v\\) in (3.16), we can query the value of \\(x(t)\\) and \\(\\dot{x}(t)\\) at any time \\(t \\in [0,T]\\). In particular, we will query the values of \\(x(t)\\) and \\(\\dot{x}(t)\\) at the midpoints to obtain \\[ x_k^c = x\\left( t_k + \\frac{h_k}{2} \\right) = \\frac{1}{2} (x_k + x_{k+1}) + \\frac{h_k}{8} (f(x_k,u_k) - f(x_{k+1},u_{k+1})), \\] and \\[ \\dot{x}_k^c = \\dot{x} \\left( t_k + \\frac{h_k}{2} \\right) = - \\frac{3}{2h_k} (x_k - x_{k+1}) - \\frac{1}{4} \\left( f(x_k,u_k) + f(x_{k+1},u_{k+1}) \\right). \\] At the midpoint, we assume the control is \\[ u_k^c = \\frac{1}{2}(u_k + u_{k+1}). \\] Therefore, we can enforce the dynamics constraint at the midpoint as \\[\\begin{equation} \\dot{x}^c_k = f(x_k^c, u_k^c). \\tag{3.20} \\end{equation}\\] Transcribe other constraints. The other constraints in the continuous-time formulation (3.9) can be transcribed to the knot variables in a straightforward way: \\[\\begin{align} x_k \\in \\mathcal{X} \\Rightarrow c_i^x(x_k)\\geq 0, i=1,\\dots,q_x, \\quad k=1,\\dots,N \\\\ u_k \\in \\mathcal{X} \\Rightarrow c^u_i(u_k) \\geq 0, i=1,\\dots,q_u, \\quad k=0,\\dots,N-1\\\\ \\phi_i(x_k, u_k) \\geq 0, i=1,\\dots,q, \\quad k=0,\\dots,N. \\end{align}\\] Transcribe the objective. We can write the objective as \\[ g_T(x_N) + \\sum_{k=0}^{N-1} \\frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k. \\] Summary. In summary, the final optimization problem becomes \\[\\begin{equation} \\begin{split} \\min_{u_0,\\dots,u_{N-1},x_1,\\dots,x_N} &amp; \\quad g_T(x_N) + \\sum_{k=0}^{N-1} \\frac{g(x_k,u_k) + g(x_{k+1},u_{k+1})}{2} h_k \\\\ \\text{subject to} &amp; \\quad \\dot{x}_k^c = f(x_k^c,u_k^c), \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad c_i^x(x_k) \\geq 0,i=1,\\dots,q_x, \\quad k=1,\\dots,N \\\\ &amp; \\quad c_i^u(u_k) \\geq 0,i=1,\\dots,q_u, \\quad k=0,\\dots,N-1 \\\\ &amp; \\quad \\phi_i (x_k,u_k) \\geq 0, i=1,\\dots,q, \\quad k=0,\\dots,N. \\end{split} \\tag{3.21} \\end{equation}\\] Let us apply direct collocation to the same double integrator Example 3.6. Example 3.7 (Direct Collocation for Minimum-Time Double Integrator) Consider the same minimum-time optimal control problem in Example 3.6. To apply direct collocation, we have our NLP variable \\[ v = (T,x_0,\\dots,x_N,u_0,\\dots,u_N). \\] Similarly we can enforce the control constraints and the initial / terminal constraints. The following script shows how to enforce the collocation constraint. function [c,ceq] = collocation(v,N,initial_state) T = v(1); h = T/(N-1); x = reshape(v(2:2*N+1),2,N); u = v(2*N+2:end); c = []; ceq = []; for k=1:N-1 uk = u(k); ukp1 = u(k+1); xk = x(:,k); xkp1 = x(:,k+1); fk = double_integrator(xk,uk); fkp1 = double_integrator(xkp1,ukp1); % collocation points xkc = 0.5*(xk+xkp1) + h/8 * (fk - fkp1); ukc = 0.5*(uk + ukp1); dxkc = -3/(2*h) * (xk-xkp1) - 0.25*(fk + fkp1); % collocation constraint ceq = [ceq; dxkc - double_integrator(xkc,ukc)]; end ceq = [ceq; x(:,1) - initial_state; % initial condition x(:,end)]; % land at zero end As you can see, we do not need any numerical integrator such as ode45. The only thing we need is the continuous-time double integrator dynamics. Running the complete code with \\(x_0 = (-10,0)\\) and \\(N=51\\), we get the control signal in Fig. 3.9 that is bang-bang. Figure 3.9: Optimal control signal by direct collocation. Using ode45 to integrate the double integrator dynamics from \\(x_0\\) with the controller in Fig. 3.9, we obtain the following state trajectory. Comparing Fig. 3.10 with Fig. 3.7, we can observe that the terminal state of the trajectory obtained from direct collocation is more accurate than that obtained from direct multiple shooting. Figure 3.10: ODE45 integration with the optimal control signal found by direct collocation. Not only is direct collocation more accurate for this example, it is also faster. This is evident because in direct multiple shooting, evaluating the nonlinear constraints requires runing ode45, while in direct collocation, evaluating the nonlinear constraints simply requires calling the original continuous-time dynamics. We can easily optimize with a larger \\(N=101\\) and get the following results. Figure 3.11: Direct collocation with N=101. If you are interested in direct collocation, there is a nice tutorial with Matlab examples in (Kelly 2017). 3.2.4 Direct Orthogonal Collocation 3.2.5 Failure of Open-Loop Control Trajectory optimization produces very satisfying trajectories when the nonlinear programming algorithms work as expected, as shown by the previous Examples 3.7 and 3.6 on the double integrator system. However, does this imply that if we simply run the planned controls on a real system, i.e., doing open-loop control, the trajectory will behave as planned? Unfortunately the answer is No, for two major reasons. Trajectory optimization only produces an approximate solution to the optimal control problem (OCP) (3.9), regardless of what transcription method is used (such as multiple shooting and direct collocation). This means every \\(u_k\\) we obtained is an approximation to the true optimal controller \\(u(t_k)\\), and the approximation errors will accumulate as the dynamical system is evolving. Even the OCP (3.9) is an imperfect approximation of the true control task. This is due to we assumed we have perfect knowledge of the system dynamics \\[ \\dot{x} = f(x(t),u(t)), \\] which rarely holds in practice. For example, in the double integrator example, there is always friction between the mass and the ground. A more realistic assumption is that the system dynamics is \\[ \\dot{x} = f(x(t),u(t)) + w_t, \\] where \\(w_t\\) is some unknown disturbance, or modeling error. Let us observe the failure of open-loop control on our favorite pendulum example. Example 3.8 (Failure of Open-Loop Control on A Simple Pendulum) Consider an “ideal” pendulum dynamics model \\[ x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix}, \\] with \\(m=1,l=1,g=9.8,b=0.1\\). We enforce control saturation \\[ u \\in [-u_{\\max}, u_{\\max}] \\] with \\(u_{\\max} = 4.9\\). Figure 3.12: Simple pendulum. Consider the optimal control problem with \\(T=10\\) \\[\\begin{equation} \\begin{split} \\min_{u(t), t \\in [0,T]} &amp; \\quad \\int_{t=0}^T (\\cos \\theta(t) + 1)^2 + \\sin^2\\theta(t) + \\dot{\\theta}^2(t) + u^2 dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(t),u(t)) \\\\ &amp; \\quad x(0) = [0,0]^T, \\quad x(T) = [\\pi,0]^T, \\\\ &amp; \\quad u(t) \\in [-u_{\\max},u_{\\max}], \\end{split} \\tag{3.22} \\end{equation}\\] where we start from the initial state \\([0,0]^T\\), i.e., the bottomright position, and we want to swing up the pendulum to the final state \\([\\pi,0]^T\\), i.e., the upright position. The cost function in (3.22) penalizes the deviation of the state trajectory from the target state (the target state has \\(\\cos \\theta = -1\\), \\(\\sin \\theta = 0\\) and \\(\\dot{\\theta} = 0\\)), together with the magnitude of control. Trajectory optimization with direct collocation. We perform trajectory optimization for the OCP (3.22) with direct collocation. We choose \\(N=101\\) break points with \\(h=0.1\\) equal interval to discretize the time. The result of trajectory optimization is shown in Fig. 3.13. We can see that the pendulum is perfectly swung up to \\([\\pi,0]^T\\) and stabilized there. Figure 3.13: Pendulum swing-up with direct collocation Deploy the optimized plan. We then deploy the optimized controls in Fig. 3.13 on the “ideal” pendulum. We deploy the control every \\(0.1\\) seconds with zero-order hold, and use Matlab ode89 to integrate the pendulum dynamics. Fig. 3.14 shows the true trajectory of the pendulum. Unfortunately, the pendulum swing-up and stabilization is unsuccessful. Figure 3.14: Deploy optimized controls Adding noise. What if the true pendulum dynamics is \\[ x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. \\] That is, there is a constant unmodelled angular acceleration of \\(1\\), which maybe come from some unknown external force. Fig. 3.15 shows the trajectory of the pendulum with the optimized controlls. We can clearly see the unmodelled dynamics makes the performance much worse. Figure 3.15: Deploy optimized controls on a system with disturbance You can play with the code here. The failure of open-loop control motivates feedback control. 3.2.6 LQR Trajectory Tracking 3.3 Model Predictive Control The model predictive control framework leverages the idea of receding horizon control (RHC) to turn an open-loop controller into a closed-loop controller, i.e., a feedback controller. For example, suppose we have an optimal control problem with horizon \\(T\\): \\[\\begin{equation} \\begin{split} \\min_{u(t), t \\in [0,T]} &amp; \\quad g_T(x(T)) + \\int_{t=0}^T g(x(t),u(t)) dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(t),u(t)), \\quad x(0) = x_0 \\\\ &amp; (x(t),u(t)) \\in \\mathcal{X} \\times \\mathcal{U}, \\quad x(T) \\in \\mathcal{X}_T \\end{split} \\tag{3.23} \\end{equation}\\] where \\(\\dot{x} = f(x(t),u(t))\\) is the best “ideal” model we have about our system. The idea of RHC is as follows. At time \\(t\\), we obtain a measurement of the current state of the system, denoted as \\(x_t\\), and we solve the following OCP with horizon \\(H &lt; T\\): \\[\\begin{equation} \\begin{split} \\min_{u(\\tau), \\tau \\in [0,H]} &amp; \\quad g_t(x(H)) + \\int_{\\tau = 0}^H g(x(\\tau),u(\\tau)) d\\tau \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(\\tau),u(\\tau)), \\quad x(0) = x_t \\\\ &amp; \\quad (x(t),u(t)) \\in \\mathcal{X} \\times \\mathcal{U}, \\quad x(H) \\in \\mathcal{X}_{t} \\end{split} \\tag{3.24} \\end{equation}\\] where notice that I used a different notation for the terminal cost \\(g_t\\) (as supposed to \\(g_T\\) in (3.23)), and the terminal constraint \\(\\mathcal{X}_t\\) (as supposed to \\(\\mathcal{X}_T\\) in (3.24)). Using a different terminal cost and terminal constraint is meant to make the problem (3.24) always feasible. Suppose we solve the open-loop control problem (3.24) using trajectory optimization with \\(N\\) time intervals and obtained the following solution \\[ u_{t,0},u_{t,1},\\dots,u_{t,N}. \\] Then RHS will only take the first control \\(u_{t,0}\\) and apply it to the system. Therefore, the closed-loop dynamics is effectively \\[\\begin{equation} \\dot{x} = f(x(t),u_{t,0}), \\tag{3.25} \\end{equation}\\] where \\(u_{t,0}\\) is the first control solution to the OCP (3.24). Figure 3.16: Receding horizon control It is worth noting that RHS effectively introduces feedback control, because the controller \\(u_{t,0}\\) in (3.25) depends on \\(x(t)\\) via the open-loop optimal control problem (3.24). Trajectory optimization and model predictive control are the workhorse for control of highly dynamic robots. For example, here is a talk by Scott Kuindersma on how trajectory optimization and model predictive control enable a diverse set of behaviors of Boston Dynamics’s humanoid robot. 3.3.1 Turn Trajectory Optimization into Feedback Control Let us turn our trajectory optimization algorithm in Example 3.8 into a feedback controller through receding horizon control. Example 3.9 (Pendulum Swingup with Model Predictive Control) Consider again the pendulum swingup task in Example 3.8. This time, we will apply MPC to this task. Again, we discretize the time windown \\([0,T]\\) into \\(N-1 = 100\\) equal intervals with \\(h = 0.1\\). At each timestep \\(t_k = kh\\), \\(k=0,\\dots,N-1\\), we first measure the current state of the pendulum as \\(x_k\\), then solve the following OCP with planning horizon \\(H=5\\) \\[\\begin{equation} \\begin{split} \\min_{u(t), t \\in [0,H]} &amp; \\quad \\int_{t=0}^H (\\cos \\theta(t) + 1)^2 + \\sin^2\\theta(t) + \\dot{\\theta}^2(t) + u^2 dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x(t),u(t)) \\\\ &amp; \\quad x(0) = x_k, \\quad x(H) = [\\pi,0]^T, \\\\ &amp; \\quad u(t) \\in [-u_{\\max},u_{\\max}]. \\end{split} \\tag{3.26} \\end{equation}\\] We solve the OCP (3.26) using trajectory optimization with direct collocation (using \\(M\\) break points), which gives us a sequence of controls \\[ u_{k,1},\\dots,u_{k,M}, \\] we deploy the first control \\(u_{k,1}\\) to the pendulum system. MPC results. Fig. 3.17 shows the control and state trajectories when using MPC to swing up the pendulum. We can see that it works very well. Figure 3.17: MPC for pendulum swing-up Robustness to noise. MPC is naturally robust to modelling errors and noises. When the true system dynamics is \\[ x = \\begin{bmatrix} \\theta \\\\ \\dot{\\theta} \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} \\dot{\\theta} \\\\ - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) + \\frac{1}{ml^2} u \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. \\] The MPC controller still works very well, as shown in Fig. 3.18! Figure 3.18: MPC for pendulum swing-up with noise Feel free to play with the code here. Software tools. Matlab implements a nice package for nonlinear MPC, you can refer to the documentation and examples. In Python, the do-mpc package is quite popular. However, it is important to recognize that trajectory optimization for the nonconvex open-loop optimal control problen (3.24) (a) only guarantees a locally optimal solution and hence can be brittle, (b) can be time-consuming when the problem is high-dimensional. In fact, MPC was originally developed in the context of chemical plant control (Borrelli, Bemporad, and Morari 2017), which has linear system dyanmics, as we will introduce soon. Before we introduce the MPC formulation and understand its theoretical properties, we need to take a detour and introduce various important notions of sets. 3.3.2 Controllability, Reachability, and Invariance Consider the autonomous system \\[\\begin{equation} x_{t+1} = f_a(x_t) \\tag{3.27} \\end{equation}\\] and the controlled system \\[\\begin{equation} x_{t+1} = f(x_t, u_t) \\tag{3.28} \\end{equation}\\] for \\(t \\geq 0\\). Both systems are subject to state and input constraints \\[\\begin{equation} x_t \\in \\mathcal{X}, \\quad u_t \\in \\mathcal{U}, \\forall t \\geq 0, \\tag{3.29} \\end{equation}\\] with \\(\\mathcal{X}\\) and \\(\\mathcal{U}\\) both being polyhedral sets (see Definition B.9) \\[\\begin{equation} \\begin{split} x_t \\in \\mathcal{X} = \\left\\{ x \\in \\mathbb{R}^n \\mid C x \\leq c \\right\\}, \\quad C \\in \\mathbb{R}^{l_x \\times n}, c \\in \\mathbb{R}^{l_x}, \\\\ u_t \\in \\mathcal{U} = \\left\\{ u \\in \\mathbb{R}^m \\mid D u \\leq d \\right\\}, \\quad D \\in \\mathbb{R}^{l_u \\times m}, d \\in \\mathbb{R}^{l_u}. \\end{split} \\tag{3.30} \\end{equation}\\] We are going to make a few definitions. 3.3.2.1 Controllable and Reachable Sets Definition 3.1 (Precursor Set) For the autonomous system (3.27), we define the precursor set to a set \\(\\mathcal{S}\\) as \\[ \\text{Pre}(\\mathcal{S}) = \\left\\{ x \\in \\mathbb{R}^n \\mid f_a(x) \\in \\mathcal{S} \\right\\}. \\] In words, \\(\\text{Pre}(\\mathcal{S})\\) is the set of states which evolve into the target set \\(\\mathcal{S}\\) in one time step. For the controlled system (3.28), we define the precursor set to the set \\(\\mathcal{S}\\) as \\[ \\text{Pre}(\\mathcal{S}) = \\left\\{ x \\in \\mathbb{R}^n : \\exists u \\in \\mathcal{U} \\text{ s.t. } f(x,u) \\in \\mathcal{S} \\right\\}. \\] In words, \\(\\text{Pre}(\\mathcal{S})\\) is the set of states that can be driven into the target set \\(\\mathcal{S}\\) while satisfying control and state constraints. A related concept is the successor set. Definition 3.2 (Successor Set) For the autonomous system (3.27), we define the successor set to a set \\(\\mathcal{S}\\) as \\[ \\text{Suc}(\\mathcal{S}) = \\left\\{ x \\in \\mathbb{R}^n \\mid \\exists x&#39; \\in \\mathcal{S} \\text{ s.t. } x = f_a(x&#39;)\\right\\}. \\] In words, the states in \\(\\mathcal{S}\\) get mapped into the states in \\(\\text{Suc}(\\mathcal{S})\\) after one time step. For the controlled system (3.28), we define the successor set to the set \\(\\mathcal{S}\\) as \\[ \\text{Suc}(\\mathcal{S}) = \\left\\{ x \\in \\mathbb{R}^n \\mid \\exists x&#39; \\in \\mathcal{S}, \\exists u&#39; \\in \\mathcal{U} \\text{ s.t. } x = f(x&#39;,u&#39;). \\right\\} \\] In words, the states in \\(\\mathcal{S}\\) get mapped into the states in \\(\\text{Suc}(\\mathcal{S})\\) after one time step while satisfying the control constraints. The \\(N\\)-step controllable set is defined by iterating \\(\\text{Pre}(\\cdot)\\) computations. Definition 3.3 (N-Step Controllable Set) Let \\(\\mathcal{S} \\subseteq \\mathcal{X}\\) be a target set. The \\(N\\)-step controllable set \\(\\mathcal{K}_{N}(\\mathcal{S})\\) of the system (3.27) or (3.28) subject to the constraints (3.29) is defined recursively as: \\[ \\mathcal{K}_0 (\\mathcal{S}) = \\mathcal{S}, \\quad \\mathcal{K}_{i}(\\mathcal{S}) = \\text{Pre}(\\mathcal{K}_{i-1}(\\mathcal{S})) \\cap \\mathcal{X}, i=1,\\dots,N. \\] According to this definition, given any \\(x_0 \\in \\mathcal{K}_N(\\mathcal{S})\\): For the autonomous system (3.27), its state will evolve into the target set \\(\\mathcal{S}\\) in \\(N\\) steps while satisfying state constraints For the controlled system (3.28), there exists a sequence of admissible controls (i.e., satisfying control constraints) such that its state will evolve into the target set \\(\\mathcal{S}\\) in \\(N\\) steps while satisfying state constraints. Similarly, the \\(N\\)-step reachable set is defined by iterating \\(\\text{Suc}(\\cdot)\\) computations. Definition 3.4 (N-Step Reachable Set) Let \\(\\mathcal{X}_0 \\subseteq \\mathcal{X}\\) be an initial set. The \\(N\\)-step reachable set \\(\\mathcal{R}_N (\\mathcal{X}_0)\\) of the system (3.27) or (3.28) subject to the constraints (3.29) is defined recursively as: \\[ \\mathcal{R}_0(\\mathcal{X}_0) = \\mathcal{X}_0, \\quad \\mathcal{R}_{i}(\\mathcal{X}_0) = \\text{Suc}(\\mathcal{R}_{i-1}(\\mathcal{X}_0)) \\cap \\mathcal{X}, i=1,\\dots,N. \\] According to this definition, given any \\(x_0 \\in \\mathcal{X}_0\\): For the autonomous system (3.27), its state will evolve into \\(\\mathcal{R}_N(\\mathcal{X}_0)\\) in \\(N\\) steps while satisfying state constraints For the controlled system (3.28), there exists a sequence of admissible controls (i.e., satisfying control constraints) such that its state will evolve into \\(\\mathcal{R}_N(\\mathcal{X}_0)\\) in \\(N\\) steps while satisfying state constraints. In the literature, the controllable set is often referred to as the backwards reachable set. 3.3.2.2 Computation of Controllable and Reachable Sets For a linear time-invariant system, when the state constraint set \\(\\mathcal{X}\\), control constraint set \\(\\mathcal{U}\\), the target set \\(\\mathcal{S}\\), and the initial set \\(\\mathcal{X}_0\\) are all polytopes, then the \\(N\\)-step controllable set \\(\\mathcal{K}_{N}(\\mathcal{S})\\) and the \\(N\\)-step reachable set \\(\\mathcal{R}_{N}(\\mathcal{X}_0)\\) are both polytopes and can be computed exactly and efficiently. We will not describe the underlying algorithm for computing \\(\\mathcal{K}_{N}(\\mathcal{S})\\) and \\(\\mathcal{R}_{N}(\\mathcal{X}_0)\\) (these details can be found in (Borrelli, Bemporad, and Morari 2017) and they are not very difficult to understand, so I suggest you to read them), but we now show you how to use the Multi-Parametric Toolbox (MPT) to compute them. Example 3.10 (Compute Controllable and Reachable Sets) Consider a linear system \\[ x_{t+1} = \\begin{bmatrix} 1.5 &amp; 0 \\\\ 1 &amp; -1.5 \\end{bmatrix} x_t + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} u_t \\] with state and control constraints \\[ \\mathcal{X} = [-10,10]^2, \\quad \\mathcal{U} = [-5,5]. \\] Given \\(\\mathcal{S} = \\mathcal{X}_0 = [-1,1]^2\\), I want to compute \\(\\mathcal{K}_i(\\mathcal{S})\\) and \\(\\mathcal{R}_i(\\mathcal{X}_0)\\), for \\(i=0,1,2,3,4\\). Dynamical system. We first define the linear time-invariant system as follows. A = [1.5, 0; 1, -1.5]; B = [1; 0]; sys = LTISystem(&#39;A&#39;,A,&#39;B&#39;,B); We then define the state and control constraints. calX = Polyhedron(&#39;A&#39;,... [1,0;0,1;-1,0;0,-1], ... &#39;b&#39;,[10;10;10;10]); calU = Polyhedron(&#39;A&#39;,[1;-1],&#39;b&#39;,[5;5]); Note that in the MPT toolbox, to define a polytope \\(P = \\{ x \\in \\mathbb{R}^n \\mid A x \\leq b \\}\\), we just need to call the Polyhedron function with inputs A and b. Controllable sets. We have from Definition 3.3 the recursion \\[ \\mathcal{K}_0(\\mathcal{S}) = \\mathcal{S}, \\quad \\mathcal{K}_i(\\mathcal{S}) = \\text{Pre}(\\mathcal{K}_{i-1}(\\mathcal{S})) \\cap \\mathcal{X}. \\] To implement the above set operation, we need two functions, one for the “\\(\\cap\\)” intersection operation, and the other for the “\\(\\text{Pre}(\\cdot)\\)” precursor set operation. These two functions are both available through MPT. intersect(P,S) computes the intersection of two sets P and S, both defined as polytopes. system.reachableSet('X', X, 'U', U, 'N', 1, 'direction', 'backwards') computes the one-step backwards reachable set (which is just the precursor set) of the system with target set X and control constraints U. Using these two functions, the following code snippet computes recursively the controllable sets for \\(N\\) steps. % target set S = Polyhedron(&#39;A&#39;,[1,0;0,1;-1,0;0,-1],&#39;b&#39;,[1;1;1;1]); K = [S]; N = 4; for i = 1:N Si = sys.reachableSet(&#39;X&#39;,K(i),&#39;U&#39;,calU,&#39;N&#39;,1,&#39;direction&#39;,&#39;backwards&#39;); K = [K; intersect(Si,calX)]; end Fig. 3.19 plots the controllable sets computed by running the above code. Figure 3.19: Computation of controllable sets using MPT. The bottom plot shifts the sets horizontally for better visualization. Reachable sets. We have Definition 3.4 the recursion \\[ \\mathcal{R}_0(\\mathcal{X}_0) = \\mathcal{X}_0, \\quad \\mathcal{R}_i(\\mathcal{X}_0) = \\text{Suc}(\\mathcal{R}_{i-1}(\\mathcal{X}_0)) \\cap \\mathcal{X}. \\] To implement the recursion above, we need “\\(\\cap\\)” set intersection, which is available via intersect(P,S), and the “\\(\\text{Suc}(\\cdot)\\)” successor set operation, which is available as system.reachableSet('X', X, 'U', U, 'N', 1, 'direction', 'forward') computes the one-step forward reachable set (which is just the successor set) of the system with initial set X and control constraints U. Therefore, we can compute the reachable sets recursively using the following code snippet % initial set X0 = Polyhedron(&#39;A&#39;,[1,0;0,1;-1,0;0,-1],&#39;b&#39;,[1;1;1;1]); R = [X0]; N = 4; for i = 1:N Ri = sys.reachableSet(&#39;X&#39;,R(i),&#39;U&#39;,calU,&#39;N&#39;,1,&#39;direction&#39;,&#39;forward&#39;); R = [R; intersect(Ri,calX)]; end Fig. 3.20 plots the reachable sets computed by running the above code. Feel free to play with the code here. Note that you need to install the MPT toolbox before running the code. Figure 3.20: Computation of reachable sets using MPT. The bottom plot shifts the sets horizontally for better visualization. 3.3.2.3 Invariant Sets Definition 3.5 (Positive Invariant Set) A set \\(\\mathcal{O} \\subseteq \\mathcal{X}\\) is said to be a positive invariant set for the autonomous system (3.27) subject to the constraints in (3.29) if \\[ x_0 \\in \\mathcal{O} \\Longrightarrow x_t \\in \\mathcal{O}, \\forall t \\geq 0. \\] That is, if the system starts in \\(\\mathcal{O}\\), it stays in \\(\\mathcal{O}\\) for all future timesteps. The maximal positive invariant set is the largest positive invariant set. Definition 3.6 (Maximal Positive Invariant Set) A set \\(\\mathcal{O}_{\\infty} \\subseteq \\mathcal{X}\\) is said to be the maximal positive invariant set for the autonomous system (3.27) subject to the constraints in (3.29) if (a) \\(\\mathcal{O}_{\\infty}\\) is positive invariant and (b) \\(\\mathcal{O}_{\\infty}\\) contains all the invariant sets contained in \\(\\mathcal{X}\\). Essentially, the (maximal) positive invariant set is the set of initial conditions under which the system does not blow up. For the controlled system (3.28), we have the similar notion of a control invariant set. Definition 3.7 (Control Invariant Set) A set \\(\\mathcal{C} \\subseteq \\mathcal{X}\\) is said to be a control invariant set for the controlled system (3.28) subject to the constraints in (3.29) if \\[ x_0 \\in \\mathcal{C} \\Longrightarrow \\exists u_t \\in \\mathcal{U} \\text{ s.t. } f(x_t,u_t) \\in \\mathcal{C}, \\forall t \\geq 0 \\] That is, if the system starts in \\(\\mathcal{C}\\), it can be controlled to stay in \\(\\mathcal{C}\\) for all future time steps. The maximal control invariant set is the largest control invariant set. Definition 3.8 (Maximal Control Invariant Set) A set \\(\\mathcal{C}_{\\infty} \\subseteq \\mathcal{X}\\) is said to be the maximal control invariant set for the controlled system (3.28) subject to the constraints in (3.29) if (a) \\(\\mathcal{C}_{\\infty}\\) is control invariant, and (b) \\(\\mathcal{C}_{\\infty}\\) contains all control invariant sets contained in \\(\\mathcal{X}\\). Essentially, the (maximal) control invariant set is the set of initial conditions under which the system can be controlled to not blow up. We now state a necessary and sufficient condition for a set to be positive invariant and control invariant. Theorem 3.1 (Geometric Condition for Invariance) For the autonomous system (3.27) subject to the constraint (3.29), a set \\(\\mathcal{O} \\subseteq \\mathcal{X}\\) is positive invariant if and only if \\[\\begin{equation} \\mathcal{O} \\subseteq \\text{Pre}(\\mathcal{O}). \\tag{3.31} \\end{equation}\\] Similarly, for the controlled system (3.28) subject to the constraint (3.29), a set \\(\\mathcal{C}\\) is control invariant if and only if \\[\\begin{equation} \\mathcal{C} \\subseteq \\text{Pre}(\\mathcal{C}). \\tag{3.32} \\end{equation}\\] Proof. We only prove (3.31) since (3.32) can be proved using similar arguments. “\\(\\Leftarrow\\)”: we want to show \\(\\mathcal{O}\\) is positive invariant if (3.31) holds. We prove this by contradiction. If \\(\\mathcal{O}\\) is not positive invariant, the \\(\\exists \\hat{x} \\in \\mathcal{O}\\) such that \\(f_a(\\hat{x}) \\not\\in \\mathcal{O}\\). This implies we have found \\(\\hat{x} \\in \\mathcal{O}\\) but \\(\\hat{x} \\not\\in \\text{Pre}(\\mathcal{O})\\), contradicting (3.31). “\\(\\Rightarrow\\)”: we want to show (3.31) holds if \\(\\mathcal{O}\\) is positive invariant. We prove this by contradiction. Suppose (3.31) does not hold, then \\(\\exists \\hat{x}\\) such that \\(\\hat{x} \\in \\mathcal{O}\\) but \\(\\hat{x} \\not\\in \\text{Pre}(\\mathcal{O})\\). This implies we have found \\(\\hat{x} \\in \\mathcal{O}\\) that does not remain in \\(\\mathcal{O}\\) in the next step, contradicting \\(\\mathcal{O}\\) being positive invariant. This shows that (3.31) is a sufficient and necessary condition. Theorem 3.1 immediately suggests an algoithm for computing (control) invariant sets, as we will describe in the next section. 3.3.2.4 Computation of Invariant Sets Observe that the geometric conditions (3.31) and (3.32) are equivalent to the following conditions \\[ \\mathcal{O} = \\text{Pre}(\\mathcal{O}) \\cap \\mathcal{O}, \\quad \\mathcal{C} = \\text{Pre}(\\mathcal{C}) \\cap \\mathcal{C}. \\] Based on the equation above, we can design an algoithm that iteratively evaluates \\(\\text{Pre}(\\mathcal{\\Omega}) \\cap \\mathcal{\\Omega}\\) until it converges. Algorithm: Computation of \\(\\mathcal{O}_{\\infty}\\) Input: \\(f_a\\), \\(\\mathcal{X}\\) Output: \\(\\mathcal{O}_{\\infty}\\)    \\(\\Omega_0 \\leftarrow \\mathcal{X}\\), \\(k=0\\)    Repeat      \\(\\Omega_{k+1} \\leftarrow \\text{Pre}(\\Omega_k) \\cap \\Omega_k\\)      \\(k \\leftarrow k+1\\)    Until \\(\\Omega_{k+1} = \\Omega_k\\).    \\(\\mathcal{O}_{\\infty} \\leftarrow \\Omega_k\\) The algorithm above generates a sequence of sets \\(\\{ \\Omega_k \\}\\) satisfying \\(\\Omega_{k+1} \\subseteq \\Omega_k\\) for any \\(k\\), and it terminates when \\(\\Omega_{k+1} = \\Omega_k\\). If it terminates, then \\(\\Omega_k\\) is the maximal positive invariant set \\(\\mathcal{O}_{\\infty}\\). If \\(\\Omega_k = \\emptyset\\) for some intege \\(k\\) then \\(\\mathcal{O}_{\\infty} = \\emptyset\\). In general, the algoithm above may never terminate. If the algoithm does not terminate in a finite number of iterations, then it can be proven that (Kolmanovsky, Gilbert, et al. 1998) \\[ \\mathcal{O}_{\\infty} = \\lim_{k \\rightarrow \\infty} \\Omega_k. \\] Conditions for finite time termination of the algoithm can be found in (Gilbert and Tan 1991). A simple sufficient condition requires the dynamics \\(f_a\\) to be linear and stable, and the constraint set \\(\\mathcal{X}\\) to be bounded and contain the origin. The same algoithm can be used to compute the maximal control invariant set \\(\\mathcal{C}_{\\infty}\\). Algorithm: Computation of \\(\\mathcal{C}_{\\infty}\\) Input: \\(f\\), \\(\\mathcal{X}\\), \\(\\mathcal{U}\\) Output: \\(\\mathcal{C}_{\\infty}\\)    \\(\\Omega_0 \\leftarrow \\mathcal{X}\\), \\(k=0\\)    Repeat      \\(\\Omega_{k+1} \\leftarrow \\text{Pre}(\\Omega_k) \\cap \\Omega_k\\)      \\(k \\leftarrow k+1\\)    Until \\(\\Omega_{k+1} = \\Omega_k\\).    \\(\\mathcal{C}_{\\infty} \\leftarrow \\Omega_k\\) Similarly, the above algoithm generates \\(\\{\\Omega_k \\}\\) such that \\(\\Omega_{k+1} \\subseteq \\Omega_k\\) for any \\(k\\). If the algoithm terminates, then \\(\\Omega_k = \\mathcal{C}_{\\infty}\\). In general, the algoithm may never terminate. If the algoithm does not terminate, then in general convergence is not guaranteed \\[ \\mathcal{C}_{\\infty} \\neq \\lim_{k \\rightarrow \\infty} \\Omega_k. \\] The work in (Bertsekas 1972) reports examples of nonlinear systems for which the above equation can be observed. A sufficient condition for the convergence of \\(\\Omega_k\\) to \\(\\mathcal{C}_{\\infty}\\) as \\(k \\rightarrow \\infty\\) requires the polyhedral sets \\(\\mathcal{X}\\) and \\(\\mathcal{U}\\) to be bounded and the system \\(f(x,u)\\) to be continuous (Bertsekas 1972). Let us apply the algoithm to compute the maximal control invariant set for the linear system in Example 3.10. Example 3.11 (Computation of the Maximal Control Invariant Set) Consider the linear system in Example 3.10 with same state constraint and control constraint. The following code snippet shows how to apply the iterative algorithm introduced above to compute the maximal control invariant set. Omega = [calX]; while true last_Omega = Omega(end); pre_omega = sys.reachableSet(&#39;X&#39;,last_Omega,&#39;U&#39;,calU,&#39;N&#39;,1,... &#39;direction&#39;,&#39;backwards&#39;); new_Omega = intersect(pre_omega,last_Omega); Omega = [Omega;new_Omega]; if new_Omega == last_Omega fprintf(&quot;Converged to maximal control invariant set.\\n&quot;); break; end end The algoithm converges in 37 iterations and Fig. 3.21 plots the sequence of sets generated by the algorithm. Figure 3.21: Maximal control invariant set. The MPT toolbox actually implements this algoithm for us to use directly. If we use C = sys.invariantSet(&#39;X&#39;, calX, &#39;U&#39;, calU); we get the same result as before. You can play with the code here. 3.3.3 Basic Formulation for Linear Systems We are now ready to introduce the basic formulation of MPC for linear systems and study its theoretical properties. Consider the problem of regulating the following discrete-time linear system to the origin \\[\\begin{equation} x_{t+1} = A x_t + B u_t, \\tag{3.33} \\end{equation}\\] where the state \\(x_t\\) and control \\(u_t\\) are constrained to lie in polyhedral sets \\(\\mathcal{X}\\) and \\(\\mathcal{U}\\), respectively. We assume \\(\\mathcal{X}\\) contains the origin \\(0\\). We can formulate the following optimal control problem to regulate the system to the origin \\[\\begin{equation} \\begin{split} J^\\star(x_0) = \\min_{u_t,t=0,\\dots} &amp; \\quad \\sum_{t=0}^{\\infty} x_t^T Q x_t + u_t^T R u_t \\\\ \\text{subject to} &amp; \\quad x_{t+1} = A x_t + B u_t, \\\\ &amp; \\quad (u_t,x_t) \\in \\mathcal{U} \\times \\mathcal{X}, \\forall t = 0,\\dots \\end{split} \\tag{3.34} \\end{equation}\\] with \\(Q \\succeq 0, R \\succ 0\\). Had we not included the constraints \\((u_t,x_t) \\in \\mathcal{U} \\times \\mathcal{X}\\), then problem (3.34) is exactly the infinite-horizon LQR problem in Section 2.1.1, for which we know the optimal controller is \\(u_t = - K x_t\\) with \\(K\\) computed in closed-form as (2.12). However, in the presence of constraints, problem (3.34) does not admit a simple closed-form solution. In fact, problem (3.34) is commonly referred to as the contrained LQR (CLQR) problem and it is known that the optimal controller is piece-wise affine, see Theorem 11.4 in (Borrelli, Bemporad, and Morari 2017). We have asked you to numerically play with a toy example of CLQR in Exercise 5.2. Receding horizon control. Leveraging the receding horizon control framework, we can approach problem (3.34) by online solving convex optimization problems. At time \\(t\\), suppose we can measure the current state of the system \\(x_t\\), then we solve the following optimal control problem with a finite horizon \\(N\\) \\[\\begin{equation} \\begin{split} J_t^\\star(x_t) = \\min_{u(0),\\dots,u(N-1)} &amp; \\quad p(x(N)) + \\sum_{k=0}^{N-1} q(x(k),u(k)) \\\\ \\text{subject to} &amp; \\quad x(k+1) = A x(k) + B u(k), k =0, \\dots, N-1, \\quad x(0) = x_t \\\\ &amp; \\quad (x(k),u(k)) \\in \\mathcal{X} \\times \\mathcal{U}, k=0,\\dots,N-1 \\\\ &amp; \\quad x(N) \\in \\mathcal{X}_f, \\end{split} \\tag{3.35} \\end{equation}\\] and \\(p(x)\\), \\(q(x,u)\\) convex functions. For example, a simple choice is \\(p(x) = x^T P x\\) and \\(q = x^T Q x + u^T R u\\) with \\(P,Q\\succeq 0\\) and \\(R \\succ 0\\). I hope you could pay attention to the notation in (3.35). I used \\(x_t, u_t\\) to denote the state and control for the original linear system (3.33) at time \\(t\\), as well as in the CLQR problem (3.34). However, in every subproblem (3.35) of RHS at time \\(t\\), I used \\(x(k),u(k)\\), with \\(k\\) as the time step, to denote the state and control in the finite-horizon optimal control problem starting at \\(x_t\\), with \\(x_t = x(0)\\) (\\(k\\) is the shifted time horizon in RHS that always starts at zero). In addition to the difference in notation between (3.35) and (3.34), problem (3.35) is also different in the following two ways: Terminal cost \\(p(x)\\): in the objective of problem (3.35), there is an additional terminal cost \\(p(x(N))\\). Terminal constraint set \\(\\mathcal{X}_f\\): problem (3.35) has an additional constraint that the final state \\(x(N)\\) must belong to the set \\(\\mathcal{X}_f\\). We assume \\(\\mathcal{X}_f\\) is also polyheral (and convex). Feasible sets. We denote as \\(\\mathcal{X}_0 \\subseteq \\mathcal{X}\\) the set of initial states \\(x_t\\) such that the RHC subproblem (3.35) is feasible, i.e., \\[\\begin{equation} \\hspace{-14mm} \\begin{split} \\mathcal{X}_0 = \\{ x(0) \\in \\mathbb{R}^n \\mid \\exists (u(0),\\dots,u(N-1)) \\text{ such that } x(k) \\in \\mathcal{X},u(k) \\in \\mathcal{U},k=0,\\dots,N-1, \\\\ x(N) \\in \\mathcal{X}_f \\text{ with } x(k+1) = A x(k) + B u(k), k=0,\\dots,N-1 \\}. \\end{split} \\tag{3.36} \\end{equation}\\] Similarly, we denote as \\(\\mathcal{X}_i\\) the set of states such that the RHC subproblem is feasible from step \\(k=i\\): \\[\\begin{equation} \\hspace{-14mm} \\begin{split} \\mathcal{X}_i = \\{ x(i) \\in \\mathbb{R}^n \\mid \\exists (u(i),\\dots,u(N-1)) \\text{ such that } x(k) \\in \\mathcal{X}, u(k) \\in \\mathcal{U}, k=i,\\dots,N-1, \\\\ x(N) \\in \\mathcal{X}_f \\text{ with } x(k+1) = A x(k) + B u(k), k=i,\\dots,N-1 \\}. \\end{split} \\tag{3.37} \\end{equation}\\] Clearly, by definition we have \\[ \\mathcal{X}_N = \\mathcal{X}_f, \\] and \\[ \\mathcal{X}_i = \\{ x \\in \\mathcal{X} \\mid \\exists u \\in \\mathcal{U} \\text{ such that } A x + Bu \\in \\mathcal{X}_{i+1} \\},i=0,\\dots,N-1, \\] or written in a compact way as \\[\\begin{equation} \\mathcal{X}_i = \\text{Pre}(\\mathcal{X}_{i+1}) \\cap \\mathcal{X}. \\tag{3.38} \\end{equation}\\] Note that from equation (3.38) we have that, if we pick \\(x_i \\in \\mathcal{X}_i\\) and let \\(U(x_i)\\) be the set of feasible controls at \\(x_i\\), then pick any \\((u(0),\\dots,u(N-1)) \\in U(x_i)\\) and apply \\(u(0)\\) to the system to get \\(x_{i+1} = A x_i + B u(0)\\), we have \\(x_{i+1} \\in \\mathcal{X}_{i+1}\\). Since the definitions (3.36) and (3.37) are rather abstract, let us visualize them using the double integrator example. Example 3.12 (Double Integrator RHC Feasible Sets) Consider the discrete-time double integrator dynamics \\[ x_{t+1} = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} x_t + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u_t, \\] subject to control constraint \\[ u \\in \\mathcal{U} = [-0.5,0.5], \\] and state constraint \\[ x \\in \\mathcal{X} = [-5, 5] \\times [-5, 5]. \\] We use \\(N=3\\) and visualize the feasible sets (3.36) and (3.37) for two choices of \\(\\mathcal{X}_f\\). Choice 1. \\(\\mathcal{X}_f = \\mathcal{X}\\) is the full state space. We use the recursion (3.38) to compute the feasible sets \\(\\mathcal{X}_i\\) for \\(i=0,1,2,3\\). Fig. 3.22 plots the feasible sets. Observe that in this case \\(\\mathcal{X}_0 \\subset \\mathcal{X}_1 \\subset \\mathcal{X}_2 \\subset \\mathcal{X}_3\\). This creates a concern: suppose the RHC starts at \\(x_0 \\in \\mathcal{X}_0\\) that is feasible, in the next iteration we have \\(x_1 \\in \\mathcal{X}_1\\). However, since \\(\\mathcal{X}_1\\) is larger than \\(\\mathcal{X}_0\\), \\(x_1\\) is not guaranteed to be feasible. Figure 3.22: Feasible sets of the double integrator receding horizon controller without terminal constraint. Choice 2. \\(\\mathcal{X}_f = \\{(0,0)\\}\\) is the origin. Fig. 3.23 plots the feasible sets. Observe that in this case \\(\\mathcal{X}_3 \\subset \\mathcal{X}_2 \\subset \\mathcal{X}_1 \\subset \\mathcal{X}_0\\). This is a nice case, because if RHC starts at \\(x_0 \\in \\mathcal{X}_0\\) that is feasible, in the next iteration we have \\(x_1 \\in \\mathcal{X}_1 \\subset \\mathcal{X}_0\\), which implies that \\(x_1\\) is guaranteed to remain feasible! In fact, as we will soon show, in the first choice the RHC does suffer from infeasibility. Figure 3.23: Feasible sets of the double integrator receding horizon controller with terminal constraint. The code for this example can be found here. Algorithm. Above all, problem (3.35) is a convex optimization problem that we know how to solve efficiently (you have solved such convex optimization problems in Exercise 5.2). Let \\(u^\\star(0),\\dots,u^\\star(N-1)\\) be the optimal solution of problem (3.35) when it is feasible, the RHS framework will only apply the first control \\(u^\\star(0)\\) to the system, and hence the closed-loop system is \\[\\begin{equation} x_{t+1} = A x_t + u^\\star_{x_t}(0) = f_{\\mathrm{cl}}(x_t), \\tag{3.39} \\end{equation}\\] where I have used \\(u^\\star_{x_t}(0)\\) to make it explicit that the control is the first optimal control of solving (3.35) with an initial state \\(x_t\\). The following algorithm summarizes the receding horizon control algorithm. Algorithm: Online Receding Horizon Control Input: State \\(x_t\\) at time \\(t\\) Output: Control \\(u^\\star_{x_t}(0)\\) Solve problem (3.35) to get the optimal controls \\(u^\\star(0), \\dots, u^\\star(N-1)\\) if the problem is infeasible, then stop else return \\(u^\\star_{x_t}(0) = u^\\star(0)\\) RHC main questions. Two main questions arise regarding the RHC controller. Persistent feasibility. If the RHC algorithm starts at a state \\(x_0\\) for which the convex optimization (3.35) is feasible, i.e., \\(x_0 \\in \\mathcal{X}_0\\), will the convex optimization (3.35) remain feasible for all future time steps? Stability. Assuming the convex optimization is always feasible. Will the closed-loop system (3.39) (induced by the RHC controller) converge to the desired origin \\(x=0\\)? Let us use a couple of examples to illustrate that, in general, the answers to the above two questions are both NO. Example 3.13 (Receding Horizon Control for Double Integrator) Consider the discrete-time double integrator dynamics \\[ x_{t+1} = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} x_t + \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} u_t, \\] subject to control constraint \\[ u \\in \\mathcal{U} = [-0.5,0.5], \\] and state constraint \\[ x \\in \\mathcal{X} = [-5, 5] \\times [-5, 5]. \\] In the RHS subproblem (3.35), we use \\(N = 3\\), \\(p(x) = x^T P x\\), \\(q(x,u) = x^T Q x + u^T R u\\) with \\(P = Q = I\\), \\(R=10\\), and \\(\\mathcal{X}_f = \\mathbb{R}^2\\) (i.e., there is not terminal constraint). The subproblem is implemented using CVX in Matlab. Fig. 3.24 shows the state trajectory of executing RHC starting at \\(x_0 = [-4.5;2]\\) and \\(x_0 = [-4.5;3]\\), respectively. We can see that when the initial state is \\(x_0 = [-4.5;2]\\), the trajectory successfully converges to the origin (the blue line). However, when the initial state is \\(x_0 = [-4.5;3]\\), RHC fails in the third iteration because the subproblem becomes infeasible (the red line). You can find code for this example here. Figure 3.24: Receding horizon control for the double integrator with two initial states. We now show another example, adapted from (Borrelli, Bemporad, and Morari 2017) where the design of \\(N\\), \\(p(x)\\) and \\(q(x,u)\\) affects the closed-loop performance. Example 3.14 (RHC Performance Affected by Parameters) Consider the system \\[ x_{t+1} = \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 0.5 \\end{bmatrix} x_t + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} u_t, \\] with control constraint \\[ u \\in \\mathcal{U} = [-1,1], \\] and state constraint \\[ x \\in \\mathcal{X} = [-10,10] \\times [-10,10]. \\] In the RHC problem (3.35), we choose \\(p(x) = x^T P x\\), \\(q(x,u) = x^T Q x + u^T R u\\). We fix \\(P=0\\), \\(Q = I\\), and \\(\\mathcal{X}_f = \\mathbb{R}^2\\), but vary \\(N\\) and \\(R\\): Setting 1: \\(N=2\\), \\(R = 10\\); Setting 2: \\(N=3\\), \\(R = 2\\); Setting 3: \\(N=4\\), \\(R=1\\). Fig. 3.25 shows the closed-loop trajectories of three different settings. As we can see, the closed-loop performance depends on the parameters in a very complicated manner. Figure 3.25: Closed-loop trajectories for different settings of horizon N and weight R. Boxes (circles) are initial points leading to feasible (infeasible) closed-loop trajectories. 3.3.4 Persistent Feasibility Under what conditions can we guarantee the RHC subproblem (3.35) is always feasible? Intuitively, we will show that, by designing the terminal constraint set \\(\\mathcal{X}_f\\), we can guarantee the configuration of the feasible sets will look like choice 2 in Example 3.12. There are various sets here of interest for answering this question. \\(\\mathcal{C}_{\\infty}\\): The maximal control invariant set \\(\\mathcal{C}_{\\infty}\\) is only affected by the system dynamics (3.33) and the constraint sets \\(\\mathcal{X} \\times \\mathcal{U}\\). It is the largest set over which we can expect any controller to work, because otherwise the system trajectory will blow up. \\(\\mathcal{X}_0\\): The set of states at which the RHC subproblem (3.35) is feasible. The set \\(\\mathcal{X}_0\\) depends on the system dynamics, the constraint sets \\(\\mathcal{X} \\times \\mathcal{U}\\), as well as the RHC horizon \\(N\\) and the terminal constraint set \\(\\mathcal{X}_f\\). It is worth noting that it does not depend on the objective function in (3.35) (i.e., \\(P,Q,R\\)). \\(\\mathcal{O}_{\\infty}\\): The maximal positive invariant set for the closed-loop system (3.39) induced by the RHC control law. This depends on the RHC controller and hence it depends on the system dynamics, the constraint set \\(\\mathcal{X} \\times \\mathcal{U}\\), \\(N\\), \\(\\mathcal{X}_f\\) and the objective function of (3.35) \\(P,Q,R\\). A few relationships between these sets are easy to observe. \\(\\mathcal{O}_{\\infty} \\subseteq \\mathcal{X}_0\\). Any state \\(x \\in \\mathcal{O}_{\\infty}\\) needs to stay in \\(\\mathcal{O}_{\\infty}\\) for all future time steps. Thus, the subproblem (3.35) must be feasible for any \\(x \\in \\mathcal{O}_{\\infty}\\), otherwise the closed-loop system (3.39) is not even well defined. \\(\\mathcal{O}_{\\infty} \\subseteq \\mathcal{C}_{\\infty}\\). The set \\(\\mathcal{O}_{\\infty}\\) is control invariant because there exists a controller (specifically, the RHC controller) that makes it positive invariant. Therefore, \\(\\mathcal{O}_{\\infty}\\) must belong to the maximal control invariant set \\(\\mathcal{C}_{\\infty}\\). We can now state necessary and sufficient conditions guaranteeing persistent feasibility. Lemma 3.1 (Sufficient and Necessary Condition for Persistent Feasibility) The RHC subproblem (3.35) is persistently feasible if and only if \\(\\mathcal{X}_0 = \\mathcal{O}_{\\infty}\\). Proof. We have already argued that \\(\\mathcal{O}_{\\infty} \\subseteq \\mathcal{X}_0\\). It remains to show \\(\\mathcal{X}_0 \\subseteq \\mathcal{O}_{\\infty}\\). By definition, \\(\\mathcal{X}_0\\) is persistently feasible implies that \\[ x \\in \\mathcal{X}_0 \\Longrightarrow f_{\\mathrm{cl}}^t(x) \\in \\mathcal{X}_0, \\forall t \\] where \\(f_{\\mathrm{cl}}^t\\) means applying the closed-loop dynamics \\(f_{\\mathrm{cl}}\\) in (3.39) \\(t\\) times. This shows that \\(\\mathcal{X}_0\\) is a positive invariant set for the closed-loop system, and hence \\(\\mathcal{X}_0 \\subseteq \\mathcal{O}_{\\infty}\\). We argued that \\(\\mathcal{X}_0\\) does not depend on the RHC parameters \\(P,Q,R\\) but \\(\\mathcal{O}_{\\infty}\\) does. Therefore, in general only some \\(P,Q,R\\) are allowed for persistent feasibility to hold. Due to the complicated relationship between \\(P,Q,R\\) and \\(\\mathcal{O}_{\\infty}\\), it is generally difficult to design \\(P,Q,R\\) such that RHC has persistent feasibility. We now state a sufficient condition for persistent feasibility to hold. Lemma 3.2 (Sufficient Condition for Persistent Feasibility) Consider the RHC subproblem (3.35) with \\(N \\geq 1\\). If \\(\\mathcal{X}_1\\) is a control invariant set for the linear system (3.33), then the RHC controller is persistently feasible. Proof. If \\(\\mathcal{X}_1\\) is control invariant, then by definition \\(\\mathcal{X}_1 \\subseteq \\text{Pre}(\\mathcal{X}_1)\\). Since \\(\\mathcal{X}_1 \\subseteq \\mathcal{X}\\), we have \\[ \\mathcal{X}_1 \\subseteq \\text{Pre}(\\mathcal{X}_1) \\cap \\mathcal{X} = \\mathcal{X}_0. \\] where the last equality in the equation above is due to (3.38). Now pick any \\(x_0 \\in \\mathcal{X}_0\\), the set of controls that make \\(x_0\\) feasible is denoted as \\[ U = \\{ u(0),\\dots,u(N-1) \\mid x(0)=x_0, x(k) \\in \\mathcal{X},k=0,\\dots,N-1,x(N) \\in \\mathcal{X}_f \\}. \\] The RHC will pick some control sequence from \\(U\\), say \\(\\hat{u}(0),\\dots,\\hat{u}(N-1)\\) and apply the first control \\(\\hat{u}(0)\\) to the system, which will bring the system to a new state \\[ x_1 = A x_0 + B \\hat{u}(0). \\] Observe that \\(x_1 \\in \\mathcal{X}_1\\) by definition. Since \\(\\mathcal{X}_1 \\subseteq \\mathcal{X}_0\\), we have \\(x_1 \\in \\mathcal{X}_0\\). This proves that the RHC is persistently feasible, i.e., starting with any \\(x_0 \\in \\mathcal{X}_0\\), the RHC subproblem (3.35) is always feasible. Lemma 3.2 states that if \\(\\mathcal{X}_1\\) is control invariant, then the RHC subproblem is persistently feasible. An immediate result of this Lemma is that when \\(N=1\\), then \\(\\mathcal{X}_1 = \\mathcal{X}_f\\). Therefore, if we choose the terminal constraint set \\(\\mathcal{X}_f\\) to be control invariant, then RHC is has persistent feasibility. The next theorem states that when \\(N \\geq 1\\), as long as we choose the terminal constraint set \\(\\mathcal{X}_f\\) to be control invariant, then persistent feasibility also holds. Theorem 3.2 (Control Invariant Terminal Constraint Set Guarantees Persistent Feasibility) Consider the RHC subproblem (3.35) with \\(N \\geq 1\\). If \\(\\mathcal{X}_f\\) is a control invariant set for the linear system (3.33), then the RHC controller is persistently feasible. Proof. We will prove that \\(\\mathcal{X}_f\\) being control invariant implies \\[ \\mathcal{X}_{N-1}, \\mathcal{X}_{N-2},\\dots,\\mathcal{X}_1 \\] are all control invariant, and then by Lemma 3.2, we can guarantee persistent feasibility. Fig. 3.26 shows the nested control invariant sets when \\(\\mathcal{X}_f\\) is control invariant. Figure 3.26: Nested control invariant sets. It suffices to show \\(\\mathcal{X}_{i+1}\\) being control invariant leads to \\(\\mathcal{X}_i\\) being control invariant. First, by \\(\\mathcal{X}_{i+1}\\) control invariant, we have \\(\\mathcal{X}_{i+1} \\subseteq \\text{Pre}(\\mathcal{X}_i) \\cap \\mathcal{X} = \\mathcal{X}_i\\). Now pick any \\(x_{i} \\in \\mathcal{X}_{i}\\), for any feasible \\(\\hat{u}(0),\\dots,\\hat{u}(N-1)\\), applying the first control \\(\\hat{u}(0)\\) brings the system to a new state \\[ x_{i+1} = A x_i + B \\hat{u}(0) \\in \\mathcal{X}_{i+1} \\subseteq \\mathcal{X}_i. \\] This shows \\(\\mathcal{X}_i\\) is control invariant. Persistent feasibility does not guarantee the closed-loop system will converge to the origin. In fact, from Theorem 3.2, it is clear that if the system starts at \\(x_0 \\in \\mathcal{X}_0\\), then we can only guarantee \\(x_t \\in \\mathcal{X}_1\\) for all \\(t \\geq 0\\). 3.3.5 Stability We now answer the question of how can we guarantee the RHC controller will drive the system to the desired origin. Theorem 3.3 (Sufficient Condition for Stability) Consider the linear system (3.33), and the RHC algorithm (3.35). Assume that The stage cost \\(q(x,u)\\) and terminal cost \\(p(x)\\) are continuous and positive definite functions. The sets \\(\\mathcal{X}\\), \\(\\mathcal{X}_f\\) and \\(\\mathcal{U}\\) contain the origin in their interior and are closed. \\(\\mathcal{X}_f \\subseteq \\mathcal{X}\\) is control invariant. For any \\(x \\in \\mathcal{X}_f\\), the following inequality holds \\[\\begin{equation} \\min_{u \\in \\mathcal{U}, A x + Bu \\in \\mathcal{X}_f} \\left( - p(x) + q(x,u) + p(Ax + Bu) \\right) \\leq 0. \\tag{3.40} \\end{equation}\\] Then, the origin of the closed-loop system (3.39) is asymptotically stable with domain of attraction \\(\\mathcal{X}_0\\). In words, for any \\(x_0 \\in \\mathcal{X}_0\\), if the closed-loop system (3.39) starts at \\(x_0\\), then the system trajectory converges to the origin as \\(t\\) tends to infinity. Let us interpret Theorem 3.3 before proving it. It should be clear that the first three assumptions are easy to satisfy. For example, if we choose \\(p(x) = x^T P x\\) and \\(q(x,u) = x^T Q x + u^T R u\\) with \\(P, Q, R \\succ 0\\), then assumption 1 is satisfied. Usually \\(\\mathcal{X}\\) and \\(\\mathcal{U}\\) are both polyhedral sets containing the origin in the interior, so assumption 2 also holds naturally. Finding a control invariant set \\(\\mathcal{X}_f\\) is not a trivial task but there exists numerical algorithms for this task (e.g., using the algorithms introduced in Section 3.3.2.4). After we find a control invariant \\(\\mathcal{X}_f\\), by Theorem 3.2, we know persistent feasibility will hold. We now prove the theorem by showing that Assumption 4 guarantees stability. Before we show the proof, we need the concept of a Lyapunov function. Below we introduce Lyapunov function for a discrete-time dynamical system, but we will study more details of Lyapunov function for continuous-time dynamical systems in Chapter 4. Lemma 3.3 (Discrete-time Lyapunov Function) Consider the discrete-time autonomous system (3.27), restated below for convenience: \\[ x_{t+1} = f_a(x_t), \\] and assume \\(x=0\\) is an equilibrium point of the system, i.e., \\(0 = f_a (0)\\) (if the system starts at the origin, it stays at the origin). Let \\(\\Omega \\subset \\mathbb{R}^n\\) be a closed and bounded set containing the origin. If there exists a function \\(V: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) that is (a) continuous at the origin, (b) finite for every \\(x \\in \\Omega\\), and (c) satisfies \\(V\\) is positive definite: \\(V(0) = 0\\) and \\(V(x) &gt; 0,\\forall x \\in \\Omega \\backslash \\{ 0\\}\\), \\(V(x_{t+1}) - V(x_t) \\leq - \\alpha(x_t) &lt; 0\\) for any \\(x_t \\in \\Omega \\backslash \\{ 0\\}\\), where \\(\\alpha: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is a continuous positive definite function. Then \\(x=0\\) is asymptotically stable in \\(\\Omega\\), i.e, if the system starts within \\(\\Omega\\), then its trajectory tends to \\(0\\) as \\(t \\rightarrow \\infty\\). A function \\(V\\) satisfying the conditions above is called a Lyapunov function for the system. One can think of the Lyapunov function \\(V\\) as an energy function for the system (3.27) that maps a system state to a single scalar. Condition 1 in Lemma 3.3 states that \\(V\\) (the energy function) is strictly positive except at the origin. Condition 2 in Lemma 3.3 states that, the system energy \\(V\\) is strictly decreasing along any system trajectory. Therefore, we conclude that \\(V\\) must converge to \\(V=0\\), and hence any state trajectory must converge to the origin. We will now use Lemma 3.3 to prove RHC stability under Theorem 3.3. Proof. Our goal is to prove that the optimal cost of the RHC problem (3.35), \\(J_t^\\star(x_t)\\), is a Lyapunov function for the closed-loop system (3.39) on the domain \\(\\mathcal{X}_0\\). Then using Lemma 3.3 we can conclude that the closed-loop system (3.39) will converge to the origin. Positive definite. Clearly, we have \\(J_t^\\star(0) = 0\\) by the positive definiteness of \\(p\\) and \\(q\\) and the fact that \\(x=0\\) is an equilibrium point of the linear system (3.33). For \\(x_t \\neq 0\\), it is also clear that \\(J_t^\\star(x) &gt; 0\\). Therefore, \\(J_t^\\star(x)\\) is positive definite on \\(\\mathcal{X}_0\\). Strictly decrease. It suffices to show \\(J_1^\\star(x_1) - J_0^\\star(x_0) &lt; 0\\) because the constraints of the RHC problem (3.35) is time-invariant. Pick any \\(x_0 \\in \\mathcal{X}_0\\), and let \\[\\begin{equation} \\left( u^\\star(0),u^\\star(1),\\dots,u^\\star(N-1) \\right) \\tag{3.41} \\end{equation}\\] be an optimal control trajectory to problem (3.35). Denote \\[\\begin{equation} \\left( x_0,x(1),\\dots,x(N) \\right) \\tag{3.42} \\end{equation}\\] as the associated optimal state trajectory for problem (3.35). The RHC controller will apply \\(u^\\star(0)\\) to the system, leading to the next state \\[ x_1 = x(1) = A x_0 + B u^\\star(0). \\] Then we will solve problem (3.35) to get \\(J^\\star_1(x_1)\\), and we want to show \\(J^\\star_1(x_1) &lt; J^\\star_0(x_0)\\). Towards this, we will construct a feasible solution to (3.35) starting at \\(x_1\\), and hence an upper bound on \\(J^\\star_1(x_1)\\). Consider the control sequence \\[\\begin{equation} \\left( u^\\star(1),\\dots,u^\\star(N-1),v \\right) \\tag{3.43} \\end{equation}\\] which is different from the control sequence in (3.41) by removing \\(u^\\star(0)\\) and appending \\(v\\). The corresponding state trajectory to (3.43) is \\[\\begin{equation} \\left( x_1 = x(1), x(2),\\dots,x(N), Ax(N) + Bv \\right) \\tag{3.44} \\end{equation}\\] which is different from the state trajectory in (3.42) by removing \\(x_0\\) and appending \\(A x(N) + Bv\\). Since \\(x(N) \\in \\mathcal{X}_f\\) and \\(\\mathcal{X}_f\\) is control invariant, there exists \\(v\\) such that \\(Ax(N) + Bv \\in \\mathcal{X}_f\\) and the corresponding control trajectory (3.43) is feasible. Applying the control trajectory (3.43) to the optimization problem (3.35) will lead to the total cost \\[ J_1(x_1) = J_0^\\star(x_0) - q(x_0,u^\\star(0)) \\underbrace{- p(x(N)) + q(x(N),v) + p(Ax(N) + Bv)}_{s(x(N),v)}. \\] Now by assumption 4 in Theorem 3.3, we can choose \\(v\\) such that the sum \\(s(x(N),v) \\leq 0\\). Consequently, we have \\[ J_1^\\star(x_1) \\leq J_1(x_1) \\leq J_0^\\star(x_0) - q(x_0,u^\\star(0)), \\] which leads to \\[ J_1^\\star(x_1) - J_0^\\star(x_0) \\leq - q(x_0,u^\\star(0)). \\] Because the choice of \\(x_0\\) was arbitrary, we conclude that \\(J_t^\\star(x_t)\\) strictly decreases along any system trajectory that starts within \\(\\mathcal{X}_0\\). Continuity at the origin. We will show that \\(J_0^\\star(x) \\leq p(x)\\) for any \\(x \\in \\mathcal{X}_f\\). With this argument, since \\(p(x)\\) is positive definite and continuous, then \\(J_0^\\star(x)\\) must be continuous at the origin. We now prove \\(J_0^\\star(x) \\leq p(x)\\) for any \\(x \\in \\mathcal{X}_f\\). Since \\(\\mathcal{X}_f\\) is control invariant, pick any \\(x \\in \\mathcal{X}_f\\), there exists a sequence of controls \\((u(0),u(1),\\dots,u(N-1))\\) such that the state trajectory \\((x(0)=x,x(1),\\dots,x(N))\\) stays in \\(\\mathcal{X}_f\\). Such a control sequence lead to an upper bound on \\(J^\\star_0(x)\\): \\[ J^\\star_0(x(0)) \\leq p(x(N)) + \\sum_{i=0}^{N-1} q(x(i),u(i)) = p(x(0)) + \\sum_{i=0}^{N-1} \\left( q(x(i),u(i)) + p(x(i+1)) - p(x(i)) \\right), \\] since each \\(x(i) \\in \\mathcal{X}_f\\), according to Assumption 4, we can choose \\(u(i)\\) such that \\[ J^\\star_0(x(0)) \\leq p(x(0)) \\] for any \\(x(0) \\in \\mathcal{X}_f\\). In conclusion, we have shown that \\(J^\\star_t(x_t)\\) is a Lyapunov function, and by Lemma 3.3, the closed-loop system is asymptotically stable. A function \\(p(x)\\) that satisfies Assumption 4 in Theorem 3.3 is typicalled known as a control Lyapunov function. Now two natural problems arise: Given \\(p(x)\\), how to verify if assumption 3 holds in Theorem 3.3? How to synthesize a \\(p(x)\\) that satisfies assumption 3 in Theorem 3.3? Unfortunately, both problems are hard. To see this, suppose we are given a candidate function \\(p(x) = x^T P x\\) with \\(P \\succ 0\\) that is clearly positive definite. Assume \\(q(x,u) = x^T Q x + u^T R u\\) with \\(Q, R \\succ 0\\) and \\(\\mathcal{U}, \\mathcal{X}_f\\) are given polyhedral sets. Then verifying if \\(p(x)\\) satisifies Assumption 4 boils down to checking if \\[ \\min_{u \\in \\mathcal{U}, Ax + Bu \\in \\mathcal{X}_f} x^T Q x + u^T R u + (Ax + Bu)^T P (Ax + Bu) - x^T P x \\] is non-positive for any possible \\(x \\in \\mathcal{X}_f\\). Although for each possible \\(x\\), the above problem is a convex optimization problem, there are an infinite number of points in the set \\(\\mathcal{X}_f\\) and enumerating over all points is a daunting task. We will see in Chapter 4 that convex relaxations, in particular semidefinite relaxations, can help us partially solve these hard problems. A simple control lyapunov function. One can solve the infinite-horizon unconstrained LQR problem \\[ \\min_{u_t} \\sum_{t=0}^{\\infty}x_t ^T Q x_t + u_t^T R u_t, \\] for which the optimal cost-to-go is \\[ J_{\\infty}(x) = x^T S x, \\] with \\(S\\) the solution to the algebraic Riccati equation (2.13). Denote \\[ u_t = \\Pi_{\\mathcal{U}} (- K x_t) \\] as the optimal controller (\\(\\Pi_{\\mathcal{U}}\\) is the projection of the controller to the feasible set \\(\\mathcal{U}\\)), and use \\(\\mathcal{X}_f\\) as the maximal positive invariant set of the closed-loop system \\[ x_{t+1} = A x_t + B u_t. \\] Then \\(J_{\\infty}(x) = x^T S x\\) is a control Lyapunov function over the set \\(\\mathcal{X}_f\\). 3.3.6 Explicit MPC See the original paper (Bemporad et al. 2002), and check out Matlab’s explict MPC design. 3.4 Policy Gradient References Bemporad, Alberto, Manfred Morari, Vivek Dua, and Efstratios N Pistikopoulos. 2002. “The Explicit Linear Quadratic Regulator for Constrained Systems.” Automatica 38 (1): 3–20. Bertsekas, Dimitri. 1972. “Infinite Time Reachability of State-Space Regions by Using Feedback Control.” IEEE Transactions on Automatic Control 17 (5): 604–13. Borrelli, Francesco, Alberto Bemporad, and Manfred Morari. 2017. Predictive Control for Linear and Hybrid Systems. Cambridge University Press. Gilbert, Elmer G, and K Tin Tan. 1991. “Linear Systems with State and Control Constraints: The Theory and Application of Maximal Output Admissible Sets.” IEEE Transactions on Automatic Control 36 (9): 1008–20. Kelly, Matthew. 2017. “An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation.” SIAM Review 59 (4): 849–904. Kolmanovsky, Ilya, Elmer G Gilbert, et al. 1998. “Theory and Computation of Disturbance Invariant Sets for Discrete-Time Linear Systems.” Mathematical Problems in Engineering 4: 317–67. Levine, Nir, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. 2017. “Shallow Updates for Deep Reinforcement Learning.” Advances in Neural Information Processing Systems 30. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. Nocedal, Jorge, and Stephen J Wright. 1999. Numerical Optimization. Springer. Yang, Alan, and Stephen Boyd. 2023. “Value-Gradient Iteration with Quadratic Approximate Value Functions.” arXiv Preprint arXiv:2307.07086. "],["stability.html", "Chapter 4 Stability Analysis 4.1 Autonomous Systems 4.2 Controlled Systems 4.3 Non-autonomous Systems", " Chapter 4 Stability Analysis Optimal control formulates a control problem via the language of mathematical optimization. However, there are control problems, and sometimes even the very basic control problems, that cannot be easily stated in the optimal control formulation. For example, suppose our goal is to swing up a pendulum to the upright position and stabilize it there. You may want to formalize the problem as \\[\\begin{equation} \\min_{u(t) \\in \\mathbb{U}} \\int_{0}^{\\infty} \\Vert x(t) - x_d \\Vert^2 dt, \\quad \\text{subject to} \\quad \\dot{x} = f(x,u), x(0) = x_0, \\tag{4.1} \\end{equation}\\] where \\(x_d\\) is the desired upright position for the pendulum. However, does the solution of problem (4.1), if exists, guarantee the stabilization of the pendulum at the upright position? The answer is unclear without a rigorous proof. However, after a slight change of perspective, the optimal control problem may be formulated to better match the goal. Suppose there exists a region, \\(\\Omega\\), in the state space such that as long as the pendulum enters \\(\\Omega\\), there always exists a sequence of control to bring the pendulum to the goal state \\(x_d\\), then we can simply formulate a different optimal control problem \\[\\begin{equation} \\min_{u(t) \\in \\mathbb{U}} \\int_{0}^{T} \\Vert u(t) \\Vert^2 dt, \\quad \\text{subject to} \\quad x(0)=x_0, x(T) \\in \\Omega, \\dot{x} = f(x,u), \\tag{4.2} \\end{equation}\\] where now it is very clear, if a solution exists to problem (4.2), then we will definitely achieve our goal. This is because the constraint \\(x(T) \\in \\Omega\\) guarantees that we will be able to stabilize the pendulum, and the cost function of (4.2) simply encourages minimum control effort along the way. This highlights that, sometimes the formulation of a problem may deserve more thoughts than the actual solution. Of course the formulation (4.2) may be much more difficult to solve. In fact, does the set \\(\\Omega\\) exist, and if so, how to describe it? This is the main focus of this chapter: to introduce tools that can help us analyze the stability of uncontrolled and controlled nonlinear systems. Specifically, we will introduce the notion of stability certificates, which are conditions that, if hold, certify the stability of the system (e.g., in the set \\(\\Omega\\)). Interestingly, you will see that the notion of stability certificates is intuitive and easy, but what is really challenging is to find and compute the stability certificates. We will highlight the power and also limitation of computational tools, especially those that are based on convex optimization (see Appendix B for a review of convex optimization). 4.1 Autonomous Systems Let us first focus on autonomous systems, i.e., systems whose dynamics do not depent on time (and control). We introduce different concepts of stability and ways to certify them. 4.1.1 Concepts of Stability Consider the autonomous system \\[\\begin{equation} \\dot{x} = f(x) \\tag{4.3} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) is the state and \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is the (potentially nonlinear) dynamics. Before talking about concepts of stability, we need to define an equilibrium point. Definition 4.1 (Equilibrium Point) A state \\(x^\\star\\) is called an equilibrium point of system (4.3) if \\(f(x^\\star) = 0\\), i.e., once the system reaches \\(x^\\star\\), it stays at \\(x^\\star\\). For example, a linear system \\[ \\dot{x} = A x \\] has a single equilibrium point \\(x^\\star = 0\\) when \\(A\\) is nonsingular, and an infinite number of equilibrium points when \\(A\\) is singular (those equilibrium points lie in the kernel of matrix \\(A\\)). When analyzing the behavior of a dynamical system around the equilibrium point, it is often helpful to “shift” the dynamics equation so that \\(0\\) is the equilibrium point. For example, if we are interested in the behavior of system (4.3) near the equilibrium point \\(x^\\star\\), we can create a new variable \\[ z = x - x^\\star, \\] so that \\[\\begin{equation} \\dot{z} = \\dot{x} = f(x) = f(z + x^\\star). \\tag{4.4} \\end{equation}\\] Clearly, \\(z^\\star = 0\\) is an equilibrium point for the shifted system (4.4). Let us find the equilibrium points of a simple pendulum. Example 4.1 (Equilibrium Points of A Simple Pendulum) Consider the dynamics of an uncontrolled pendulum \\[\\begin{equation} \\begin{cases} \\dot{\\theta} = \\dot{\\theta} \\\\ \\ddot{\\theta} = - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) \\end{cases} \\tag{4.5} \\end{equation}\\] where \\(\\theta\\) is the angle between the pendulum and the vertical line, and \\(x = [\\theta,\\dot{\\theta}]^T\\) is the state of the pendulum (\\(m,g,l,b\\) denote the mass, gravity constant, length, and damping constant, respectively). To find the equilibrium points of the pendulum, we need the right hand sides of (4.5) to be equal to zero: \\[ \\dot{\\theta} = 0, \\quad - \\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) = 0. \\] The solutions are easy to find \\[ x^\\star = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\quad \\text{or} \\quad \\begin{bmatrix} \\pi \\\\ 0 \\end{bmatrix}, \\] corresponding to the bottomright and upright positions of the pendulum, respectively. The pendulum dynamics has two equilibrium points, but our physics intuition tells us these two equilibrium points are dramatically different. Specifically, the bottomright equilibrium \\(x^\\star = [0,0]^T\\) is such that if you perturb the pendulum around the equilibrium, the pendulum will go back to that equilibrium; the upright equilibrium \\(x^\\star = [\\pi,0]^T\\) is such that if you perturb the pendulum (even just a little bit) around the equilibrium, it will diverge from that equilibrium. This physical intuition is exactly what we want to formalize as the concepts of stability. In the following, we focus on the nonlinear autonomous system (4.3) with \\(f(0) = 0\\), i.e., \\(x^\\star = 0\\) is an equilibrium point. We now formally define the different concepts of stability. Definition 4.2 (Lyapunov Stability) The equilibrium point \\(x=0\\) is said to be stable in the sense of Lyapunov if, for any \\(R &gt; 0\\), there exists \\(r &gt;0\\) such that if \\(\\Vert x(0) \\Vert &lt; r\\), then \\(\\Vert x(t) \\Vert &lt; R\\) for all \\(t \\geq 0\\). Otherwise, the equilibrium point is unstable. For a system that is Lyapunov stable around \\(x=0\\), the definition says that, if we want to constrain the trajectory of the system to be within the ball \\(B_R = \\{ x \\mid \\Vert x \\Vert &lt; R \\}\\), then we can always find a smaller ball \\(B_r = \\{ x \\mid \\Vert x \\Vert &lt; r \\}\\) such that if the system starts within \\(B_r\\), it will remain in the larger ball \\(B_R\\). On the other hand, if the system is not Lyapunov stable at \\(x=0\\), then there exists at least one ball \\(B_R\\), such that no matter how close the system’s initial condition is to the origin, it will eventually exit the ball \\(B_R\\). The following exercise is left for you to verify the instability of the Van der Pol oscillator. Exercise 4.1 (Instability of the Van der Pol oscillator) Show that the Van der Pol oscillator \\[ \\begin{cases} \\dot{x}_1 = x_2 \\\\ \\dot{x}_2 = - x_1 + (1-x_1^2) x_2 \\end{cases} \\] is unstable at the equilibrium point \\(x = 0\\). Lyapunov stability does not guarantee the system trajectory will actually converge to \\(x =0\\). Instead, asymptotic stability will ask the system trajectory to converge to \\(x=0\\). Definition 4.3 (Asymptotic Stability and Domain of Attraction) The equilibrium point \\(x = 0\\) is said to be asymptotically stable if (i) it is Lyapunov stable, and (ii) there exists some \\(r &gt; 0\\) such that \\(x(0) \\in B_r\\) implies \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). The domain of attraction (for the equilibrium \\(x=0\\)) is the largest set of points in the state space such that trajectories initiated at those points will converge to the equilibrium point. That is, \\[ \\Omega(x^\\star) = \\{ x \\in \\mathbb{X} \\mid x(0) = x \\Longrightarrow \\lim_{t \\rightarrow \\infty} x(t) = x^\\star \\}. \\] The ball \\(B_r\\) is a domain of attraction for the equilibrium point \\(x=0\\), but not necessarily the largest domain of attraction. You may immediately realize that in the definition of asymptotic stability, we require Lyapunov stability to hold first. Is this necessary? i.e., does there exist a system where trajectories eventually converge to zero, but is not stable in the sense of Lyapunov? You should work out the following exercise. Exercise 4.2 (Vinograd System) Show that for the Vinograd dynamical system (Vinograd 1957) \\[ \\begin{cases} \\dot{x} = \\frac{x^2(y-x) + y^5}{(x^2+y^2)(1 + (x^2+y^2)^2)} \\\\ \\dot{y} = \\frac{y^2 (y - 2x)}{(x^2+y^2)(1 + (x^2+y^2)^2)} \\end{cases}, \\] all system trajectories converge to the equilibrium point \\((x,y) = 0\\), but the equilibrium point is not stable in the sense of Lyapunov. (Hint: the system trajectories will behave like the following plot.) Figure 4.1: Trajectories of the Vinograd system. Copied from the original article of Vinograd. In many cases, we want the convergence of the system trajectory towards \\(x=0\\) to be fast, thus bringing in the notion of exponential stability. Definition 4.4 (Exponential Stability) An equilibrium point \\(x=0\\) is said to be exponentially stable, if there exists a ball \\(B_r\\) such that as long as as \\(x(0) \\in B_r\\), then \\[ \\Vert x(t) \\Vert \\leq \\alpha \\Vert x(0) \\Vert e^{-\\lambda t}, \\quad \\forall t, \\] for some \\(\\alpha &gt; 0\\) and \\(\\lambda &gt; 0\\) (\\(\\lambda\\) is called the rate of exponential convergence). Exponential stability implies asymptotic stability (and certainly also Lyapunov stability). What is nice about exponential stability is that we can quantify the distance of the system trajectory to the equilibrium point as a function of time (as long as we know the constants \\(\\alpha, \\Vert x(0) \\Vert, \\lambda\\)). In many safety-critical applications, we need such performance guarantees. For example, in Chapter ??, we will see the application of exponential stability in observer-feedback control. All the concepts of stability we have mentioned so far only talk about the stability of the system locally around the equilibrium point \\(x=0\\) (via arguments like \\(B_r\\) and \\(B_R\\)). It would be much nicer if we can guarantee stability of the system globally, i.e., no matter where the system starts in the state space \\(\\mathbb{X}\\), its trajectoy will converge to \\(x=0\\). Definition 4.5 (Global Asymptotic and Exponential Stability) The equilibrium point \\(x = 0\\) is said to be globally asymptotically (exponentially) stable if asymptotic (exponential) stability holds for any initial states. That is, \\[ \\forall x \\in \\mathbb{X}, \\quad x(0) = x \\Longrightarrow \\begin{cases} \\lim_{t \\rightarrow \\infty} x(t) = 0 &amp; \\text{global asymptotic stability} \\\\ \\exists \\alpha, \\lambda &gt; 0, \\text{ s.t. } \\Vert x(t) \\Vert \\leq \\alpha \\Vert x(0) \\Vert e^{-\\lambda t} &amp; \\text{global exponential stability} \\end{cases} \\] This concludes our definitions of stability for nonlinear systems (Definition 4.2-4.5). It is worth mentioning that the concepts of stability are complicated (refined) here due to our focus on nonlinear systems. For linear systems, the concepts of stability are simpler. Specifically, all local stability properties of linear systems are also global and asymptotic stability is equal to exponential stability. In fact, for a linear time-invariant system \\(\\dot{x} = Ax\\), it is either asymptotically (exponentially) stable, or marginally stable, or unstable. Moreover, we can fully characterize the stability property by inspecting the eigenvalues of \\(A\\) (you can find a refreshment of this in Appendix C.1). How do we characterize the stability property of a nonlinear system? If someone gave me a nonlinear system (4.3), how can I provide a certificate to her that the system is stable or unstable (I cannot use eigenvalues anymore in this case)? Let us describe some of these certificates below. 4.1.2 Stability by Linearization A natural idea is to linearize, if possible, the nonlinear system (4.3) at a given equilibrium point \\(x^\\star\\) and inspect the stability of the linearized system (for which we can compute eigenvalues). Therefore, the key question here is how does the stability and instability of the linearized system relate to the stability and instability of the original nonlinear system. Theorem 4.1 (Stability by Linearization) Assume \\(x=0\\) is an equilibrium point of system (4.3) and \\(f\\) is continuously differentiable. Let \\[\\begin{equation} \\dot{x} = Ax, \\quad A = \\frac{\\partial f}{\\partial x} \\Big\\vert_{x=0} \\tag{4.6} \\end{equation}\\] be the linearized system at \\(x=0\\). The following statements are true about the stability relationship between (4.3) and (4.6). If the linearized system (4.6) is strictly stable (i.e., all eigenvalues of \\(A\\) have strictly negative real parts), then the original system (4.3) is asymptotically stable at \\(x=0\\). If the linearized system (4.6) is unstable (i.e., at least one eigenvalue of \\(A\\) has strictly positive real part), then the original system (4.3) is unstable at \\(x=0\\). If the linearized system (4.6) is marginally stable (i.e., all eigenvalues of \\(A\\) have nonpositive real parts, and at least one eigenvalue has zero real part), then the stability of the original system (4.3) at \\(x=0\\) is indeterminate. Theorem 4.1 is actually quite useful when we want to quickly examine the local stability of a nonlinear system around a given equilibrium point, as we will show in the next example. Example 4.2 (Stability of A Simple Pendulum by Linearization) Consider the simple pendulum dynamics (4.5) in Example 4.1. Without loss of generality, let \\(m=1,l=1,b=0.1\\). The Jacobian of the nonlinear dynamics reads \\[ A = \\frac{\\partial f}{\\partial x} = \\begin{bmatrix} 0 &amp; 1 \\\\ -\\frac{g}{l} \\cos \\theta &amp; -\\frac{b}{ml^2} \\end{bmatrix}. \\] At the bottomright equilibrium point \\(\\theta =0, \\dot{\\theta} = 0\\), the matrix \\(A\\) has two eigenvalues \\[ -0.0500 \\pm 3.13i, \\] and hence the pendulum is asymptotically stable at the bottomright equilibrium point. At the upright equilibrium point \\(\\theta =\\pi, \\dot{\\theta} = 0\\), the matrix \\(A\\) has two eigenvalues \\[ 3.08, \\quad -3.18, \\] and hence the pendulum is unstable at the upright equilibrium point. The linearization method is easy to carry out. However, it tells us nothing about global stability or exponential stability. Moreover, when the linearized system is marginally stable, the stability of the orignal system is inconclusive. In the next, we will introduce a more general, and perhaps the most popular framework for analyzing the stability of nonliear systems. 4.1.3 Lyapunov Analysis The basic idea of Lyapunov analysis is quite intuitive: if can find an “energy-like” scalar function for a system such that the scalar function is zero at an equilibrium point and positive everywhere else, and the time-derivative of the scalar function is zero at the equilibrium point but negative otherwise, then we know that the energy of the system will eventually converge to zero, and hence the state trajectory will converge to the equilibrium point. Lyapunov analysis was originally inspired by the energy function of a mechanical system: the total energy of a mechanical system (potental energy plus kinetic energy) will settle down to its minimum value if it is constantly dissipated (e.g., due to damping). However, the concept of a Lyapunov function is much broader than the energy function, i.e., it can be an arbitrary abstract function without any physical meaning. Let us now introduce the concept of a Lyapunov function. Definition 4.6 (Positive Definite Function) A scalar function \\(V(x)\\) is said to be locally positive definite in a ball \\(B_R\\) if \\[ V(0) = 0 \\quad \\text{and} \\quad V(x) &gt; 0, \\forall x \\in B_R \\backslash \\{0\\}, \\] and globally positive definite if \\[ V(0) = 0 \\quad \\text{and} \\quad V(x) &gt; 0, \\forall x \\in \\mathbb{X} \\backslash \\{0\\}, \\] where \\(\\mathbb{X}\\) is the entire state space. A function \\(V(x)\\) is said to be negative definite if \\(-V(x)\\) is positive definite. A function \\(V(x)\\) is said to be positive semidefinite if the “\\(&gt;\\)” sign is replaced by the “\\(\\geq\\)” sign in the above equations. A function \\(V(x)\\) is said to be negative semidefinite if \\(-V(x)\\) is positive semidefinite. For example, when \\(\\mathbb{X} = \\mathbb{R}^2\\), the function \\(V(x) = x_1^2 + x_2^2\\) is positive definite, but the function \\(V(x) = x_1^2\\) is only positive semidefinite. Definition 4.7 (Lyapunov Function) In the ball \\(B_R\\), if a function \\(V(x)\\) is positive definite, and its time derivative along any system trajectory \\[ \\dot{V}(x) = \\frac{\\partial V}{\\partial x} f(x) \\] is negative semidefinite (we assume the partial derivative \\(\\frac{\\partial f}{\\partial x}\\) exists and is continuous), then \\(V(x)\\) is said to be a Lyapunov function for system (4.3). Note that \\(\\dot{V}(x^\\star) = 0\\) at any equilibrium point \\(x^\\star\\) by definition. With the introduction of positive definite and Lyapunov functions, we are now ready to use them to certify different concepts of stability. Theorem 4.2 (Lyapunov Local Stability) Consider the nonlinear system (4.3) in a ball \\(B_R\\) with equilibrium point \\(x=0\\), if there exists a scalar function \\(V(x)\\) (with continuous partial derivatives) such that \\(V(x)\\) is positive definite (in \\(B_R\\)) \\(\\dot{V}(x)\\) is negative semidefinite (in \\(B_R\\)) then the equilibrium point \\(x=0\\) is stable in the sense of Lyapunov (cf. Definition 4.2). Moreover, if \\(\\dot{V}(x)\\) is negative definite in \\(B_R\\), then the equilibrium point is asymptotically stable (cf. Definition 4.3). if \\(\\dot{V}(x) \\leq - \\alpha V(x)\\) for any \\(x \\in B_R\\), then the equilibrium point is exponentially stable (cf. Definition 4.4). Let us apply Theorem 4.2 to the simple pendulum. Example 4.3 (Lyapunov Local Stability for A Simple Pendulum) Consider the pendulum dynamics (4.5). The total energy of a pendulum is \\[\\begin{equation} V(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + mgl (1 - \\cos \\theta). \\tag{4.7} \\end{equation}\\] Clearly, \\(V(x)\\) is positive definite on the entire state space, and the only point where \\(V(x) = 0\\) is the equilibrium point \\(\\theta = 0, \\dot{\\theta} = 0\\). Let us compute the time derivative of \\(V(x)\\): \\[ \\dot{V}(x) = ml^2 \\dot{\\theta} \\ddot{\\theta} + mgl \\sin \\theta \\dot{\\theta} = ml^2 \\dot{\\theta} \\left( -\\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\sin\\theta) \\right) + mgl \\sin \\theta \\dot{\\theta} = -b \\dot{\\theta}^2 , \\] which is clearly negative semidefinite. In fact, \\(\\dot{V}(x)\\) is precisely the energy dissipation rate due to damping. By Theorem 4.2 we conclude that the equilibrium point is stable in the sense of Lyapunov. Note that with this choice of \\(V(x)\\) as in (4.7), we actually cannot certify asymptotic local stability of the bottomright equilibrium point. So a natural question is, can we find a better Lyapunov function that indeed certifies asymptotic stability? The answer is yes. Consider a different Lyapunov function \\[\\begin{equation} \\tilde{V}(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + \\frac{1}{2} ml^2 \\left( \\frac{b}{ml^2}\\theta + \\dot{\\theta} \\right)^2 + 2mgl (1 - \\cos \\theta), \\tag{4.8} \\end{equation}\\] which is positive definite and admits a single zero-value point \\(\\theta = 0, \\dot{\\theta} = 0\\) that is also the bottomright equilibrium point. Simplifying \\(\\tilde{V}(x)\\) we can get \\[\\begin{align} \\tilde{V}(x) &amp;= ml^2 \\dot{\\theta}^2 + 2mgl(1-\\cos \\theta) + \\frac{1}{2} ml^2 \\left( \\frac{b^2}{m^2 l^4} \\theta^2 + \\frac{2b}{ml^2} \\theta \\dot{\\theta} \\right) \\\\ &amp;= 2V(x) + \\frac{1}{2} ml^2 \\left( \\frac{b^2}{m^2 l^4} \\theta^2 + \\frac{2b}{ml^2} \\theta \\dot{\\theta} \\right). \\end{align}\\] The time derivative of the new function \\(\\tilde{V}(x)\\) is \\[\\begin{align} \\dot{\\tilde{V}}(x) &amp;= 2 \\dot{V}(x) + \\frac{ml^2}{2} \\left( \\frac{2b^2}{m^2 l^4} \\theta \\dot{\\theta} + \\frac{2b}{ml^2} (\\dot{\\theta}^2 + \\theta \\ddot{\\theta}) \\right) \\\\ &amp; = 2\\dot{V}(x) + b\\dot{\\theta}^2 + \\left( \\frac{b^2}{ml^2} \\theta\\dot{\\theta} + b \\theta \\left( -\\frac{1}{ml^2} (b\\dot{\\theta} + mgl \\sin \\theta) \\right) \\right) \\\\ &amp; = -b \\left( \\dot{\\theta}^2 + \\frac{g}{l} \\theta \\sin \\theta \\right). \\end{align}\\] \\(\\dot{\\tilde{V}}(x)\\) is negative definite locally around the equilibrium point (locally \\(\\sin\\theta \\approx \\theta\\)). Therefore, with the new Lyapunov function \\(\\tilde{V}(x)\\) we can certify asymptotic stability. Interestingly, \\(V(x)\\) is intuitive (the total energy of the pendulum system), but it fails to certify asymptotic local stability (as least by just using Theorem 4.2). \\(\\tilde{V}(x)\\) does not have any physical intuition, but it successfully certifies local asymptotic stability. In Section 4.1.4, we will see that when using \\(V(x)\\) with the invariant set theorem, we can actually still certify the asymptotic stability of the pendulum around the bottomright equilibrium. In many applications, we desire to certify the global stability of an equilibrium point. The following theorem states that if in addition the scalar function \\(V(x)\\) is radially unbounded, then global stability can be certified. Theorem 4.3 (Lyapunov Global Stability) For the autonomous system (4.3), suppose there exists a scalar function \\(V(x)\\) with (continuous partial derivatives) such that \\(V(x)\\) is positive definite; \\(\\dot{V}(x)\\) is negative definite; \\(V(x) \\rightarrow \\infty\\) as \\(\\Vert x \\Vert \\rightarrow \\infty\\), then the equilibrium point \\(x = 0\\) is globally asymptotically stable (cf. Definition 4.5). Moreover, if in addition to the three conditions above \\(\\dot{V}(x) \\leq - \\alpha V(x)\\) for some \\(\\alpha &gt; 0\\), then the equilibrium point is globally exponentially stable. 4.1.4 Invariant Set Theorem Through Theorem 4.2, Theorem 4.3, and Example 4.3, we see that in order to certify asymptotic stability, the time derivative \\(\\dot{V}(x)\\) is required to be positive definite. However, in many cases, with Example 4.3 being a typical one, \\(\\dot{V}(x)\\) is only negative semidefinite, which makes it difficult to certify asymptotic stability. In this section, we will introduce the invariant set theorem that can help us reason about asymptotic stability even when \\(\\dot{V}(x)\\) is only negative semidefinite. Let us first introduce the notion of an invariant set. Definition 4.8 (Invariant Set) A set \\(G\\) is an invariant set for a dynamical system (4.3) if every system trajectory that starts within \\(G\\) remains in \\(G\\) for all future time. Formally, \\[ x(0) \\in G \\Longrightarrow x(t) \\in G,\\forall t. \\] A trivial invariant set is the entire state space \\(\\mathbb{X}\\). Another example of an invariant set is the singleton \\(\\{x^\\star \\}\\) with \\(x^\\star\\) being an equilibrium point. A nontrivial invariant set is the domain of attraction of an equilibrium point (cf. Definition 4.3). We now state the local invariant set theorem. Theorem 4.4 (Local Invariant Set) Consider the autonomous system (4.3), and let \\(V(x)\\) be a scalar function with continuous partial derivatives. Assume that the sublevel set \\(\\Omega_{\\rho} = \\{ x \\in \\mathbb{X} \\mid V(x) &lt; \\rho \\}\\) is bounded for some \\(\\rho &gt; 0\\), and \\(\\dot{V}(x) \\leq 0\\) for all \\(x \\in \\Omega_{\\rho}\\). Let \\(\\mathcal{R}\\) be the set of all points within \\(\\Omega_{\\rho}\\) such that \\(\\dot{V}(x) = 0\\), and \\(\\mathcal{M}\\) be the largest invariant set in \\(\\mathcal{R}\\). Then, every trajectory that starts in \\(\\Omega_{\\rho}\\) will converge to \\(\\mathcal{M}\\) as \\(t \\rightarrow \\infty\\). With this theorem, we can now revisit the pendulum example 4.3. Example 4.4 (Revisiting the Local Stability of A Simple Pendulum) In Example 4.3, using the Lyapunov function \\[ V(x) = \\frac{1}{2} ml^2 \\dot{\\theta}^2 + mgl(1 - \\cos\\theta), \\] with time derivative \\[ \\dot{V}(x) = -b\\dot{\\theta}^2, \\] we were only able to verify the stability of the bottomright equilibrium point in the sense of Lyapunov. Now let us use the invariant set theorem 4.4 to show the asymptotic stability of the bottomright equilibrium point. First it is easy to see that the sublevel set of \\(V(x)\\) is bounded. For example, with \\(\\rho = \\frac{1}{4} mgl\\), \\[\\begin{align} V(x) &lt; \\frac{1}{4} mgl \\Rightarrow \\frac{1}{2} ml^2 \\dot{\\theta}^2 &lt; \\frac{1}{4} mgl \\Rightarrow \\dot{\\theta}^2 &lt; \\frac{1}{2} \\frac{g}{l} \\\\ V(x) &lt; \\frac{1}{4} mgl \\Rightarrow mgl(1-\\cos\\theta) &lt; \\frac{1}{4} mgl \\Rightarrow \\cos\\theta &gt; \\frac{3}{4} \\Rightarrow \\theta \\in (-\\arccos \\frac{3}{4}, \\arccos \\frac{3}{4}). \\end{align}\\] The set \\(\\mathcal{R}\\), including all the points in \\(\\Omega_{\\rho}\\) such that \\(\\dot{V}(x) = 0\\) is \\[ \\mathcal{R} = \\{ x \\in \\Omega_{\\rho} \\mid \\dot{\\theta} = 0 \\}. \\] We now claim that the largest invariant set \\(\\mathcal{M}\\) in \\(\\mathcal{R}\\) is just the single equilibrium point \\(x = [0,0]^T\\). We can prove this by contradiction. Suppose there is a different point \\(x&#39; = [\\theta,0]^T\\) with \\(\\theta \\neq 0\\) also belonging to the invariant set \\(\\mathcal{M}\\), then \\[ \\ddot{\\theta} = -\\frac{1}{ml^2} (b \\dot{\\theta} + mgl \\sin \\theta) = - \\frac{g}{l} \\sin\\theta \\neq 0, \\] which means \\(\\dot{\\theta}\\) will immediately become nonzero, and hence the trajectory will exit \\(\\mathcal{R}\\) and also \\(\\mathcal{M}\\). So that point cannot belong to the invariant set. Now by Theorem 4.4, we conclude the bottomright equilibrium point is asymptotically stable. Note that through this analysis we also obtain \\(\\Omega_{\\rho}\\) as a domain of attraction for the bottomright equilibrium point. Similarly, with the addition of the radial unboundedness of \\(V(x)\\), we have a global version of the invariant set theorem. Theorem 4.5 (Global Invariant Set) For the autonomous system (4.3), let \\(V(x)\\) be a scalar function with continuous partial derivatives that satisfies \\(V(x) \\rightarrow \\infty\\) as \\(\\Vert x \\Vert \\rightarrow \\infty\\), and \\(\\dot{V}(x) \\leq 0\\) over the entire state space. Let \\(\\mathcal{R} = \\{ x\\in \\mathbb{X} \\mid \\dot{V}(x)= 0 \\}\\), and \\(\\mathcal{M}\\) be the largest invariant set in \\(\\mathcal{R}\\). Then all system trajectories asymptotically converge to \\(\\mathcal{M}\\) as \\(t \\rightarrow \\infty\\). 4.1.5 Computing Lyapunov Certificates All the Theorems we have stated so far (Theorems 4.2, 4.3, 4.4, and 4.5) are very general and powerful tools for certifying stability of nonlinear systems. However, the key requirement for applying the results is a Lyapunov function \\(V(x)\\) that verifies different types of nonnegativity constraints. How to find these functions? In Example 4.3, we have seen that physical intuition can help us find a good Lyapunov function (4.7). Nevertheless, it did not quite give us what we want in terms of asymptotic stability. Instead, a hand-crafted function (4.8) helped us certify local asymptotic stability. Wouldn’t it be cool that we can design an algorithm to find the Lyapunov certificates for us? A closer look at the Theorems 4.2, 4.3, 4.4, and 4.5 tells us the key property of a Lyapunov certificate is that it needs to satisfy the positivity (or negativity) constraint for all states inside a set. This is a nontrivial and difficult requirement, because even if we were given a function \\(V(x)\\), naively evaluting if \\(V(x)\\) is nonnegative inside a set requires enumeration over all the states in the set, which is impractical given that the set is continuous and has infinite number of states.4 When the dynamics (4.3) is linear, searching for Lyapunov functions is well understood and presented in Appendix C.1. However, when the dynamics is nonlinear, things can get very complicated. In the next, I want to introduce a general framework for searching Lyapunov certificates for nonlinear systems that is based on convex optimization. This framework, although having deep connections with many other disciplines such as algebraic geometry, theoretical computer science, and mathematical optimization, is based on a very simple intution that we all have since high school. Example 4.5 (A Simple Example for Certifying Nonnegativity) Suppose I give you a polynomial of a single variable \\(x \\in \\mathbb{R}\\) \\[ p(x) = x^2 + 2x + 1 \\] and ask you if \\(p(x) \\geq 0\\) for all \\(x\\). You would not hesitate to answer “yes”, because you know \\[ p(x) = (x+1)^2 \\] is the square of \\(x+1\\) and hence must be nonnegative. Let me make it more challenging. Suppose I give you a different polynomial \\[ p(x) = -x^4 + 2 x^2 + x + 1 \\] and ask you if \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\) (instead of any \\(x \\in \\mathbb{R}\\)). At first glance, it seems much harder to answer this question because (i) we have a constraint set \\(x \\in [-1,1]\\), and (ii) the polynomial \\(p(x)\\) has a higher degree and it is not a polynomial that we are very famliar with (compared to \\(p(x) = x^2 + 2x +1\\)). However, if I show you that \\(p(x)\\) can be written as \\[\\begin{equation} p(x) = -x^4 + 2 x^2 + 2x + 1 = (x+1)^2 + x^2 (1 - x^2), \\tag{4.9} \\end{equation}\\] it becomes easy again to certify that \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\). Why? First notice that \\((x+1)^2 \\geq 0\\) for any \\(x \\in \\mathbb{R}\\), Then notice that \\(1 - x^2 \\geq 0\\) for any \\(x \\in [-1,1]\\), and \\(x^2 \\geq 0\\) for any \\(x\\). Therefore, \\(x^2 (1-x^2) \\geq 0\\) for any \\(x \\in [-1,1]\\). Combining the above two reasonings, it becomes clear \\(p(x)\\) is nonnegative for any \\(x \\in [-1,1]\\). What we have learned from this simple example is that Given a polynomial \\(p(x)\\) and a constraint set \\(x \\in \\mathcal{X} \\subseteq \\mathbb{R}^n\\), if we can write \\(p(x)\\) as a sum of a finite number of products \\[ p(x) = \\sum_{i=1}^K \\sigma_i(x) g_i(x) \\] where \\(\\sigma_i(x)\\) is a polynomial that we know is always nonnegative for any \\(x \\in \\mathbb{R}^n\\) (just like \\((x+1)^2\\) and \\(x^2\\) in (4.9)), and \\(g_i(x)\\) is a polynomial that we know is always nonnegative for any \\(x\\) in the constraint set \\(\\mathcal{X}\\) (just like \\(1-x^2\\) for the set \\([-1,1]\\) in (4.9)), then we have a certificate that \\(p(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\). With this simple intuition, let me now formalize the framework of sum of squares (SOS) certificates for proving nonnegativity (also known as positivstellensatz, or in short P-satz). Positivstellensatz, Sum of Squares, and Convex Optimization Basic Semialgebraic Set. Let \\(x = [x_1,\\dots,x_n] \\in \\mathbb{R}^n\\) be a list of variables, we define a basic semialgebraic set as \\[\\begin{equation} \\mathcal{X} = \\{ x \\in \\mathbb{R}^{n} \\mid p_i(x) = 0, i = 1,\\dots,l_{\\mathrm{eq}}; p_i(x) \\geq 0, i=l_{\\mathrm{eq}}+1,\\dots,l_{\\mathrm{eq}} + l_{\\mathrm{ineq}} \\} \\tag{4.10} \\end{equation}\\] where \\(p_i(x),i=1,\\dots,l_{\\mathrm{eq}}+l_{\\mathrm{ineq}}\\) are polynomial functions in \\(x\\). In other words, the set \\(\\mathcal{X}\\) is a subset of \\(\\mathbb{R}^n\\) that is defined by \\(l_{\\mathrm{eq}}\\) equality constraints and \\(l_{\\mathrm{ineq}}\\) inequality constraints. Observe that a basic semialgebraic set can capture a lot of the common constraint sets, such as a unit sphere, a unit ball, and a box (try this for yourself). Positivstellensatz. We are now given the same question as in Example 4.5. Suppose I give you another polynomial function \\(p_0(x)\\), how can you tell me if \\(p_0(x)\\) is nonnegative for any \\(x\\) in the basic semialgebraic set \\(\\mathcal{X}\\)? That is, to verify if \\[ p_0(x) \\geq 0, \\quad \\forall x \\in \\mathcal{X}. \\] Formalizing the intution obtained from Example 4.5, you will say if someone can produce a decomposition of \\(p_0(x)\\) as \\[\\begin{equation} p_0(x) = \\sigma_0(x) + \\sum_{i=1}^{l_{\\mathrm{ineq}}} \\sigma_i(x) p_{i + l_{\\mathrm{eq}}}(x) + \\sum_{i=1}^{l_{\\mathrm{eq}}} \\lambda_i(x) p_{i}(x), \\tag{4.11} \\end{equation}\\] where \\(\\sigma_0,\\sigma_1,\\dots,\\sigma_{l_{\\mathrm{ineq}}}\\) are “some type of” polynomials that we know are always nonnegative (for any \\(x \\in \\mathbb{R}^n\\)), and \\(\\lambda_1,\\dots,\\lambda_{l_{\\mathrm{eq}}}\\) are arbitrary polynomials. Then I have a “certificate” that \\(p_0(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\). Why? The reasoning is exactly the same as before. \\(\\sigma_0(x) \\geq 0\\) for any \\(x\\), \\(\\sigma_i(x) p_{i+l_{\\mathrm{eq}}}(x) \\geq 0, i=1,\\dots,l_{\\mathrm{ineq}}\\) for any \\(x \\in \\mathcal{X}\\), because (a) \\(\\sigma_i(x) \\geq 0\\) for any \\(x\\), and (b) \\(p_{i+l_{\\mathrm{eq}}}(x) \\geq 0\\) for any \\(x \\in \\mathcal{X}\\) by definition of the basic semialgebraic set (4.10), \\(\\lambda_i(x) p_i (x) = 0, i=1,\\dots,l_{\\mathrm{eq}}\\) for any \\(x \\in \\mathcal{X}\\) by definition of the basic semialgebraic set (4.10). We call \\(\\sigma_i\\)’s “nonnegative polynomial multipliers”, and \\(\\lambda_i\\)’s “polynomial multipliers”. Sum-of-Squares. Now it comes the key question: what type of polynomials should we choose as the nonnegative polynomial multipliers? Ideally, this type of polynomials should be always (trivially) nonnegative, and have a nice representation for its unknown parameters (coefficients). Looking back at our choice of multipliers, i.e., \\((x+1)^2\\) and \\(x^2\\) in Example 4.5, it is natural to come up with the choice of a “sum-of-squares” (SOS) polynomial. Definition 4.9 (Sum-of-Squares Polynomial) A polynomial \\(\\sigma(x)\\) is called an SOS polynomial if \\[ \\sigma(x) = \\sum_{i=1}^k q_i^2(x), \\] i.e., \\(\\sigma(x)\\) can be written as a sum of \\(k\\) squared polynomials. OK, an SOS polynomial is trivially nonnegative (satisfying requirement (a) above), but does it have a nice representation for its parameters? The following Lemma gives us an affirmative answer. Lemma 4.1 (SOS Polynomial and Positive Semidefinite Matrix) A polynomial \\(\\sigma(x)\\) is SOS if and only if \\[ \\sigma(x) = [x]_d^T Q [x]_d \\] for some \\(Q \\succeq 0\\), where \\([x]_d\\) is the vector of monomials in \\(x\\) of degree up to \\(d\\). For example, if \\(x \\in \\mathbb{R}^2\\) and \\(d = 2\\), then \\[ [x]_2 = [1,x_1,x_2,x_1^2,x_1x_2,x_2^2]^T. \\] With the choice of \\(\\sigma(x)\\) as SOS polynomials, we are now ready to explicitly search for a nonnegativity certificate in the form of (4.11): \\[\\begin{equation} \\begin{split} \\text{find} &amp; \\quad \\{\\sigma_i \\}_{i=0}^{l_{\\mathrm{ineq}}}, \\{ \\lambda_i \\}_{i=1}^{l_{\\mathrm{eq}}} \\\\ \\text{subject to} &amp; \\quad p_0(x) = \\sigma_0(x) + \\sum_{i=1}^{l_{\\mathrm{ineq}}} \\sigma_i(x) p_{i + l_{\\mathrm{eq}}}(x) + \\sum_{i=1}^{l_{\\mathrm{eq}}} \\lambda_i(x) p_{i}(x), \\\\ &amp; \\quad \\sigma_i \\text{ is SOS}, i=0,\\dots,l_{\\mathrm{ineq}}, \\\\ &amp; \\quad \\lambda_i \\text{ is polynomial}, i=1,\\dots,l_{\\mathrm{eq}} \\\\ &amp; \\quad \\mathrm{deg}(\\sigma_0) \\leq 2 \\kappa, \\mathrm{deg}(\\sigma_i p_{i+l_{\\mathrm{eq}}}) \\leq 2 \\kappa, i=1,\\dots,l_{\\mathrm{ineq}}, \\\\ &amp; \\quad \\mathrm{deg}(\\lambda_i p_i) \\leq 2 \\kappa, i=1,\\dots,l_{\\mathrm{eq}}. \\end{split} \\tag{4.12} \\end{equation}\\] Bounding the Degree. The careful reader realizes that in (4.12) we have added constraints on the degrees of the polynomial multipliers \\(\\sigma_i\\)’s and \\(\\lambda_i\\)’s.5 Precisely, we choose an integer \\(\\kappa\\), which we call the relaxation order, such that \\[ 2 \\kappa \\geq \\max \\{ \\mathrm{deg}(p_i(x)) \\}_{i=0}^{l_{\\mathrm{eq}} + l_{\\mathrm{ineq}}}, \\] and restrict the products \\(\\sigma_i p_{i+l_{\\mathrm{eq}}}\\)’s and \\(\\lambda_i p_i\\)’s to have degrees at most \\(2\\kappa\\). With this, we are explicitly limiting the degrees of the multipliers \\(\\sigma_i\\)’s and \\(\\lambda_i\\)’s, and hence asking the formulation (4.12) to search for a finite number of parameters (otherwise, if the degree of the multipliers is unbounded, then the number of parameters to be searched is infinite). Convex Optimization. The last crucial (and surprising) observation is that the problem (4.12) is a convex optimization! This is due to the following three reasons The polynomial multipliers \\(\\lambda_i\\)’s can be fully parametrized by their coefficients, and these coefficients can be arbitrary vectors. Precisely, if \\(\\lambda(x)\\) is a polynomial with degree up to \\(d\\), then \\[ \\lambda(x) = c^T [x]_d, \\] where \\([x]_d\\) is the vector of monomials in \\(x\\) of degree up to \\(d\\), and \\(c\\) is the vector of coefficients. The SOS multipliers \\(\\sigma_i\\)’s can be fully parametrized by their coefficients, and these coefficients are positive semidefinite matrices, according to Lemma 4.1. The equality constraint of decomposing \\(p_0(x)\\) as a sum of products in (4.12) therefore becomes a set of affine equality constraints on the parameters of \\(\\lambda_i\\)’s and \\(\\sigma_i\\)’s, by matching coefficients of the monomials on the left-hand size and the right-hand side. Therefore, the problem (4.12) is a convex semidefinite program (SDP). There are multiple software packages, e.g., SOSTOOLS, YALMIP, SumOfSquares.py, that allow us to model our problem in the form of (4.12), convert the formulation into SDPs, and pass them to SDP solvers (such as MOSEK). We will see an example of this soon. Extensions. I want to congratulate, and welcome you to enter the world of SOS relaxations! Like I said before, this is an active area of research and the framework I just introduced is just a tip of the iceberg. Therefore, before I end this tutorial, I want to point out several extensions of the SOS framework. Necessary Condition. We have seen that a decomposition in the form of (4.12) is a sufficient condition to prove the nonnegativity of \\(p_0(x)\\). Is it also a necessary condition? That is, for any \\(p_0(x)\\) that is nonnegative on the set \\(\\mathcal{X}\\), does it admit a decomposition in the form of (4.12)? In general, the answer is no, and there exist nonnegative polynomials that cannot be written in the form of SOS decompositions (e.g., the Motzkin’s polynomial). However, with certain assumptions on the set \\(\\mathcal{X}\\), the decomposition (4.12) is also necessary for nonnegativity! A well-known assumption is called the Archimedean condition (which, roughly speaking, requires the set \\(\\mathcal{X}\\) to be compact)). I suggest you to read (Blekherman, Parrilo, and Thomas 2012) for more details. Global Polynomial Optimization. The SOS framework can be used for global optimization of polynomials in a straightforward way. Consider the polynomial optimization problem (POP) \\[ \\min_{x \\in \\mathcal{X}} p_0(x), \\] where one seeks the global minimum of the polynomial \\(p_0(x)\\) on the set \\(\\mathcal{X}\\). A POP is generally a nonconvex optimization problem, and it is difficult to obtain a globally optimal solution. However, with a slight change of perspective, we can write the problem above equivalently as \\[\\begin{equation} \\begin{split} \\max &amp; \\quad \\gamma \\\\ \\text{subject to} &amp; \\quad p_0(x) - \\gamma \\geq 0, \\quad \\forall x \\in \\mathcal{X}. \\end{split} \\tag{4.13} \\end{equation}\\] Basically I want to push the lower bound \\(\\gamma\\) as high as possible. The constraint in (4.13) asks \\(p_0(x) -\\gamma\\) to be nonnegative on \\(\\mathcal{X}\\). With the SOS framework introduced above, we can naturally relax it to \\[\\begin{equation} \\begin{split} \\max &amp; \\quad \\gamma \\\\ \\text{subject to} &amp; \\quad p_0(x) - \\gamma \\quad \\text{is SOS on} \\quad \\mathcal{X}, \\end{split} \\tag{4.14} \\end{equation}\\] where the “SOS on \\(\\mathcal{X}\\)” constraint is exactly the problem (4.12). Therefore, we have relaxed the nonconvex optimization (4.13) into a convex problem (4.14)! Moreover, by increasing the relaxation order \\(\\kappa\\), we obtain a sequence of lower bounds that asymptotically converge to the true global optimum of the nonconvex problem (4.13). This is called Lasserre’s hierarchy of moment-SOS relaxations, originally proposed by Lasserre in the seminal work (Jean B. Lasserre 2001). As this name suggests, the dual problem to the SOS relaxation (4.14) is called the moment relaxation. Lasserre’s hierarchy has recently gained a lot of attention due to the empirical observation in many engineering disciplines that the convergence to global optimum is finite, i.e., by solving the convex problem (4.14) at a finite relaxation order \\(\\kappa\\), an exact global optimizer of the original nonconvex problem (4.13) can be extracted. For a pragmatic introduction to the moment relaxation, I suggest to read Section 2.2 of (H. Yang and Carlone 2022). For more applications of Lasserre’s hierarchy, please refer to (Jean Bernard Lasserre 2009). Scalability. I have to warn you that there is no free lunch. The fact that so many challenging problems can be relaxed or restated as convex optimization problems should send you an alert. Does this mean that we can use convex optimization to solve all the challenging problems? Well, although we hope this is the case, in practice we are limited by the computational resources. The caveat is that the problem (4.12) and (4.14), despite being convex, grows very large as the dimension \\(n\\) and relaxation order \\(\\kappa\\) increases. Another way of saying this is that, we seek to solve small-to-medium scale nonconvex problems with large-scale convex problems. Unfortunately, today’s SDP solver cannot solve all the problems we formulate, and hence a major research direction in the mathematical optimization community is to develop SDP solvers that are more scalable. You can read (H. Yang et al. 2022) and references therein for more details. Non-SOS Certificates. Nobody is preventing us to use a different choice of nonnegative polynomial multipliers (other than SOS multipliers) in (4.11). For example, one can use a decomposition as the sum of nonnegative circuit polynomials (Wang 2022) or signomials (Murray, Chandrasekaran, and Wierman 2021). However, to the best of my knowledge, non-SOS certificates are far less popular than SOS certificates. There are many other extensions to the SOS framework, and a complete enumeration is beyond the scope of this lecture notes. For the connection between SOS and theoretical computer science, you can see the lecture notes by Boaz Barak and David Steurer. There are also more recent monographs about SOS, for example (Magron and Wang 2023) and (Nie 2023). I plan to introduce these in more details in an upcoming graduate-level class at Harvard. That was a long detour from Lyapunov analysis! The SOS machinery will come back later when we study multiple other topics in optimal control and estimation. But now let us show how to tackle the problem of computing Lyapunov certificates using the SOS machinery. According to Theorem 4.2, given a set \\(\\mathcal{X}\\) that contains an equilibrium point \\(x^\\star\\), if we can find a Lyapunov function \\(V(x)\\) such that \\(V(x)\\) is positive definite on \\(\\mathcal{X}\\) and \\(\\dot{V}(x)\\) is negative definite on \\(\\mathcal{X}\\), then the equilibrium point \\(x^\\star\\) is locally asymptotically stable. With the SOS machinery, we can search for a \\(V(x)\\) that is a polynomial as \\[\\begin{align} \\text{find} &amp; \\quad V(x) \\\\ \\text{subject to} &amp; \\quad V(x) - \\epsilon_1 \\Vert x - x^\\star \\Vert^2 \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ &amp; \\quad - \\epsilon_2 \\Vert x - x^\\star \\Vert^2 - \\frac{\\partial V(x)}{\\partial x} f(x) \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ &amp; \\quad V(x^\\star) = 0, \\end{align}\\] where \\(\\epsilon_1, \\epsilon_2 &gt; 0\\) are (small) positive constants. This is a convex optimization problem, just like (4.12) (try to convince yourself my claim is true). Similarly, we can choose a relaxation order \\(\\kappa\\) and solve the above problem. If a solution exists, then we find a valid Lyapunov certificate. Let us apply it to the simple pendulum to synthesize local stability certificates. Example 4.6 (Computing Lyapunov Local Stability Certificate for the Simple Pendulum with Convex Optimization) The SOS framework works with polynomials, so let us first write the pendulum dynamics in polynomial form via a change of coordinate \\(x = [\\mathfrak{s}, \\mathfrak{c}, \\dot{\\theta}]^T\\) with \\(\\mathfrak{s} = \\sin \\theta\\), \\(\\mathfrak{c} = \\cos\\theta\\): \\[ \\begin{cases} \\dot{\\mathfrak{s}} = \\mathfrak{c} \\dot{\\theta} \\\\ \\dot{\\mathfrak{c}} = -\\mathfrak{s} \\dot{\\theta} \\\\ \\ddot{\\theta} = - \\frac{1}{ml^2}(b \\dot{\\theta} + mgl \\mathfrak{s}) \\end{cases}. \\] We will use \\(m = 1, l = 1, b=0.1\\) for our numerical experiment. We want to find a local Lyapunov certificate in the compact set \\[\\begin{equation} \\theta \\in \\left[-\\arccos \\frac{3}{4}, \\arccos \\frac{3}{4} \\right], \\quad \\dot{\\theta} \\in \\left[- \\frac{\\pi}{2}, \\frac{\\pi}{2} \\right]. \\tag{4.15} \\end{equation}\\] In the new coordinates \\(x\\), this is equivalent to the semialgebraic set \\[ \\mathcal{X} = \\left\\{ x \\in \\mathbb{R}^{3} \\mid \\mathfrak{s}^2 + \\mathfrak{c}^2 = 1, \\dot{\\theta}^2 \\leq \\frac{\\pi^2}{4}, \\mathfrak{c} \\geq \\frac{3}{4} \\right\\}. \\] Denoting the bottomright equilibrium point as \\(x_e = [0,1,0]^T\\), and with \\(\\epsilon_1,\\epsilon_2 &gt; 0\\) two positive constants, we can seek a Lyapunov function \\(V(x)\\) that satisfies the following conditions \\[\\begin{align} V(x) \\geq \\epsilon_1 (x - x_e)^T (x - x_e), \\quad \\forall x \\in \\mathcal{X} \\tag{4.16}\\\\ \\dot{V}(x) = \\frac{\\partial V}{\\partial x} \\dot{x} \\leq - \\epsilon_2 (x - x_e)^T (x - x_e), \\quad \\forall x \\in \\mathcal{X} \\tag{4.17}\\\\ V(x_e) = 0, \\quad \\dot{V}(x_e) = 0 \\tag{4.18} \\end{align}\\] where (4.16) ensures \\(V(x)\\) is positive definite, (4.17) ensures \\(\\dot{V}(x)\\) is negative definite, and (4.18) ensures \\(V(x),\\dot{V}(x)\\) vanish at the equilibrium point. To leverage the power of convex optimization, we can relax the positivity constraints as SOS constraints \\[\\begin{align} V(x) - \\epsilon_1 (x - x_e)^T (x - x_e) \\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ - \\epsilon_2 (x - x_e)^T (x - x_e) - \\frac{\\partial V}{\\partial x} \\dot{x}\\quad \\text{is SOS on} \\quad \\mathcal{X} \\\\ V(x_e) = 0, \\quad \\dot{V}(x_e) = 0. \\end{align}\\] If we limit the degree of \\(V\\) to \\(2\\), choose the relaxation order \\(\\kappa = 2\\), and \\(\\epsilon_1 = \\epsilon_2 = 0.01\\), we obtain a solution \\[ V(x) = 2.7982 \\mathfrak{s}^2 + 0.086248 \\mathfrak{s} \\dot{\\theta} + 2.4548\\mathfrak{c}^2 + 0.88117 \\dot{\\theta}^2 - 16.6277 \\mathfrak{c} + 14.1728 \\] with the time derivative \\[ \\dot{V}(x) = 0.68675 \\mathfrak{s} \\mathfrak{c} \\dot{\\theta} + 0.086248* \\mathfrak{c} \\dot{\\theta}^2 - 0.84523 \\mathfrak{s}^2 - 0.65191 \\mathfrak{s} \\dot{\\theta} - 0.17623 \\dot{\\theta}^2. \\] Plotting \\(V(x)\\) in the constraint set (4.15) using \\((\\theta, \\dot{\\theta})\\) coordinates, we get Figure 4.2: Lyapunov local stability certificate computed via convex optimization. and verify that \\(V(x)\\) is locally positive definite. Plotting \\(\\dot{V}(x)\\) in the constraint set (4.15) using \\((\\theta, \\dot{\\theta})\\) coordinates, we get Figure 4.3: Derivative of the Lyapunov local stability certificate computed via convex optimization. and verify that \\(\\dot{V}(x)\\) is locally negative definite. You should try the code for this example here. 4.2 Controlled Systems We can generalize the notion of a Lyapunov function for an autonomous system to the notion of a Control Lyapunov Function (CLF) for a controlled system (Sontag 1983). Consider a controlled system \\[\\begin{equation} \\dot{x} = f(x,u) \\tag{4.19} \\end{equation}\\] where \\(x \\in \\mathbb{X} \\subseteq \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{U} \\subseteq \\mathbb{R}^m\\) the control, and \\(f\\) is continuously differentiable. Let \\(x = 0, u=0\\) be an equilibrium point, i.e., \\(f(0,0) = 0\\). The existence of a CLF guarantees that we can design a controller to stabilize the equilibrium point. Theorem 4.6 (Control Lyapunov Function) Let \\(V(x): \\mathbb{X} \\rightarrow \\mathbb{R}\\) be a continuously differentiable positive definite function, i.e., \\(V(0) = 0\\) and \\(V(x) &gt; 0, \\forall x \\in \\mathbb{X} \\backslash \\{ 0 \\}\\). If there exists \\(\\rho &gt; 0\\) such that \\(\\Omega := \\{ x \\in \\mathbb{X} \\mid V(x) \\leq \\rho \\}\\) is bounded and \\[\\begin{equation} \\min_{u \\in \\mathbb{U}} \\dot{V}(x) = \\frac{\\partial V}{\\partial x} f(x,u) &lt; 0, \\quad \\forall x \\in \\Omega \\backslash \\{ 0 \\}, \\tag{4.20} \\end{equation}\\] then \\(V(x)\\) is called a control Lyapunov function. In this case, if the system starts within the set \\(\\Omega\\), then there exists a controller \\(u(t)\\) that drives the system towards the equilibrium point \\(0\\). In Theorem 4.6, it is clear that \\(Omega\\) is a control invariant set. In fact, \\(Omega\\) is an inner approximation of the region of attraction to the equilibrium point \\(0\\). Deploying a CLF. Now we observe that, when a CLF \\(V(x)\\) is given, it can help us design an asymptotically stablizing controller. Consider a special instance of the controlled system (4.19) that is control-affine \\[\\begin{equation} \\dot{x} = f_1(x) + f_2(x)u. \\tag{4.21} \\end{equation}\\] Clearly, the dynamics (4.21) is affine in the control \\(u\\). Almost all robotics systems are control affine, so (4.21) is quite general. Suppose we are given an arbitrary controller \\(u_d(t)\\), which may or may not stabilize the system towards the equilibrium point (e.g., \\(u_d(t)\\) may be obtained from neural networks). We can use the CLF \\(V(x)\\) to correct \\(u_d(t)\\) so that we always obtain a stabilizing feedback controller \\[\\begin{equation} \\begin{split} u(x(t)) = \\arg\\min_{u \\in \\mathbb{U}} &amp; \\quad \\Vert u - u_d(t) \\Vert^2 \\\\ \\text{subject to} &amp; \\quad \\dot{V}(x) = \\frac{\\partial V}{\\partial x} (f_1(x) + f_2(x) u) &lt; 0. \\end{split} \\tag{4.22} \\end{equation}\\] Note that the optimization (4.22) is actually a convex optimization problem! Since we are minimizing over \\(u\\) (the state \\(x(t)\\) is given), the objective in (4.22) is quadratic in \\(u\\) and the constraint in (4.22) is linear in \\(u\\), implying (4.22) is a quadratic optimization problem. The CLF condition (4.20) ensures that (4.22) is always feasible. Intuitively, optimization (4.22) seeks to find the feedback controller that (a) minimally modifies the given arbitrary controller \\(u_d(t)\\), and (b) guarantees stabilization towards the equilibrium. Verification and Synthesis of a CLF. When a CLF is given, deploying it to compute a stablizing controller is easy. However, verifying if a candidate function is indeed a control Lyapunov function is a highly nontrivial task. In fact, it is an active area of research. The interested reader can refer to (Kang et al. 2023) and (Dai and Permenter 2023). 4.3 Non-autonomous Systems Lemma 4.2 (Barbalat's Lemma) Let \\(f(t)\\) be differentiable, if \\(\\lim_{t \\rightarrow \\infty} f(t)\\) is finite, and \\(\\dot{f}(t)\\) is uniformly continuous,6 then \\[ \\lim_{t \\rightarrow \\infty} \\dot{f}(t) = 0. \\] Theorem 4.7 (Barbalat's Stability Certificate) If a scalar function \\(V(x,t)\\) satisfies \\(V(x,t)\\) is lower bounded, \\(\\dot{V}(x,t)\\) is negative semidefinite \\(\\dot{V}(x,t)\\) is uniformly continuous then \\(\\dot{V}(x,t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\). Proof. \\(V(x,t)\\) is lower bounded and \\(\\dot{V}\\) is negative semidefinite implies the limit of \\(V\\) as \\(t \\rightarrow \\infty\\) is finite (note that \\(V(x,t) \\leq V(x(0),0)\\)). Then the theorem clearly follows from Barbalat’s Lemma 4.2. References Blekherman, Grigoriy, Pablo A Parrilo, and Rekha R Thomas. 2012. Semidefinite Optimization and Convex Algebraic Geometry. SIAM. Dai, Hongkai, and Frank Permenter. 2023. “Convex Synthesis and Verification of Control-Lyapunov and Barrier Functions with Input Constraints.” In 2023 American Control Conference (ACC), 4116–23. IEEE. Dawson, Charles, Sicun Gao, and Chuchu Fan. 2023. “Safe Control with Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction Methods for Robotics and Control.” IEEE Transactions on Robotics. Kang, Shucheng, Yuxiao Chen, Heng Yang, and Marco Pavone. 2023. “Verification and Synthesis of Robust Control Barrier Functions: Multilevel Polynomial Optimization and Semidefinite Relaxation.” In 2023 62nd IEEE Conference on Decision and Control (CDC). Lasserre, Jean B. 2001. “Global Optimization with Polynomials and the Problem of Moments.” SIAM Journal on Optimization 11 (3): 796–817. Lasserre, Jean Bernard. 2009. Moments, Positive Polynomials and Their Applications. Vol. 1. World Scientific. Magron, Victor, and Jie Wang. 2023. Sparse Polynomial Optimization: Theory and Practice. World Scientific. Murray, Riley, Venkat Chandrasekaran, and Adam Wierman. 2021. “Signomial and Polynomial Optimization via Relative Entropy and Partial Dualization.” Mathematical Programming Computation 13: 257–95. Nie, Jiawang. 2023. Moment and Polynomial Optimization. SIAM. Sontag, Eduardo D. 1983. “A Lyapunov-Like Characterization of Asymptotic Controllability.” SIAM Journal on Control and Optimization 21 (3): 462–71. Vinograd, Robert Èlyukimovich. 1957. “Inapplicability of the Method of Characteristic Exponents to the Study of Non-Linear Differential Equations.” Matematicheskii Sbornik 83 (4): 431–38. Wang, Jie. 2022. “Nonnegative Polynomials and Circuit Polynomials.” SIAM Journal on Applied Algebra and Geometry 6 (2): 111–33. Yang, Heng, and Luca Carlone. 2022. “Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite Relaxations and Scalable Global Optimization.” IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (3): 2816–34. Yang, Heng, Ling Liang, Luca Carlone, and Kim-Chuan Toh. 2022. “An Inexact Projected Gradient Method with Rounding and Lifting by Nonlinear Programming for Solving Rank-One Semidefinite Relaxation of Polynomial Optimization.” Mathematical Programming, 1–64. In fact, many of the recent works verify “neural” Lyapunov certificates (and other types of certificates) using this idea, see for example (Dawson, Gao, and Fan 2023).↩︎ The degree of a monomial is the sum of its exponents. For example, \\(\\mathrm{deg}(x_1 x_2^4 x_3^2) = 1 + 4 + 2 = 7\\). The degree of a polynomial is the maximum degree of its monomials. For example, the polynomial \\(p(x) = 1 + x_2 + x_1^2 x_2^3\\) has three monomials with degrees \\(0\\), \\(1\\), and \\(5\\), respectively. Therefore, \\(\\mathrm{deg}(p) = 5\\).↩︎ A sufficient condition for this to hold is that \\(\\ddot{f}\\) exists and is bounded.↩︎ "],["psets.html", "Chapter 5 Problem Sets", " Chapter 5 Problem Sets Exercise 5.1 (Inscribed Polygon of Maximal Perimeter) In this exercise, we will use dynamic programming to solve a geometry problem, i.e., to find the \\(N\\)-side polygon inscribed inside a circle with maximum perimeter. We will walk you through the key steps of formulating and solving the problem, while leaving a few mathematical details for you to fill in. Given a circle with radius \\(1\\), we can randomly choose \\(N\\) distinct points on the circle to form a polygon with \\(N\\) vertices and sides, as shown in Fig. 5.1 with \\(N=3,4,5\\). Figure 5.1: Polygons inscribed inside a circle Once the \\(N\\) points are chosen, the \\(N\\)-polygon will have a perimeter, i.e., the sum of the lengths of its edges. What is the configuration of the \\(N\\) points such that the resulting \\(N\\)-polygon has the maximum perimeter? I claim that the answer is when the \\(N\\)-polygon has edges of equal lengths, or in other words, when the \\(N\\) points are placed on the circle evenly. Let us use dynamic programming to prove the claim. To use dynamic programming, we need to definie a dynamical system and an objective function. Figure 5.2: Sequential placement of N points on the circle. Dynamical system. We will use \\(\\{x_1,\\dots,x_N \\}\\) to denote the angular positions of the \\(N\\) points to be placed on the circle (with slight abuse of notation, we will call each of those points \\(x_k\\) as well). In particular, as shown in Fig. 5.2, let us use \\(x_k\\) to denote the angle between the line \\(O-x_k\\) and the vertical line (\\(O\\) is the center of the circle), with zero angle starting at \\(12\\) O’clock and clockwise being positive. Without loss of generality, we assume \\(x_1 = 0\\). (if \\(x_1\\) is nonzero, we can always rotate the entire circle so that \\(x_1 = 0\\)). After the \\(k\\)-th point is placed, we can “control” where the next point \\(x_{k+1}\\) will be, by deciding the incremental angle between \\(x_{k+1}\\) and \\(x_k\\), denoted as \\(u_k &gt; 0\\) in Fig. 5.2. This is simply saying the dynamics is \\[ x_{k+1} = x_k + u_k, \\quad k=1,\\dots,N-1, \\quad x_1 = 0. \\] Cost-to-go. The perimeter of the \\(N\\)-polygon is therefore \\[ g_N(x_N) + \\sum_{k=1}^{N-1} g_k(x_k, u_k), \\] with the terminal cost \\[ g_N(x_N) = 2 \\sin \\left( \\frac{2\\pi - x_N}{2} \\right) \\] the distance between \\(x_N\\) and \\(x_1\\) (see Fig. 5.2), and the running cost \\[ g_k(x_k,u_k) = 2 \\sin \\left( \\frac{u_k}{2} \\right) \\] the distance between \\(x_{k+1}\\) and \\(x_k\\). Dynamic programming. We are now ready to invoke dynamic programming. We start by setting \\[ J_N(x_N) = g_N(x_N) = 2 \\sin \\left( \\frac{2\\pi - x_N}{2} \\right). \\] We then compute \\(J_{N-1}(x_{N-1})\\) as \\[\\begin{equation} J_{N-1}(x_{N-1}) = \\max_{0&lt; u_{N-1} &lt; 2\\pi - x_{N-1}} \\left\\{ \\underbrace{ 2 \\sin \\left( \\frac{u_{N-1}}{2} \\right) + J_N(x_{N-1} + u_{N-1}) }_{Q_{N-1}(x_{N-1}, u_{N-1})} \\right\\}, \\tag{5.1} \\end{equation}\\] where \\(u_{N-1} &lt; 2\\pi - x_{N-1}\\) because we do not want \\(x_N\\) to cross \\(2\\pi\\). Show that \\[ Q_{N-1}(x_{N-1},u_{N-1}) = 2 \\sin \\left( \\frac{u_{N-1}}{2} \\right) + 2 \\sin \\left( \\frac{2\\pi - x_{N-1} - u_{N-1} }{2} \\right), \\] and \\[ \\frac{\\partial Q_{N-1}(x_{N-1},u_{N-1})}{\\partial u_{N-1}} = \\cos \\left( \\frac{u_{N-1}}{2} \\right) - \\cos \\left( \\frac{2\\pi - x_{N-1} - u_{N-1} }{2} \\right). \\] Show that \\(Q_{N-1}(x_{N-1},u_{N-1})\\) is concave (i.e., \\(-Q_{N-1}(x_{N-1},u_{N-1})\\) is convex) in \\(u_{N-1}\\) for every \\(x_{N-1} \\in (0, \\pi)\\) and \\(u_{N-1} \\in (0, 2\\pi - x_{N-1})\\). (Hint: compute the second derivative of \\(Q_{N-1}(x_{N-1},u_{N-1})\\) with respect to \\(u_{N-1}\\) and use Proposition B.2). With a and b, show that the optimal \\(u_{N-1}\\) that solves (5.1) is \\[ u_{N-1}^\\star = \\frac{2\\pi - x_{N-1}}{2}, \\] and therefore \\[ J_{N-1}(x_{N-1}) = 4 \\sin \\left( \\frac{2\\pi - x_{N-1}}{4} \\right). \\] (Hint: the point at which a concave function’s gradient vanishes must be the unique maximizer of that function) Now use induction to show that the \\(k\\)-th step dynamic programming \\[ J_k(x_k) = \\max_{0&lt; u_k &lt; 2\\pi - x_k} \\left\\{ 2 \\sin\\left( \\frac{u_k}{2} \\right) + J_{k+1}(x_k + u_k) \\right\\} \\] admits an optimal control \\[ u_k^\\star = \\frac{2\\pi - x_k}{N-k+1}, \\] and optimal cost-to-go \\[ J_k(x_k) = 2(N-k+1)\\sin\\left( \\frac{2\\pi - x_k}{2(N-k+1)} \\right). \\] Starting from \\(x_1 = 0\\), what is the optimal sequence of controls? Hopefully now you see why my original claim is true! (Bonus) We are not yet done for this exercise. Since you have probably already spent quite some time on this exercise, I will leave the rest of the exercise a bonus. In case you found this simple geometric problem interesting, you should keep reading as we will use numerical techniques to prove the same claim. In Fig. 5.2, by denoting \\[ u_N = 2\\pi - x_N = 2\\pi - (u_1 + \\dots + u_{N-1}) \\] as the angle between the line \\(O-x_{N}\\) and the line \\(O-x_1\\), it is not hard to observe that the perimeter of the \\(N\\)-polygon is \\[ \\sum_{k=1}^N 2 \\sin \\left( \\frac{u_k}{2} \\right). \\] Consequently, to maximize the perimeter, we can formulate the following optimization \\[\\begin{equation} \\begin{split} \\max_{u_1,\\dots,u_N} &amp;\\quad \\sum_{k=1}^N 2 \\sin \\left( \\frac{u_k}{2} \\right) \\\\ \\text{subject to} &amp;\\quad u_k &gt; 0, k=1,\\dots,N \\\\ &amp;\\quad u_1 + \\dots + u_N = 2 \\pi \\end{split} \\tag{5.2} \\end{equation}\\] where \\(u_k\\) can be seen as the angle spanned by the line \\(x_k - x_{k+1}\\) with respect to the center \\(O\\) so that they are positive and sum up to \\(2\\pi\\). Show that the optimization (5.2) is convex. (Hint: first show the feasible set is convex, and then show the objective function is concave over the feasible set.) Now that we have shown (5.2) is a convex optimization problem, we know that pretty much any numerical algorithm will guarantee convergence to the globally optimal solution. It is too much to ask you to implement a numerical algorithm on your own, as that can be a one-semester graduate-level course (Nocedal and Wright 1999). However, Matlab provides a nice interface, fmincon, to many such numerical algorithms, and let me show you how to use fmincon to solve (5.2) so we can numerically prove our claim. I have provided most of the code necessary for solving (5.2) below. Please fill in the definition of the function perimeter(u), and then run the code in Matlab. Show your results for \\(N=3,10,100\\). Do the solutions obtained from fmincon verify our claim? clc; clear; close all; % number of points to be placed N = 10; % define the objective function % fmincon assumes minimization % We minimize the negative perimeter so as to maximize the perimeter objective = @(u) -1*perimeter(u); % choose which algorithm to use for solving options = optimoptions(&#39;fmincon&#39;, &#39;Algorithm&#39;, &#39;interior-point&#39;); % supply an initial guess % since this is a convex problem, we can use any initial guess u0 = rand(N,1); % solve uopt = fmincon(objective,u0,... % objective and initial guess -eye(N),zeros(N,1),... % linear inequality constraints ones(1,N),2*pi,... % linear equality constraints [],[],[],... % we do not have lower/upper bounds and nonlinear constraints options); % plot the solution x = zeros(N,1); for k = 2:N x(k) = x(k-1) + uopt(k-1); end figure; % plot a circle viscircles([0,0],1); hold on % scatter the placed points scatter(cos(x),sin(x),&#39;blue&#39;,&#39;filled&#39;); axis equal; %% helper functions % The objective function function f = perimeter(u) % TODO: define the perimeter function here. end   Exercise 5.2 (LQR with Constraints) In class we worked on the LQR problem where the states and controls are unbounded. This is rarely the case in real life – you only have a limited amount of control power, and you want your states to be bounded (e.g., not entering some dangerous zones). For linear systems with convex constraints on the control and states, the seminal paper (Bemporad et al. 2002) investigates the landscape of the optimal cost-to-go and controller. In this exercise, let us use convex optimization to numerically study a toy problem. Consider a variant of the LQR problem (2.2) where the controls are bounded between \\([-u_{\\max}, u_{\\max}]\\), the system matrices \\(A_k, B_k\\) are constant, and the dynamics is deterministic: \\[\\begin{equation} \\begin{split} J(x_0) = \\min_{u_{0},\\dots,u_{N-1} \\in [-u_{\\max},u_{\\max}]} &amp;\\quad x_N^T Q_N x_N + \\sum_{k=0}^{N-1} (x_k^T Q_k x_k + u_k^T R_k u_k) \\\\ \\text{subject to} &amp;\\quad x_{k+1} = A x_k + B u_k, k=0,\\dots,N-1 \\end{split} \\tag{5.3} \\end{equation}\\] We assume \\(Q_k\\succeq 0\\) for \\(k=0,\\dots,N\\) and \\(R_k \\succ 0\\) for all \\(k=0,\\dots,N-1\\). Show that Problem (5.3), when \\(x_0\\) is given, is a convex optimization problem. Discretize the continuous-time double integrator dynamics \\[ \\ddot{q} = u, \\quad u \\in [-1,1] \\] in the form of \\(x_{k+1} = A x_k + B u_k\\) with a constant \\(dt\\) time discretization. (Hint: take \\(x = [q,\\dot{q}]\\) as the state vector.) Fix \\(N=50\\), \\(dt=0.1\\) and choose your favorite \\(Q_k\\) and \\(R_k\\). Solve the convex optimization (5.3) at a dense grid of \\(x_0\\) (e.g., using CVX or cvxpy). Plot the optimal cost-to-go \\(J(x_0)\\), and the optimal controls \\(u_0(x_0),\\dots,u_{N-1}(x_0)\\). For the optimal controls, you can just plot one of the controls such as \\(u_0(x_0)\\). You may want to use the Matlab function surf. (Hint: you will most likely benefit from Appendix B.2.) Increase \\(N\\) and decrease \\(dt\\), repeat (c). Do you get more fine-grained plots of the optimal cost-to-go and controls? (When you increase \\(N\\), the convex optimization has more variables to optimize, so there is a limit at which the solver takes too much time.) (Bonus) We only have constraints on the control so far. What if you add constraints to the states as well? For example, you can try limiting the velocity \\(\\dot{q}\\) to be at least \\(0.1\\) by adding \\(\\dot{q}_k \\geq 0.1\\) for some k. How will \\(J\\) and \\(u\\) change? (Bonus) Can you write down the KKT optimality conditions of (5.3) and explain what you have observed from the numerical experiments? (Hint: KKT optimality conditions can be found in Theorem B.2.)   Exercise 5.3 (Cart Pole System) In this exercse, let us study the cart-pole system (we saw the video of human-controlled version in our first lecture), another interesting nonlinear control problem, and reinforce our knowledge about LQR. Our task is to balance a pendulum on a cart by horizontally moving the cart. Fig. 5.3 gives an illustration of the system. See this video for an actual robotic implementation. Figure 5.3: Illustration of cart-pole problem With the above illustration, we parameterize the system with two scalars: \\(x\\) represents the current location of the cart, while \\(\\theta\\) is the angle between current pole and the stable equilibrium. Therefore, our goal is to study the motion of the cart-pole system with a horizonal control \\(f\\). We assume hereafter the system is ideal such that there is no friction, and the mass of the pole concentrates at the free end point. (Bonus) For those of you who have background in rigid-body dynamics, this is an opportunity for you to apply your knowledge. However, feel free to skip this subproblem and it won’t affect the rest of the exercise. Denote the mass of cart and pole as \\(m_c\\) and \\(m_p\\), respectively. Derive the equations of motion: \\[\\begin{equation} \\left(m_c+m_p\\right) \\ddot{x}+m_p l \\ddot{\\theta} \\cos \\theta-m_p l \\dot{\\theta}^2 \\sin \\theta=f, \\tag{5.4} \\end{equation}\\] \\[\\begin{equation} m_p l \\ddot{x} \\cos \\theta+m_p l^2 \\ddot{\\theta}+m_p g l \\sin \\theta=0. \\tag{5.5} \\end{equation}\\] (Hints: compute the Lagrangian of the system and the corresponding Lagrangian equations. Analyzing the two objects separately also works.) Translate the equations in (a) into the basic state-space dynamics form \\[\\begin{equation} \\dot{\\mathbf{x}}=F(\\mathbf{x}, \\mathbf{u}). \\tag{5.6} \\end{equation}\\] What are \\(\\mathbf{x},\\mathbf{u},F\\) here? (Hint: try \\(\\mathbf{x}=[x,\\theta,\\dot{x},\\dot{\\theta}]^\\top\\).) Linearize the dynamics in (b) around the unstable equilibrium where \\(\\theta^*=\\pi\\) and \\(x^*=\\dot{x}^*=\\dot{\\theta}^*=0.\\) (i.e., the pole is in the upright position and the cart stay at zero.) The result should be in the form of \\[\\begin{equation} \\dot{\\Delta\\mathbf{x}}=A\\Delta\\mathbf{x}+B\\Delta\\mathbf{u}, \\tag{5.7} \\end{equation}\\] where \\(\\Delta\\mathbf{x} = \\mathbf{x}-\\mathbf{x}^*\\) and \\(\\Delta\\mathbf{u}=\\mathbf{u}-\\mathbf{u}^*\\). Define the linearization error \\(e(\\mathbf{x}, \\mathbf{u}):=\\|F(\\mathbf{x}, \\mathbf{u}) - (A\\Delta\\mathbf{x}+B\\Delta\\mathbf{u})\\|^2\\). Simulate the original system (5.6) and the linearized system (5.7) with the same initial condition. How does the linearization error change over time? Provide at least three different initialization results. (Hints: (i) Sanity check: intuitively the error should not depend on the initial location \\(x\\), and it should have symmetry. Is that true in your simulation? (ii) In the same unstable position, how does push/pull (positive/negative) force change the results?) Convert the continuous-time dynamics in (5.7) to discrete-time with a fixed time-discretization. Then design an LQR controller to stabilize the cart-pole at the unstable equilibrium. Does the LQR controller succeed for all initial conditions you tested? (Hint: try several initial conditions where the end point of the pole is above or below the horizontal line.) You may want to take a look at the LQR example for the simple pendulum in Example 2.1.   Exercise 5.4 (Trajectory Optimization) Let us use this exercise to practice your skills in implementing trajectory optimization. Consider a dynamical system \\[ x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\quad \\dot{x} = f(x,u) = \\begin{bmatrix} (1-x_2^2)x_1 - x_2 + u \\\\ x_1 \\end{bmatrix}, \\quad x(0) = x_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}. \\] With \\(T=10\\), consider the following optimal control problem \\[\\begin{equation} \\begin{split} \\min_{u(t),t \\in [0,T]} &amp; \\quad \\int_{t=0}^T \\Vert x(t) \\Vert^2 + u(t)^2 dt \\\\ \\text{subject to} &amp; \\quad \\dot{x} = f(x,u), \\quad x(0) = x_0 \\\\ &amp; \\quad u(t) \\in [-1,1],\\forall t \\in [0,T]. \\end{split} \\end{equation}\\] Solve the problem using direct multiple shooting with \\(N=50\\) time intervals. Solve the problem using direct collocation with \\(N=50\\). Plot the optimized control signal and resulting state trajectory for both a and b. You probably want to refer to the source codes of Example 3.6 and 3.7.   Exercise 5.5 (Policy Iteration of Shortest Path) In Example 2.2, we used value iteration to solve the shortest path problem with obstacles. In this exercise, we will implement policy iteration. Instead of value iteration that updates the Q-function by \\(Q^{(k+1)} = \\mathcal{T}Q^{(k)}\\) where \\(\\mathcal{T}\\) is the Bellman optimality operator defined in (2.27), we use a different update rule: Initialize the policy \\(\\pi_0\\). For \\(k=0,1,2,\\ldots\\), compute \\(Q_{\\pi_k}\\) via (2.24). Then update policy by greedy method \\(\\pi_{k+1}=\\pi_{Q^{\\pi_k}}\\) (see (2.26) for the definition of \\(\\pi_Q\\)). Now try to solve the following problems. With the same obstacles in Fig. 2.3, implement the above policy iteration algorithm. Can you recover the cost-to-go and shortest path found in the example? We designed the cost function \\(g\\) to be \\(20\\) when there is an obstacle. Is there a lower bound on this value, such that the shortest path with any starting point will still avoid obstacles? If yes, how will the lower bound change with different obstacle structures? (Bonus) With the assumption that the grids are connected, give a conjecture on the maximum of the lower bound. If it is allowed to go diagonally (for example, from (4,4) to (3,5)), how will the results change? (Bonus) Beyond iterative algorithms, there is another formulation of the problem in linear programming. Try to implement the following problem: \\[\\max_{J} \\sum_{x}J(x),\\quad s.t. J(x)\\le g(x,u) + \\gamma\\sum_{x&#39;}P(x&#39;|x,u)J(x&#39;),\\ \\forall x,u.\\] What can you find? (Hint: formulate the problem as a linear programming problem.)   Exercise 5.6 (Verifying Control Lyapunov Function for the Double Integrator) Consider the following discrete-time double integrator dynamics \\[ x_{t+1} = \\underbrace{\\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}}_{A} x_t + \\underbrace{\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}}_{B} u_t, \\] with control constraints \\[ u \\in \\mathcal{U} = [-1, 1]. \\] We do not enforce any state constraint on \\(x\\), that is \\(\\mathcal{X} = \\mathbb{R}^2\\). Receding horizon controller. We aim to regulate the system at the origin, and design the following receding horizon controller (RHC) with horizon \\(N=3\\) \\[\\begin{equation} \\begin{split} \\min_{u(0),\\dots,u(N-1)} &amp; \\quad x(N)^T P x(N) + \\sum_{k=0}^{N-1} x(k)^T Q x(k) + u(k)^T R u(k) \\\\ \\text{subject to} &amp; \\quad u(k) \\in \\mathcal{U}, \\forall k=0,\\dots,N-1 \\\\ &amp; \\quad x(k+1) = A x(k) + B u(k), \\forall k = 0,\\dots,N-1 \\\\ &amp; \\quad x(0) = x_t, \\end{split} \\tag{5.8} \\end{equation}\\] where \\(x_t\\) is the system state at time \\(t\\), and \\(P,Q,R\\) matrices are designed as follows \\[ Q = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}, \\quad R = 1, \\quad P = \\begin{bmatrix} 0.1 &amp; 0 \\\\ 0 &amp; 0.1 \\end{bmatrix}. \\] Implement the RHC in (5.8), and run the RHC with initial system state \\(x_0 = [2;1]\\). Plot the system trajectory on a 2D plane like Fig. 3.24. From Theorem 3.3, we know that a sufficient condition for the stability of RHC is that the terminal cost, in our example is \\(p(x) = x^T P x\\), needs to satisfy (3.40). When \\(p(x)\\) satisfies (3.40), we say \\(p(x)\\) is a control Lyapunov function (CLF). Instantiating (3.40) for this exercise, it becomes (recall that we do not have terminal constraint, i.e., \\(\\mathcal{X}_f = \\mathbb{R}^2\\)): \\[\\begin{equation} \\rho = \\min_{u \\in \\mathcal{U}} \\ \\ (Ax + Bu)^T P (Ax + Bu) - x^T P x + x^T Q x + u^T R u \\leq 0, \\quad \\forall x \\in \\mathbb{R}^2. \\tag{5.9} \\end{equation}\\] Computing \\(\\rho\\) for all \\(x\\) in \\(\\mathbb{R}^2\\) is difficult, so I will ask you to compute \\(\\rho\\) along the state trajectory generated by RHC in (a). Compute and plot the trajectory of \\(\\rho\\). From Proposition 2.2, we know the optimal cost-to-go of the unconstrained infinite-horizon LQR problem with \\(x(0) = x\\) \\[ J_{\\infty}(x) = \\min_{u(0),\\dots} \\ \\ \\sum_{k=0}^{\\infty} u(k)^T R u(k) + x(k)^T Q x(k), \\quad \\text{subject to} \\quad x(k+1) = A x(k) + B u(k), \\] is a quadratic function \\[ J_{\\infty}(x) = x^T S x, \\] where \\(S\\) can be computed by solving the algebraic Riccati equation. Now use Matlab or Python to compute \\(S\\) (e.g., using dlqr in Matlab) for the double integrator. Redo (a) and (b) by using \\(S\\) in (c) as the terminal cost, i.e., setting \\(P=S\\). Plot the state trajectory, as well as the trajectory of \\(\\rho\\). Redo (a) and (b) by using \\(0.1S\\) and \\(10S\\) as the terminal cost, i.e., setting \\(P=0.1S\\) and \\(P=10S\\). Plot the state trajectory, as well as the trajectory of \\(\\rho\\).   Exercise 5.7 (Polyhedral Controllable and Reachable Sets) In this exercise, we aim to compute controllable and reachable sets explicitly, rather than implementing MPT. A bounded polyhedron in \\(\\mathbb{R}^n\\) can be expressed as the convex hull of a finite set of points (vertices) \\(V = \\{V_1, \\ldots, V_k\\}\\subset \\mathbb{R}^n\\). Show that a linear transform of such a polyhedron is simply tranforming its vertices, namely, \\[A\\text{conv}(V)+b = \\text{conv}(AV)+b,\\] where \\(A\\in\\mathbb{R}^{m\\times n},\\ b\\in\\mathbb{R}^m\\). The exact definition of a (closed) polyhedron \\(\\mathcal{P}\\) is a set defined by finite number of linear inequalities \\(\\mathcal{P} = \\{x\\in\\mathbb{R}^n\\mid Hx\\le h \\}\\) where \\(H\\in\\mathbb{R}^{m\\times n},\\ h\\in\\mathbb{R}^m\\). Let \\(A\\in\\mathbb{R}^{n\\times n}\\) be an invertible matrix, \\(b\\in\\mathbb{R}^n\\). Suppose the linear transformed polyhedron \\(A\\mathcal{P}+b\\) has representation \\[\\{y\\in\\mathbb{R}^n\\mid H^\\prime y\\le h^\\prime \\}.\\] What are \\(H^\\prime\\) and \\(h^\\prime\\)? With the same setting as Example 3.10, consider linear system \\[ x_{t+1} = \\begin{bmatrix} 1.5 &amp; 0 \\\\ 1 &amp; -1.5 \\end{bmatrix} x_t + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} u_t \\] with state and control constraints \\[ \\mathcal{X} = [-10,10]^2, \\quad \\mathcal{U} = [-5,5]. \\] Compute \\(\\text{Pre}(\\mathcal{X})\\) and \\(\\text{Suc}(\\mathcal{X})\\) in the form of \\(\\{x\\in\\mathbb{R}^n\\mid Hx\\le h \\}\\). What are the vertices of two sets respectively? (Hints: Denote the dynamics as \\(x_{t+1} = Ax_t+Bu_t\\). For the successor set, \\[\\text{Suc}(\\mathcal{X}) = \\cup_{u\\in\\mathcal{U}}\\{x:\\exists x&#39;\\in\\mathcal{X},\\text{ s.t., }Ax&#39;+Bu = x\\} = \\cup_{u\\in\\mathcal{U}}(A\\mathcal{X} + Bu).\\] What is \\(A\\mathcal{X} + Bu\\) for each \\(u\\)? Will the shape of \\(A\\mathcal{X} + Bu\\) change when changing \\(u\\)? The conclusion in (b) might help. Similarly, for the precursor set, \\[\\text{Pre}(\\mathcal{X}) = \\cup_{u\\in\\mathcal{U}}\\{x:Ax+Bu \\in \\mathcal{X}\\} = \\cup_{u\\in\\mathcal{U}}A^{-1}(\\mathcal{X} - Bu).\\] Mimic the computation of the successor set.) Continued with the results in (c), compute one-step controllable set \\(\\mathcal{K}_1(\\mathcal{X}) = \\text{Pre}(\\mathcal{X}) \\cap \\mathcal{X}\\). (Hints: There are 4 inequalities for each of the two sets. Any redundancy in those 8 inequalities when combining them together?) From the above questions, we get the intuition of how the MPT compute controllable set: Obtain the representation of precursor set; Intersect the precursor set with the feasible domain \\(\\mathcal{X}\\); Remove redundant (useless) inequalities.        Provided the following algorithm that removes redundancy of a polyhedral representation, write the codes to find \\(\\mathcal{K}_1(\\mathcal{X})\\) without using MPT. Does the outcome match your results in (d)? Algorithm: Find redundacy-free representation of a polyhedron Input: \\(H\\in\\mathbb{R}^{m\\times n}\\) and \\(h\\in\\mathbb{R}^m\\) that represents \\(\\mathcal{P} = \\{x\\in\\mathbb{R}^n\\mid Hx\\le h \\}\\) Output: \\(H_0\\in\\mathbb{R}^{m_0\\times n}\\) and \\(h_0\\in\\mathbb{R}^{m_0}\\) that represents \\(\\mathcal{P}\\) without redundancy    \\(\\mathcal{I} \\leftarrow \\{1,\\ldots, m\\}\\)    For \\(i=1\\) to \\(m\\)      \\(\\mathcal{I}\\leftarrow \\mathcal{I}\\setminus\\{i\\}\\)      \\(f^*\\leftarrow\\max_x H_i x,\\ s.t. H_{\\mathcal{I}}x\\le h_{\\mathcal{I}}, H_i x\\le h_i + 1\\)      %% \\(H_i\\) is the \\(i\\)-th row of \\(H\\), \\(H_{\\mathcal{I}}\\) concatenates rows of \\(H\\) with index in \\(\\mathcal{I}\\)      If \\(f^* &gt; h_i\\) Then \\(\\mathcal{I}\\leftarrow \\mathcal{I}\\cup \\{i\\}\\)    \\(H_0\\leftarrow H_{\\mathcal{I}}, h_0\\rightarrow h_{\\mathcal{I}}\\)   Exercise 5.8 (Certified Region of Attraction of Clipped LQR Controller) In this exercise, let us use the simple pendulum as an example to (i) recognize the difficulty of nonlinear control in the presence of control limits, and (ii) appreciate the power of Lyapunov analysis and Sums-of-Squares (SOS) programming. A starting script for this exercise can be found here, where you need to fill out some specific details to finish this exercise. Let us consider the continuous-time pendulum dynamics \\[\\begin{equation} \\dot{x} = \\begin{bmatrix} x_2 \\\\ \\frac{1}{ml^2} (u - b x_2 + mgl \\sin x_1) \\end{bmatrix} \\tag{5.10} \\end{equation}\\] where \\(x_1\\) is the angular position of the pendulum, and \\(x_2\\) is the angular velocity. Note that the dynamics above is written such that \\(x=0\\) represents the upright position. In the case of no control saturation, i.e., \\(u \\in \\mathbb{R}\\), stabilizing the pendulum at \\(x=0\\) is easy, we can linearize the dynamics at \\(x=0\\) and obtain \\[ \\dot{x} \\approx \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ \\frac{g}{l} &amp; - \\frac{b}{ml^2} \\end{bmatrix}}_{A} x + \\underbrace{\\begin{bmatrix} 0 \\\\ \\frac{1}{ml^2} \\end{bmatrix}}_{B} u. \\] We then solve an infinite-horizon LQR problem \\[ \\min \\int_{t=0}^{\\infty} x(t)^T Q x(t) + u(t)^T R u(t) dt, \\quad \\text{subject to} \\quad x(0) = x_0. \\] The solution to this LQR problem can be written in closed form, see Section ??. Calling the Matlab lqr function (which is for continuous-time LQR), we can get a feedback controller \\[\\begin{equation} u(x) = - Kx, \\tag{5.11} \\end{equation}\\] with the optimal cost-to-go \\[\\begin{equation} J(x_0) = x_0^T S x_0, \\tag{5.12} \\end{equation}\\] with \\(K\\) and \\(S\\) constant matrices. Control saturation. Assume now our control has hard bounds, i.e., \\[ u \\in [-u_{\\max}, u_{\\max}]. \\] We wish to keep using the LQR controller (5.11). Therefore, our saturated controller will be \\[\\begin{equation} \\bar{u}(x) = \\text{clip}(u(x),-u_{\\max},u_{\\max}) = \\text{clip}(-Kx,-u_{\\max},u_{\\max}), \\tag{5.13} \\end{equation}\\] where the \\(\\text{clip}\\) function is defined as \\[ \\text{clip}(u,-u_{\\max},u_{\\max}) = \\begin{cases} u_{\\max} &amp; \\text{if} \\quad u \\geq u_{\\max} \\\\ u &amp; \\text{if} \\quad -u_{\\max} \\leq u \\leq u_{\\max} \\\\ -u_{\\max} &amp; \\text{if} \\quad u \\leq - u_{\\max} \\end{cases}. \\] A natural question is: can the saturated controller \\(\\bar{u}(t)\\) in (5.13) still stabilize the pendulum? Let’s simulate the pendulum and the controller to investigate this. We will use the parameter \\(m=1,g=9.8,l=1,b=0.1\\), and \\(u_{\\max} = 2\\). For the LQR cost matrix, let us use \\(Q = I\\) and \\(R = 1\\). Compute the LQR gain \\(K\\) and implement the saturated controller (5.13). In a region \\(\\mathcal{X} = [-0.2\\pi, 0.2\\pi] \\times [-0.2\\pi, 0.2\\pi]\\), sample \\(N=1000\\) initial states, and for each initial state, simulate the system under the saturated controller using ode45 or ode89. There will be some initial states from which the pendulum gets stabilized and others from which the pendulum fails to be stabilized. Plot the stabilized initial states as “circles”, and the non-stabilized initial states as “squares” on a 2D plot. (Hint: you should get something like Fig. 5.4.) Figure 5.4: Example of stabilized and non-stabilized initial states. The plot in Fig. 5.4 intuitively makes sense: when the initial state is very close to \\(0\\), the saturated controller can still stabilize the pendulum. Now our goal is to use Sums-of-Squares and Lyapunov analysis to get a certified region of stabilization, also known as the region of attraction (ROA), under the saturated control. Approximate polynomial dynamics. The SOS tool requires the system dynamics to be polynomial. The dynamics in (5.10) is polynomial except the term \\(\\sin x_1\\). Therefore, we perform a Taylor expansion of \\(\\sin x_1\\) \\[ \\sin x_1 = x_1 - \\frac{x_1^3}{3!} + \\frac{x_1^5}{5!} + \\dots, \\] leading to an approximate polynomial dynamics \\[\\begin{equation} \\dot{x} = \\bar{f}(x,u) = \\begin{bmatrix} x_2 \\\\ \\frac{1}{ml^2} \\left(u - b x_2 + mgl \\left(x_1 - \\frac{x_1^3}{3!} + \\frac{x_1^5}{5!} \\right) \\right) \\end{bmatrix}. \\tag{5.14} \\end{equation}\\] Candidate ROA. We want to certify the following candidate ROA \\[ \\Omega_\\rho = \\{x \\in \\mathbb{R}^2 \\mid J(x) :=x^T S x \\leq \\rho \\} \\] for some \\(\\rho &gt; 0\\), and \\(S\\) is exactly the LQR cost-to-go in (5.12). Since \\(J(x)\\) is already positive definite (because \\(S \\succ 0\\)), \\(\\Omega_\\rho\\) will look like an elliptical region around the origin. To certify all initial states inside \\(\\Omega_\\rho\\) can be stabilized, we need \\(\\dot{J}(x)\\) to be negative definite on \\(\\Omega\\), under the saturated LQR controller (5.13) (according to Theorem 4.2), that is to say \\[ \\dot{J}(x) = \\frac{\\partial J}{\\partial x} \\bar{f}(x, \\bar{u}(x)) &lt; 0, \\quad \\forall x \\in \\Omega_\\rho \\backslash \\{ 0 \\}. \\] A sufficient condition for the above equation to hold is \\[\\begin{equation} - \\frac{\\partial J}{\\partial x} \\bar{f}(x, \\bar{u}(x)) - \\epsilon \\Vert x \\Vert^2 \\quad \\text{ is SOS on } \\Omega_\\rho. \\tag{5.15} \\end{equation}\\] for some \\(\\epsilon &gt; 0\\) (do you see this?). The condition (5.15) is almost ready for us to implement in SOSTOOLS. However, there is one last issue. The saturated controller \\(\\bar{u}(x)\\) is not a polynomial! The last trick. Fortunately, we can use a trick here to save us. A closer look at (5.15) and the saturated controller \\(\\bar{u}(x)\\) shows that it is equivalent to asking \\[\\begin{equation} \\begin{split} - \\frac{\\partial J}{\\partial x} \\bar{f}(x, u_{\\max}) - \\epsilon \\Vert x \\Vert^2 \\quad \\text{ is SOS on } \\quad \\Omega_\\rho \\cap \\{ x \\mid u(x) \\geq u_{\\max} \\} \\\\ - \\frac{\\partial J}{\\partial x} \\bar{f}(x, u(x)) - \\epsilon \\Vert x \\Vert^2 \\quad \\text{ is SOS on } \\quad \\Omega_\\rho \\cap \\{ x \\mid -u_{\\max} \\leq u(x) \\leq u_{\\max} \\} \\\\ - \\frac{\\partial J}{\\partial x} \\bar{f}(x, - u_{\\max}) - \\epsilon \\Vert x \\Vert^2 \\quad \\text{ is SOS on } \\quad \\Omega_\\rho \\cap \\{ x \\mid u(x) \\leq -u_{\\max} \\}. \\end{split} \\tag{5.16} \\end{equation}\\] Essentially, (5.16) breaks the SOS condition into three cases, each corresponding to one case in the \\(\\text{clip}\\) function. (1) If \\(u(x) \\geq u_{\\max}\\), then the first equation takes \\(u_{\\max}\\) to be the controller, (2) If \\(u(x) \\leq -u_{\\max}\\), then the third equation takes \\(-u_{\\max}\\) to be the controller, (3) otherwise, the second equation takes \\(u(x)=-Kx\\) to be the controller. Condition (5.16) has three SOS constraints and can be readily implemented using SOSTOOLS. We will use \\(\\epsilon = 0.01\\). Choose \\(\\rho = 1\\), and implement the SOS conditions in (5.16) with a chosen relaxation order \\(\\kappa\\) (in the code I choose \\(\\kappa=4\\)). Is the SOS program feasible (i.e., does the SOS program produce a certificate)? If so, plot the boundary of \\(\\Omega_\\rho\\) on top of the samples plot in Fig. 5.4. Does \\(\\Omega_\\rho\\) agree with the samples? (Hint: you should see a plot similar to Fig. 5.5.) Figure 5.5: Certified Region of Attraction. Now try \\(\\rho = 2,3,4,5\\), for which values of \\(\\rho\\) the SOS program is feasible, and for which values of \\(\\rho\\) the SOS program becomes infeasible?   Exercise 5.9 (Energy pumping of Simple Pendulum) In Exercise 5.8, we have studied the region of attraction of the Clipped LQR Controller. The conclusion we got there is, once we enter the region of attraction \\(\\Omega_\\rho\\) (the green elliptical region in Fig. 5.5), we can switch to the clipped LQR controller and it guarantees stabilization towards the upright position. In this exercise, we are going to design a controller that can swing up the pendulum to enter the region of attraction. The total energy of the pendulum is given by \\[ E = \\frac{1}{2} m l^2 x_2^2 + mgl\\cos x_1,\\] where \\(x_1\\) and \\(x_2\\) are notations in (5.10). With the dynamics in (5.10), one can obtain the change of energy over time: \\[ \\dot E = \\color{red}{h(x,u)}\\cdot x_2.\\] Find the expression of \\(h(x,u)\\). Our desired energy is the one with the upright equilibruim: \\(E^d = mgl.\\) Define the energy of difference \\[\\tilde{E} = E - E^d.\\] Find the expression of \\(\\dot{\\tilde{E}}\\). Now consider the feedback controller of the form \\[ u = -k x_2 \\tilde{E} + b x_2.\\] Find the expression of \\(\\dot{\\tilde{E}}\\). (Bonus) Explain heuristically why the controller works. However, our controller has saturation as before: \\[\\bar{u}(x) = \\text{clip}(u(x),-u_{\\max},u_{\\max}).\\] Set the parameters the same as the previous exercise and \\(k=1\\). Simulate the system under the saturated controller with initial state \\((x_1,x_2)=(\\pi,1).\\) Will the state hit the candidate ROA \\(\\Omega_\\rho\\) with \\(\\rho=2\\)? If yes, stop the system once it hits the region, and draw the trajectory on the Figure similar to Fig. 5.5. Try \\(k=0.01, 0.1,1,10\\), which one hits the region the fastest? Congrats! We have designed a full nonlinear controller (energy pumping + clipped LQR) that guarantees swing-up of the pendulum from any initial state to the upright position. Try the controller for yourself. References Bemporad, Alberto, Manfred Morari, Vivek Dua, and Efstratios N Pistikopoulos. 2002. “The Explicit Linear Quadratic Regulator for Constrained Systems.” Automatica 38 (1): 3–20. Nocedal, Jorge, and Stephen J Wright. 1999. Numerical Optimization. Springer. "],["acknowledgement.html", "Acknowledgement", " Acknowledgement "],["linear-algebra-and-differential-equations.html", "A Linear Algebra and Differential Equations A.1 Linear Algebra A.2 Solving an Ordinary Differential Equation", " A Linear Algebra and Differential Equations In this Chapter, we provide basic concepts in linear algebra and ordinary differential equations (ODE), which can be a cheatsheet for readers. A.1 Linear Algebra Most of the linear algebraic concepts used in this textbook are provided in this section. A.1.1 Matrix Exponential Definition A.1 (Matrix exponential) Given a \\(n\\times n\\) matrix \\(A\\), the matrix exponential of \\(A\\), denoted as \\(e^A\\), is defined as: \\[e^A = \\sum_{p=0}^\\infty \\frac{A^p}{p!}.\\] Note that the matrix exponential is well defined, and every entry converges absolutely. We show some special cases of matrix exponential. Diagonal matrix. Note that if \\(A=\\text{diag}(a_1,a_2,\\ldots,a_n)\\), then \\(A^p=\\text{diag}(a_1^p,a_2^p,\\ldots,a_n^p)\\), and \\(e^A=\\text{diag}(e^{a_1},e^{a_2},\\ldots,e^{a_n})\\). Diagonalizable matrix: Definition A.2 (Diagonablizable matrix) A square matrix \\(A\\) is said to be diagonalizable or non-defective, if there exists an invertible matrix \\(P\\), such that \\(P^{-1}AP\\) is a diagonal matrix. In words, after change of coordination, the matrix becomes diagonal. If a matrix is not diagonalizable, it is defective. With diagonalization \\(A=PDP^{-1}\\) where \\(D\\) is a diagonal matrix (e.g. symmetric matrix is diagonalizable), we have \\(e^A=Pe^DP^{-1}.\\) Specifically if \\(U\\) consists of eigenvectors of \\(A\\) and \\(D\\) is the spectrum, then matrix exponential is the same as taking exponents of eigenvalues of \\(A\\) while keeping the eigenbasis invariant. Exchangable matrices. If \\(A_1\\) and \\(A_2\\) are exchangeable, that is, \\(A_1A_2=A_2A_1\\), then \\(e^{A_1+A_2}=e^{A_1}e^{A_2}\\) (exercise!). Nilpotent matrix. If \\(N\\) is a nilpotent matrix, which means that \\(N^K=0\\) for some integer \\(K\\), then \\(e^N=I+N+\\frac12N^2+\\ldots+\\frac1{(K-1)!}N^{K-1}.\\) Any matrix with Jordan canonical form (*This is out of scope of this textbook. We leave it for readers with interest). Any matrix \\(A\\) can be decomposed as \\(P(D+N)P^{-1}\\), where \\(D\\) is diagonal and exchangeable with the nilpotent matrix \\(N\\). Then based on the above discussions, \\(e^A=P(e^De^N)P^{-1}\\). Projection matrix \\(A^2=A\\). Then \\(e^A=I+(e-1)A\\) (exercise!). A.1.2 Gradients In matrix calculus, index may not be consistent in different references. It should be noted that in neural network literature, gradients may have a transpose on our results here. For any function \\(f(X):\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}\\) of an \\(m\\)-by-\\(n\\) matrix \\(X\\) (if \\(n=1\\) then it is a vector), the gradient \\(\\nabla f(X)\\) is another \\(m\\)-by-\\(n\\) matrix with the \\((i,j)\\)-th entry \\(\\frac{\\partial f(X)}{\\partial X_{ij}}\\). For any function \\(f(x):\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^n\\) that maps an \\(m\\)-dimensional vector \\(x\\) to \\(n\\)-dimensional space, \\(\\nabla f(x)\\) is an \\(m\\)-by-\\(n\\) matrix with the \\((i,j)\\)-th entry \\(\\frac{\\partial (f(x))_j}{\\partial x_i}\\). Some important examples include: Vector inner-product. \\(\\nabla(a^\\top x)=a\\). Quadratic form \\(\\nabla(x^\\top Ax) = (A+A^\\top)x\\). Composition. Suppose \\(f(y):\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^k\\) and \\(g(x):\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^n\\), then \\(\\nabla f(g(x))\\) is an \\(m\\)-by-\\(k\\) matrix \\(\\nabla g(x) \\nabla f(y=g(x))\\). A.2 Solving an Ordinary Differential Equation A differential equation is an equation with a function and its derivatives. Compared to partial differential equation (PDE) that consists of partial derivatives, throughout this textbook we will mainly focus on ordinary differential equations (ODEs), including but not limited to, the solutions, convergence and stability analysis. An example of ODE looks like this: \\[\\begin{equation} x+\\frac{dx}{dt}=5t,\\quad t\\in[0,T]. \\tag{A.1} \\end{equation}\\] Definition A.3 (Ordinary Differential Equation) In general, an ODE of order \\(k\\), or, a \\(k\\)-th order ODE, is in the form of \\[ F(t, x, x^\\prime, \\ldots, x^{(k)})=0.\\] Further, if \\(F\\) is linear in \\(x,x&#39;,\\ldots,x^{(k)}\\), we call it a linear ODE. If a linear ODE with \\(F=x^{(k)} + \\sum_{i=0}^{k-1}a_i(t)x^{(i)}\\) that does not independently relate to \\(t\\), we call it homogeneous. Note that \\(x(t)\\equiv 0\\) will always be a trivial solution for homogeneous ODE. For example, the equation (A.1) is a linear ODE but not homogeneous. The solution, a function \\(x=x(t)\\) is not unique in general, and additional conditions are required. For example, we can have one of the following conditions for the above ODE: Initial condition, e.g., \\(x(0)=0\\) gives the constraint that the function at initial time \\(t=0\\) starts at zero point. End-point condition, e.g., \\(x(T)=0\\) implies that the dynamics should end at zero. (Initial) velocity condition, e.g., \\(\\frac{dx}{dt}|_{t=0}=0\\) suggests that at the start time the “slope”/“velocity” of \\(x\\) is zero. For higher-order ODEs, the conditions may be much more complicated to guarantee uniqueness. A.2.1 Separation of Variables A.2.2 First-order Linear ODE A.2.3 Gronwall Inequality A.2.4 Matlab "],["appconvex.html", "B Convex Analysis and Optimization B.1 Theory B.2 Practice", " B Convex Analysis and Optimization B.1 Theory B.1.1 Sets Convex set is one of the most important concepts in convex optimization. Checking convexity of sets is crucial to determining whether a problem is a convex problem. Here we will present some definitions of some set notations in convex optimization. Definition B.1 (Affine set) A set \\(C\\subset \\mathbb{R}^n\\) is affine if the line through any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in \\mathbb{R}\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition B.2 (Convex set) A set \\(C\\subset \\mathbb{R}^n\\) is convex if the line segment between any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition B.3 (Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a cone if for any \\(x\\in C\\) and any \\(\\theta\\geq 0\\), we have \\(\\theta x \\in C\\). Definition B.4 (Convex Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a convex cone if \\(C\\) is convex and a cone. Below are some important examples of convex sets: Definition B.5 (Hyperplane) A hyperplane is a set of the form \\[\\{x|a^Tx = b\\}\\] Definition B.6 (Halfspaces) A (closed) halfspace is a set of the form \\[\\{x|a^Tx \\leq b\\}\\] Definition B.7 (Balls) A ball is a set of the form \\[B(x,r) = \\{y|\\|y-x\\|_2 \\leq r\\} = \\{x+ru|\\|u\\|_2\\leq 1\\}\\] where \\(r &gt;0\\). Definition B.8 (Ellipsoids) A ellipsoid is a set of the form \\[\\mathcal{E} = \\{y|(y-x)^TP^{-1}(y-x)\\leq 1\\}\\] where \\(P\\) is symmetric and positive definite. Definition B.9 (Polyhedra) A polyhedra is defined as the solution set of a finite number of linear equalities and inequalities: \\[\\mathcal{P} = \\{x|a_j^Tx\\leq b_j, j=1,...,m, c_k^Tx=d_k,k=1,...,p\\}\\] Definition B.10 (Norm ball) A norm ball \\(B\\) of radius \\(r\\) and a center \\(x_c\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[B = \\{x|\\|x-x_c\\|\\leq r\\}\\] Definition B.11 (Norm cone) A norm cone \\(C\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[C = \\{(x,t)|\\|x\\|\\leq t\\}\\subset \\mathbb{R}^{n+1}\\] Simplexes are important family of polyhedra. Suppose the \\(k+1\\) points \\(v_0,...,v_k\\in \\mathbb{R}^n\\) are affinely independent, which means \\(v_1-v_0,...,v_k-v_0\\) are linearly independent. Definition B.12 (Simplex) A simplex \\(C\\) defined by points \\(v_0,...,v_k\\) is: \\[C = \\textbf{conv}\\{v_0,...,v_k\\} = \\{\\theta_0v_0 + ... \\theta_kv_k|\\theta \\succeq 0, \\textbf{1}^T\\theta = 1\\}\\] Extremely important examples of convex sets are positive semidefinite cones: Definition B.13 (Symmetric,positive semidefinite,positive definite matrices) Symmetric matrices: \\(\\textbf{S}^n = \\{X\\in\\mathbb{R}^{n\\times n}| X=X^T\\}\\) Symmetric Positive Semidefinite matrices: \\(\\textbf{S}_+^n = \\{X\\in\\textbf{S}^n| X\\succeq0\\}\\) Symmetric Positive definite matrices: \\(\\textbf{S}_{++}^n = \\{X\\in\\textbf{S}^n| X\\succ0\\}\\) In most scenarios, the set we encounter is more complicated. In general it is extermely hard to determine whether a set in convex or not. But if the set is ‘generated’ by some convex sets, we can easily determine its convexity. So let’s focus on operations that preserve convexity: Proposition B.1 Assume \\(S\\) is convex, \\(S_\\alpha,\\alpha\\in\\mathcal{A}\\) is a family of convex sets. Following operations on convex sets will preserve convexity: Intersection: \\(\\bigcap_{\\alpha\\in\\mathcal{A}}S_\\alpha\\) is convex. Image under affine function: A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is affine if it has the form \\(f(x) = Ax+b\\). The image of \\(S\\) under affine function \\(f\\) is convex. I.e. \\(f(S) = \\{f(x)|x\\in S\\}\\) is convex Image under perspective function: We define the perspective function \\(P:\\mathbb{R}^{n+1}\\), with domain \\(\\textbf{dom}P = \\mathbb{R}^n\\times \\mathbb{R}_{++}\\)(where \\(\\mathbb{R}_{++}=\\{x\\in \\mathbb{R}|x&gt;0\\}\\)) as \\(P(z,t) = z/t\\). The image of \\(S\\) under perspective function is convex. Image under linear-fractional function: We define linear fractional function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) as:\\(f(x) = (Ax+b)/(c^Tx+d)\\) with \\(\\textbf{dom}f = \\{x|c^Tx+d&gt;0\\|\\). The image of \\(S\\) under linear fractional functions is convex. In some cases, the restrictions of interior is too strict. For example, imagine a plane in \\(\\mathbb{R}^3\\). The interior of the plane is \\(\\emptyset\\). But intuitively many property should be extended to this kind of situation. Because the points in the plane also lies ‘inside’ the convex set. Thus, we will define relative interior. First we will define affine hull. Definition B.14 (Affine hull) The affine hull of a set \\(S\\) is the smallest affine set that contains \\(S\\), which can be written as: \\[\\text{aff}(S) = \\{\\sum_{i=1}^k\\alpha_ix_i|k&gt;0,x_i\\in S,\\alpha_i\\in\\mathbb{R},\\sum_{i=1}^k\\alpha_i=1\\}\\] Definition B.15 (Relative Interior) The relative interior of a set \\(S\\) (denoted \\(\\text{relint}(S)\\)) is defined as its interior within the affine hull of \\(S\\). I.e. \\[\\text{relint}(S):=\\{x\\in S: \\text{there exists } \\epsilon&gt;0 \\text{ such that }N_\\epsilon \\cap \\text{aff}(S)\\subset S\\}\\] where \\(N_\\epsilon(x)\\) is a ball of radius \\(\\epsilon\\) centered on \\(x\\). B.1.2 Convex function In this section, let’s define convex functions: Definition B.16 (Convex function) A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if \\(\\textbf{dom}\\ f\\) is convex and \\(\\forall x,y\\in \\textbf{dom}\\ f\\) and with \\(\\theta \\in [0,1]\\), we have:\\[f(\\theta x +(1-\\theta)y)\\leq \\theta f(x) + (1-\\theta)f(y)\\] The function is strictly convex if the inequality holds whenever \\(x\\neq y\\) and \\(\\theta\\in (0,1)\\). If a function is differentiable, it will be easier for us to check its convexity: Proposition B.2 (Conditions for Convex function) 1.(First order condition) Suppose \\(f\\) is differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x,y\\in \\textbf{dom} f\\), \\[f(y)\\geq f(x) +\\nabla f(x)^T(y-x)\\] 2.(Second order conditions) Suppose \\(f\\) is twice differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x\\in \\textbf{dom} f\\), \\[\\nabla^2 f(x) \\succeq \\textbf{0}\\] For the same purpose, some operations that preserve the convexity of the convex functions are presented here: Proposition B.3 (Operations that preserve convexity) Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be a convex function and \\(g_1,...,g_n\\) be convex functions. The following operations will preserve convexity of the function: 1.(Nonnegative weighted sum): A nonnegative weighted sum of convex functions: \\[f = \\omega_1f_1 + ... +\\omega_mf_m\\] 2.(Composition with an affine mapping) Suppose \\(A\\in \\mathbb{R}^{n\\times m}\\) and \\(b\\in \\mathbb{R}^n\\), then \\(g(x) = f(Ax+b)\\) is convex. 3.(Pointwise maximum and supremum) \\(g(x) = \\max\\{g_1(x),...,g_n(x)\\}\\) is convex. If \\(h(x,y)\\) is convex in \\(x\\) for each \\(y\\in\\mathcal{A}\\), then \\(\\sup_{y\\in\\mathcal{A}} h(x,y)\\) is also convex in \\(x\\). 4.(Minimization) If \\(h(x,y)\\) is convex in \\((x,y)\\), and \\(C\\) is a convex nonempty set, then \\(\\inf_{x\\in C} h(x,y)\\) is convex in \\(x\\). 5.(Perspective of a function) The perspective of \\(f\\) is the function \\(h:\\mathbb{R}^{n+1}\\to\\mathbb{R}\\) defined by: \\(h(x,t) = tf(x/t)\\) with domain \\(\\textbf{dom}\\ h=\\{(x,t)|x/t\\in\\textbf{dom} f,t&gt;0\\}\\). And \\(h\\) is convex. B.1.3 Lagrange dual We consider an optimization problem in the standard form (without assuming convexity of anything): \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; h_i(x) = 0\\quad i=1,...,p \\\\ \\end{aligned} \\end{equation}\\] Definition B.17 (Lagrange dual function) The Lagrangian related to the problem above is defined as: \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\] The Lagrange dual function is defined as: \\[g(\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}}L(x,\\lambda,\\nu)\\] When the Lagrangian is unbounded below in \\(x\\), the dual function takes on the value \\(-\\infty\\). Note that since the Lagrange dual function is a pointwise infimum of a family of affine functions of \\((\\lambda,\\nu)\\), so it’s concave. The Lagrange dual function will give us lower bounds of the optimal value of the original problem: \\[g(\\lambda,\\nu)\\leq p^*\\]. We can see that, the dual function can give a nontrivial lower bound only when \\(\\lambda\\succeq 0\\). Thus we can solve the following dual problem to get the best lower bound. Definition B.18 (Lagrange dual problem) The lagrangian dual problem is defined as follows: \\[\\begin{equation} \\begin{aligned} d^* = \\quad \\max_{\\lambda,\\nu} \\quad &amp; g(\\lambda,\\nu)\\\\ \\textrm{s.t.} \\quad &amp; \\lambda\\succeq 0 \\end{aligned} \\end{equation}\\] This is a convex optimization problem. We can easily see that \\[d^*\\leq p^*\\] always hold. This property is called weak duality. If \\[d^*=p^*\\], it’s called strong duality. Strong duality does not hold in general, but it usually holfs for convex problems. We can find conditions that guarantee strong duality in convex problems, which are called constrained qualifications. Slater’s constraint qualification is a useful one. Theorem B.1 (Slater's constraint qualification) Strong duality holds for a convex problem \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; Ax=b \\\\ \\end{aligned} \\end{equation}\\] if it is strictly feasible, i.e. \\[\\exists x\\in\\textbf{relint}\\mathcal{D}:\\quad f_i(x)&lt;0,\\quad i=1...m,\\quad Ax=b\\] And the linear inequalities do not need to hold with strict inequality. B.1.4 KKT condition Note that if strong duality holds, denote \\(x^*\\) to be primal optimal, and \\((\\lambda^*,\\nu^*)\\) to be dual optimal. Then: \\[\\begin{equation} \\begin{aligned} f_0(x^*) = g(\\lambda^*,\\nu^*) = &amp; \\inf_x(f_0(x)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x))\\\\ \\leq &amp; f_0(x^*)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x)\\\\ \\leq &amp; f_0(x^*)\\\\ \\end{aligned} \\end{equation}\\] from this, combining \\(\\lambda^*\\geq 0\\) and \\(f_i(x^*)\\leq 0\\), we can know that: \\(\\lambda_i^*f_i(x^*)=0\\quad i=1\\cdots m\\). This means for \\(\\lambda_i^*\\) and \\(f_i(x^*)\\), one of them must be zero, which is known as complementary slackness). Thus we arrived at the following four conditions, which are called KKT conditions. Theorem B.2 (Karush-Kuhn-Tucker(KKT) Conditions) The following four conditions are called KKT conditions (for a problem with differentiable \\(f_i,h_i\\)) Primal feasible: \\(f_i(x) \\leq 0,i,\\cdots ,m,\\ h_i(x) = 0,i=1,\\cdots ,p\\) Dual feasible: \\(\\lambda\\succeq0\\) Complementary slackness: \\(\\lambda_if_i(x)=0,i=1,\\cdots,m\\) Gradient of Lagrangian with respect to \\(x\\) vanishes:\\(\\nabla f_0(x)+\\sum_{i=1}^m\\lambda_i\\nabla f_i(x)+\\sum_{i=1}^p\\nu_i\\nabla h_i(x) = 0\\) From the discussion above, we know that if strong duality holds and \\(x,\\lambda,\\nu\\) are optimal, then they must satisfy the KKT conditions. Also if \\(x,\\lambda,\\nu\\) satisfy KKT for a convex problem, then they are optimal. However, the converse is not generally true, since KKT condition implies strong duality. If Slater’s condition is satisfied, then \\(x\\) is optimal if and only if there exist \\(\\lambda,\\nu\\) that satisfy KKT conditions. Sometimes, by solving the KKT system, we can derive the closed-form solution of a optimization directly. Also, sometimes we will use the residual of the KKT system as the termination condition. In general, \\(f_i,h_i\\) may not be differentiable. There are also KKT conditions for them, which will include knowledge of subdifferential and will not be included here. B.2 Practice B.2.1 CVX Introduction In the last section, we have learned basic concepts and theorems in convex optimization. In this section, on the other hand, we will introduce you how to model basic convex optimization problems with CVX, an easy-to-use MATLAB package. To install CVX, please refer to this page. Note that every time you what to use the CVX package, you should add it to your MATLAB path. For example, if I install CVX package in the parent directory of my current directory with default directory name cvx, the following line should be added before your CVX codes: addpath(genpath(&quot;../cvx/&quot;)); With CVX, it is incredibly easy for us to define and solve a convex optimization problem. You just need to: define the variables. define the objective function you want to minimize or maximize. define the constraints. After running your codes, the optimal objective value is stored in the variable cvx_optval, and the problem status is stored in the variable cvx_status (when your problem is well-defined, this variable’s value will be Solved). The optimal solutions will be stored in the variables you define. Throughout this section, we will study five types of convex optimization problems: linear programming (LP), quadratic programming (QP), (convex) quadratically constrained quadratic programming (QCQP), second-order cone programming (SOCP), and semidefinite programming (SDP). Given two types of optimization problems \\(A\\) and \\(B\\), we say \\(A &lt; B\\) if \\(A\\) can always be converted to \\(B\\) while the inverse is not true. Under this notation, we have \\[\\begin{equation*} \\text{LP} &lt; \\text{QP} &lt; \\text{QCQP} &lt; \\text{SOCP} &lt; \\text{SDP} \\end{equation*}\\] B.2.2 Linear Programming (LP) Definition. An LP has the following form: \\[\\begin{equation} \\tag{B.1} \\begin{aligned} \\min_{x \\in \\mathbb{R}^n} &amp; \\ c^T x \\\\ \\text{subject to } &amp; A x \\le b \\end{aligned} \\end{equation}\\] where \\(x\\) is the variable, \\(A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m\\), and \\(c \\in \\mathbb{R}^n\\) are the parameters. Note that the constraint \\(A x \\le b\\) already incorporates linear equality constraints. To see this, consider the constraint \\(A&#39; x = b&#39;\\), we can reformulate it as \\(A x \\le b\\) by \\[\\begin{equation*} \\begin{bmatrix} A&#39; \\\\ -A&#39; \\end{bmatrix} x \\le \\begin{bmatrix} b&#39; \\\\ -b&#39; \\end{bmatrix} \\end{equation*}\\] Example. Consider the problem of minimizing a linear function \\(c_1 x_1 + c_2 x_2\\) over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). We can convert it to the standard LP form in (B.1) by simply setting \\(c\\) as \\([c_1, \\ c_2]^T\\) and the linear inequality constraint as \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the LP example setting c1 = 2; c2 = -5; l1 = 3; l2 = 7; % parameters: c, A, b c = [c1; c2]; A = [1, 0; -1, 0; 0, 1; 0, -1]; b = [l1; l1; l2; l2]; %% solve LP cvx_begin variable x(2); % define variables [x1, x2] minimize(c&#39; * x); % define the objective subject to A * x &lt;= b; % define the linear constraint cvx_end B.2.3 Quadratic Programming (QP) Definition. A QP has the following form: \\[\\begin{align} \\tag{B.2} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P x + q^T x \\\\ \\text{subject to } &amp; Gx \\le h \\\\ &amp; Ax = b \\end{align}\\] where \\(P \\in \\mathcal{S}_+^n, q\\in \\mathbb{R}^n, G \\in \\mathbb{R}^{m \\times n}, h\\in \\mathbb{R}^m, A \\in \\mathbb{R}^{p \\times n}, b \\in \\mathbb{R}^p\\). Here \\(\\mathcal{S}_+^n\\) denotes the set of positive semidefinite matrices of size \\(n\\times n\\). Obviously, if we set \\(P\\) as zero, QP will degenerate to LP. Example. Consider the problem of minimizing a quadratic function \\[\\begin{equation*} f(x_1, x_2) = p_1 x_1^2 + 2p_2 x_1 x_2 + p_3 x_2^2 + q_1 x_1 + q_2 x_2 \\end{equation*}\\] over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). Since \\(P = 2 \\begin{bmatrix} p_1 &amp; p_2 \\\\ p_2 &amp; p_3 \\end{bmatrix} \\succeq 0\\), the following two conditions must hold: \\[\\begin{equation*} \\begin{cases} p_1 \\ge 0 \\\\ p_1 p_3 - 4 p_2^2 \\ge 0 \\end{cases} \\end{equation*}\\] Same as in the LP example, \\(G\\) and \\(h\\) can be expressed as: \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the QP example setting p1 = 2; p2 = 0.5; p3 = 4; q1 = -3; q2 = -6.5; l1 = 2; l2 = 2.5; % check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0); tmp2 = (p1*p3 - 4*p2^2 &gt;= 0); if ~(tmp1 &amp;&amp; tmp2) error(&quot;P is not positve semidefinite!&quot;); end % parameters: P, q, G, h P = 2 * [p1, p2; p2, p3]; q = [q1; q2]; G = [1, 0; -1, 0; 0, 1; 0, -1]; h = [l1; l1; l2; l2]; %% Solve the QP problem cvx_begin variable x(2); % define variables [x1; x2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P) + q&#39; * x; minimize(obj); subject to G * x &lt;= h; % define the linear constraint cvx_end B.2.4 Quadratically Constrained Quadratic Programming (QCQP) Definition. An (convex) QCQP has the following form: \\[\\begin{align} \\tag{B.3} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P_0 x + q_0^T x \\\\ \\text{subject to } &amp; \\frac{1}{2} x^T P_i x + q_i^T x + r_i \\le 0, \\ i = 1 \\dots m \\\\ &amp; Ax = b \\end{align}\\] where \\(P_i \\in \\mathcal{S}_+^n, i = 0 \\dots m\\), \\(q_i \\in \\mathbb{R}^n, i = 0 \\dots m\\), \\(A \\in \\mathbb{R}^{p \\times n}\\), and \\(b \\in \\mathbb{R}^p\\). Note that in other literature, you may find a more general form of QCQP: they don’t require \\(P_i\\)’s to be positive semidefinite. Yet in this case, the problem is non-convex and beyond our scope. Example. We study the problem of getting the minimum distance between two ellipses. By convention, when the ellipses overlap, we set the minimum distance as \\(0\\). This problem can be exactly solved by (convex) QCQP. Consider two ellipses of the following form: \\[\\begin{equation*} \\begin{cases} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 \\le 0 \\\\ \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 \\le 0 \\\\ \\end{cases} \\end{equation*}\\] where \\([y_1, z_1]^T\\) and \\([y_2, z_2]^T\\) are arbitrary points inside the two ellipses respectively. Also, two ensure the ellipses are well defined, we should enforce the following properties in \\((K_i, k_i, c_i), i = 1, 2\\): (1) \\(K_i \\succ 0\\); (2) Let \\(K_i = L_i L_i^T\\) be the Cholesky decomposition of \\(K_i\\). Then, ellipse \\(i\\) can be rewritten as: \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^T \\begin{bmatrix} y_i \\\\ z_i \\end{bmatrix} - L_i^{-1} k_i \\parallel^2 \\le \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i \\end{equation*}\\] Thus, \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i &gt; 0 \\end{equation*}\\] With these two assumptions, we want to minimize: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 \\end{equation*}\\] Now, we construct \\(P, q, r\\)’s in QCQP with the above parameters. Define the variable \\(x\\) as \\([y_1, z_1, y_2, z_2]\\). \\(P_0\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 = \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix}^T \\begin{bmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix} \\end{equation*}\\] \\(P_1, q_1, r_1\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 = \\frac{1}{2} x^T \\begin{bmatrix} K_1 &amp; O \\\\ O &amp; O \\end{bmatrix} + \\begin{bmatrix} k_1 \\\\ O \\end{bmatrix}^T x + c_1 \\le 0 \\end{equation*}\\] \\(P_2, q_2, r_2\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 = \\frac{1}{2} x^T \\begin{bmatrix} O &amp; O \\\\ O &amp; K_2 \\end{bmatrix} + \\begin{bmatrix} O \\\\ k_2 \\end{bmatrix}^T x + c_2 \\le 0 \\end{equation*}\\] The corresponding codes are shown below. In this example, we test the minimum distance between a circle \\(y_1^2 + z_1^2 \\le 1\\) and another circle \\((y_2 - 2)^2 + (z_2 - 2)^2 \\le 1\\). You can check whether the result from QCQP aligns with your manual calculation. %% Define the QCQP example setting K1 = eye(2); k1 = zeros(2, 1); c1 = -0.5; K2 = eye(2); k2 = [2; 2]; c2 = 3.5; if ~(if_ellipse(K1, k1, c1) &amp;&amp; if_ellipse(K2, k2, c2)) error(&quot;The example setting is not correct&quot;); end % define parameters P0, P1, P2, q1, q2, r1, r2 P0 = [1,0,-1,0; 0,1,0,-1; -1,0,1,0; 0,-1,0,1]; P1 = zeros(4, 4); P1(1:2, 1:2) = K1; P2 = zeros(4, 4); P2(3:4, 3:4) = K2; q1 = [k1; zeros(2, 1)]; q2 = [zeros(2, 1); k2]; r1 = c1; r2 = c2; %% Solve the QCQP problem cvx_begin variable x(4); % define variables [y1; z1; y2; z2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P0); minimize(obj); subject to 0.5 * quad_form(x, P1) + q1&#39; * x + r1 &lt;= 0; 0.5 * quad_form(x, P2) + q2&#39; * x + r2 &lt;= 0; cvx_end %% detect whether (K, k, c) generates a ellipse function flag = if_ellipse(K, k, c) L = chol(K); radius_square = 0.5 * norm(L \\ k)^2 - c; % L \\ k = inv(L) * k flag = (radius_square &gt; 0); end B.2.5 Second-Order Cone Programming (SOCP) Definition. An SOCP has the following form: \\[\\begin{align} \\tag{B.4} \\min_{x \\in \\mathbb{R}^n} \\ &amp; f^T x \\\\ \\text{subject to } &amp; || A_i x + b_i ||_2 \\le c_i^T x + d_i, \\ i = 1 \\dots m \\\\ &amp; Fx = g \\end{align}\\] where \\(f \\in \\mathbb{R}^n, A_i \\in \\mathbb{R}^{n_i \\times n}, b_i \\in \\mathbb{R}^{n_i}, c_i \\in \\mathbb{R}^n, d_i \\in \\mathbb{R}, F \\in \\mathbb{R}^{p \\times n}\\), and \\(g \\in \\mathbb{R}^p\\). Example. We consider the problem of stochastic linear programming: \\[\\begin{align} \\min_x \\ &amp; c^T x \\\\ \\text{subject to } &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p, \\ i = 1 \\dots m \\\\ &amp; a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i), \\ i = 1 \\dots m \\end{align}\\] Here \\(p\\) should be more than \\(0.5\\). We show that this problem can be converted to a SOCP: Since \\(a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i)\\), then \\((a_i^T x - b_i) \\sim \\mathcal{N}(\\bar{a}_i^T x - b_i, x^T \\Sigma_i x)\\). Standardize it: \\[\\begin{equation*} t := ||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1} \\left\\{ (a_i^T x - b_i) - (\\bar{a}_i^T x - b_i) \\right\\} \\sim \\mathcal{N}(0, 1) \\end{equation*}\\] Then, \\[\\begin{align} \\mathbb{P}(a_i^T x \\le b_i) &amp; = \\mathbb{P}(a_i^T x - b_i \\le 0) \\\\ &amp; = \\mathbb{P}(t \\le -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\\\ &amp; = \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\end{align}\\] Here \\(\\Phi(\\cdot)\\) is the cumulative distribution function of the standard normal distribution: \\[\\begin{equation*} \\Phi(\\xi) = \\int_{-\\infty}^{\\xi} e^{-\\frac{1}{2} t^2} \\ dt \\end{equation*}\\] Thus, \\[\\begin{align} &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p \\\\ \\Longleftrightarrow &amp; \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\ge p \\\\ \\Longleftrightarrow &amp; -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i) \\ge \\Phi^{-1}(p) \\\\ \\Longleftrightarrow &amp; \\Phi^{-1}(p) ||\\Sigma_i^{\\frac{1}{2}} x||_2 \\le b_i - \\bar{a}_i^T x \\end{align}\\] which is exactly the same as inequality constraints in SOCP formulation. (You can see why we enforce \\(p &gt; 0.5\\) here: otherwise \\(\\Phi^{-1}(p)\\) will be negative and the constraint with not be an second-order cone.) In the following code example, we set up four inequality constraints and let \\(\\bar{a}_i^T x \\le b_i, \\ i = 1 \\dots 4\\) form an square located at the origin of size \\(2\\). Then, for convenience, we set \\(\\Sigma_i \\equiv \\sigma^2 I\\). %% Define the SOCP example setting bar_a1 = [1; 0]; b1 = 1; bar_a2 = [0; 1]; b2 = 1; bar_a3 = [-1; 0]; b3 = 1; bar_a4 = [0; -1]; b4 = 1; sigma = 0.1; c = [2; 3]; p = 0.9; % p should be more than 0.5 Phi_inv = norminv(p); % get Phi^{-1}(p) %% Solve the SOCP problem cvx_begin variable x(2); % define variables [x1; x2] minimize(c&#39; * x); subject to sigma*Phi_inv * norm(x) &lt;= b1 - bar_a1&#39; * x; sigma*Phi_inv * norm(x) &lt;= b2 - bar_a2&#39; * x; sigma*Phi_inv * norm(x) &lt;= b3 - bar_a3&#39; * x; sigma*Phi_inv * norm(x) &lt;= b4 - bar_a4&#39; * x; cvx_end B.2.6 Semidefinite Programming (SDP) Definition. An SDP has the following form: \\[\\begin{align} \\tag{B.5} \\min_{X_i, x_i} \\ &amp; \\sum_{i=1}^{n_s} C_i \\cdot X_i + \\sum_{i=1}^{n_u} c_i \\cdot x_i \\\\ \\text{subject to } &amp; \\sum_{i=1}^{n_s} A_{i,j} \\cdot X_i + \\sum_{i=1}^{n_u} a_{i,j} \\cdot x_i = b_j, \\quad j = 1 \\dots m \\\\ &amp; X_i \\in \\mathcal{S}_+^{D_i}, \\quad i = 1 \\dots n_s \\\\ &amp; x_i \\in \\mathbb{R}^{d_i}, \\quad i = 1 \\dots n_u \\end{align}\\] where \\(C_i, A_{i, j} \\in \\mathbb{R}^{D_i \\times D_i}\\), \\(c_i, a_{i, j} \\in \\mathbb{R}^{d_i}\\), and \\(\\cdot\\) means element-wise product. For two square matrices \\(A, B\\), the dot product \\(A \\cdot B\\) is equal to \\(\\text{tr}(A B)\\); for two vectors \\(a, b\\), the dot product \\(a \\cdot b\\) is the same as inner product \\(a^T b\\). Note that actually there are many “standard” forms of SDP. For example, in the convex optimization theory part, you may find an SDP that looks like: \\[\\begin{align} \\min_X \\ &amp; C \\cdot X \\\\ \\text{subject to } &amp; A \\cdot X = b \\\\ &amp; X \\succeq 0 \\end{align}\\] It is convenient for us to analyze the theoretical properties of SDP with this form. Also, in SDP solvers’ User Guide, you may see more complex SDP forms which involve more general convex cones. For example, see MOSEK’s MATLAB API docs. Here we turn to use the form of (B.5) for two reasons: (1) it is general enough: our SDP example below can be converted to this form (also, SDPs from sum-of-squares programming in this book are exactly of the form (B.5)); (2) it is more readable than more complex forms. Example. We consider the problem of finding the minimum eigenvalue for a positive semidefinite matrix \\(S\\). We will show that this problem can be converted to (B.5). Since \\(S\\) is positive semidefinite, the finding procedure can be cast as \\[\\begin{align} \\max_\\lambda &amp; \\ \\lambda \\\\ \\text{subject to } &amp; S - \\lambda I \\succeq 0 \\end{align}\\] Now define an auxiliary matrix \\(X := S - \\lambda I\\). We have \\[\\begin{align} \\min_{\\lambda, X} &amp; \\ -\\lambda \\\\ \\text{subject to } &amp; X + \\lambda I = S \\\\ &amp; X \\succeq 0 \\end{align}\\] It is obvious that the linear matrix equality constraint \\(X + \\lambda I = S\\) can be divided into several linear scalar equality constraints in (B.5). For example, we consider \\(S \\in \\mathbb{S}_+^3\\). Thereby \\(X + \\lambda I = S\\) will lead to \\(6\\) linear equality constraints (We don’t consider \\(X\\) is a symmetric matrix here, since most solvers will implicitly consider this. Thus, only the upper-triangular part of \\(X\\) and \\(S\\) are actually used in the equality construction.): \\[\\begin{align} &amp; \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[0, 0], \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 2] \\\\ &amp; \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[1, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[1, 2], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\cdot X + \\lambda = S[2, 2] \\end{align}\\] Seems tedious? Fortunately, CVX provides a high-level API to handle these linear equality constraints: you just need to write down X + lam * eye(3) == S; % linear equality constraints: X + lam *I = S CVX will autometically convert this high-level constraint to (B.5) and pass them to the underlying solver. To generate a ramdom \\(S \\in \\mathcal{S}_+^3\\), you just need to assign three nonnegative eigenvalues to the program. After that, an random \\(S\\) will be generated by \\(S = Q \\ \\text{diag}(\\lambda_1, \\lambda_2, \\lambda_3) \\ Q^T\\), where \\(Q\\) is random orthonormal matrix. %% Define the SDP example setting lam_list = [0.7; 2.4; 3.7]; S = generate_random_PD_matrix(lam_list); % get a PD matrix S %% Solve the SDP problem cvx_begin variable X(3, 3) symmetric; variable lam; maximize(lam); subject to % here &quot;==&quot; should be read as &quot;is in&quot; X == semidefinite(3); X + lam * eye(3) == S; cvx_end % this function help to generate PD matrix of size 3*3 % if you provide the eigenvalues [lam_1, lam_2, lam_3] function S = generate_random_PD_matrix(lam_list) if ~all(lam_list &gt;= 0) % all eigenvalues &gt;= 0 error(&quot;All eigenvalues must be nonnegative.&quot;); end D = diag(lam_list); % use QR factorization to generate a random orthonormal matrix Q [Q, ~] = qr(rand(3, 3)); S = Q * D * Q&#39;; end B.2.7 CVXPY Introduction and Examples Apart from CVX MATLAB, we also have a Python package called CVXPY, which functions almost the same as CVX MATLAB. To define and solve a convex optimization problem CVXPY, basically, there are three steps (apart from importing necessary packages): Step 1: Define parameters and variables in a certain type of convex problem. Here variables are what you are trying to optimize or “learn”. Parameters are the “coefficients” of variables in the objective and constraints. Step 2: Define the objective function and constraints. Step 3: Solve the problem and get the results. Here we provide the CVXPY codes for the above five convex optimization examples. B.2.7.1 LP import cvxpy as cp import numpy as np ## Define the LP example setting c1 = 2 c2 = -5 l1 = 3 l2 = 7 ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: c, A, b c = np.array([c1, c2]) A = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) b = np.array([l1, l1, l2, l2]) ## Step 2: define objective and constraints obj = cp.Minimize(c.T @ x) constraints = [A @ x &lt;= b] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x B.2.7.2 QP import cvxpy as cp import numpy as np ## Define the LP example setting p1 = 2 p2 = 0.5 p3 = 4 q1 = -3 q2 = -6.5 l1 = 2 l2 = 2.5 # check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0) tmp2 = (p1*p3 - 4*p2**2 &gt;= 0) assert(tmp1 and tmp2, &quot;P is not positve semidefinite!&quot;) ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: P, q, G, h P = 2*np.array([[p1, p2], [p2, p3]]) q = np.array([q1, q2]) G = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) h = np.array([l1, l1, l2, l2]) ## Step 2: define the objective and constraints fx = 0.5 * cp.quad_form(x, P) + q.T @ x obj = cp.Minimize(fx) constraints = [G @ x &lt;= h] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve the problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x B.2.7.3 QCQP import cvxpy as cp import numpy as np from numpy.linalg import cholesky, inv, norm ## Define the QCQP example setting def if_ellipse(K, k, c): # examine whether 0.5*x^T K x + k^T x + c &lt;= 0 is a ellipse # if K is not positive semidefinite, Cholesky will raise an error L = cholesky(K) radius_square = 0.5 * norm(inv(L) @ k)**2 - c return radius_square &gt; 0 K1 = np.eye(2) k1 = np.zeros(2) c1 = -0.5 K2 = np.array([[1, 0], [0, 1]]) k2 = np.array([2, 2]) c2 = 3.5 if not (if_ellipse(K1, k1, c1) and if_ellipse(K2, k2, c2)): raise ValueError(&quot;The example setting is not correct&quot;) ## Step 1: define variables and parameters P0 = np.array([[1,0,-1,0], [0,1,0,-1], [-1,0,1,0], [0,-1,0,1]]) P1 = np.zeros((4,4)) P1[:2, :2] = K1 P2 = np.zeros((4,4)) P2[2:, 2:] = K2 q1 = np.concatenate([k1, np.zeros(2)]) q2 = np.concatenate([np.zeros(2), k2]) r1 = c1 r2 = c2 ## Step 2: define objective and constraints x = cp.Variable(4) # variable: x = [y1, z1, y2, z2]^T fx = 0.5 * cp.quad_form(x, P0) obj = cp.Minimize(fx) con1 = (0.5 * cp.quad_form(x, P1) + q1.T @ x + r1 &lt;= 0) # ellipse 1 con2 = (0.5 * cp.quad_form(x, P2) + q2.T @ x + r2 &lt;= 0) # ellipse 2 constraints = [con1, con2] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x B.2.7.4 SOCP import cvxpy as cp import numpy as np from scipy.stats import norm ## Define the SOCP example setting # define bar_ai, bi (i = 1, 2, 3, 4) bar_a1 = np.array([1, 0]) b1 = 1 bar_a2 = np.array([0, 1]) b2 = 1 bar_a3 = np.array([-1, 0]) b3 = 1 bar_a4 = np.array([0, -1]) b4 = 1 sigma = 0.1 c = np.array([2, 3]) p = 0.9 # p should be more than 0.5 ## Step 1: define variables and parameters Phi_inv = norm.ppf(p) # get Phi^{-1}(p) ## Step 2: define objective and constraints x = cp.Variable(2) # variable: x = [x1, x2]^T obj = cp.Minimize(c.T @ x) # use cp.SOC(t, x) to create the SOC constraint ||x||_2 &lt;= t constraints = [ cp.SOC(b1 - bar_a1.T @ x, sigma*Phi_inv*x), cp.SOC(b2 - bar_a2.T @ x, sigma*Phi_inv*x), cp.SOC(b3 - bar_a3.T @ x, sigma*Phi_inv*x), cp.SOC(b4 - bar_a4.T @ x, sigma*Phi_inv*x), ] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x B.2.7.5 SDP import cvxpy as cp import numpy as np from scipy.stats import ortho_group ## Define the SDP example setting # this function help to generate PD matrix of size 3*3 # if you provide the eigenvalues [lam_1, lam_2, lam_3] def generate_random_PD_matrix(lam_list): assert np.all(lam_list &gt;= 0) # all eigenvalues &gt;= 0 # S = Q @ D @ Q.T D = np.diag(lam_list) Q = ortho_group.rvs(3) return Q @ D @ Q.T lam_list = np.array([0.5, 2.4, 3.7]) S = generate_random_PD_matrix(lam_list) # get a PD matrix S ## Step 1: define variables and parameters # get coefficients for equality constraints A_00 = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_00 @ X) + lam = S_00 A_01 = np.array([[0, 1, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_01 @ X) = S_01 A_02 = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]]) # tr(A_02 @ X) = S_02 A_11 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]) # tr(A_11 @ X) + lam = S_11 A_12 = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 0]]) # tr(A_12 @ X) = S_12 A_22 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1]]) # tr(A_22 @ X) + lam = S_22 ## Step 2: define objective and constraints # define a PD matrix variable X of size 3*3 X = cp.Variable((3, 3), symmetric=True) constraints = [X &gt;&gt; 0] # the operator &gt;&gt; denotes matrix inequality lam = cp.Variable(1) constraints += [ cp.trace(A_00 @ X) + lam == S[0,0], cp.trace(A_01 @ X) == S[0,1], cp.trace(A_02 @ X) == S[0,2], cp.trace(A_11 @ X) + lam == S[1,1], cp.trace(A_12 @ X) == S[1,2], cp.trace(A_22 @ X) + lam == S[2,2], ] obj = cp.Minimize(-lam) prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, lam.value) # optimal lam "],["app-lti-system-theory.html", "C Linear System Theory C.1 Stability C.2 Controllability and Observability C.3 Stabilizability And Detectability", " C Linear System Theory Thanks to Shucheng Kang for writing this Appendix. C.1 Stability C.1.1 Continuous-Time Stability Consider the continuous-time linear time-invariant (LTI) system \\[\\begin{equation} \\dot{x} = A x. \\tag{C.1} \\end{equation}\\] the system is said to be “diagonalizable” if \\(A\\) is diagonalizable. Definition C.1 (Asymptotic and Marginal Stability) The diagonalizable, LTI system (C.1) is “asymptotically stable” if \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “marginally stable” if \\(x(t) \\nrightarrow 0\\) but remains bounded as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “stable” if it is either asymptotically or marginally stable “unstable” if it is not stable One can show that \\(A\\)’s eigenvalues determine the LTI system’s stability, as the following Theorem states: Theorem C.1 (Stability of Continuous-Time LTI System) The diagonalizable7, LTI system (C.1) is asymptotically stable if \\(\\text{Re} (\\lambda_i) &lt; 0\\) for all \\(i\\) marginally stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(\\text{Re} (\\lambda_i) = 0\\) stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) unstable if \\(\\text{Re} (\\lambda_i) &gt; 0\\) for at least one \\(i\\) Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(\\dot{x} = Ax\\) can be restated as \\(\\dot{z} = \\Lambda z\\). Consider the \\(i\\)’s component of \\(z\\): \\[\\begin{equation*} \\dot{z}_i = \\lambda_i z_i \\Longrightarrow z_i(t) = e^{\\lambda_i t} z_i(0) \\end{equation*}\\] Since \\(\\text{Re}(\\lambda_i) &lt; 0\\), \\(z_i(t)\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_i(0)\\). C.1.2 Discrete-Time Stability Now consider the diagonalizable, discrete-time linear time-invariant (LTI) system \\[\\begin{equation} x_{t+1} = A x_t. \\tag{C.2} \\end{equation}\\] Theorem C.2 (Stability of Discrete-Time LTI System) The diagonalizable, discrete-time LTI system (C.2) is asymptotically stable if \\(|\\lambda_i| &lt; 1\\) for all \\(i\\) marginally stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(|\\lambda_i| = 1\\) stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) unstable if \\(|\\lambda_i| &gt; 1\\) for at least one \\(i\\). Note that \\(|\\lambda_i| &lt; 1\\) means the eigenvalue lies strictly inside the unit circle in the complex plane. Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(x_{t+1} = Ax\\) can be restated as \\(z_{t+1} = \\Lambda z_t\\). Expanding the recursion, we have \\[\\begin{equation*} z_{t} = \\Lambda^{t-1} z_0 \\Longrightarrow z_{t,i} = \\lambda_i^{t-1} z_{0,i} \\end{equation*}\\] Since \\(|\\lambda_i| &lt; 1\\), \\(z_{t,i}\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_{0,i}\\). C.1.3 Lyapunov Analysis Theorem C.3 (Lyapunov Equation) The following is equivalent for a linear time-invariant system \\(\\dot{x} = A x\\) The system is globally asymptotically stable, i.e., \\(A\\) is Hurwitz and \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) regardless of the initial condition; For any positive definite matrix \\(Q\\), the unique solution \\(P\\) to the Lyapunov equation \\[\\begin{equation} A^T P + P A = -Q \\tag{C.3} \\end{equation}\\] is positive definite. Proof. (a): \\(2 \\Rightarrow 1\\). Suppose we are given two positive definite matrices \\(P, Q \\succ 0\\) that satisfies the Lyapunov equation (C.3). Define a scalar function \\[ V(x) = x^T P x. \\] It is clear that \\(V &gt; 0\\) for any \\(x \\neq 0\\) and \\(V(x) = 0\\) (i.e., \\(V(x)\\) is positive definite). We also see \\(V(x)\\) is radially unbounded because: \\[ V(x) \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2 \\Rightarrow \\lim_{x \\rightarrow \\infty} V(x) \\rightarrow \\infty. \\] The time derivative of \\(V\\) reads \\[ \\dot{V} = 2 x^T P \\dot{x} = x^T (A^T P + P A) x = - x^T Q x. \\] Clearly, \\(\\dot{V} &lt; 0\\) for any \\(x \\neq 0\\) and \\(\\dot{V}(0) = 0\\). According to Lyapunov’s global stability theorem 4.3, we conclude the linear system \\(\\dot{x} = Ax\\) is globally asymptotically stable at \\(x = 0\\). (b): \\(1 \\Rightarrow 2\\). Suppose \\(A\\) is Hurwitz, we want to show that, for any \\(Q \\succ 0\\), there exists a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (C.3). In fact, consider the matrix \\[ P = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt. \\] Because \\(A\\) is Hurwitz, the integral exists, and clearly \\(P \\succ 0\\) due to \\(Q \\succ 0\\). To show this choice of \\(P\\) satisfies the Lyapunov equation, we write \\[\\begin{align} A^T P + P A &amp;= \\int_{t=0}^{\\infty} \\left( A^T e^{A^T t} Q e^{At} + e^{A^T t} Q e^{At} A \\right) dt \\\\ &amp;=\\int_{t=0}^{\\infty} d \\left( e^{A^T t} Q e^{At} \\right) \\\\ &amp; = e^{A^T t} Q e^{At}\\vert_{t = \\infty} - e^{A^T t} Q e^{At}\\vert_{t = 0} = - Q, \\end{align}\\] where the last equality holds because \\(e^{A \\infty} = 0\\) (recall \\(A\\) is Hurwitz). To show the uniqueness of \\(P\\), we assume that there exists another matrix \\(P&#39;\\) that also satisfies the Lyapunov equation. Therefore, \\[\\begin{align} P&#39; &amp;= e^{A^T t} P&#39; e^{At} \\vert_{t=0} - e^{A^T t} P&#39; e^{At} \\vert_{t=\\infty} \\\\ &amp;= - \\int_{t=0}^{\\infty} d \\left( e^{A^T t} P&#39; e^{At} \\right) \\\\ &amp;= - \\int_{t=0}^{\\infty} e^{A^T t} \\left( A^T P&#39; + P&#39; A \\right) e^{At} dt \\\\ &amp; = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt = P, \\end{align}\\] leading to \\(P&#39; = P\\). Hence, the solution is unique. Convergence rate estimation. We now show that Theorem C.3 can allow us to quantify the convergence rate of a (stable) linear system towards zero. For a Hurwitz linear system \\(\\dot{x} = Ax\\), let us pick a positive definite matrix \\(Q\\). Theorem C.3 tells us we can find a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (C.3). In this case, we can upper bound the scalar function \\(V = x^T P x\\) as \\[ V \\leq \\lambda_{\\max}(P) \\Vert x \\Vert^2. \\] The time derivative of \\(V\\) is \\(\\dot{V} = - x^T Q x\\), which can be upper bounded by \\[\\begin{align} \\dot{V} &amp; \\leq - \\lambda_{\\min} (Q) \\Vert x \\Vert^2 \\\\ &amp; = - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} \\underbrace{ \\left( \\lambda_{\\max} (P) \\Vert x \\Vert^2 \\right)}_{\\geq V} \\\\ &amp; \\leq - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} V. \\end{align}\\] Denoting \\(\\gamma(Q) = \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max}(P)}\\), the above inequality implies \\[ V(0) e^{-\\gamma(Q) t} \\geq V(t) = x^T P x \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2. \\] As a result, \\(\\Vert x \\Vert^2\\) converges to zero exponentially with a rate at least \\(\\gamma(Q)\\), and \\(\\Vert x \\Vert\\) converges to zero exponentially with a rate at least \\(\\gamma(Q) / 2\\). Best convergence rate estimation. I have used \\(\\gamma (Q)\\) to make it explict that the rate \\(\\gamma\\) depends on the choice of \\(Q\\), because \\(P\\) is computed from the Lyapunov equation as an implicit function of \\(Q\\). Naturally, choosing different \\(Q\\) will lead to different \\(\\gamma (Q)\\). So what is the choice of \\(Q\\) that maximizes the convergence rate estimation? Corollary C.1 (Maximum Convergence Rate Estimation) \\(Q = I\\) maximizes the convergence rate estimation. Proof. let us denote \\(P_0\\) as the solution to the Lyapunov equation with \\(Q = I\\) \\[ A^T P_0 + P_0 A = - I. \\] Let \\(P\\) be the solution corresponding to a different choice of \\(Q\\) \\[ A^T P + P A = - Q. \\] Without loss of generality, we can assume \\(\\lambda_{\\min}(Q) = 1\\), because rescaling \\(Q\\) will recale \\(P\\) by the same factor, which does not affect \\(\\gamma(Q)\\). Subtracting the two Lyapunov equations above we get \\[ A^T (P - P_0) + (P - P_0) A = - (Q - I). \\] Since \\(Q - I \\succeq 0\\) (due to \\(\\lambda_{\\min}(Q) = 1\\)), we know \\(P - P_0 \\succeq 0\\) and \\(\\lambda_{\\max} (P) \\geq \\lambda_{\\max} (P_0)\\). As a result, \\[ \\gamma(Q) = \\frac{\\lambda_{\\min}(Q)}{\\lambda_{\\max}(P)} = \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P)} \\leq \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P_0)} = \\gamma(I), \\] and \\(Q = I\\) maximizes the convergence rate estimation. C.2 Controllability and Observability Consider the following linear time-invariant (LTI) system \\[\\begin{equation} \\tag{C.4} \\begin{split} \\dot{x} = A x + B u \\\\ y = C x + D u \\end{split} \\end{equation}\\] where \\(x \\in \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{R}^m\\) the control input, \\(y \\in \\mathbb{R}^p\\) the output, and \\(A,B,C,D\\) are constant matrices with proper sizes. If we know the initial state \\(x(0)\\) and the control inputs \\(u(t)\\) over a period of time \\(t \\in [0, t_1]\\), the system trajectory \\((x(t), y(t))\\) can be determined as \\[\\begin{equation} \\tag{C.5} \\begin{split} x(t) &amp; = e^{At} x(0) + \\int_{0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau \\\\ y(t) &amp; = C x(t) + D u(t) \\end{split} \\end{equation}\\] To study the internal structure of linear systems, two important properties should be considered: controllability and observability. In the following analysis, we will see that they are actually dual concepts. Their definitions (Chen 1984) are given below. Definition C.2 (Controllability) The LTI system (C.4), or the pair \\((A, B)\\), is controllable, if for any initial state \\(x(0) = x_0\\) and final state \\(x_f\\), there exists a sequence of control inputs that transfer the system from \\(x_0\\) to \\(x_f\\) in finite time. Definition C.3 (Observability) The LTI system (C.4), or the pair \\((C, A)\\), is observable, if for any unknown initial state \\(x(0)\\), there exists a finite time \\(t_1 &gt; 0\\), such that knowing \\(y\\) and \\(u\\) over \\([0, t_1]\\) suffices to determine \\(x(0)\\). Sometimes it will become more convenient for us to analyze the system (C.4) under another coordinate basis, i.e., \\(z = T x\\), where the coordinate transformation \\(T\\) is nonsingular (i.e., full-rank). Define \\(A&#39; = TAT^{-1}, B&#39; = PB, C&#39; = CT^{-1}, D&#39; = D\\), we get \\[\\begin{equation*} \\begin{split} \\dot{z} = A&#39; z + B&#39; u \\\\ y = C&#39; z + D&#39; u \\end{split} \\end{equation*}\\] Since the coordinate transformation only changes the system’s coordinate basis, physical properties like controllability and observability will not change. C.2.1 Cayley-Hamilton Theorem In the analysis of controllability and observability, Cayley Hamilton Theorem lays the foundation. The statement of the theory and its (elegant) proof are given blow. Some useful corollaries are also presented. Theorem C.4 (Cayley-Hamilton) Let \\(A \\in \\mathbb{C}^{n \\times n}\\) and denote the characteristic polynomial of \\(A\\) as \\[ \\text{det}(\\lambda I - A) = \\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n \\in \\mathbb{C}[\\lambda], \\] which is a polynomial in a single variable \\(\\lambda\\) with coefficients \\(a_1,\\dots,a_n\\). Then \\[ A^n + a_1 A^{n-1} + \\dots + a_n I = 0 \\] Proof. Define the adjugate of \\(\\lambda I - A\\) as \\[ B = \\text{adj}(\\lambda I - A) \\] From \\(B\\)’s definition, we have \\[\\begin{equation} (\\lambda I - A) B = \\text{det}(\\lambda I - A) I = (\\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n) I \\tag{C.6} \\end{equation}\\] Also, \\(B\\) is a polynomial matrix over \\(\\lambda\\), whose maximum degree is no more than \\(n - 1\\). Therefore, we write \\(B\\) as follows: \\[ B = \\sum_{i=0}^{n-1} \\lambda^i B_i \\] where \\(B_i\\)’s are constant matrices. In this way, we unfold \\((\\lambda I - A)B\\): \\[\\begin{equation} \\tag{C.7} \\begin{split} (\\lambda I - A) B &amp; = (\\lambda I - A) \\sum_{i=0}^{n-1} \\lambda^i B_i \\\\ &amp; = \\lambda^n B_{n-1} + \\sum_{i=1}^{n-1} \\lambda^i (-A B_i + B_{i-1}) - A B_0 \\end{split} \\end{equation}\\] Since \\(\\lambda\\) can be arbitrarily set, matching the coefficients of (C.6) and (C.7), we have \\[\\begin{equation*} \\begin{split} B_{n-1} &amp; = I \\\\ -A B_i + B_{i-1} &amp; = a_{n-i} I, \\quad i = 1 \\dots n - 1 \\\\ -A B_0 &amp; = a_n I \\end{split} \\end{equation*}\\] Thus, we have \\[\\begin{equation*} \\begin{split} &amp; B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I \\\\ = &amp; I \\cdot A^n + \\sum_{i=1}^{n-1} (a_{n-i} I) \\cdot A^i + (a_n I) \\cdot I \\\\ = &amp; A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I \\end{split} \\end{equation*}\\] On the other hand, one can easily check that \\[ B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I = 0 \\] since each term offsets completely. Therefore, \\[ A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I = 0, \\] concluding the proof. Here are some corollaries of the Cayley-Hamilton Theorem. Corollary C.2 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k \\ge n\\), \\(A^k B\\) is a linear combination of \\(B, AB, A^2B, \\dots, A^{n-1}B\\). Proof. Directly from Cayley Hamilton Theorem, \\(A^n\\) can be expressed as a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). By recursion, it is easy to show that for all \\(m &gt; n\\), \\(A^m\\) is also a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). Post-multiply both sides with \\(B\\), we get what we want. Corollary C.3 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k &gt; n\\), the following equality always holds: \\[ \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix}) = \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix}) \\] Proof. First prove LHS \\(\\le\\) RHS. \\(\\forall v \\in \\mathbb{C}^n\\) such that \\[ v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B &amp; \\dots A^{k-1}B \\end{bmatrix} = 0 \\] \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) must hold. Second prove LHS \\(\\ge\\) RHS. For any \\(v \\in \\mathbb{C}^n\\) such that \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) and any \\(k &gt; n\\), by Corollary C.2, there exists a sequence \\(c_i, i = 0 \\dots n-1\\) satisfy the following: \\[ v^* A^k B = v^* \\sum_{i=0}^{n-1} c_i A^i B = 0 \\] Therefore, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = 0\\). Corollary C.4 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}\\), define \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] If \\(\\text{rank}(\\mathcal{C}) = k_1 &lt; n\\), there exist a similarity transformation \\(T\\) such that \\[ T A T^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, T B = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}, \\bar{B}_c \\in \\mathbb{C}^{k_1 \\times m}\\). Moreover, the matrix \\[\\begin{equation*} \\bar{\\mathcal{C}} := \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c \\end{bmatrix} \\end{equation*}\\] has full row rank. Proof. Since \\(\\mathcal{C}\\) is not full row rank, we pick \\(k_1\\) linearly independent columns from \\(\\mathcal{C}\\). Denote them as \\(q_1\\dots q_{k_1}\\), \\(q_i \\in \\mathbb{C}^n\\). Then, we arbitrarily set other \\(n-k_1\\) vectors \\(q_{k_1+1} \\dots q_{n}\\) as long as \\[ Q = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\] is invertible. Define the similarity transformation matrix by \\(T = Q^{-1}\\). Note that \\(A q_i\\) can be seen as a column picked from \\(A^{k} B, k \\in \\left\\{1 \\dots n\\right\\}\\), which is guaranteed to be a linear combination of \\(B, AB, \\dots, A^{n-1}B\\) from Cayley Hamilton Theorem. Thus, \\(A q_i\\) is bound to be a linear transformation of columns from \\(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = \\mathcal{C}\\). Since \\(q_1\\dots q_{k_1}\\) is the largest linearly independent column vector set from \\(\\mathcal{C}\\), this implies \\(A q_i\\) can be expressed as a linear combination of \\(q_1\\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} A Q &amp; = A T^{-1} = A \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} = T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} \\end{split} \\end{equation*}\\] Similarly, \\(B\\) itself is part of \\(\\mathcal{C}\\). Therefore, each column of \\(B\\) is naturally a linear combination of \\(q_1 \\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} B = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{split} = T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] To see \\(\\bar{\\mathcal{C}}\\) has full row rank, note that \\(\\text{rank} \\mathcal{C} = k_1\\) and \\[\\begin{equation*} \\mathcal{C} = T^{-1} \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\end{equation*}\\] Thus, \\[\\text{rank}\\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\end{bmatrix} = k_1. \\] By Corollary C.3, \\(\\text{rank}\\bar{\\mathcal{C}} = k_1\\). The following Corollary is especially useful in the study of pole assignment in the single-input-multiple-output (SIMO) LTI system. Corollary C.5 For any \\(A \\in \\mathbb{C}^{n \\times n}, b \\in \\mathbb{C}^{n}\\), if \\[\\begin{equation*} \\mathcal{C} = \\begin{bmatrix} b &amp; Ab &amp; \\dots &amp; A^{n-1}b \\end{bmatrix} \\in \\mathbb{C}^{n \\times n} \\end{equation*}\\] has full rank, then there exists a similarity transformation \\(T\\) such that \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(a_1, \\dots, a_n\\) are the coefficients of \\(A\\)’s characteristic polynomial: \\[\\begin{equation*} \\det(A - \\lambda I) = \\lambda^{n} + a_1 \\lambda^{n-1} + \\dots + a_n \\lambda \\end{equation*}\\] Proof. Since \\(\\mathcal{C}\\) is invertible, define its inverse \\[\\begin{equation*} \\mathcal{C}^{-1} = \\begin{bmatrix} M_1 \\\\ M_2 \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] where \\(M_i \\in \\mathbb{C}^{1 \\times n}\\). Then, \\[\\begin{equation*} I = \\mathcal{C}^{-1} \\mathcal{C} = \\begin{bmatrix} M_1 b &amp; M_1 Ab &amp; \\dots &amp; M_1 A^{n-1}b \\\\ M_2 b &amp; M_2 Ab &amp; \\dots &amp; M_2 A^{n-1}b \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} \\Longrightarrow \\begin{cases} M_n A^{n-1}b = 1 \\\\ M_n A^ib = 0, \\ i = 0,\\dots, n-2 \\end{cases} \\end{equation*}\\] Now we claim that the transformation matrix \\(T\\) can be constructed as follows: \\[\\begin{equation*} T = \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] We first show \\(T\\) is invertible by calculating \\(T \\mathcal{C}\\): \\[\\begin{equation*} T \\mathcal{C} = \\begin{bmatrix} M_n A^{n-1}b &amp; \\star &amp; \\dots &amp; \\star \\\\ M_n A^{n-2}b &amp; M_n A^{n-1}b &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\star &amp; \\dots &amp; \\star \\\\ 0 &amp; 1 &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\end{equation*}\\] Then we calculate \\(Tb\\) and \\(TA\\): \\[\\begin{equation*} \\begin{split} Tb &amp; = \\begin{bmatrix} M_n A^{n-1}b \\\\ M_n A^{n-2}b \\\\ \\vdots \\\\ M_n b \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ T A &amp; = \\begin{bmatrix} M_n A^n \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} = \\begin{bmatrix} -M_n \\cdot \\sum_{i=0}^{n-1} a_{n-i} A^i \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\vdots \\\\ M_n A \\\\ M_n \\end{bmatrix} = A_1 T \\end{split} \\end{equation*}\\] where the penultimate equality uses Cayley Hamilton Theorem. C.2.2 Equivalent Statements for Controllability There are a few equivalent statements to express an LTI system’s controllability that one should be familiar with: Theorem C.5 (Equivalent Statements for Controllability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((A, B)\\) is controllable. The matrix \\[ W_c(t) := \\int_{0}^{t} e^{A\\tau} B B^* e^{A^* \\tau} d\\tau \\] is positive definite for any \\(t &gt; 0\\). The controllability matrix \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; A^2 B &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] has full row rank. The matrix \\([A - \\lambda I, B]\\) has full row rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(x\\) be any eigenvalue and any corresponding left eigenvector \\(A\\), i.e., \\(x^* A = x^* \\lambda\\), then \\(x^* B \\ne 0\\). The eigenvalues of \\(A+BF\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(F\\). If, in addition, all eigenvalues of \\(A\\) have negative real parts, then the unique solution of \\[ A W_c + W_c A^* = -B B^* \\] is positive definite. The solution is called the controllability Gramian and can be expressed as \\[ W_c = \\int_{0}^{\\infty} e^{A \\tau} B B^* e^{A^* \\tau} d\\tau \\] Proof. (\\(1. \\Rightarrow 2.\\)) Prove by contradiction. Assume that \\((A, B)\\) is controllable but \\(W_c(t_1)\\) is singular for some \\(t_1 &gt; 0\\). This implies there exists a real vector \\(v \\ne 0 \\in \\mathbb{R}^n\\), s.t. \\[ v^* W_c(t_1) v = v^* (\\int_{0}^{t_1} e^{At} B B^* e^{A^*t} dt) v = \\int_{0}^{t_1} v^* (e^{At} B B^* e^{A^*t}) v \\ dt = 0 \\] Since \\(e^{At} BB^* e^{A^*t} \\succeq 0\\) for all \\(t\\), we must have \\[\\begin{equation*} \\begin{split} &amp; v^* (e^{At} B B^* e^{A^*t}) v = \\parallel v^* B e^{At} \\parallel^2 = 0, \\quad \\forall t \\in [0, t_1] \\\\ \\Longrightarrow &amp; v^* B e^{At} = 0, \\quad \\forall t \\in [0, t_1] \\end{split} \\end{equation*}\\] Setting \\(x(t_1) = 0\\), from (C.5), we have \\[ 0 = e^{A t_1} x(0) + \\int_{0}^{t_1} e^{A (t_1 - \\tau)} B u(\\tau) d\\tau = 0 \\] Pre-multiply the above equation by \\(v^*\\), then \\[ 0 = v^* e^{A t_1} x(0) \\] Since \\(x(0)\\) can be chosen arbitrarily, we set \\(x(0) = v e^{-A t_1}\\), which results in \\(v = 0\\). Contradiction! (\\(2. \\Rightarrow 1.\\)) For any \\(x(0) = x_0, t_1 &gt; 0, x(t_1) = x_1\\), since \\(W_c(t_1) \\succ 0\\), we set the control inputs as \\[ u(t) = -B^* e^{A^*(t_1 - t)} W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\] We claim that the picked \\(u(t)\\) satisfies (C.5) by \\[\\begin{equation*} \\begin{split} &amp; e^{At} x_0 + \\int_{0}^{t_1} e^{A(t_1-t)} B u(t) dt \\\\ &amp; = e^{At} x_0 - \\int_{0}^{t_1} e^{A(t_1-t)} B B^* e^{A^*(t_1-t)} dt \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; \\overset{\\tau = t_1-t}{=} e^{At} x_0 - \\underbrace{\\int_{0}^{t_1} e^{A\\tau} BB^* e^{A^*\\tau} d\\tau}_{W_c(t_1)} \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; = e^{At} x_0 - [e^{At_1} x_0 - x_1] = x_1 \\end{split} \\end{equation*}\\] (\\(2. \\Rightarrow 3.\\)) Prove by contradiction. Suppose \\(W_c(t) \\succ 0, \\forall t &gt; 0\\) but \\(\\mathcal{C}\\) is not of full row rank. Then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[ v^* A^k B = 0, \\quad k = 0 \\dots n - 1 \\] By Corollary C.2, we have \\[ v^* A^k B = 0, \\ \\forall k \\in \\mathbb{N} \\Longrightarrow v^* e^{At} B = 0, \\ \\forall t &gt; 0 \\] which implies \\[ v^* W_c(t) v = v^* (\\int_{0}^{t} e^{A\\tau} B B^* e^{A^*\\tau} d\\tau) v = 0, \\quad \\forall t &gt; 0 \\] Contradiction! (\\(3. \\Rightarrow 2.\\)) Prove by contradiction. Suppose \\(\\mathcal{C}\\) has full row rank but \\(W_c(t_1)\\) is singular at some \\(t_1 &gt; 0\\). Then, similar to the proof in (\\(1. \\Rightarrow 2.\\)), there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(F(t) := v^* e^{At} B \\equiv 0, \\forall t \\in [0, t_1]\\). Since \\(F(t)\\) is infinitely differentiable, we get its \\(i\\)’s derivative at \\(t=0\\), where \\(i = 0, 1, \\dots n-1\\). This results in \\[\\begin{equation*} \\left. \\frac{d^i F}{dt^i} \\right|_{t=0} = \\left. v^* A^{i} e^{At} B \\right|_{t=0} = v^* A^i B = 0, \\quad i = 0 \\dots n-1 \\end{equation*}\\] Thus, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\). Contradiction! (\\(3. \\Rightarrow 4.\\)) Proof by contradiction. Suppose \\([A - \\lambda I, B]\\) does not have full row rank for some \\(\\lambda \\in \\mathbb{C}\\). Then, there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(v^* [A - \\lambda I, B] = 0\\). This implies \\(v^* A = v^* \\lambda\\) and \\(v^* B = 0\\). On the other hand, \\[\\begin{equation*} v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; \\lambda B &amp; \\dots &amp; \\lambda^{n-1} B \\end{bmatrix} = 0 \\end{equation*}\\] Contradiction! (\\(4. \\Rightarrow 5.\\)) Proof by contradiction. If there exists a left eigenvector and eigenvalue pair \\((x, \\lambda)\\), s.t. \\(x^* A = \\lambda x^*\\) while \\(x^*B = 0\\), then \\(x^* [A - \\lambda I, B] = 0\\). Contradiction! (\\(5. \\Rightarrow 3.\\)) Proof by contradiction. If the controllability matrix \\(\\mathcal{C}\\) does not have full row rank, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then, from Corollary C.4, there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{R}^{k \\times k}, \\bar{A}_{\\bar{c}} \\in \\mathbb{R}^{(n-k) \\times (n-k)}\\). Now arbitrarily pick one of \\(\\bar{A}_{\\bar{c}}\\)’s left eigenvector \\(x_{\\bar{c}}\\) and its corresponding eigenvalue \\(\\lambda_1\\). Define the vector \\(x = \\begin{bmatrix} 0 \\\\ x_{\\bar{c}} \\end{bmatrix}\\). Then, \\[\\begin{equation*} \\begin{split} x^* (TAT^{-1}) = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\end{bmatrix} \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\bar{A}_{\\bar{c}} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; \\lambda_1 x_{\\bar{c}}^* \\end{bmatrix} = \\lambda_1 x^* \\\\ x^* (TB) &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{x}} \\end{bmatrix} \\begin{bmatrix} B_{\\bar{c}} \\\\ 0 \\end{bmatrix} = 0 \\end{split} \\end{equation*}\\] which implies \\((TAT^{-1}, TB)\\) is not controllable. However, similarity transformation does not change controllability. Contradiction! (\\(6. \\Rightarrow 1.\\)) Prove by contradiction. If \\((A, B)\\) is not controllable, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then from Corollary C.4, there exists a similarity transformation \\(T\\) s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] Now arbitrarily pick \\(F \\in \\mathbb{R}^{m\\times n}\\) and define \\(FT^{-1} = [F_1, F_2]\\), where \\(F_1 \\in \\mathbb{R}^{m\\times k}, F_2 \\in \\mathbb{R}^{m\\times (n-k)}\\). Thus, \\[\\begin{equation*} \\begin{split} \\text{det}(A+BF-\\lambda I) &amp; = \\text{det}\\left( T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} T + T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} F - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det}\\left( T^{-1} \\left\\{ \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} FT^{-1} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right\\} T \\right) \\\\ &amp; = \\text{det}\\left( \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\begin{bmatrix} F_1 &amp; F_2 \\end{bmatrix} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1 &amp; \\bar{A}_{12} + \\bar{B}_c F_2 \\\\ 0 &amp; \\bar{A}_{\\bar{c}} - \\lambda I_2 \\end{bmatrix} \\\\ &amp; = \\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1) \\cdot \\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2) \\end{split} \\end{equation*}\\] where \\(I_1\\) is the identity matrix of size \\(k\\). Similarly, \\(I_2\\) of size \\(n-k\\). Thus, at least \\(n-k\\) eigenvalues of \\(A+BF\\) cannot be freely assigned by choosing \\(F\\). Contradiction! (\\(1. \\Rightarrow 6.\\)) Here we only represent the SIMO case. For the MIMO case, the proof is far more complex. Interesting readers can refer to (Davison and Wonham 1968) (the shortest proof I can find). Since there is only one input, the matrix \\(B\\) degenerate to vector \\(b\\). From Corollary C.5, there exist a similarity transformation matrix \\(T\\), s.t. \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] For any \\(F \\in \\mathbb{C}^{1 \\times n}\\), denote \\(FT^{-1}\\) as \\([f_1, f_2, \\dots, f_n]\\). Calculating the characteristic polynomial of \\(A + bF\\): \\[\\begin{equation*} \\begin{split} \\text{det}(\\lambda I - A - bF) &amp; = \\text{det}(\\lambda I - T^{-1}A_1 T - T^{-1} b_1 F) \\\\ &amp; = \\text{det}(\\lambda I - A_1 - b_1 F T^{-1}) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\lambda + a_1 - f_1 &amp; \\lambda + a_2 - f_2 &amp; \\dots &amp; \\lambda + a_{n-1} - f_{n-1} &amp; \\lambda + a_n - f_n \\\\ -1 &amp; \\lambda &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; -1 &amp; \\lambda \\end{bmatrix} \\\\ &amp; = \\lambda^n + (a_1 - f_1) \\lambda^{n-1} + \\dots + (a_n - f_n) \\end{split} \\end{equation*}\\] By choosing \\([f_1, f_2, \\dots, f_n]\\), \\(A+bF\\)’s eigenvalues can be arbitrarily set. (\\(7. \\Rightarrow 1.\\)) Prove by contradiction. Assume that \\((A, B)\\) is not controllable. Then from 2., there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\) and \\(t_1 &gt; 0\\), \\[\\begin{equation*} F(t) = v^* e^{At} B = 0, \\quad \\forall t \\in [0, t_1] \\end{equation*}\\] Now consider \\(F(z) = v^* e^{Az} B, z\\in \\mathcal{C}\\), which is a vector of analytic function in complex analysis. For a arbitrary \\(t_2 \\in (0, t_1)\\), we have \\(F^{(i)}(t_2) = 0, \\forall i \\in \\mathbb{N}\\). Then, by invoking the fact from complex analysis: “Let \\(G\\) a connected open set and \\(f: G \\rightarrow \\mathbb{C}\\) be analytic, then \\(f \\equiv 0\\) on \\(G\\), if and only if there is a point \\(a \\in G\\) such that \\(f^{(i)}(a) = 0, \\forall n \\in \\mathbb{N}\\)”, we have \\(f(z) \\equiv 0, \\forall z \\in \\mathbb{C}\\). On the other hand, however, \\(W_c \\succ 0\\) implies there exists \\(t_3 &gt; 0\\), such that for the above \\(v\\), we have \\(v^* e^{At_3} B \\ne 0\\). Contradiction! (\\(1. \\Rightarrow 7.\\)) Since \\((A, B)\\) is controllable, from 2., \\(W_c(t) \\succ 0, \\forall t\\). Therefore, \\(W_c \\succ 0\\). The existence and uniqueness of the solution for \\(AW_c + W_cA^* = -BB^*\\) can be obtained directly from the proof of Theorem C.3, by setting \\(Q\\) there to be positive semidefinite. C.2.3 Duality Although controllability and observability seemingly have no direct connections from their definitions C.2 and C.3, the following theorem (Chen 1984) states their tight relations. Theorem C.6 (Theorem of Duality) The pair \\((C,A)\\) is observable if and only if \\((A^*,C^*)\\) is controllable. Proof. We first show that \\((C,A)\\) is observable if and only if the \\(n \\times n\\) matrix \\(W_o(t) = \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau}\\) is positive definite (nonsingular) for any \\(t&gt;0\\): “\\(\\Longleftarrow\\)”: From (C.5), given initial state \\(x(0)\\) and the inputs \\(u(t)\\), \\(y(t)\\) can be expressed as \\[\\begin{equation*} y(t) = Ce^{At} x(0) + C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau + Du(t) \\end{equation*}\\] Define a known function \\(\\bar{y}(t)\\) as \\(y(t) - C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau - Du(t)\\) and we will get \\[\\begin{equation*} Ce^{At} x(0) = \\bar{y}(t) \\end{equation*}\\] Pre-multiply the above equation by \\(e^{A^*t}C^*\\) and integrate it over \\([0,t_1]\\) to yield \\[\\begin{equation*} (\\int_{0}^{t_1} e^{A^*t} C^*C e^{At} dt) x(0) = W_o(t_1) x(0) = \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] Since \\(W_o(t_1) \\succ 0\\), \\[\\begin{equation*} x(0) = W_o(t_1)^{-1} \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] can be observed. “\\(\\Longrightarrow\\)”: Prove by contradiction. Suppose \\((C,A)\\) is observable but there exists \\(t_1 &gt;0\\), s.t. \\(W_o(t_1)\\) is singular. This implies there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* W_o(t_1) v = 0 \\Longrightarrow Ce^{At} v \\equiv 0, \\ \\forall t \\in [0,t_1] \\end{equation*}\\] Similar to the proof of Theorem C.5 (\\(7. \\Rightarrow 1.\\)), we can use conclusions from complex analysis to claim that \\(Ce^{At} v \\equiv 0, \\forall t &gt;0\\). On the other hand, we set \\(u(t) \\equiv 0\\), which results in \\(y(t) = Ce^{At}x(0)\\). In this case \\(x(0) = 0\\) and \\(x(0) = v \\ne 0\\) will lead to the same output responses \\(y(t)\\) over \\(t&gt;0\\), which implies \\((C,A)\\) is not observable. Contradiction! Next we show the duality of controllability and observability: From (1) we know \\((C,A)\\) is controllable if and only of \\[\\begin{equation*} \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau} d\\tau = \\int_{0}^{t} e^{(A^*)\\tau} (C^*)^* (C^*) e^{(A^*)^*\\tau} d\\tau \\end{equation*}\\] is nonsingular for all \\(t &gt;0\\). The latter is exactly the definition of \\((A^*, C^*)\\)’s controllability Gramian \\(W_c(t)\\). C.2.4 Equivalent Statements for Observability With the Theorem of Duality C.6, we can directly write down the equivalent statements of observability without any additional proofs: Theorem C.7 (Equivalent Statements for Observability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((C, A)\\) is observable. The matrix \\[\\begin{equation*} W_o(t) := \\int_{0}^{t} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] is positive definite for any \\(t&gt;0\\). The observability matrix \\[\\begin{equation*} \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^2 \\\\ \\dots \\\\ CA^{n-1} \\end{bmatrix} \\end{equation*}\\] has full column rank. The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full column rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(y\\) be any eigenvalue and any corresponding right eigenvector of \\(A\\), i.e., \\(Ay = \\lambda y\\), then \\(Cy \\ne 0\\). The eigenvalues of \\(A+LC\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(L\\). \\((A^*, C^*)\\) is controllable. If, in addition, all eigenvalues of \\(A\\) have negative parts, then the unique solution of \\[\\begin{equation*} A^* W_o + W_o A = -C^* C \\end{equation*}\\] is positive definite. The solution is called the observability Gramian and can be expressed as \\[\\begin{equation*} W_o = \\int_{0}^{\\infty} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] C.3 Stabilizability And Detectability To define stabilizability and detectability of an LTI system, we first introduce the concept of system mode, which can be naturally derived from the fifth definition of controllability C.5 (observability C.7). Definition C.4 (System Mode) \\(\\lambda\\) is a mode of an LTI system, if it is an eigenvalue of \\(A\\). The mode \\(\\lambda\\) is said to be: stable, if \\(\\text{Re}\\lambda &lt; 0\\), controllable, if \\(x^* B \\ne 0\\) for all left eigenvectors of \\(A\\) associated with \\(\\lambda\\), observable, if \\(C x \\ne 0\\) for all right eigenvectors of \\(A\\) associated with \\(\\lambda\\). Otherwise, the mode is said to be uncontrollable (unobservable). With the concept of system mode, the fifth definition of controllability C.5 (observability C.7) can be restated as An LTI system is controllable (observable) if and only if all modes are controllable (observable). Stabilizability (detectability) is defined similarly via loosening part of controllability (observability) conditions. Definition C.5 (Stabilizability) An LTI system is said to be stabilizable if all of its unstable modes are controllable. Definition C.6 (Detectability) An LTI system is said to be detectable if all of its unstable modes are observable. Like in the case of controllability and observability, duality also holds in stabilizability and detectability. Moreover, similarity transformation will not influence an LTI system’s stabilizability and detectability. C.3.1 Equivalent Statements for Stabilizability Theorem C.8 (Equivalent Statements for Stabilizability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((A,B)\\) is stabilizable. For all \\(\\lambda\\) and \\(x\\) such that \\(x^* A = \\lambda x^*\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(x^* B \\ne 0\\). The matrix \\([A-\\lambda I, B]\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(F\\) such that \\(A+BF\\) are Hurwitz. Proof. (\\(1. \\Leftrightarrow 2.\\)) Directly from stabilizability’s definition. (\\(2. \\Leftrightarrow 3.\\)) If 2. holds but 3. not hold, then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* [A-\\lambda I, B] = 0 \\Leftrightarrow v^* A = \\lambda v^*, v^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Contradiction! Vice versa. (\\(4. \\Rightarrow 2.\\)) Prove by contradiction. Suppose there \\(x \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} x^* [A-\\lambda I, B] = 0 \\Leftrightarrow x^* A = \\lambda x^*, x^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Thus, for any \\(F\\), \\[\\begin{equation*} x^* (A+BF) = \\lambda x^*, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] On the other hand, suppose \\(A+BF\\) has \\(I\\) Jordon blocks, with each equipped with an eigenvalue \\(\\eta_i, i = 1\\dots I\\) (note that \\(\\eta_\\alpha\\) may be equal to \\(\\eta_\\beta\\), i.e., they are equivalent eigenvalues with different Jordon blocks). Since \\(A+BF\\)’s eigenvalues all have negative real parts, \\(\\text{Re} (\\eta_i) &lt; 0, i = 1\\dots I\\). For each \\(\\eta_i,i \\in \\left\\{1\\dots i\\right\\}\\), denote its \\(K_i\\) generalized left eigenvectors as \\(v_{i,1}, v_{i,2}, \\dots v_{i,K_i}\\). By definition, \\(\\sum_{i=1}^{I} K_i = n\\) and \\[\\begin{equation*} \\begin{split} v_{i,1}^* (A+BF) &amp; = v_{i,1}^* \\cdot \\eta_i \\\\ v_{i,2}^* (A+BF) &amp; = v_{i,1}^* + v_{i,2}^* \\cdot \\eta_i \\\\ &amp; \\vdots \\\\ v_{i,K_i}^* (A+BF) &amp; = v_{i,K_i-1}^* + v_{i,K_i}^* \\cdot \\eta_i \\end{split} \\end{equation*}\\] for all \\(i \\in \\left\\{1\\dots i\\right\\}\\). Also, \\(v_{i,k},i=1\\dots I, k=1\\dots K_i\\) are linearly independent and spans \\(\\mathbb{C}^n\\). Therefore, \\[\\begin{equation*} x^* = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\end{equation*}\\] which leads to \\[\\begin{equation*} \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* (A+BF) = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot \\lambda \\cdot v_{i,k}^* \\end{equation*}\\] Since \\(v_{i,k}\\)’s are \\(A+BF\\)’s generalized eigenvectors, we have \\[\\begin{equation*} \\begin{split} &amp; \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\cdot (A+BF) \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\xi_{i,1} \\cdot \\eta_i \\cdot v_{i,1}^* + \\sum_{k=2}^{K_i} \\xi_{i,k} (v_{i,k-1}^* + \\eta_i \\cdot v_{i,k}^* ) \\right\\} \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} (\\xi_{i,k}\\cdot \\eta_i + \\xi_{i,k+1}) v_{i,k}^* + \\xi_{i,K_i} \\cdot \\eta_i \\cdot v_{i,K_i}^* \\right\\} \\end{split} \\end{equation*}\\] Combining the above two equations: \\[\\begin{equation*} \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} \\left[ \\xi_{i,k}\\cdot (\\eta_i - \\lambda) + \\xi_{i,k+1} \\right] v_{i,k}^* + \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) \\cdot v_{i,K_i}^* = 0 \\right\\} \\end{equation*}\\] Since \\(v_{i,k}\\)’s are linearly independent, for any \\(i \\in \\left\\{i\\dots I\\right\\}\\): \\[\\begin{equation*} \\begin{split} \\xi_{i,1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,2} &amp; = 0 \\Rightarrow \\xi_{i,2} = (-1) \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda) \\\\ \\xi_{i,2} \\cdot (\\eta_i - \\lambda) + \\xi_{i,3} &amp; = 0 \\Rightarrow \\xi_{i,3} = (-1)^2 \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^2 \\\\ &amp; \\vdots \\\\ \\xi_{i,K_i-1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,K_i} &amp; = 0 \\Rightarrow \\xi_{i,K_i} = (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i-1} \\\\ \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) &amp; = 0 \\end{split} \\end{equation*}\\] Thus, \\[\\begin{equation*} (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i} = 0 \\end{equation*}\\] Denote \\(\\xi_{i,1}\\) as \\(r_1 e^{\\theta_1}\\), \\((\\eta_i - \\lambda)\\) as \\(r_2 e^{\\theta_2}\\). Since \\(\\text{Re} \\lambda \\ge 0, \\text{Re}(\\eta_i) &lt; 0\\), \\(r_2 &gt; 0\\). On the other hand, the following equation suggests \\[\\begin{equation*} r_1 r_2^{K_i-1} e^{j[\\theta_1 + \\theta_2 (K_i-1)]} = 0 \\end{equation*}\\] Thus, \\(r_1\\) has to be \\(0\\), which implies \\(\\xi_{i,1} = 0\\). By recursion, \\(\\xi_{i,k} = 0, \\forall k = 1\\dots K_i\\). Contradiction! (\\(1. \\Rightarrow 4.\\)) If \\((A,B)\\) is controllable, then from Theorem (thm:lticontrollable)’s sixth definition, we can freely assign the poles of \\(A+BF\\) via choosing \\(F\\) properly. Otherwise, if \\((A,B)\\) is uncontrollable, then from Corollary C.4 and proof of Theorem C.5 (\\(6. \\Rightarrow 1.\\)), there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] and \\[\\begin{equation*} \\text{det}(A+BF-\\lambda I) = \\underbrace{\\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1)}_{\\chi_c(\\lambda)} \\cdot \\underbrace{\\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2)}_{\\chi_{\\bar{c}}(\\lambda)} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}\\), \\(I_1\\) identity matrix of size \\(k_1\\), \\([F_1,F_2] = FT^{-1}\\), and \\(k_1 = \\text{rank} \\mathcal{C}\\). Additionally, \\((\\bar{A}_c, \\bar{B}_c)\\) is controllable. Thus, \\(\\chi_c(\\lambda)\\)’s zeros can be freely assigned by choosing proper \\(F\\), i.e., system modes with \\(\\chi_c(\\lambda)\\) is controllable, regardless of its stability. On the other hand, system modes with \\(\\chi_{\\bar{c}}(\\lambda)\\) must be stable. Otherwise, we cannot affect it by assigning \\(F\\), which is a contradiction to statement (1). Therefore, \\((TAT^{-1}, TB)\\) is stabilizable. Since similarity transformation does not change stabilizability, \\((A,B)\\) is stabilizable. C.3.2 Equivalent Statements for Detectability Thanks to duality, we can directly write down the equivalent statements of observability without any additional proofs: Theorem C.9 (Equivalent Statements for Detectability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((C,A)\\) is detectable. For all \\(\\lambda\\) and \\(x\\) such that \\(A x = \\lambda x\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(C x \\ne 0\\). The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(L\\) such that \\(A+LC\\) are Hurwitz. \\((A^*, C^*)\\) is stabilizable. References Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. when \\(A\\) is not diagonalizable, similar results can be derived via Jordan decomposition.↩︎ "],["algebraic-techniques-and-sum-of-squares.html", "D Algebraic Techniques and Sum-of-Squares D.1 Algebra", " D Algebraic Techniques and Sum-of-Squares D.1 Algebra D.1.1 Polynomials Definition D.1 (Monomial,Polynomial) A monomial in \\(x_1,\\cdots,x_n\\) is a product of the form \\(x_1^{\\alpha_1}\\cdot x_2^{\\alpha_2}\\cdots x_n^{\\alpha_n}\\). The total degree of the monomial is \\(\\alpha_1+\\cdots+\\alpha_n\\). A polynomial \\(f\\) in \\(x_1,\\cdots,x_n\\) with coefficients in \\(\\mathbb{R}\\) is a finite linear combination (with coefficients in \\(\\mathbb{R}\\)) of monomials. We will write a polynomial \\(f\\) in the form: \\(\\Sigma_\\alpha a_\\alpha x^\\alpha\\). where the sum is over a finite number of n-tuples \\(\\alpha = (\\alpha_1,\\cdots,\\alpha_n)\\). The set of all polynomials in \\(x_1,\\cdots,x_n\\) with coefficients in \\(\\mathbb{R}\\) is denoted \\(\\mathbb{R}[x_1,\\cdots,x_n]\\) Definition D.2 (Affine Variety) Let \\(f_1,\\cdots,f_s\\in\\mathbb{R}[x_1,\\cdots,x_n]\\), we set \\[V(f_1,\\cdots,f_s) = \\{(a_1,\\cdots,a_n)\\in\\mathbb{R}^n|f_i(a_1,\\cdots a_n)=0 \\quad \\forall i\\leq i\\leq s\\}\\] We call \\(V(f_1,\\cdots,f_s)\\) the affine variety defined bt \\(f_1,\\cdots,f_s\\) Definition D.3 (Ideal) A subset \\(I\\subset \\mathbb{R}[x_1,\\cdots,x_n]\\) is an ideal if it satisfies: (i) Contains additive identity: \\(0\\in I\\) (ii) Closed under addition: For all \\(f,g\\in I\\), \\(f+g\\in I\\) (iii) Absorption of multiplication: If \\(f\\in I\\) and \\(h\\in \\mathbb{R}[x_1,\\cdots,x_n]\\), then \\(hf\\in I\\) Definition D.4 (Sum of squares,Quadratic Module and Preordering) Sum of squares D.1.2 Representation of nonnegative polynomial: Univariate case Theorem D.1 (Global version) A polynomial \\(p\\in\\mathbb{R}[x]\\) of even degree is nonnegative if and only if it can be written as a sum of squares of other polynomials, i.e., \\(p(x) = \\Sigma^k_{i=1}[h_i(x)]^2\\), with \\(h_i\\in R[x], i = 1,\\cdots, k\\). Theorem D.2 (Compact interval version) A polynomial \\(p\\in\\mathbb{R}[x]\\) of even degree is nonnegative if and only if it can be written as a sum of squares of other polynomials, i.e., \\(p(x) = \\Sigma^k_{i=1}[h_i(x)]^2\\), with \\(h_i\\in R[x], i = 1,\\cdots, k\\). "],["the-kalman-yakubovich-lemma.html", "E The Kalman-Yakubovich Lemma", " E The Kalman-Yakubovich Lemma Lemma E.1 (Kalman-Yakubovich) Consider a controllable linear time-invariant system \\[ \\dot{x} = A x + b u \\\\ y = c^T x. \\] The transfer function \\[ h(p) = c^T (p I - A)^{-1} b \\] is strictly positive real (SPR) if and only if there exist positive definite matrices \\(P\\) and \\(Q\\) such that \\[ A^T P + P A = - Q \\\\ Pb = c. \\] "],["feedbacklinearization.html", "F Feedback Linearization", " F Feedback Linearization "],["slidingcontrol.html", "G Sliding Control", " G Sliding Control "],["references.html", "References", " References Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. “Reinforcement Learning: Theory and Algorithms.” CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 32. Arnold, William F, and Alan J Laub. 1984. “Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations.” Proceedings of the IEEE 72 (12): 1746–54. Bemporad, Alberto, Manfred Morari, Vivek Dua, and Efstratios N Pistikopoulos. 2002. “The Explicit Linear Quadratic Regulator for Constrained Systems.” Automatica 38 (1): 3–20. Bertsekas, Dimitri. 1972. “Infinite Time Reachability of State-Space Regions by Using Feedback Control.” IEEE Transactions on Automatic Control 17 (5): 604–13. ———. 2012. Dynamic Programming and Optimal Control: Volume i. Vol. 1. Athena scientific. Blekherman, Grigoriy, Pablo A Parrilo, and Rekha R Thomas. 2012. Semidefinite Optimization and Convex Algebraic Geometry. SIAM. Borrelli, Francesco, Alberto Bemporad, and Manfred Morari. 2017. Predictive Control for Linear and Hybrid Systems. Cambridge University Press. Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Dai, Hongkai, and Frank Permenter. 2023. “Convex Synthesis and Verification of Control-Lyapunov and Barrier Functions with Input Constraints.” In 2023 American Control Conference (ACC), 4116–23. IEEE. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Dawson, Charles, Sicun Gao, and Chuchu Fan. 2023. “Safe Control with Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction Methods for Robotics and Control.” IEEE Transactions on Robotics. Gilbert, Elmer G, and K Tin Tan. 1991. “Linear Systems with State and Control Constraints: The Theory and Application of Maximal Output Admissible Sets.” IEEE Transactions on Automatic Control 36 (9): 1008–20. Kang, Shucheng, Yuxiao Chen, Heng Yang, and Marco Pavone. 2023. “Verification and Synthesis of Robust Control Barrier Functions: Multilevel Polynomial Optimization and Semidefinite Relaxation.” In 2023 62nd IEEE Conference on Decision and Control (CDC). Kelly, Matthew. 2017. “An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation.” SIAM Review 59 (4): 849–904. Kolmanovsky, Ilya, Elmer G Gilbert, et al. 1998. “Theory and Computation of Disturbance Invariant Sets for Discrete-Time Linear Systems.” Mathematical Problems in Engineering 4: 317–67. Lasserre, Jean B. 2001. “Global Optimization with Polynomials and the Problem of Moments.” SIAM Journal on Optimization 11 (3): 796–817. Lasserre, Jean Bernard. 2009. Moments, Positive Polynomials and Their Applications. Vol. 1. World Scientific. Levine, Nir, Tom Zahavy, Daniel J Mankowitz, Aviv Tamar, and Shie Mannor. 2017. “Shallow Updates for Deep Reinforcement Learning.” Advances in Neural Information Processing Systems 30. Magron, Victor, and Jie Wang. 2023. Sparse Polynomial Optimization: Theory and Practice. World Scientific. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. Murray, Riley, Venkat Chandrasekaran, and Adam Wierman. 2021. “Signomial and Polynomial Optimization via Relative Entropy and Partial Dualization.” Mathematical Programming Computation 13: 257–95. Nie, Jiawang. 2023. Moment and Polynomial Optimization. SIAM. Nocedal, Jorge, and Stephen J Wright. 1999. Numerical Optimization. Springer. Sontag, Eduardo D. 1983. “A Lyapunov-Like Characterization of Asymptotic Controllability.” SIAM Journal on Control and Optimization 21 (3): 462–71. Vinograd, Robert Èlyukimovich. 1957. “Inapplicability of the Method of Characteristic Exponents to the Study of Non-Linear Differential Equations.” Matematicheskii Sbornik 83 (4): 431–38. Wang, Jie. 2022. “Nonnegative Polynomials and Circuit Polynomials.” SIAM Journal on Applied Algebra and Geometry 6 (2): 111–33. Yang, Alan, and Stephen Boyd. 2023. “Value-Gradient Iteration with Quadratic Approximate Value Functions.” arXiv Preprint arXiv:2307.07086. Yang, Heng, and Luca Carlone. 2022. “Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite Relaxations and Scalable Global Optimization.” IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (3): 2816–34. Yang, Heng, Ling Liang, Luca Carlone, and Kim-Chuan Toh. 2022. “An Inexact Projected Gradient Method with Rounding and Lifting by Nonlinear Programming for Solving Rank-One Semidefinite Relaxation of Polynomial Optimization.” Mathematical Programming, 1–64. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
