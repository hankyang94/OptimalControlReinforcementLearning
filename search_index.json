[["index.html", "Optimal Control and Reinforcement Learning Preface Feedback Offerings", " Optimal Control and Reinforcement Learning Heng Yang 2025-10-06 Preface This is the textbook for Harvard ES/AM 158: Introduction to Optimal Control and Reinforcement Learning. Feedback I would like to invite you to provide feedback to the textbook via inline comments with Hypothesis: Go to Hypothesis and create an account Install the Chrome extension of Hypothesis Provide public comments to textbook contents and I will try to address them Offerings 2025 Fall Time: Mon/Wed 2:15 - 3:30pm Location: SEC 1.413 Instructor: Heng Yang Teaching Fellow: Haoyu Han, Han Qi [Syllabus], [Problem Sets], [Canvas] 2023 Fall The course was previously offered as Introduction to Optimal Control and Estimation. Starting Fall 2025, contents about reinforcement learning have been added to the course. "],["mdp.html", "Chapter 1 Markov Decision Process 1.1 Finite-Horizon MDP 1.2 Infinite-Horizon MDP", " Chapter 1 Markov Decision Process Optimal control (OC) and reinforcement learning (RL) address the problem of making optimal decisions in the presence of a dynamic environment. In optimal control, this dynamic environment is often referred to as a plant or a dynamical system. In reinforcement learning, it is modeled as a Markov decision process (MDP). The goal in both fields is to evaluate and design decision-making strategies that optimize long-term performance: RL typically frames this as maximizing a long-term reward. OC often formulates it as minimizing a long-term cost. The emphasis on long-term evaluation is crucial. Because the environment evolves over time, decisions that appear beneficial in the short term may lead to poor long-term outcomes and thus be suboptimal. With this motivation, we now formalize the framework of Markov Decision Processes (MDPs), which are discrete-time stochastic dynamical systems. 1.1 Finite-Horizon MDP We begin with finite-horizon MDPs and introduce infinite-horizon MDPs in the following section. An abstract definition of the finite-horizon case will be presented first, followed by illustrative examples. A finite-horizon MDP is given by the following tuple: \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, T), \\] where \\(\\mathcal{S}\\): state space (set of all possible states) \\(\\mathcal{A}\\): action space (set of all possible actions) \\(P(s&#39; \\mid s, a)\\): probability of transitioning to state \\(s&#39;\\) from state \\(s\\) under action \\(a\\) (i.e., dynamics) \\(R(s,a)\\): reward of taking action \\(a\\) in state \\(s\\) \\(T\\): horizon, a positive integer For now, let us assume both the state space and the action space are discrete and have a finite number of elements. In particular, denote the number of elements in \\(\\mathcal{S}\\) as \\(|\\mathcal{S}|\\), and the number of elements in \\(\\mathcal{A}\\) as \\(|\\mathcal{A}|\\). This is also referred to as a tabular MDP. Policy. Decision-making in MDPs is represented by policies. A policy is a function that, given any state, outputs a distribution of actions: \\(\\pi: \\mathcal{S} \\mapsto \\Delta(\\mathcal{A})\\). That is, \\(\\pi(a \\mid s)\\) returns the probability of taking action \\(a\\) in state \\(s\\). In finite-horizon MDPs, we consider a tuple of policies: \\[\\begin{equation} \\pi = (\\pi_0, \\dots, \\pi_t, \\dots, \\pi_{T-1}), \\tag{1.1} \\end{equation}\\] where each \\(\\pi_t\\) denotes the policy at step \\(t \\in [0,T-1]\\). Trajectory and Return. Given an initial state \\(s_0 \\in \\mathcal{S}\\) and a policy \\(\\pi\\), the MDP will evolve as Start at state \\(s_0\\) Take action \\(a_0 \\sim \\pi_0(a \\mid s_0)\\) following policy \\(\\pi_0\\) Collect reward \\(r_0 = R(s_0, a_0)\\) (assume \\(R\\) is deterministic) Transition to state \\(s_1 \\sim P(s&#39; \\mid s_0, a_0)\\) following the dynamics Go to step 2 and continue until reaching state \\(s_T\\) This evolution generates a trajectory of states, actions, and rewards: \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T). \\] The cumulative reward of this trajectory is \\(g_0 = \\sum_{t=0}^{T-1} r_t\\), which is called the return of the trajectory. Clearly, \\(g_0\\) is a random variable due to the stochasticity of both the policy and the dynamics. Similarly, if the state at time \\(t\\) is \\(s_t\\), we denote: \\[ g_t = r_t + \\dots + r_{T-1} \\] as the return of the policy starting at \\(s_t\\). 1.1.1 Value Functions State-Value Function. Given a policy \\(\\pi\\) as in (1.1), which states are preferable at time \\(t\\)? The (time-indexed) state-value function assigns to each \\(s\\in\\mathcal{S}\\) the expected return from \\(t\\) onward when starting in \\(s\\) and following \\(\\pi\\) thereafter. Formally, define \\[\\begin{equation} V_t^\\pi(s) := \\mathbb{E} \\left[g_t \\mid s_t=s\\right] = \\mathbb{E} \\left[\\sum_{i=t}^{T-1} R(s_i,a_i) \\middle| s_t=s,a_i\\sim \\pi_i(\\cdot\\mid s_i), s_{i+1}\\sim P(\\cdot\\mid s_i,a_i)\\right]. \\tag{1.2} \\end{equation}\\] The expectation is over the randomness induced by both the policy and the dynamics. Thus, if \\(V_t^\\pi(s_1)&gt;V_t^\\pi(s_2)\\), then at time \\(t\\) under policy \\(\\pi\\) it is better in expectation to be in \\(s_1\\) than in \\(s_2\\) because the former yields a larger expected return. \\(V^{\\pi}_t(s)\\): given policy \\(\\pi\\), how good is it to start in state \\(s\\) at time \\(t\\)? Action-Value Function. Similarly, the action-value function assigns to each state-action pair \\((s,a)\\in\\mathcal{S}\\times\\mathcal{A}\\) the expected return obtained by starting in state \\(s\\), taking action \\(a\\) first, and then following policy \\(\\pi\\) thereafter: \\[\\begin{equation} \\begin{split} Q_t^\\pi(s,a) := &amp; \\mathbb{E} \\left[R(s,a) + g_{t+1} \\mid s_{t+1} \\sim P(\\cdot \\mid s,a)\\right] \\\\ = &amp; \\mathbb{E} \\left[R(s,a) + \\sum_{i=t+1}^{T-1} R(s_i, a_i) \\middle| s_{t+1} \\sim P(\\cdot \\mid s,a) \\right]. \\end{split} \\tag{1.3} \\end{equation}\\] The key distinction is that the action-value function evaluates the return when the first action may deviate from policy \\(\\pi\\), whereas the state-value function assumes strict adherence to \\(\\pi\\). This flexibility makes the action-value function central to improving \\(\\pi\\), since it reveals whether alternative actions can yield higher returns. \\(Q^{\\pi}_t(s,a)\\): At time \\(t\\), how good is it to take action \\(a\\) in state \\(s\\), then follow the policy \\(\\pi\\)? It is easy to verify that the state-value function and the action-value function satisfy: \\[\\begin{align} V_t^{\\pi}(s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi_t(a \\mid s) Q_t^{\\pi}(s,a), \\tag{1.4} \\\\ Q_t^{\\pi}(s,a) &amp; = R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}_{t+1} (s&#39;). \\tag{1.5} \\end{align}\\] From these two equations, we can derive the Bellman Consistency equations. Proposition 1.1 (Bellman Consistency (Finite Horizon)) The state-value function \\(V^{\\pi}_t(\\cdot)\\) in (1.2) satisfies the following recursion: \\[\\begin{equation} \\begin{split} V^{\\pi}_t(s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi_t(a\\mid s) \\left( R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}_{t+1} (s&#39;) \\right) \\\\ &amp; =: \\mathbb{E}_{a \\sim \\pi_t(\\cdot \\mid s)} \\left[ R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} [V^{\\pi}_{t+1}(s&#39;)] \\right]. \\end{split} \\tag{1.6} \\end{equation}\\] Similarly, the action-value function \\(Q^{\\pi}_t(s,a)\\) in (1.3) satisfies the following recursion: \\[\\begin{equation} \\begin{split} Q^{\\pi}_t (s, a) &amp; = R(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) \\left( \\sum_{a&#39; \\in \\mathcal{A}} \\pi_{t+1}(a&#39; \\mid s&#39;) Q^{\\pi}_{t+1}(s&#39;, a&#39;)\\right) \\\\ &amp; =: R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[\\mathbb{E}_{a&#39; \\sim \\pi_{t+1}(\\cdot \\mid s&#39;)} [Q^{\\pi}_{t+1}(s&#39;, a&#39;)] \\right]. \\end{split} \\tag{1.7} \\end{equation}\\] 1.1.2 Policy Evaluation The Bellman consistency result in Proposition 1.1 is fundamental because it directly yields an algorithm for evaluating a given policy \\(\\pi\\)—that is, for computing its state-value and action-value functions—provided the transition dynamics of the MDP are known. Policy evaluation for the state-value function proceeds as follows: Initialization: set \\(V^{\\pi}_T(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). Backward recursion: for \\(t = T-1, T-2, \\dots, 0\\), update each \\(s \\in \\mathcal{S}\\) by \\[ V^{\\pi}_{t}(s) = \\mathbb{E}_{a \\sim \\pi_t(\\cdot \\mid s)} \\left[ R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\big[ V^{\\pi}_{t+1}(s&#39;) \\big] \\right]. \\] Similarly, policy evaluation for the action-value function is given by: Initialization: set \\(Q^{\\pi}_T(s,a) = 0\\) for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). Backward recursion: for \\(t = T-1, T-2, \\dots, 0\\), update each \\((s,a) \\in \\mathcal{S}\\times\\mathcal{A}\\) by \\[ Q^{\\pi}_t(s,a) = R(s, a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[ \\mathbb{E}_{a&#39; \\sim \\pi_{t+1}(\\cdot \\mid s&#39;)} \\big[ Q^{\\pi}_{t+1}(s&#39;, a&#39;) \\big] \\right]. \\] The essential feature of this algorithm is its backward-in-time recursion: the value functions are first set at the terminal horizon \\(T\\), and then propagated backward step by step through the Bellman consistency equations. Example 1.1 (MDP, Transition Graph, and Policy Evaluation) It is often useful to visualize small MDPs as transition graphs, where states are represented by nodes and actions are represented by directed edges connecting those nodes. As a simple illustrative example, consider a robot navigating on a two-state grid. At each step, the robot can either Stay in its current state or Move to the other state. This finite-horizon MDP is fully specified by the tuple of states, actions, transition dynamics, rewards, and horizon: States: \\(\\mathcal{S} = \\{\\alpha, \\beta \\}\\) Actions: \\(\\mathcal{A} = \\{\\text{Move} , \\text{Stay} \\}\\) Transition dynamics: we can specify the transition dynamics in the following table State \\(s\\) Action \\(a\\) Next State \\(s&#39;\\) Probability \\(P(s&#39; \\mid s, a)\\) \\(\\alpha\\) Stay \\(\\alpha\\) 1 \\(\\alpha\\) Move \\(\\beta\\) 1 \\(\\beta\\) Stay \\(\\beta\\) 1 \\(\\beta\\) Move \\(\\alpha\\) 1 Reward: \\(R(s,a)=1\\) if \\(a = \\text{Move}\\) and \\(R(s,a)=0\\) if \\(a = \\text{Stay}\\) Horizon: \\(T=2\\). This MDP can be represented by the transition graph in Fig. 1.1. Note that for this MDP, the transition dynamics is deterministic. We will see a stochastic MDP soon. Figure 1.1: A Simple Transition Graph. At time \\(t=0\\), if the robot starts at \\(s_0 = \\alpha\\), first chooses action \\(a_0 = \\text{Move}\\), and then chooses action \\(a_1 = \\text{Stay}\\), the resulting trajectory is \\[ \\tau = (\\alpha, \\text{Move}, +1, \\beta, \\text{Stay}, 0, \\beta). \\] The return of this trajectory is: \\[ g_0 = +1 + 0 = +1. \\] Policy Evaluation. Given a policy \\[\\begin{equation} \\pi = (\\pi_0, \\pi_1), \\quad \\pi_0(a \\mid s) = \\begin{cases} 0.5 &amp; a = \\text{Move} \\\\ 0.5 &amp; a = \\text{Stay} \\end{cases}, \\quad \\pi_1( a \\mid s) = \\begin{cases} 0.8 &amp; a = \\text{Move} \\\\ 0.2 &amp; a = \\text{Stay} \\end{cases}. \\end{equation}\\] We can use the Bellman consistency equations to compute the state-value function. We first initialize: \\[ V^{\\pi}_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\] where the first row contains the value at \\(s = \\alpha\\) and the second row contains the value at \\(s = \\beta\\). We then perform the backward recursion for \\(t=1\\). For \\(s = \\alpha\\), we have \\[\\begin{equation} V^{\\pi}_1(\\alpha) = \\begin{bmatrix} \\pi_1(\\text{Move} \\mid \\alpha) \\\\ \\pi_1(\\text{Stay} \\mid \\alpha) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\alpha, \\text{Move}) + V^{\\pi}_2(\\beta) \\\\ R(\\alpha, \\text{Stay}) + V^{\\pi}_2(\\alpha) \\end{bmatrix} = \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 0.8 \\end{equation}\\] For \\(s = \\beta\\), we have \\[\\begin{equation} V^{\\pi}_1(\\beta) = \\begin{bmatrix} \\pi_1(\\text{Move} \\mid \\beta) \\\\ \\pi_1(\\text{Stay} \\mid \\beta) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\beta, \\text{Move}) + V^{\\pi}_2(\\alpha) \\\\ R(\\beta, \\text{Stay}) + V^{\\pi}_2(\\beta) \\end{bmatrix} = \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 0.8. \\end{equation}\\] Therefore, we have \\[ V^{\\pi}_1 = \\begin{bmatrix} 0.8 \\\\ 0.8 \\end{bmatrix}. \\] We then proceed to the backward recursion for \\(t=0\\): \\[\\begin{align} V_0^{\\pi}(\\alpha) &amp; = \\begin{bmatrix} \\pi_0(\\text{Move} \\mid \\alpha) \\\\ \\pi_0(\\text{Stay} \\mid \\alpha) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\alpha, \\text{Move}) + V^{\\pi}_1(\\beta) \\\\ R(\\alpha, \\text{Stay}) + V^{\\pi}_1(\\alpha) \\end{bmatrix} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1.8 \\\\ 0.8 \\end{bmatrix} = 1.3. \\\\ V_0^{\\pi}(\\beta) &amp; = \\begin{bmatrix} \\pi_0(\\text{Move} \\mid \\beta) \\\\ \\pi_0(\\text{Stay} \\mid \\beta) \\end{bmatrix}^{\\top} \\begin{bmatrix} R(\\beta, \\text{Move}) + V^{\\pi}_0(\\alpha) \\\\ R(\\beta, \\text{Stay}) + V^{\\pi}_0(\\beta) \\end{bmatrix} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}^{\\top} \\begin{bmatrix} 1.8 \\\\ 0.8 \\end{bmatrix} = 1.3. \\end{align}\\] Therefore, the state-value function at \\(t=0\\) is \\[ V^{\\pi}_0 = \\begin{bmatrix} 1.3 \\\\ 1.3 \\end{bmatrix}. \\] You are encouraged to carry out the similar calculations for the action-value function. The toy example was small enough to carry out policy evaluation by hand; in realistic MDPs, we will need the help from computers. Consider now an MDP whose transition graph is shown in Fig. 1.2. This example is adapted from here. Figure 1.2: Hangover Transition Graph. This MDP has six states: \\[ \\mathcal{S} = \\{\\text{Hangover}, \\text{Sleep}, \\text{More Sleep}, \\text{Visit Lecture}, \\text{Study}, \\text{Pass Exam} \\}, \\] and two actions: \\[ \\mathcal{A} = \\{\\text{Lazy}, \\text{Productive} \\}. \\] The stochastic transition dynamics are labeled in the transition graph. For example, at state “Hangover”, taking action “Productive” will lead to state “Visit Lecture” with probability \\(0.3\\) and state “Hangover” with probability \\(0.7\\). The rewards of the MDP are defined as: \\[ R(s,a) = \\begin{cases} +1 &amp; s = \\text{Pass Exam} \\\\ -1 &amp; \\text{otherwise}. \\end{cases}. \\] Policy Evaluation. Consider a time-invariant random policy \\[ \\pi = \\{\\pi_0,\\dots,\\pi_{T-1} \\}, \\quad \\pi_t(a \\mid s) = \\begin{cases} \\alpha &amp; a = \\text{Lazy} \\\\ 1 - \\alpha &amp; a = \\text{Productive} \\end{cases}, \\] that takes “Lazy” with probability \\(\\alpha\\) and “Productive” with probability \\(1-\\alpha\\). The following Python code performs policy evaluation for this MDP, with \\(T=10\\) and \\(\\alpha = 0.4\\). # Finite-horizon policy evaluation for the Hangover MDP from collections import defaultdict from typing import Dict, List, Tuple State = str Action = str # --- MDP spec --------------------------------------------------------------- S: List[State] = [ &quot;Hangover&quot;, &quot;Sleep&quot;, &quot;More Sleep&quot;, &quot;Visit Lecture&quot;, &quot;Study&quot;, &quot;Pass Exam&quot; ] A: List[Action] = [&quot;Lazy&quot;, &quot;Productive&quot;] # P[s, a] -&gt; list of (s_next, prob) P: Dict[Tuple[State, Action], List[Tuple[State, float]]] = { # Hangover (&quot;Hangover&quot;, &quot;Lazy&quot;): [(&quot;Sleep&quot;, 1.0)], (&quot;Hangover&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.3), (&quot;Hangover&quot;, 0.7)], # Sleep (&quot;Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Sleep&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.6), (&quot;More Sleep&quot;, 0.4)], # More Sleep (&quot;More Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;More Sleep&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 0.5), (&quot;More Sleep&quot;, 0.5)], # Visit Lecture (&quot;Visit Lecture&quot;, &quot;Lazy&quot;): [(&quot;Study&quot;, 0.8), (&quot;Pass Exam&quot;, 0.2)], (&quot;Visit Lecture&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 1.0)], # Study (&quot;Study&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Study&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 0.9), (&quot;Study&quot;, 0.1)], # Pass Exam (absorbing) (&quot;Pass Exam&quot;, &quot;Lazy&quot;): [(&quot;Pass Exam&quot;, 1.0)], (&quot;Pass Exam&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 1.0)], } def R(s: State, a: Action) -&gt; float: &quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot; return 1.0 if s == &quot;Pass Exam&quot; else -1.0 # --- Policy: time-invariant, state-independent ------------------------------ def pi(a: Action, s: State, alpha: float) -&gt; float: &quot;&quot;&quot;pi(a|s): Lazy with prob alpha, Productive with prob 1-alpha.&quot;&quot;&quot; return alpha if a == &quot;Lazy&quot; else (1.0 - alpha) # --- Policy evaluation ------------------------------------------------------- def policy_evaluation(T: int, alpha: float): &quot;&quot;&quot; Compute {V_t(s)} and {Q_t(s,a)} for t=0..T with terminal condition V_T = Q_T = 0. Returns: V: Dict[int, Dict[State, float]] Q: Dict[int, Dict[Tuple[State, Action], float]] &quot;&quot;&quot; assert T &gt;= 0 # sanity: probabilities sum to 1 for each (s,a) for key, rows in P.items(): total = sum(p for _, p in rows) if abs(total - 1.0) &gt; 1e-9: raise ValueError(f&quot;Probabilities for {key} sum to {total}, not 1.&quot;) V: Dict[int, Dict[State, float]] = defaultdict(dict) Q: Dict[int, Dict[Tuple[State, Action], float]] = defaultdict(dict) # Terminal boundary for s in S: V[T][s] = 0.0 for a in A: Q[T][(s, a)] = 0.0 # Backward recursion for t in range(T - 1, -1, -1): for s in S: # First compute Q_t(s,a) for a in A: exp_next = sum(p * V[t + 1][s_next] for s_next, p in P[(s, a)]) Q[t][(s, a)] = R(s, a) + exp_next # Then V_t(s) = E_{a~pi}[Q_t(s,a)] V[t][s] = sum(pi(a, s, alpha) * Q[t][(s, a)] for a in A) return V, Q # --- Example run ------------------------------------------------------------- if __name__ == &quot;__main__&quot;: T = 10 # horizon alpha = 0.4 # probability of choosing Lazy V, Q = policy_evaluation(T=T, alpha=alpha) # Print V_0 print(f&quot;V_0(s) with T={T}, alpha={alpha}:&quot;) for s in S: print(f&quot; {s:13s}: {V[0][s]: .3f}&quot;) The code returns the following state values at \\(t=0\\): \\[\\begin{equation} V^{\\pi}_0 = \\begin{bmatrix} -3.582 \\\\ -2.306 \\\\ -2.180 \\\\ 1.757 \\\\ 2.939 \\\\ 10 \\end{bmatrix}, \\tag{1.8} \\end{equation}\\] where the ordering of the states follows that defined in \\(\\mathcal{S}\\). You can find the code here. 1.1.3 Principle of Optimality Every policy \\(\\pi\\) induces a value function \\(V_0^{\\pi}\\) that can be evaluated by policy evaluation (assuming the transition dynamics are known). The goal of reinforcement learning is to find an optimal policy that maximizes the value function with respect to a given initial state distribution: \\[\\begin{equation} V^\\star_0 = \\max_{\\pi}\\; \\mathbb{E}_{s_0 \\sim \\mu(\\cdot)} \\big[ V_0^{\\pi}(s_0) \\big], \\tag{1.9} \\end{equation}\\] where we have used the superscript “\\(\\star\\)” to denote the optimality of the value function. \\(V^\\star_0\\) is often known as the optimal value function. At first glance, (1.9) appears daunting: a naive approach would enumerate all stochastic policies \\(\\pi\\), evaluate their value functions, and select the best. A central result in reinforcement learning and optimal control—rooted in the principle of optimality—is that the optimal value functions satisfy a Bellman-style recursion, analogous to Proposition 1.1. This Bellman optimality recursion enables backward computation of the optimal value functions without enumerating policies. Theorem 1.1 (Bellman Optimality (Finite Horizon, State-Value)) Consider a finite-horizon MDP \\(\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,T)\\) with finite state and action sets and bounded rewards. Define the optimal value functions \\(\\{V_t^\\star\\}_{t=0}^{T}\\) by the following Bellman optimality recursion \\[\\begin{equation} \\begin{split} V_T^\\star(s)&amp; \\equiv 0, \\\\ V_t^\\star(s)&amp; = \\max_{a\\in\\mathcal{A}}\\Big\\{ R(s,a)\\;+\\;\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;)\\Big\\},\\ t=T-1,\\ldots,0. \\end{split} \\tag{1.10} \\end{equation}\\] Then, the optimal value functions are optimal in the sense of statewise dominance: \\[\\begin{equation} V_t^{\\star}(s)\\;\\ge\\; V_t^{\\pi}(s) \\quad\\text{for all policies }\\pi,\\; s\\in\\mathcal{S},\\; t=0,\\ldots,T. \\tag{1.11} \\end{equation}\\] Moreover, the deterministic policy \\(\\pi^\\star=(\\pi^\\star_0,\\ldots,\\pi^\\star_{T-1})\\) with \\[\\begin{equation} \\begin{split} \\pi_t^\\star(s)\\;\\in\\;\\arg\\max_{a\\in\\mathcal{A}} \\Big\\{ R(s,a)\\;+\\;\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;)\\Big\\}, \\\\ \\text{for any } s\\in\\mathcal{S},\\; t=0,\\dots,T-1 \\end{split} \\tag{1.12} \\end{equation}\\] is optimal, where ties can be broken by any fixed rule. Proof. We first show that the value functions defined by the Bellman optimality recursion (1.10) are optimal in the sense that they dominate the value functions of any other policy. The proof proceeds by backward induction. Base case (\\(t=T\\)). For every \\(s\\in\\mathcal{S}\\), \\[ V^\\star_T(s)\\;=\\;0\\;=\\;V_T^{\\pi}(s), \\] so \\(V^\\star_T(s)\\ge V_T^{\\pi}(s)\\) holds trivially. Inductive step. Assume \\(V^\\star_{t+1}(s)\\ge V^{\\pi}_{t+1}(s)\\) for all \\(s\\in\\mathcal{S}\\). Then, for any \\(s\\in\\mathcal{S}\\), \\[\\begin{align*} V_t^{\\pi}(s) &amp;= \\sum_{a\\in\\mathcal{A}} \\pi_t(a\\mid s)\\!\\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\pi}(s&#39;)\\right) \\\\ &amp;\\le \\sum_{a\\in\\mathcal{A}} \\pi_t(a\\mid s)\\!\\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\star}(s&#39;)\\right) \\\\ &amp;\\le \\max_{a\\in\\mathcal{A}} \\left(R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^{\\star}(s&#39;)\\right) \\;=\\; V_t^\\star(s), \\end{align*}\\] where the first inequality uses the induction hypothesis and the second uses that an expectation is bounded above by a maximum. Hence \\(V_t^\\star(s)\\ge V_t^{\\pi}(s)\\) for all \\(s\\), completing the induction. Therefore, \\(\\{V_t^\\star\\}_{t=0}^T\\) dominates the value functions attainable by any policy. Next, we show that \\(\\{V_t^\\star\\}\\) is attainable by some policy. Since \\(\\mathcal{A}\\) is finite (tabular setting), the maximizer in the Bellman optimality operator exists for every \\((t,s)\\); thus we can define a (deterministic) greedy policy \\[ \\pi_t^\\star(s)\\;\\in\\;\\arg\\max_{a\\in\\mathcal{A}} \\Big\\{ R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;) \\Big\\}. \\] A simple backward induction then shows \\(V_t^{\\pi^\\star}(s)=V_t^\\star(s)\\) for all \\(t\\) and \\(s\\): at \\(t=T\\) both are \\(0\\), and if \\(V_{t+1}^{\\pi^\\star}=V_{t+1}^\\star\\), then by construction of \\(\\pi_t^\\star\\) the Bellman equality yields \\(V_t^{\\pi^\\star}=V_t^\\star\\). Consequently, the optimal value functions are achieved by the greedy (deterministic) policy \\(\\pi^\\star\\). Corollary 1.1 (Bellman Optimality (Finite Horizon, Action-Value)) Given the optimal (state-)value functions \\(V^{\\star}_{t},t=0,\\dots,T\\), define the optimal action-value function \\[\\begin{equation} Q_t^\\star(s,a)\\;=\\;R(s,a)+\\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\,V_{t+1}^\\star(s&#39;), \\quad t=0,\\dots,T-1. \\tag{1.13} \\end{equation}\\] Then we have \\[\\begin{equation} V_t^\\star(s)=\\max_{a\\in\\mathcal{A}} Q_t^\\star(s,a),\\qquad \\pi_t^\\star(s)\\in\\arg\\max_{a\\in\\mathcal{A}} Q_t^\\star(s,a). \\tag{1.14} \\end{equation}\\] The optimal action-value functions satisfy: \\[\\begin{equation} \\begin{split} Q_T^\\star(s,a) &amp; \\equiv 0,\\\\ Q_t^\\star(s,a) &amp; = R(s,a) \\;+\\; \\mathbb{E}_{s&#39;\\sim P(\\cdot\\mid s,a)} \\!\\left[ \\max_{a&#39;\\in\\mathcal{A}} Q_{t+1}^\\star(s&#39;,a&#39;) \\right], \\quad t=T-1,\\ldots,0. \\end{split} \\tag{1.15} \\end{equation}\\] 1.1.4 Dynamic Programming The principle of optimality in Theorem 1.1 yields a constructive procedure to compute the optimal value functions and an associated deterministic optimal policy. This backward-induction procedure is the dynamic programming (DP) algorithm. Dynamic programming (finite horizon). Initialization. Set \\(V_T^\\star(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). Backward recursion. For \\(t = T-1, T-2, \\dots, 0\\): Optimal value: for each \\(s \\in \\mathcal{S}\\), \\[ V_t^\\star(s) = \\max_{a \\in \\mathcal{A}} \\left\\{ R(s,a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s,a)} \\big[ V_{t+1}^\\star(s&#39;) \\big] \\right\\}. \\] Greedy policy (deterministic): for each \\(s \\in \\mathcal{S}\\), \\[ \\pi_t^\\star(s) \\in \\arg\\max_{a \\in \\mathcal{A}} \\left\\{ R(s,a) + \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s,a)} \\big[ V_{t+1}^\\star(s&#39;) \\big] \\right\\}. \\] Exercise 1.1 How does dynamic programming look like when applied to the action-value function? Exercise 1.2 What is the computational complexity of dynamic programming? Let us try dynamic programming for the Hangover MDP presented before. Example 1.2 (Dynamic Programming for Hangover MDP) Consider the Hangover MDP defined by the transition graph shown in Fig. 1.2. With slight modification to the policy evaluation code, we can find the optimal value functions and optimal policies. # Dynamic programming (finite-horizon optimal control) for the Hangover MDP from collections import defaultdict from typing import Dict, List, Tuple State = str Action = str # --- MDP spec --------------------------------------------------------------- S: List[State] = [ &quot;Hangover&quot;, &quot;Sleep&quot;, &quot;More Sleep&quot;, &quot;Visit Lecture&quot;, &quot;Study&quot;, &quot;Pass Exam&quot; ] A: List[Action] = [&quot;Lazy&quot;, &quot;Productive&quot;] # P[s, a] -&gt; list of (s_next, prob) P: Dict[Tuple[State, Action], List[Tuple[State, float]]] = { # Hangover (&quot;Hangover&quot;, &quot;Lazy&quot;): [(&quot;Sleep&quot;, 1.0)], (&quot;Hangover&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.3), (&quot;Hangover&quot;, 0.7)], # Sleep (&quot;Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Sleep&quot;, &quot;Productive&quot;): [(&quot;Visit Lecture&quot;, 0.6), (&quot;More Sleep&quot;, 0.4)], # More Sleep (&quot;More Sleep&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;More Sleep&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 0.5), (&quot;More Sleep&quot;, 0.5)], # Visit Lecture (&quot;Visit Lecture&quot;, &quot;Lazy&quot;): [(&quot;Study&quot;, 0.8), (&quot;Pass Exam&quot;, 0.2)], (&quot;Visit Lecture&quot;, &quot;Productive&quot;): [(&quot;Study&quot;, 1.0)], # Study (&quot;Study&quot;, &quot;Lazy&quot;): [(&quot;More Sleep&quot;, 1.0)], (&quot;Study&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 0.9), (&quot;Study&quot;, 0.1)], # Pass Exam (absorbing) (&quot;Pass Exam&quot;, &quot;Lazy&quot;): [(&quot;Pass Exam&quot;, 1.0)], (&quot;Pass Exam&quot;, &quot;Productive&quot;): [(&quot;Pass Exam&quot;, 1.0)], } def R(s: State, a: Action) -&gt; float: &quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot; return 1.0 if s == &quot;Pass Exam&quot; else -1.0 # --- Dynamic programming (Bellman optimality) ------------------------------- def dynamic_programming(T: int): &quot;&quot;&quot; Compute optimal finite-horizon tables: - V[t][s] = V_t^*(s) - Q[t][(s,a)] = Q_t^*(s,a) - PI[t][s] = optimal action at (t,s) with terminal condition V_T^* = 0. &quot;&quot;&quot; assert T &gt;= 0 # sanity: probabilities sum to 1 for each (s,a) for key, rows in P.items(): total = sum(p for _, p in rows) if abs(total - 1.0) &gt; 1e-9: raise ValueError(f&quot;Probabilities for {key} sum to {total}, not 1.&quot;) V: Dict[int, Dict[State, float]] = defaultdict(dict) Q: Dict[int, Dict[Tuple[State, Action], float]] = defaultdict(dict) PI: Dict[int, Dict[State, Action]] = defaultdict(dict) # Terminal boundary for s in S: V[T][s] = 0.0 for a in A: Q[T][(s, a)] = 0.0 # Backward recursion (Bellman optimality) for t in range(T - 1, -1, -1): for s in S: # compute Q*_t(s,a) for a in A: exp_next = sum(p * V[t + 1][s_next] for s_next, p in P[(s, a)]) Q[t][(s, a)] = R(s, a) + exp_next # greedy action and optimal value # tie-breaking is deterministic by the order in A best_a = max(A, key=lambda a: Q[t][(s, a)]) PI[t][s] = best_a V[t][s] = Q[t][(s, best_a)] return V, Q, PI # --- Example run ------------------------------------------------------------- if __name__ == &quot;__main__&quot;: T = 10 # horizon V, Q, PI = dynamic_programming(T=T) print(f&quot;Optimal V_0(s) with T={T}:&quot;) for s in S: print(f&quot; {s:13s}: {V[0][s]: .3f}&quot;) print(&quot;\\nGreedy policy at t=0:&quot;) for s in S: print(f&quot; {s:13s}: {PI[0][s]}&quot;) print(&quot;\\nAction value at t=0:&quot;) for s in S: print(f&quot; {s:13s}: {Q[0][s, A[0]]: .3f}, {Q[0][s, A[1]]: .3f}&quot;) The optimal value function at \\(t=0\\) is: \\[\\begin{equation} V^\\star_0 = \\begin{bmatrix} 1.259 \\\\ 3.251 \\\\ 3.787 \\\\ 6.222 \\\\ 7.778 \\\\ 10 \\end{bmatrix}. \\tag{1.16} \\end{equation}\\] Clearly, the optimal value function dominates the value function shown in (1.8) of the random policy at every state. The optimal actions at \\(t=0\\) are: \\[\\begin{equation} \\begin{split} \\text{Hangover} &amp; : \\text{Lazy} \\\\ \\text{Sleep} &amp; : \\text{Productive} \\\\ \\text{More Sleep} &amp; : \\text{Productive} \\\\ \\text{Visit Lecture} &amp; : \\text{Lazy} \\\\ \\text{Study} &amp; : \\text{Productive} \\\\ \\text{Pass Exam} &amp; : \\text{Lazy} \\end{split}. \\end{equation}\\] You can play with the code here. 1.2 Infinite-Horizon MDP In a finite-horizon MDP, the horizon \\(T\\) must be specified in advance in order to carry out policy evaluation and dynamic programming. The finite horizon naturally provides a terminal condition, which serves as the boundary condition that allows backward recursion to proceed. In many practical applications, however, the horizon \\(T\\) is not well defined or is difficult to determine. In such cases, it is often more natural and convenient to adopt the infinite-horizon MDP formulation. An infinite-horizon MDP is given by the following tuple: \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma), \\] where \\(\\mathcal{S}\\), \\(\\mathcal{A}\\), \\(P\\), and \\(R\\) are the same as defined before in a finite-horizon MDP. We still restrict ourselves to the tabular MDP setup where \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) both have a finite number of elements. The key difference between the finite-horizon and infinite-horizon formulations is that the fixed horizon \\(T\\) is replaced by a discount factor \\(\\gamma \\in [0,1)\\). This discount factor weights future rewards less heavily than immediate rewards, as we will see shortly. Stationary Policy. In an infinite-horizon MDP, we focus on stationary policies \\(\\pi: \\mathcal{S} \\mapsto \\Delta(\\mathcal{A})\\), where \\(\\pi(a \\mid s)\\) denotes the probability of taking action \\(a\\) in state \\(s\\). In contrast, in a finite-horizon MDP we considered a tuple of \\(T\\) policies (see (1.1)), where each \\(\\pi_t\\) could vary with time (i.e., policies were non-stationary). Intuitively, in the infinite-horizon setting, it suffices to consider stationary policies because the decision-making problem at time \\(t\\) is equivalent to the problem at time \\(t + k\\) for any \\(k \\in \\mathbb{N}\\), as both face the same infinite horizon. Trajectory and Return. Given an initial state \\(s_0 \\in \\mathcal{S}\\) and a stationary policy \\(\\pi\\), the MDP will evolve as Start at state \\(s_0\\) Take action \\(a_0 \\sim \\pi(\\cdot \\mid s_0)\\) following policy \\(\\pi\\) Collect reward \\(r_0 = R(s_0, a_0)\\) Transition to state \\(s_1 \\sim P(s&#39; \\mid s_0, a_0)\\) following the dynamics Go to step 2 and continue forever This process generates a trajectory of states, actions, and rewards: \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots). \\] The return of a trajectory is defined as \\[ g_0 = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\dots = \\sum_{t=0}^{\\infty} \\gamma^t r_t. \\] Here, the discount factor \\(\\gamma\\) plays a key role: it progressively reduces the weight of rewards received further in the future, making them less influential as \\(t\\) increases. 1.2.1 Value Functions Similar to the case of finite-horizon MDP, we can define the state-value function and the action-value function associated with a policy \\(\\pi\\). State-Value Function. The value of a state \\(s \\in \\mathcal{S}\\) under policy \\(\\pi\\) is the expected discounted return obtained when starting from \\(s\\) at time \\(0\\): \\[\\begin{equation} V^{\\pi}(s) := \\mathbb{E}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\,\\middle|\\, s_0 = s, a_t \\sim \\pi(\\cdot \\mid s_t), s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\right]. \\tag{1.17} \\end{equation}\\] Action-Value Function. The value of a state-action pair \\((s,a) \\in \\mathcal{S} \\times \\mathcal{A}\\) under policy \\(\\pi\\) is the expected discounted return obtained by first taking action \\(a\\) in state \\(s\\), and then following policy \\(\\pi\\) thereafter: \\[\\begin{equation} Q^{\\pi}(s,a) := \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\,\\middle|\\, s_0 = s, a_0 = a, a_t \\sim \\pi(\\cdot \\mid s_t), s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\right]. \\tag{1.18} \\end{equation}\\] Note that a nice feature of having a discount factor \\(\\gamma \\in [0,1)\\) is that both the state-value and the action-value functions are guaranteed to be bounded even if the horizon is unbounded (assuming the reward function is bounded). We can verify the state-value function and the action value function satisfy the following relationship: \\[\\begin{align} V^{\\pi}(s) &amp;= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) Q^{\\pi}(s,a) \\tag{1.19}\\\\ Q^{\\pi}(s,a) &amp; = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi}(s&#39;). \\tag{1.20} \\end{align}\\] Combining these two equations, we arrive at the Bellman consistency result for infinite-horizon MDP. Proposition 1.2 (Bellman Consistency (Infinite Horizon)) The state-value function \\(V^{\\pi}\\) in (1.17) satisfies the following recursion: \\[\\begin{equation} \\begin{split} V^{\\pi} (s) &amp; = \\sum_{a \\in \\mathcal{A}} \\pi (a\\mid s) \\left( R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^{\\pi} (s&#39;) \\right) \\\\ &amp; =: \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} \\left[ R(s, a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} [V^{\\pi}(s&#39;)] \\right]. \\end{split} \\tag{1.21} \\end{equation}\\] Similarly, the action-value function \\(Q^{\\pi}(s,a)\\) in (1.18) satisfies the following recursion: \\[\\begin{equation} \\begin{split} Q^{\\pi} (s, a) &amp; = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) \\left( \\sum_{a&#39; \\in \\mathcal{A}} \\pi(a&#39; \\mid s&#39;) Q^{\\pi}(s&#39;, a&#39;)\\right) \\\\ &amp; =: R(s, a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(\\cdot \\mid s, a)} \\left[\\mathbb{E}_{a&#39; \\sim \\pi(\\cdot \\mid s&#39;)} [Q^{\\pi}(s&#39;, a&#39;)] \\right]. \\end{split} \\tag{1.22} \\end{equation}\\] 1.2.2 Policy Evaluation Given a policy \\(\\pi\\), how can we compute its associated state-value and action-value functions? Finite-horizon case. We initialize the terminal value function \\(V_T^{\\pi}(s) = 0\\) for every \\(s \\in \\mathcal{S}\\), and then apply the Bellman Consistency result (Proposition 1.1) to perform backward recursion. Infinite-horizon case. The Bellman Consistency result (Proposition 1.2) takes a different form and does not provide the same simple recipe for backward recursion. System of Linear Equations. A closer look at the Bellman Consistency equation (1.21) for the state-value function shows that it defines a square system of linear equations. Specifically, the value function \\(V^{\\pi}\\) can be represented as a vector with \\(|\\mathcal{S}|\\) variables, and (1.21) provides \\(|\\mathcal{S}|\\) linear equations over these variables. Thus, one way to compute the state-value function is to set up this linear system and solve it. However, doing so typically requires matrix inversion or factorization, which can be computationally expensive. The same reasoning applies to the action-value function \\(Q^{\\pi}\\), which can be represented as a vector of \\(|\\mathcal{S}||\\mathcal{A}|\\) variables constrained by \\(|\\mathcal{S}||\\mathcal{A}|\\) linear equations. The following proposition states that, instead of solving a linear system of equations, one can use a globally convergent iterative scheme, one that is very much like the policy evaluation algorithm for the finite-horizon MDP, to evaluate the state-value function associated with a policy \\(\\pi\\). Proposition 1.3 (Policy Evaluation (Infinite Horizon, State-Value)) Consider an infinite-horizon MDP \\(\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,\\gamma)\\). Fix a policy \\(\\pi\\) and consider the iterative scheme for the state-value function: \\[\\begin{equation} V_{k+1}(s) \\;\\; \\gets \\;\\; \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a) V_k(s&#39;) \\right], \\quad \\forall s \\in \\mathcal{S}. \\tag{1.23} \\end{equation}\\] Then, starting from any initialization \\(V_0 \\in \\mathbb{R}^{|\\mathcal{S}|}\\), the sequence \\(\\{V_k\\}\\) converges to the unique fixed point \\(V^{\\pi}\\), the state-value function associated with policy \\(\\pi\\). Proof. To prove the convergence of the policy evaluation algorithm, we shall introduce the notion of a Bellman operator. Bellman Operator. Any value function \\(V(s)\\) can be interpreted as a vector in \\(\\mathbb{R}^{|\\mathcal{S}|}\\) (recall we are in the tabular MDP case). Given any value function \\(V \\in \\mathbb{R}^{|\\mathcal{S}|}\\), and a policy \\(\\pi\\), define the Bellman operator associated with \\(\\pi\\) as \\(T^{\\pi}: \\mathbb{R}^{|\\mathcal{S}|} \\mapsto \\mathbb{R}^{|\\mathcal{S}|}\\): \\[\\begin{equation} (T^{\\pi} V)(s) := \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a) V(s&#39;) \\right]. \\tag{1.24} \\end{equation}\\] We claim that \\(T^{\\pi}\\) has two important properties. Monotonicity. If \\(V \\leq W\\) (i.e., \\(V(s) \\leq W(s)\\) for any \\(s \\in \\mathcal{S}\\)), then \\(T^{\\pi} V \\leq T^{\\pi}W\\). To see this, observe that \\[\\begin{align*} (T^{\\pi}V)(s) - (T^\\pi W)(s) &amp;= \\sum_{a} \\pi(a \\mid s) \\left(\\gamma \\sum_{s&#39;} P(s&#39; \\mid s, a) (V(s&#39;) - W(s&#39;)) \\right) \\\\ &amp; = \\gamma \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s), s&#39; \\sim P(\\cdot \\mid s,a)}[V(s&#39;) - W(s&#39;)]. \\end{align*}\\] Therefore, if \\(V(s&#39;) - W(s&#39;) \\leq 0\\) for any \\(s&#39; \\in \\mathcal{S}\\), then \\(T^{\\pi}V \\leq T^{\\pi} W\\). \\(\\gamma\\)-Contraction. For any value function \\(V \\in \\mathbb{R}^{|\\mathcal{S}|}\\), define the \\(\\ell_{\\infty}\\) norm (sup norm) as \\[ \\Vert V \\Vert_{\\infty} = \\max_{s \\in \\mathcal{S}} |V(s)|. \\] We claim that the Bellman operator \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction in the sup norm, i.e., \\[\\begin{equation} \\Vert T^\\pi V - T^\\pi W \\Vert_{\\infty} \\leq \\gamma \\Vert V - W \\Vert_{\\infty}, \\quad \\forall V, W \\in \\mathbb{R}^{|\\mathcal{S}|}. \\tag{1.25} \\end{equation}\\] To prove this, observe that for any \\(s \\in \\mathcal{S}\\), we have: \\[\\begin{align*} |(T^\\pi V)(s) - (T^\\pi W)(s)| &amp;= \\left| \\sum_a \\pi(a|s)\\,\\gamma \\sum_{s&#39;} P(s&#39;|s,a)\\big(V(s&#39;) - W(s&#39;)\\big) \\right| \\\\ &amp;\\le \\gamma \\sum_a \\pi(a|s)\\sum_{s&#39;} P(s&#39;|s,a)\\,|V(s&#39;) - W(s&#39;)| \\\\ &amp;\\le \\gamma \\|V - W\\|_\\infty \\sum_a \\pi(a|s)\\sum_{s&#39;} P(s&#39;|s,a) \\\\ &amp;= \\gamma \\|V - W\\|_\\infty. \\end{align*}\\] Taking the maximum over \\(s\\) gives \\[ \\|T^\\pi V - T^\\pi W\\|_\\infty \\le \\gamma \\|V - W\\|_\\infty, \\] so \\(T^\\pi\\) is a \\(\\gamma\\)-contraction in the sup norm. With the Bellman operator defined, we observe that the value function of \\(\\pi\\), denoted \\(V^{\\pi}\\) in (1.21), is a fixed point of \\(T^{\\pi}\\). That is to say \\(V^{\\pi}\\) satisfies: \\[ T^{\\pi} V^{\\pi} = V^{\\pi}. \\] In other words, \\(V^{\\pi}\\) is fixed (remains unchanged) under the Bellman operator. Since \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction, by the Banach Fixed-Point Theorem, we know that there exists a unique fixed point to \\(T^{\\pi}\\), which is \\(V^{\\pi}\\). Moreover, since \\[ \\Vert V_{k} - V^{\\pi} \\Vert_{\\infty} = \\Vert T^{\\pi} V_{k-1} - T^{\\pi} V^{\\pi} \\Vert_{\\infty} \\leq \\gamma \\Vert V_{k-1} - V^{\\pi} \\Vert_{\\infty}, \\] we can deduce the rate of convergence \\[ \\Vert V_{k} - V^{\\pi} \\Vert_{\\infty} \\leq \\gamma^{k} \\Vert V_0 - V^{\\pi} \\Vert_{\\infty}. \\] Therefore, policy evaluation globally converges from any initialization \\(V_0\\) at a linear rate of \\(\\gamma\\). We have a similar policy evaluation algorithm for the action-value function. Proposition 1.4 (Policy Evaluation (Infinite Horizon, Action-Value)) Fix a policy \\(\\pi\\). Consider the iterative scheme on \\(Q:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}\\): \\[\\begin{equation} \\begin{split} Q_{k+1}(s,a) \\;\\gets\\; R(s,a) \\;+\\; \\gamma \\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\!\\left(\\sum_{a&#39;\\in\\mathcal{A}} \\pi(a&#39;\\mid s&#39;)\\, Q_k(s&#39;,a&#39;)\\right), \\\\ \\forall (s,a)\\in\\mathcal{S}\\times\\mathcal{A}. \\end{split} \\tag{1.26} \\end{equation}\\] Then, for any initialization \\(Q_0\\), the sequence \\(\\{Q_k\\}\\) converges to the unique fixed point \\(Q^{\\pi}\\), the action-value function associated with policy \\(\\pi\\). Proof. Define the Bellman operator on action-values \\[ (T^{\\pi}Q)(s,a) := R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\Big(\\sum_{a&#39;} \\pi(a&#39;\\mid s&#39;)\\, Q(s&#39;,a&#39;)\\Big). \\] \\(T^{\\pi}\\) is a \\(\\gamma\\)-contraction in the sup-norm on \\(\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}\\); hence by the Banach fixed-point theorem, global convergence holds regardless of initialization. Let us apply policy evaluation to an infinite-horizon MDP. Example 1.3 (Policy Evaluation for Inverted Pendulum) Figure 1.3: Inverted Pendulum. We consider the inverted pendulum with state \\(s=(\\theta, \\dot\\theta)\\) and action (torque) \\(a = u\\), as visualized in Fig. 1.3. Our goal is to swing up the pendulum from any initial state to the upright position \\(s = (0,0)\\). Continuous-Time Dynamics. The continuous-time dynamics of the inverted pendulum is \\[ \\ddot{\\theta} \\;=\\; \\frac{g}{l}\\sin(\\theta) \\;+\\; \\frac{1}{ml^2}u \\;-\\; c\\,\\dot{\\theta}, \\] where \\(m &gt; 0\\) is the mass of the pendulum, \\(l &gt; 0\\) is the length of the pole, \\(c &gt; 0\\) is the damping coefficient, and \\(g\\) is the gravitational constant. Discretization (Euler). With timestep \\(\\Delta t\\), we obtain the following discrete-time dynamics: \\[\\begin{equation} \\begin{split} \\theta_{k+1} &amp;= \\theta_k + \\Delta t \\, \\dot{\\theta}_k, \\\\ \\dot{\\theta}_{k+1} &amp;= \\dot{\\theta}_k + \\Delta t \\Big(\\tfrac{g}{l}\\sin(\\theta_k) + \\tfrac{1}{ml^2}u_k - c\\,\\dot{\\theta}_k\\Big). \\end{split} \\tag{1.27} \\end{equation}\\] We wrap angles to \\([-\\pi,\\pi]\\) via \\(\\operatorname{wrap}(\\theta)=\\mathrm{atan2}(\\sin\\theta,\\cos\\theta)\\). Tabular MDP. We convert the discrete-time dynamics into a tabular MDP. State grid. \\(\\theta \\in [-\\pi,\\pi]\\), \\(\\dot\\theta \\in [-\\pi,\\pi]\\) on uniform grids: \\[ \\mathcal{S}=\\{\\;(\\theta_i,\\dot\\theta_j)\\;:\\; i=1,\\dots,N_\\theta,\\; j=1, \\dots, N_{\\dot\\theta}\\;\\}. \\] Action grid. \\(u \\in [-mgl/2, mgl/2]\\) on \\(N_u\\) uniform points: \\[ \\mathcal{A}=\\{u_\\ell:\\ell=1,\\dots,N_u\\}. \\] Stochastic transition kernel (nearest-3 interpolation). From a grid point \\(s=(\\theta_i,\\dot\\theta_j)\\) and an action \\(u_\\ell\\), compute the next continuous state \\(s^+ = (\\theta^+,\\dot\\theta^+)\\) via the discrete-time dynamics in (1.27). If \\(s^+\\notin\\mathcal{S}\\), choose the three closest grid states \\(\\{s^{(1)},s^{(2)},s^{(3)}\\}\\) by Euclidean distance in \\((\\theta,\\dot\\theta)\\) and assign probabilities \\[ p_r \\propto \\frac{1}{\\|s^+ - s^{(r)}\\|_2 + \\varepsilon},\\quad r=1,2,3, \\qquad \\sum_r p_r=1, \\] so nearer grid points receive higher probability (use a small \\(\\varepsilon&gt;0\\) to avoid division by zero). Reward. A quadratic shaping penalty around the upright equilibrium: \\[ R(s,a) = -\\Big(\\theta^2 + 0.1\\,\\dot\\theta^2 + 0.01\\,u^2\\Big). \\] Discount. \\(\\gamma \\in [0,1)\\). We obtain a discounted, infinite-horizon, tabular MDP. Policy. For policy evaluation, consider \\(\\pi(a\\mid s)\\) be uniform over the discretized actions, i.e., a random policy. Policy Evaluation. The following python script performs policy evaluation. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 41 N_thetadot = 41 N_u = 21 theta_grid = np.linspace(-np.pi, np.pi, N_theta) thetadot_grid = np.linspace(-np.pi, np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- # Pre-compute all grid points for fast nearest neighbor search grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) # sort those 3 by distance for stability nn_idx = nn_idx[np.argsort(dists[nn_idx])] d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): # reward at current (s,a) R[s, a] = reward(th, thd, u) # next continuous state th_n, thd_n = step_euler(th, thd, u) # map to 3 nearest grid states nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ----- Fixed policy: uniform over actions ----- Pi = np.full((S, A), 1.0 / A) # ----- Iterative policy evaluation ----- V = np.zeros(S) # initialization (any vector works) tol = 1e-6 max_iters = 10000 for k in range(max_iters): V_new = np.zeros_like(V) # Compute Bellman update: V_{k+1}(s) = sum_a Pi(s,a)[ R(s,a) + gamma * sum_j P(s,a,j) V_k(ns_j) ] # First, expected next V for each (s,a) EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # shape: (S, A) # Then expectation over actions under Pi V_new = (Pi * (R + gamma * EV_next)).sum(axis=1) # shape: (S,) # Check convergence if np.max(np.abs(V_new - V)) &lt; tol: V = V_new print(f&quot;Converged in {k+1} iterations (sup-norm change &lt; {tol}).&quot;) break V = V_new else: print(f&quot;Reached max_iters={max_iters} without meeting tolerance {tol}.&quot;) V_grid = V.reshape(N_theta, N_thetadot) # V_grid: shape (N_theta, N_thetadot) # theta_grid, thetadot_grid already defined fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; # any matplotlib colormap, e.g., &quot;plasma&quot;, &quot;inferno&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^\\pi(\\theta,\\dot{\\theta})$&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V^\\pi$ (tabular policy evaluation)&quot;) plt.tight_layout() plt.show() Running the code, it shows that policy evaluation converges in 518 iterations under tolerance \\(10^{-6}\\). Fig. 1.4 plots the value function over the state grid. Figure 1.4: Value Function from Policy Evaluation. You can play with the code here. 1.2.3 Principle of Optimality In an infinite-horizon MDP, our goal is to find the optimal policy that maximizes the expected long-term discounted return: \\[ V^\\star := \\max_{\\pi} \\mathbb{E}_{s \\sim \\mu(\\cdot)} [V^\\pi(s)], \\] where \\(\\mu\\) is a given initial distribution. We call \\(V^\\star\\) the optimal value function. Given a policy \\(\\pi\\) and its associated value function \\(V^\\pi\\), how do we know if the policy is already optimal? Theorem 1.2 (Bellman Optimality (Infinite Horizon)) For an infinite-horizon MDP with discount factor \\(\\gamma \\in [0,1)\\), the optimal state-value function \\(V^\\star(s)\\) satisfies the Bellman optimality equation \\[\\begin{equation} V^\\star(s) \\;=\\; \\max_{a \\in \\mathcal{A}} \\Big[\\, R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a)\\, V^\\star(s&#39;) \\,\\Big]. \\tag{1.28} \\end{equation}\\] Define the optimal action-value function as \\[\\begin{equation} Q^\\star(s,a) = R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s, a) V^\\star(s&#39;). \\tag{1.29} \\end{equation}\\] We have that \\(Q^\\star(s,a)\\) satisfies \\[\\begin{equation} Q^\\star(s,a) \\;=\\; R(s,a) + \\gamma \\sum_{s&#39; \\in \\mathcal{S}} P(s&#39; \\mid s,a)\\, \\left[\\max_{a&#39; \\in \\mathcal{A}} Q^\\star(s&#39;,a&#39;) \\right]. \\tag{1.30} \\end{equation}\\] Moreover, any greedy policy with respect to \\(V^\\star\\) (equivalently, to \\(Q^\\star\\)) is optimal: \\[\\begin{equation} \\begin{split} \\pi^\\star(s) &amp; \\in \\arg\\max_{a \\in \\mathcal{A}} \\Big[\\, R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39; \\mid s,a)\\, V^\\star(s&#39;) \\,\\Big] \\quad\\Longleftrightarrow\\quad \\\\ \\pi^\\star(s) &amp; \\in \\arg\\max_{a \\in \\mathcal{A}} Q^\\star(s,a). \\end{split} \\tag{1.31} \\end{equation}\\] Proof. We will first show that \\(V^\\star\\) has statewise dominance over all other policies, and then show that \\(V^\\star\\) can be attained by the greedy policy. Claim. For any discounted MDP with \\(\\gamma \\in [0,1)\\) and any policy \\(\\pi\\), \\[ V^\\star(s) \\;\\ge\\; V^{\\pi}(s)\\qquad \\forall s\\in\\mathcal{S}, \\] where \\(V^\\star\\) is the unique solution of the Bellman optimality equation and \\(V^\\pi\\) solves the Bellman consistency equation for \\(\\pi\\). Proof via Bellman Operators. Define the Bellman operators \\[ (T^\\pi V)(s) := \\sum_{a}\\pi(a\\mid s)\\Big[ R(s,a)+\\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)V(s&#39;) \\Big], \\] \\[ (T^\\star V)(s) := \\max_{a}\\Big[ R(s,a)+\\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)V(s&#39;) \\Big]. \\] Key facts: (Monotonicity) If \\(V \\ge W\\) componentwise, then \\(T^\\pi V \\ge T^\\pi W\\) and \\(T^\\star V \\ge T^\\star W\\). (Dominance of \\(T^*\\)) For any \\(V\\) and any \\(\\pi\\), \\[ T^\\star V \\;\\ge\\; T^\\pi V \\] because the max over actions is at least the \\(\\pi\\)-weighted average. (Fixed points) \\(V^\\pi = T^\\pi V^\\pi\\) and \\(V^\\star = T^\\star V^\\star\\). (Contraction) Each \\(T^\\pi\\) and \\(T^\\star\\) is a \\(\\gamma\\)-contraction in the sup-norm; hence their fixed points are unique. Now start from \\(V^\\pi\\). Using (2), \\[ V^\\pi = T^\\pi V^\\pi \\;\\le\\; T^\\star V^\\pi. \\] Applying \\(T^\\star\\) repeatedly and using (1), \\[ V^\\pi \\;\\le\\; T^\\star V^\\pi \\;\\le\\; (T^\\star)^2 V^\\pi \\;\\le\\; \\cdots \\] The sequence \\((T^\\star)^k V^\\pi\\) converges (by contraction) to the unique fixed point of \\(T^\\star\\), namely \\(V^\\star\\). Taking limits preserves the inequality, yielding \\(V^\\pi \\le V^\\star\\) statewise. The Bellman optimality condition tells us, if a policy \\(\\pi\\) is already greedy with respect to its value function \\(V^\\pi\\), then \\(\\pi\\) is the optimal policy and \\(V^\\pi\\) is the optimal value function. In the next, we introduce two algorithms that can guarantee finding the optimal policy and the optimal value function. The first algorithm, policy iteration (PI), iterates over the space of policies; while the second algorithm, value iteration (VI), iterates over the space of value functions. 1.2.4 Policy Improvement The policy evaluation algorithm enables us to compute the value functions associated with a given policy \\(\\pi\\). The next result, known as the Policy Improvement Lemma, shows that once we have \\(V^{\\pi}\\), constructing a greedy policy with respect to \\(V^{\\pi}\\) guarantees performance that is at least as good as \\(\\pi\\), and strictly better in some states unless \\(\\pi\\) is already greedy with respect to \\(V^{\\pi}\\). Lemma 1.1 (Policy Improvement) Let \\(\\pi\\) be any policy and let \\(V^{\\pi}\\) be its state-value function. Define a new policy \\(\\pi&#39;\\) such that for each state \\(s\\), \\[ \\pi&#39;(s) \\in \\arg\\max_{a \\in \\mathcal{A}} \\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39; \\mid s,a) V^{\\pi}(s&#39;) \\Big]. \\] Then for all states \\(s \\in \\mathcal{S}\\), \\[ V^{\\pi&#39;}(s) \\;\\ge\\; V^{\\pi}(s). \\] Moreover, the inequality is strict for some state \\(s\\) unless \\(\\pi\\) is already greedy with respect to \\(V^\\pi\\) (which implies optimality). Proof. Let \\(V^{\\pi}\\) be the value function of a policy \\(\\pi\\), and define a new (possibly stochastic) policy \\(\\pi&#39;\\) that is greedy w.r.t. \\(V^{\\pi}\\): \\[ \\pi&#39;(\\cdot \\mid s) \\in \\arg\\max_{\\mu \\in \\Delta(\\mathcal{A})} \\sum_{a}\\mu(a)\\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V^{\\pi}(s&#39;)\\Big]. \\] Define the Bellman operators \\[\\begin{align*} (T^{\\pi}V)(s) &amp; := \\sum_a \\pi(a\\mid s)\\Big[R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;\\mid s,a)V(s&#39;)\\Big],\\\\ (T^{\\pi&#39;}V)(s) &amp; := \\sum_a \\pi&#39;(a\\mid s)\\Big[\\cdots\\Big]. \\end{align*}\\] Step 1: One-step improvement at \\(V^{\\pi}\\). By greediness of \\(\\pi&#39;\\) w.r.t. \\(V^{\\pi}\\), \\[ (T^{\\pi&#39;} V^{\\pi})(s) = \\max_{\\mu}\\sum_a \\mu(a)\\Big[R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;\\mid s,a)V^{\\pi}(s&#39;)\\Big] \\;\\;\\ge\\;\\; (T^{\\pi} V^{\\pi})(s) = V^{\\pi}(s), \\] for all \\(s\\). Hence \\[\\begin{equation} T^{\\pi&#39;} V^{\\pi} \\;\\ge\\; V^{\\pi}\\quad\\text{(componentwise).} \\tag{1.32} \\end{equation}\\] Step 2: Monotonicity + contraction yield global improvement. The operator \\(T^{\\pi&#39;}\\) is monotone (order-preserving) and a \\(\\gamma\\)-contraction in the sup-norm. Apply \\(T^{\\pi&#39;}\\) repeatedly to both sides of (1.32): \\[ (T^{\\pi&#39;})^k V^{\\pi} \\;\\ge\\; (T^{\\pi&#39;})^{k-1} V^{\\pi} \\;\\ge\\; \\cdots \\;\\ge\\; V^{\\pi},\\qquad k=1,2,\\dots \\] By contraction, \\((T^{\\pi&#39;})^k V^{\\pi} \\to V^{\\pi&#39;}\\), the unique fixed point of \\(T^{\\pi&#39;}\\). Taking limits preserves the inequality, so \\[ V^{\\pi&#39;} \\;\\ge\\; V^{\\pi}\\quad\\text{statewise.} \\] Strict improvement condition. If there exists a state \\(s\\) such that \\[ (T^{\\pi&#39;} V^{\\pi})(s) \\;&gt;\\; V^{\\pi}(s), \\] then by monotonicity we have a strict increase at that state after one iteration, and the limit remains strictly larger at that state (or at any state that can reach it with positive probability under \\(\\pi&#39;\\)). This happens precisely when \\(\\pi&#39;\\) selects, with positive probability, an action \\(a\\) for which \\[ Q^{\\pi}(s,a)=R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V^{\\pi}(s&#39;) \\;&gt;\\; V^{\\pi}(s), \\] i.e., when \\(\\pi\\) was not already greedy (optimal) at \\(s\\). 1.2.5 Policy Iteration The policy improvement lemma and the principle of optimality, combined together, leads to the first algorithm that guarantees convergence to an optimal policy. This algorithm is called policy iteration. Theorem 1.3 (Convergence of Policy Iteration) Consider a discounted MDP with finite state and action sets and \\(\\gamma\\in[0,1)\\). Let \\(\\{\\pi_k\\}_{k\\ge0}\\) be the sequence produced by Policy Iteration (PI): Policy evaluation: compute \\(V^{\\pi_k}\\) such that \\(V^{\\pi_k}=T^{\\pi_k}V^{\\pi_k}\\). Policy improvement: choose \\(\\pi_{k+1}\\) greedy w.r.t. \\(V^{\\pi_k}\\): \\[ \\pi_{k+1}(s) \\in \\arg\\max_{a}\\Big[ R(s,a)+\\gamma\\sum_{s&#39;}P(s&#39;|s,a)\\,V^{\\pi_k}(s&#39;)\\Big]. \\] Then: \\(V^{\\pi_{k+1}} \\ge V^{\\pi_k}\\) componentwise, and the inequality is strict for some state unless \\(\\pi_{k+1}=\\pi_k\\). If \\(\\pi_{k+1}=\\pi_k\\), then \\(V^{\\pi_k}\\) satisfies the Bellman optimality equation; hence \\(\\pi_k\\) is optimal and \\(V^{\\pi_k}=V^*\\). Because the number of stationary policies is finite, PI terminates in finitely many iterations at an optimal policy \\(\\pi^*\\) with value \\(V^*\\). \\(\\Vert V^{\\pi_{k+1}} - V^\\star \\Vert_{\\infty} \\leq \\gamma \\Vert V^{\\pi_k} - V^\\star \\Vert_{\\infty}\\), for any \\(k\\) (i.e., contraction). Proof. By the policy improvement lemma, we have \\[ V^{\\pi_{k+1}} \\geq V^{\\pi_k}. \\] By monotonicity of the Bellman operator \\(T^{\\pi_{k+1}}\\), we have \\[ V^{\\pi_{k+1}} = T^{\\pi_{k+1}} V^{\\pi_{k+1}} \\geq T^{\\pi_{k+1}} V^{\\pi_k}. \\] By definition of the Bellman optimality operator, we have \\[ T^{\\pi_{k+1}} V^{\\pi_k} = T^\\star V^{\\pi_k}. \\] Therefore, \\[ 0 \\geq V^{\\pi_{k+1}} - V^\\star \\geq T^{\\pi_{k+1}} V^{\\pi_k} - V^\\star = T^\\star V^{\\pi_k} - T^\\star V^\\star \\] As a result, \\[ \\Vert V^{\\pi_{k+1}} - V^\\star \\Vert_{\\infty} \\leq \\Vert T^\\star V^{\\pi_k} - T^\\star V^\\star \\Vert_{\\infty} \\leq \\gamma \\Vert V^{\\pi_k} - V^\\star \\Vert_{\\infty}. \\] This proves the contraction result (d). Let us apply Policy Iteration to the inverted pendulum problem. Example 1.4 (Policy Iteration for Inverted Pendulum) The following code performs policy iteration for the inverted pendulum problem. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 101 N_thetadot = 101 N_u = 51 theta_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_theta) thetadot_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) nn_idx = nn_idx[np.argsort(dists[nn_idx])] # sort those 3 by distance d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): # reward at current (s,a) R[s, a] = reward(th, thd, u) # next continuous state th_n, thd_n = step_euler(th, thd, u) # map to 3 nearest grid states nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ======================= # POLICY ITERATION # ======================= # Represent policy as a deterministic action index per state: pi[s] in {0..A-1} # Start from uniform-random policy (deterministic tie-breaker: middle action) pi = np.full(S, A // 2, dtype=int) def policy_evaluation(pi, V_init=None, tol=1e-6, max_iters=10000): &quot;&quot;&quot;Iterative policy evaluation for deterministic pi (action index per state).&quot;&quot;&quot; V = np.zeros(S) if V_init is None else V_init.copy() for k in range(max_iters): # For each state s, use chosen action a = pi[s] a = pi # shape (S,) # Expected next value under chosen action EV_next = (NS_prob[np.arange(S), a] * V[NS_idx[np.arange(S), a]]).sum(axis=1) # (S,) V_new = R[np.arange(S), a] + gamma * EV_next if np.max(np.abs(V_new - V)) &lt; tol: # print(f&quot;Policy evaluation converged in {k+1} iterations.&quot;) return V_new V = V_new # print(&quot;Policy evaluation reached max_iters without meeting tolerance.&quot;) return V def policy_improvement(V, pi_old=None): &quot;&quot;&quot;Greedy improvement: pi&#39;(s) = argmax_a [ R(s,a) + gamma * E[V(s&#39;)] ].&quot;&quot;&quot; # Compute Q(s,a) = R + gamma * sum_j P(s,a,j) V(ns_j) EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # (S, A) Q = R + gamma * EV_next # (S, A) pi_new = np.argmax(Q, axis=1).astype(int) # greedy deterministic policy stable = (pi_old is not None) and np.array_equal(pi_new, pi_old) return pi_new, stable # Main PI loop max_pi_iters = 100 V = np.zeros(S) for it in range(max_pi_iters): # Policy evaluation V = policy_evaluation(pi, V_init=V, tol=1e-6, max_iters=10000) # Policy improvement pi_new, stable = policy_improvement(V, pi_old=pi) print(f&quot;[PI] Iter {it+1}: policy changed = {not stable}&quot;) pi = pi_new if stable: print(&quot;Policy iteration converged: policy stable.&quot;) break else: print(&quot;Reached max_pi_iters without policy stability (may still be near-optimal).&quot;) # ----- Visualization ----- V_grid = V.reshape(N_theta, N_thetadot) fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^{\\pi}(\\theta,\\dot{\\theta})$ (final PI)&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V$ after Policy Iteration&quot;) plt.tight_layout() plt.show() # Visualize the greedy action *value* (torque) pi_grid = pi.reshape(N_theta, N_thetadot) # action indices action_values = u_grid[pi_grid] # map indices -&gt; torques plt.figure(figsize=(7,5), dpi=120) im = plt.imshow(action_values, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;coolwarm&quot;) # diverging colormap good for ± torque cbar = plt.colorbar(im) cbar.set_label(&quot;Greedy action value (torque)&quot;) plt.xlabel(r&quot;$\\dot{\\theta}$&quot;) plt.ylabel(r&quot;$\\theta$&quot;) plt.title(&quot;Greedy policy (torque) after PI&quot;) plt.tight_layout() plt.show() Running the code produces the optimal value function shown in Fig. 1.5 and the optimal policy shown in Fig. 1.6. Figure 1.5: Optimal Value Function after Policy Iteration Figure 1.6: Optimal Policy after Policy Iteration We can apply the optimal policy to the pendulum with an initial state of \\((-\\pi, 0)\\) (i.e., the bottomright position). Fig. 1.7 plots the rollout trajectory of \\(\\theta, \\dot{\\theta}, u\\). We can see that the optimal policy is capable of performing “bang-bang” control to accumulate energy before swinging up. Fig. 1.8 overlays the trajectory on top of the optimal value function. You can play with the code here. Figure 1.7: Optimal Trajectory of Pendulum Swing-Up Figure 1.8: Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function 1.2.6 Value Iteration Policy iteration—as the name suggests—iterates on policies: it alternates between (1) policy evaluation (computing \\(V^{\\pi}\\) for the current policy \\(\\pi\\)) and (2) policy improvement (making \\(\\pi\\) greedy w.r.t. \\(V^{\\pi}\\)). An alternative, often very effective, method is value iteration. Unlike policy iteration, value iteration does not explicitly maintain a policy during its updates; it iterates directly on the value function toward the fixed point of the Bellman optimality* operator. Once the value function has (approximately) converged, the optimal policy is obtained by a single greedy extraction step. Note that intermediate value iterates need not correspond to the value of any actual policy. The value iteration (VI) algorithm works as follows: Initialization. Choose any \\(V_0:\\mathcal{S}\\to\\mathbb{R}\\) (e.g., \\(V_0 \\equiv 0\\)). Iteration. For \\(k=0,1,2,\\dots\\), \\[ V_{k+1}(s) \\;\\leftarrow\\; \\max_{a\\in\\mathcal{A}} \\Big[\\, R(s,a) \\;+\\; \\gamma \\sum_{s&#39;\\in\\mathcal{S}} P(s&#39;\\mid s,a)\\; V_k(s&#39;) \\,\\Big], \\quad \\forall s\\in\\mathcal{S}. \\] Stopping rule. Stop when \\(\\lVert V_{k+1}-V_k\\rVert_\\infty \\le \\varepsilon\\) (or any chosen tolerance). Policy extraction (greedy): \\[ \\pi_{k+1}(s) \\in \\arg\\max_{a\\in\\mathcal{A}} \\Big[\\, R(s,a) \\;+\\; \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\; V_{k+1}(s&#39;) \\,\\Big]. \\] The following theorem states the convergence of value iteration. Theorem 1.4 (Convergence of Value Iteration) Let \\(T^\\star\\) be the Bellman optimality operator, \\[ (T^\\star V)(s) := \\max_{a}\\Big[ R(s,a) + \\gamma \\sum_{s&#39;} P(s&#39;\\mid s,a)\\, V(s&#39;) \\Big]. \\] For \\(\\gamma\\in[0,1)\\) and finite \\(\\mathcal{S},\\mathcal{A}\\), \\(T^\\star\\) is a \\(\\gamma\\)-contraction in the sup-norm. Hence, for any \\(V_0\\), \\[ V_k \\;=\\; (T^\\star )^k V_0 \\;\\xrightarrow[k\\to\\infty]{}\\; V^*, \\] the unique fixed point of \\(T^\\star\\). Moreover, the greedy policy \\(\\pi_k\\) extracted from \\(V_k\\) converges to an optimal policy \\(\\pi^\\star\\). In addition, after \\(k\\) iterations, we have \\[ \\lVert V_k - V^* \\rVert_\\infty \\;\\le\\; \\gamma^k \\, \\lVert V_0 - V^* \\rVert_\\infty. \\] Finally, we apply value iteration to the inverted pendulum problem. Example 1.5 (Value Iteration for Inverted Pendulum) The following code performs value iteration for the inverted pendulum problem. import numpy as np import matplotlib.pyplot as plt # ----- Physical &amp; MDP parameters ----- g, l, m, c = 9.81, 1.0, 1.0, 0.1 dt = 0.05 gamma = 0.97 eps = 1e-8 # Grids N_theta = 101 N_thetadot = 101 N_u = 51 theta_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_theta) thetadot_grid = np.linspace(-1.5*np.pi, 1.5*np.pi, N_thetadot) u_max = 0.5 * m * g * l u_grid = np.linspace(-u_max, u_max, N_u) # Helpers to index/unwrap def wrap_angle(x): return np.arctan2(np.sin(x), np.cos(x)) def state_index(i, j): return i * N_thetadot + j def index_to_state(idx): i = idx // N_thetadot j = idx % N_thetadot return theta_grid[i], thetadot_grid[j] S = N_theta * N_thetadot A = N_u # ----- Dynamics step (continuous -&gt; one Euler step) ----- def step_euler(theta, thetadot, u): theta_next = wrap_angle(theta + dt * thetadot) thetadot_next = thetadot + dt * ((g/l) * np.sin(theta) + (1/(m*l*l))*u - c*thetadot) # clip angular velocity to grid range (bounded MDP) thetadot_next = np.clip(thetadot_next, thetadot_grid[0], thetadot_grid[-1]) return theta_next, thetadot_next # ----- Find 3 nearest grid states and probability weights (inverse-distance) ----- grid_pts = np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing=&#39;ij&#39;), axis=-1).reshape(-1, 2) def nearest3_probs(theta_next, thetadot_next): x = np.array([theta_next, thetadot_next]) dists = np.linalg.norm(grid_pts - x[None, :], axis=1) nn_idx = np.argpartition(dists, 3)[:3] # three smallest (unordered) nn_idx = nn_idx[np.argsort(dists[nn_idx])] # sort those 3 by distance d = dists[nn_idx] w = 1.0 / (d + eps) p = w / w.sum() return nn_idx.astype(int), p # ----- Reward ----- def reward(theta, thetadot, u): return -(theta**2 + 0.1*thetadot**2 + 0.01*u**2) # ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] ----- R = np.zeros((S, A)) NS_idx = np.zeros((S, A, 3), dtype=int) # next-state indices (3 nearest) NS_prob = np.zeros((S, A, 3)) # their probabilities for i, th in enumerate(theta_grid): for j, thd in enumerate(thetadot_grid): s = state_index(i, j) for a, u in enumerate(u_grid): R[s, a] = reward(th, thd, u) th_n, thd_n = step_euler(th, thd, u) nn_idx, p = nearest3_probs(th_n, thd_n) NS_idx[s, a, :] = nn_idx NS_prob[s, a, :] = p # ======================= # VALUE ITERATION # ======================= # Bellman optimality update: # V_{k+1}(s) = max_a [ R(s,a) + gamma * sum_j P(s,a,j) * V_k(ns_j) ] V = np.zeros(S) tol = 1e-6 max_vi_iters = 1000 for k in range(max_vi_iters): # Expected next V for every (s,a), given current V_k EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # shape (S, A) Q = R + gamma * EV_next # shape (S, A) V_new = np.max(Q, axis=1) # greedy backup over actions delta = np.max(np.abs(V_new - V)) # Optional: a stopping rule aligned with policy loss bound could scale tol # e.g., stop when delta &lt;= tol * (1 - gamma) / (2 * gamma) if delta &lt; tol: V = V_new print(f&quot;Value Iteration converged in {k+1} iterations (sup-norm change {delta:.2e}).&quot;) break V = V_new else: print(f&quot;Reached max_vi_iters={max_vi_iters} (last sup-norm change {delta:.2e}).&quot;) # Greedy policy extraction from the final V EV_next = (NS_prob * V[NS_idx]).sum(axis=2) # recompute with final V Q = R + gamma * EV_next pi = np.argmax(Q, axis=1) # deterministic greedy policy (indices) # ----- Visualization: Value function ----- V_grid = V.reshape(N_theta, N_thetadot) fig, ax = plt.subplots(figsize=(7,5), dpi=120) im = ax.imshow( V_grid, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;viridis&quot; ) cbar = fig.colorbar(im, ax=ax) cbar.set_label(r&quot;$V^*(\\theta,\\dot{\\theta})$ (Value Iteration)&quot;) ax.set_xlabel(r&quot;$\\dot{\\theta}$&quot;) ax.set_ylabel(r&quot;$\\theta$&quot;) ax.set_title(r&quot;State-value $V$ after Value Iteration&quot;) plt.tight_layout() plt.show() # ----- Visualization: Greedy torque field ----- pi_grid = pi.reshape(N_theta, N_thetadot) # action indices action_values = u_grid[pi_grid] # map indices -&gt; torques plt.figure(figsize=(7,5), dpi=120) im = plt.imshow( action_values, origin=&quot;lower&quot;, extent=[thetadot_grid.min(), thetadot_grid.max(), theta_grid.min(), theta_grid.max()], aspect=&quot;auto&quot;, cmap=&quot;coolwarm&quot; # good for ± torque ) cbar = plt.colorbar(im) cbar.set_label(&quot;Greedy action value (torque)&quot;) plt.xlabel(r&quot;$\\dot{\\theta}$&quot;) plt.ylabel(r&quot;$\\theta$&quot;) plt.title(&quot;Greedy policy (torque) extracted from Value Iteration&quot;) plt.tight_layout() plt.show() Try it for yourself here! You should obtain the same results as policy iteration. "],["value-rl.html", "Chapter 2 Value-based Reinforcement Learning 2.1 Tabular Methods 2.2 Function Approximation", " Chapter 2 Value-based Reinforcement Learning In Chapter 1, we introduced algorithms for policy evaluation, policy improvement, and computing optimal policies in the tabular setting when the model is known. These dynamic-programming methods are grounded in Bellman consistency and optimality and come with strong convergence guarantees. A key limitation of the methods in Chapter 1 is that they require the transition dynamics \\(P(s&#39; \\mid s, a)\\) to be known. While in some applications modeling the dynamics is feasible (e.g., the inverted pendulum), in many others it is costly or impractical to obtain an accurate model of the environment (e.g., a humanoid robot interacting with everyday objects). This motivates relaxing the known-dynamics assumption and asking whether we can design algorithms that learn purely from interaction—i.e., by collecting data through environment interaction. This brings us to model-free reinforcement learning. In this chapter we focus on value-based RL methods. The central idea is to learn the value functions—\\(V(s)\\) and \\(Q(s,a)\\)—from interaction with the environment and then leverage these estimates to derive (approximately) optimal policies. We begin with tabular methods and then move to function-approximation approaches (e.g., neural networks) for problems where a tabular representation is intractable. 2.1 Tabular Methods Consider an infinite-horizon Markov decision process (MDP) \\[ \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma), \\] with a discount factor \\(\\gamma \\in [0,1)\\). We focus on the tabular setting where both the state space \\(\\mathcal{S}\\) and the action space \\(\\mathcal{A}\\) are finite, with cardinalities \\(|\\mathcal{S}|\\) and \\(|\\mathcal{A}|\\), respectively. A policy is a stationary stochastic mapping \\[ \\pi: \\mathcal{S} \\to \\Delta(\\mathcal{A}), \\] where \\(\\pi(a \\mid s)\\) denotes the probability of selecting action \\(a\\) in state \\(s\\). Unlike in Chapter 1, here we do not assume knowledge of the transition dynamics \\(P\\) or the reward function \\(R\\) (other than that \\(R\\) is deterministic). Instead, we assume we can interact with the environment and obtain trajectories of the form \\[ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots), \\] by following a policy \\(\\pi\\). 2.1.1 Policy Evaluation We first consider the problem of estimating the value function of a given policy \\(\\pi\\). Recall the definition of the state-value function associated with \\(\\pi\\) is: \\[\\begin{equation} V^{\\pi}(s) = \\mathbb{E}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right], \\tag{2.1} \\end{equation}\\] where the expectation is taken over the randomness of both the policy \\(\\pi\\) and the transition dynamics \\(P\\). 2.1.1.1 Monte Carlo Estimation The basic idea of Monte Carlo (MC) estimation is to approximate the value function \\(V^\\pi\\) by averaging empirical returns observed from sampled trajectories generated under policy \\(\\pi\\). Since the return is defined as the discounted sum of future rewards, MC methods replace the expectation in the definition of \\(V^\\pi\\) with an average over sampled trajectories. Episodic Assumption. To make Monte Carlo methods well-defined, we restrict attention to the episodic setup, where each trajectory terminates upon reaching a terminal state (and the rewards thereafter are always zero). This ensures that the return is finite and can be computed exactly for each trajectory. Concretely, if an episode terminates at time \\(T\\), the return starting from time \\(t\\) is \\[\\begin{equation} g_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k} = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t-1} r_{T-1}. \\tag{2.2} \\end{equation}\\] Algorithmic Form. Let \\(\\mathcal{D}(s)\\) denote the set of all time indices at which state \\(s\\) is visited across sampled episodes. Then the Monte Carlo estimate of the value function is \\[\\begin{equation} \\hat{V}(s) = \\frac{1}{|\\mathcal{D}(s)|} \\sum_{t \\in \\mathcal{D}(s)} g_t. \\tag{2.3} \\end{equation}\\] There are two common variants: First-visit MC: use only the first occurrence of \\(s\\) in each episode. Every-visit MC: use all occurrences of \\(s\\) within an episode. Both variants converge to the same value function in the limit of infinitely many episodes. Incremental Implementation. Monte Carlo can be written as an incremental stochastic-approximation update that uses the return \\(g_t\\) as the target and a diminishing step size. Let \\(N(s)\\) be the number of (first- or every-) visits to state \\(s\\) that have been used to update \\(\\hat V(s)\\) so far, and let \\(g_t\\) be the return computed at a particular visit time \\(t\\in\\mathcal{D}(s)\\). Then the MC update is \\[\\begin{equation} \\hat V(s) \\;\\leftarrow\\; \\hat V(s) + \\alpha_{N(s)}\\,\\big( g_t - \\hat V(s) \\big), \\qquad \\alpha_{N(s)} &gt; 0 \\text{ diminishing.} \\tag{2.4} \\end{equation}\\] A canonical choice is the sample-average step size \\(\\alpha_{N(s)} = 1/N(s)\\), which yields the recurrence \\[\\begin{align} \\hat V_{N}(s) = \\hat V_{N-1}(s) + \\tfrac{1}{N}\\big(g_t - \\hat V_{N-1}(s)\\big) &amp; = \\Big(1-\\tfrac{1}{N}\\Big)\\hat V_{N-1}(s) + \\tfrac{1}{N}\\, g_t \\\\ &amp; = \\frac{N-1}{N} \\frac{1}{N-1} \\sum_{i=1}^{N-1} g_{t,i} + \\frac{1}{N} g_t \\\\ &amp; = \\frac{1}{N} \\sum_{i=1}^N g_{t,i} \\end{align}\\] so that \\(\\hat V_{N}(s)\\) equals the average of the \\(N\\) observed returns for \\(s\\) (i.e., Eq. (2.3)). In the above equation, I have used \\(g_{t,i}\\) to denote the \\(i\\)-th return before \\(g_t\\) was collected (and \\(g_t = g_{t,N}\\)). More generally, any diminishing schedule satisfying \\[ \\sum_{n=1}^\\infty \\alpha_n = \\infty, \\qquad \\sum_{n=1}^\\infty \\alpha_n^2 &lt; \\infty \\] (e.g., \\(\\alpha_n = c/(n+t_0)^p\\) with \\(1/2 &lt; p \\le 1\\)) also ensures consistency in the tabular setting. In first-visit MC, \\(N(s)\\) increases by one per episode at most; in every-visit MC, \\(N(s)\\) increases at each occurrence of \\(s\\) within an episode. Theoretical Guarantees. Unbiasedness: For any state \\(s\\), the return \\(g_t\\) is an unbiased sample of \\(V^\\pi(s)\\). \\[ \\mathbb{E}[g_t \\mid s_t = s] = V^\\pi(s). \\] Consistency: By the law of large numbers, as the number of episodes grows, \\[ \\hat{V}(s) \\xrightarrow{\\text{a.s.}} V^\\pi(s). \\] Asymptotic Normality: The MC estimator converges at rate \\(O(1/\\sqrt{N})\\), where \\(N\\) is the number of episodes used for the estimation. Limitations. Despite its conceptual simplicity, MC estimation suffers from several drawbacks: It requires episodes to terminate, making it unsuitable for continuing tasks without artificial truncation. It can only update value estimates after an episode ends, which is data-inefficient. While unbiased, MC estimates often have high variance, leading to slow convergence. These limitations motivate the study of Temporal-Difference (TD) learning, which updates value estimates online and can handle continuing tasks. 2.1.1.2 Temporal-Difference Learning While Monte Carlo methods estimate value functions by averaging full returns from complete episodes, Temporal-Difference (TD) learning provides an alternative approach that updates value estimates incrementally after each step of interaction with the environment. The key idea is to combine the sampling of Monte Carlo with the bootstrapping of dynamic programming. High-Level Intuition. TD learning avoids waiting until the end of an episode by using the Bellman consistency equation as a basis for updates. Recall that for any policy \\(\\pi\\), the Bellman consistency equation reads: \\[\\begin{equation} V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid s)} \\left[ R(s,a) + \\gamma \\mathbb{E}_{s&#39; \\sim P(s&#39; \\mid s, a)} V(s&#39;) \\right]. \\tag{2.5} \\end{equation}\\] At a high level, TD learning turns the expectation in Bellman equation into sampling. At each step, it updates the current estimate of the value function toward a one-step bootstrap target: the immediate reward plus the discounted value of the next state. This makes TD methods more data-efficient and applicable to continuing tasks without terminal states. Algorithmic Form. Suppose the agent is in state \\(s_t\\), takes action \\(a_t \\sim \\pi(\\cdot \\mid s_t)\\), receives reward \\(r_t\\), and transitions to \\(s_{t+1}\\). The TD(0) update rule is \\[\\begin{equation} \\hat{V}(s_t) \\;\\leftarrow\\; \\hat{V}(s_t) + \\alpha \\big[ r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t) \\big], \\tag{2.6} \\end{equation}\\] where \\(\\alpha \\in (0,1]\\) is the learning rate. The term inside the brackets, \\[\\begin{equation} \\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t), \\tag{2.7} \\end{equation}\\] is called the TD error. It measures the discrepancy between the current value estimate and the bootstrap target. The algorithm updates \\(\\hat{V}(s_t)\\) in the direction of reducing this error. Theoretical Guarantees. Convergence in the Tabular Case: If each state is visited infinitely often and the learning rate sequence satisfies \\[ \\sum_t \\alpha_t = \\infty, \\; \\sum_t \\alpha_t^2 &lt; \\infty \\] then TD(0) converges almost surely to the true value function \\(V^\\pi\\). For example, choosing \\(\\alpha_t = 1/(t+1)\\) satisfies this condition. Section 2.1.2 provides a detailed proof of the convergence of TD learning. Bias–Variance Tradeoff: The TD target uses the current estimate \\(\\hat{V}(s_{t+1})\\) rather than the true value, which introduces bias. However, it has significantly lower variance than Monte Carlo estimates, often leading to faster convergence in practice. To see this, note that for TD(0), the target is a one-step bootstrap: \\[ y_t = r_t + \\gamma \\hat{V}(s_{t+1}). \\] This replaces the true value \\(V^\\pi(s_{t+1})\\) with the current estimate \\(\\hat{V}(s_{t+1})\\). As a result, \\(y_t\\) is biased relative to the true return. However, since it depends only on the immediate reward and the next state, the variance of \\(y_t\\) is much lower than that of the Monte Carlo target. Limitations. TD(0) relies on bootstrapping, which introduces bias relative to Monte Carlo methods. Convergence can be slow if the learning rate is not chosen carefully. In summary, Temporal-Difference learning addresses the major limitations of Monte Carlo estimation: it works in continuing tasks, updates online at each step, and is generally more sample-efficient. However, it trades away unbiasedness for bias–variance efficiency, motivating further extensions such as multi-step TD and TD(\\(\\lambda\\)). 2.1.1.3 Multi-Step TD Learning Monte Carlo methods use the full return \\(g_t\\), while TD(0) uses a one-step bootstrap. Multi-step TD learning generalizes these two extremes by using \\(n\\)-step returns as targets. In this way, multi-step TD interpolates between Monte Carlo and TD(0). High-Level Intuition. The motivation is to balance the high variance of Monte Carlo with the bias of TD(0). Instead of waiting for a full return (MC) or using only one step of bootstrapping (TD(0)), multi-step TD uses partial returns spanning \\(n\\) steps of real rewards, followed by a bootstrap. This provides a flexible tradeoff between bias and variance. Algorithmic Form. The \\(n\\)-step return starting from time \\(t\\) is defined as \\[\\begin{equation} g_t^{(n)} = r_t + \\gamma r_{t+1} + \\dots + \\gamma^{n-1} r_{t+n-1} + \\gamma^n \\hat{V}(s_{t+n}). \\tag{2.8} \\end{equation}\\] The \\(n\\)-step TD update is \\[\\begin{equation} \\hat{V}(s_t) \\;\\leftarrow\\; \\hat{V}(s_t) + \\alpha \\big[ g_t^{(n)} - \\hat{V}(s_t) \\big], \\tag{2.9} \\end{equation}\\] where \\(g_t^{(n)}\\) replaces the one-step target in TD(0) (2.6). For \\(n=1\\): the method reduces to TD(0). For \\(n=T-t\\) (the full episode length): the method reduces to Monte Carlo. Theoretical Guarantees. Convergence in the Tabular Case: With suitable learning rates and sufficient exploration, \\(n\\)-step TD converges to \\(V^\\pi\\). Bias–Variance Tradeoff: Larger \\(n\\): lower bias, higher variance (closer to Monte Carlo). Smaller \\(n\\): higher bias, lower variance (closer to TD(0)). Intermediate \\(n\\) provides a balance that often yields faster learning in practice. Limitations. Choosing the right \\(n\\) is problem-dependent: too small and bias dominates; too large and variance grows. Requires storing \\(n\\)-step reward sequences before updating, which can increase memory and computation. In summary, multi-step TD unifies Monte Carlo and TD(0) by introducing \\(n\\)-step returns. It allows practitioners to tune the bias–variance tradeoff by selecting \\(n\\). Later, we will see how TD(\\(\\lambda\\)) averages over all \\(n\\)-step returns in a principled way, further smoothing this tradeoff. 2.1.1.4 Eligibility Traces and TD(\\(\\lambda\\)) So far, we have seen that Monte Carlo methods use full returns \\(g_t\\), while TD(0) uses a one-step bootstrap. Multi-step TD methods generalize between these two extremes by using \\(n\\)-step returns. However, a natural question arises: can we combine information from all possible \\(n\\)-step returns in a principled way? This motivates TD(\\(\\lambda\\)), which blends multi-step TD methods into a single algorithm using eligibility traces. High-Level Intuition. TD(\\(\\lambda\\)) introduces a parameter \\(\\lambda \\in [0,1]\\) that controls the weighting of \\(n\\)-step returns: \\(\\lambda = 0\\): reduces to TD(0), relying only on one-step bootstrapping. \\(\\lambda = 1\\): reduces to Monte Carlo, relying on full returns. \\(0 &lt; \\lambda &lt; 1\\): interpolates smoothly between these two extremes by averaging all \\(n\\)-step returns with exponentially decaying weights. Formally, the \\(\\lambda\\)-return is \\[\\begin{equation} g_t^{(\\lambda)} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} g_t^{(n)}, \\tag{2.10} \\end{equation}\\] where \\(g_t^{(n)}\\) is the \\(n\\)-step return defined in (2.8). Remark. To make the \\(\\lambda\\)-return well defined, we consider two cases. Episodic Case: Well-posed. If an episode terminates at time \\(T\\), let \\(N=T-t\\) be the remaining steps. Then \\[\\begin{equation} \\begin{split} g_t^{(\\lambda)} &amp; = (1-\\lambda)\\sum_{n=1}^{N-1}\\lambda^{\\,n-1} \\, g_t^{(n)} \\;+\\; \\lambda^{\\,N-1}\\, g_t^{(N)}, \\\\ &amp; = (1-\\lambda)\\sum_{n=1}^{N}\\lambda^{\\,n-1} \\, g_t^{(n)} \\;+\\; \\lambda^{N}\\, g_t^{(N)}, \\end{split} \\tag{2.11} \\end{equation}\\] where \\(g_t^{(n)}\\) is the \\(n\\)-step return (Eq. (2.8)) and \\(g_t^{(N)}\\) is the full Monte Carlo return (Eq. (2.2)). This expression is well-defined for all \\(\\lambda\\in[0,1]\\). Note that the weights form a convex combination: \\[ (1-\\lambda)\\sum_{n=1}^{N-1}\\lambda^{n-1} + \\lambda^{N-1} = 1-\\lambda^{N-1}+\\lambda^{N-1} = 1. \\] Continuing Case: Limit. Taking \\(\\lambda\\uparrow 1\\) in (2.11) gives \\[ \\lim_{\\lambda\\uparrow 1} g_t^{(\\lambda)} = g_t^{(N)} = g_t, \\] so the \\(\\lambda\\)-return reduces to the Monte Carlo return at \\(\\lambda=1\\). For continuing tasks (no terminal \\(T\\)), \\(\\lambda=1\\) is conventionally defined by this same limiting argument, yielding the infinite-horizon discounted return when \\(\\gamma&lt;1\\). Eligibility Traces. Naively computing \\(g_t^{(\\lambda)}\\) would require storing and combining infinitely many \\(n\\)-step returns, which is impractical. Instead, TD(\\(\\lambda\\)) uses eligibility traces to implement this efficiently online. An eligibility trace is a temporary record that tracks how much each state is “eligible” for updates based on how recently and frequently it has been visited. Specifically, for each state \\(s\\), we maintain a trace \\(z_t(s)\\) that evolves as \\[\\begin{equation} z_t(s) = \\gamma \\lambda z_{t-1}(s) + \\mathbf{1}\\{s_t = s\\}, \\tag{2.12} \\end{equation}\\] where \\(\\mathbf{1}\\{s_t = s\\}\\) is an indicator that equals 1 if state \\(s\\) is visited at time \\(t\\), and 0 otherwise. TD(\\(\\lambda\\)) Update Rule. At each time step \\(t\\), we compute the TD error \\[ \\delta_t = r_t + \\gamma \\hat{V}(s_{t+1}) - \\hat{V}(s_t), \\] as in (2.7). Then, for each state \\(s\\), we update \\[\\begin{equation} \\hat{V}(s) \\;\\leftarrow\\; \\hat{V}(s) + \\alpha \\, \\delta_t \\, z_t(s). \\tag{2.13} \\end{equation}\\] Thus, all states with nonzero eligibility traces are updated simultaneously, with the magnitude of the update determined by both the TD error and the eligibility trace. See Proposition 2.1 below for a justification. Theoretical Guarantees. In the tabular case, TD(\\(\\lambda\\)) converges almost surely to the true value function \\(V^\\pi\\) under the usual stochastic approximation conditions (sufficient exploration, decaying step sizes). The parameter \\(\\lambda\\) directly controls the bias–variance tradeoff: Smaller \\(\\lambda\\): more bootstrapping, more bias but lower variance. Larger \\(\\lambda\\): less bootstrapping, less bias but higher variance. TD(\\(\\lambda\\)) can be shown to converge to the fixed point of the \\(\\lambda\\)-operator, which is itself a contraction mapping. In summary, eligibility traces provide an elegant mechanism to combine the advantages of Monte Carlo and TD learning. TD(\\(\\lambda\\)) introduces a spectrum of algorithms: at one end TD(0), at the other Monte Carlo, and in between a family of methods balancing bias and variance. In practice, intermediate values such as \\(\\lambda \\approx 0.9\\) often work well. Proposition 2.1 (Forward–Backward Equivalence) Consider one episode \\(s_0,a_0,r_0,\\ldots,s_T\\) with \\(\\hat V(s_T)=0\\). Let the forward view apply updates at the end of the episode: \\[ \\hat V(s_t) \\leftarrow \\hat V(s_t) + \\alpha \\big[g_t^{(\\lambda)}-\\hat V(s_t)\\big], \\quad t=0,\\ldots,T-1, \\] where \\(g_t^{(\\lambda)}\\) is the \\(\\lambda\\)-return in (2.10) with the \\(n\\)-step returns \\(g_t^{(n)}\\) from (2.8), and where \\(\\hat V\\) is kept fixed while computing all \\(g_t^{(\\lambda)}\\). Let the backward view run through the episode once, using the TD error \\(\\delta_t\\) from (2.7) and eligibility traces \\(z_t(s)\\) from (2.12), and then apply the cumulative update \\[ \\Delta_{\\text{back}} \\hat V(s) \\;=\\; \\alpha \\sum_{t=0}^{T-1} \\delta_t\\, z_t(s). \\] Then, for every state \\(s\\), \\[ \\Delta_{\\text{back}} \\hat V(s) \\;=\\; \\alpha \\sum_{t:\\, s_t=s}\\big[g_t^{(\\lambda)}-\\hat V(s_t)\\big], \\] i.e., the net parameter change produced by (2.13) equals that of the \\(\\lambda\\)-return updates. Proof. Fix a state \\(s\\). Using (2.12), \\[ z_t(s)=\\sum_{k=0}^{t}(\\gamma\\lambda)^{\\,t-k}\\,\\mathbf{1}\\{s_k=s\\}. \\] Hence \\[ \\sum_{t=0}^{T-1}\\delta_t z_t(s) =\\sum_{t=0}^{T-1}\\delta_t \\sum_{k=0}^{t}(\\gamma\\lambda)^{\\,t-k}\\mathbf{1}\\{s_k=s\\} =\\sum_{k:\\,s_k=s}\\; \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{\\,t-k}\\delta_t . \\tag{1} \\] Write \\(\\delta_t=r_t+\\gamma\\hat V(s_{t+1})-\\hat V(s_t)\\) and split the inner sum: \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = \\underbrace{\\sum_{t=k}^{T-1} \\gamma^{t-k}\\lambda^{t-k} r_t}_{\\text{(A)}} + \\underbrace{\\sum_{t=k}^{T-1}\\gamma^{t-k}\\lambda^{t-k}(\\gamma\\hat V(s_{t+1})-\\hat V(s_t))}_{\\text{(B)}}. \\] Term (B) telescopes. Shifting index in the first part of (B), \\[ \\sum_{t=k}^{T-1}\\gamma^{t-k}\\lambda^{t-k}\\gamma \\hat V(s_{t+1}) = \\sum_{t=k+1}^{T}\\gamma^{t-k}\\lambda^{t-1-k}\\hat V(s_t). \\] Therefore \\[ \\text{(B)}= -\\hat V(s_k) + \\sum_{t=k+1}^{T-1}\\gamma^{t-k}\\lambda^{t-1-k}(1-\\lambda)\\hat V(s_t) + \\underbrace{\\gamma^{T-k}\\lambda^{T-1-k}\\hat V(s_T)}_{=\\,0}. \\tag{2} \\] Combining (A) and (2), and reindexing with \\(n=t-k\\), \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = -\\hat V(s_k) + \\sum_{n=0}^{T-1-k}\\gamma^{n}\\lambda^{n} r_{k+n} + (1-\\lambda)\\sum_{n=1}^{T-1-k}\\gamma^{n}\\lambda^{n-1}\\hat V(s_{k+n}). \\tag{3} \\] On the other hand, expanding the \\(\\lambda\\)-return (2.10), \\[ \\begin{aligned} g_k^{(\\lambda)} &amp;=(1-\\lambda)\\sum_{n=1}^{T-k}\\lambda^{n-1} \\Bigg(\\sum_{m=0}^{n-1}\\gamma^{m} r_{k+m} + \\gamma^{n}\\hat V(s_{k+n})\\Bigg) + \\lambda^{T-k} g_k^{(T-k)}\\\\ &amp;= \\sum_{n=0}^{T-1-k}\\gamma^{n}\\lambda^{n} r_{k+n} + (1-\\lambda)\\sum_{n=1}^{T-1-k}\\gamma^{n}\\lambda^{n-1}\\hat V(s_{k+n}), \\end{aligned} \\tag{4} \\] where we used that \\(\\hat V(s_T)=0\\). Comparing (3) and (4) yields \\[ \\sum_{t=k}^{T-1} (\\gamma\\lambda)^{t-k}\\delta_t = g_k^{(\\lambda)} - \\hat V(s_k). \\tag{5} \\] Substituting (5) into (1) and multiplying by \\(\\alpha\\) completes the proof. Example 2.1 (Policy Evaluation (MC and TD Family)) We consider the classic random-walk MDP with terminal states: States: \\(\\{0,1,2,3,4,5,6\\}\\), where \\(0\\) and \\(6\\) are terminal; nonterminal states are \\(1{:}5\\). Actions: \\(\\{-1,+1\\}\\) (“Left”/“Right”). Dynamics: From a nonterminal state \\(s\\in\\{1,\\dots,5\\}\\), action \\(-1\\) moves to \\(s-1\\), and action \\(+1\\) moves to \\(s+1\\). Rewards: Transitioning into state \\(6\\) yields reward \\(+1\\); all other transitions yield \\(0\\). Discount: \\(\\gamma=1\\) (episodic task). Episodes start at state \\(s_0=3\\) and terminate upon reaching \\(\\{0,6\\}\\). We evaluate the equiprobable policy \\(\\pi\\) that chooses Left/Right with probability \\(1/2\\) each at every nonterminal state. Under this policy, the true state-value function on nonterminal states \\(s\\in\\{1,\\dots,5\\}\\) is \\[\\begin{equation} V^\\pi(s) \\;=\\; \\frac{s}{6}. \\tag{2.14} \\end{equation}\\] We compare four tabular policy-evaluation methods: Monte Carlo (MC), first-visit — using full returns as target. TD(0) — one-step bootstrap. \\(n\\)-step TD — here we use \\(n=3\\) (intermediate between MC and TD(0)). TD(\\(\\lambda\\)) — accumulating eligibility traces (we illustrate with \\(\\lambda=0.9\\)). All methods estimate \\(V^\\pi\\) from trajectories generated by \\(\\pi\\). Error Metric. We report the mean-squared error (MSE) over nonterminal states after each episode: \\[\\begin{equation} \\mathrm{MSE}_t \\;=\\; \\frac{1}{5}\\sum_{s=1}^{5}\\big(\\hat V_t(s)-V^\\pi(s)\\big)^2, \\tag{2.15} \\end{equation}\\] where \\(V^\\pi\\) is given by (2.14). Curves are averaged over multiple random seeds. Fixed Step Sizes. We first use a fixed step size \\(\\alpha=0.1\\) for all methods. Fig. 2.1 shows the trajectories of MSE versus number of episodes. We can see that, when using a constant step size, these methods do not converge to exactly the true value function, but to a small neighborhood. In addition, if the algorithm initially decays very fast, then the final variance is larger. For example, MC initially decays very fast, but has a higher variance, whereas TD(0) initially decays slower, but has a lower final variance. This agrees with the theoretical analysis in (Kearns and Singh 2000). Figure 2.1: Policy Evaluation, MC versus TD Family, Fixed Step Size Diminishing Step Sizes. We then use a diminishing step size for the TD family: \\[\\begin{equation} \\alpha_t(s) \\;=\\; \\frac{c}{\\big(N_t(s)+t_0\\big)^p}, \\qquad \\tfrac{1}{2} &lt; p \\le 1, \\tag{2.16} \\end{equation}\\] where \\(N_t(s)\\) counts how many times \\(V(s)\\) has been updated up to time \\(t\\). A common choice is \\(p=1\\) with moderate \\(c&gt;0\\) and \\(t_0&gt;0\\). Fig. 2.2 shows the MSE versus episodes for MC, TD(0), 3-step TD, and TD(\\(\\lambda\\)) under the diminishing step-size. Observe that all algorithms converge to the true value function under the diminishing step size schedule. Figure 2.2: Policy Evaluation, MC versus TD Family, Diminishing Step Size You are encouraged to play with the parameters of these algorithms in the code here. 2.1.2 Convergence Proof of TD Learning Setup. Consider a tabular MDP with finite state space \\(\\mathcal{S}\\) and action space \\(\\mathcal{A}\\), and a discount factor \\(\\gamma \\in [0,1)\\). Assume the reward function is bounded, for example, \\(R(s,a) \\in [0,1]\\) for any \\((s,a) \\in \\mathcal{S} \\times \\mathcal{A}\\). Let \\(\\pi\\) be a stochastic policy and \\(V^\\pi\\) be the true value function associated with \\(\\pi\\), the target we wish to estimate from interaction data. Denote \\[ \\mathcal{F}_t = \\sigma(s_0,a_0,r_0,\\dots,s_{t-1},a_{t-1},r_{t-1}), \\] as the \\(\\sigma\\) algebra of all state-action-reward information up to time \\(t-1\\). TD(0) Update. We maintain a tabular estimate \\(V_t\\) of the true value \\(V^\\pi\\). On visiting \\(s_t\\) and observing \\((s_t, a_t, r_t, s_{t+1})\\), the TD(0) algorithm performs \\[\\begin{equation} V_{t+1}(s_t) = V_t(s_t) + \\alpha_t(s_t) \\delta_t, \\tag{2.17} \\end{equation}\\] where \\(\\delta_t\\) is the TD error \\[ \\delta_t = r_t + \\gamma V_{t}(s_{t+1}) - V_t(s_t). \\] The update (2.17) only changes the value at \\(s_t\\), leaving the value at other states unchanged. Robbins–Monro Step Size. We assume the step size \\(\\alpha\\) satisfy the Robbins–Monro condition. That is, for any \\(s \\in \\mathcal{S}\\): \\[ \\alpha_t(s) &gt;0, \\quad \\sum_{t: s_t = s} \\alpha_t(s) = \\infty, \\quad \\sum_{t: s_t = s} \\alpha^2_t(s) &lt; \\infty. \\] Stationary Distribution. Assume the Markov chain over \\(\\mathcal{S}\\) induced by \\(\\pi\\) is ergodic, then a unique stationary state distribution \\(\\mu^\\pi\\) exists and satisfy: \\[\\begin{equation} \\mu^\\pi(s&#39;) = \\sum_{s \\in \\mathcal{S}} \\mu^\\pi(s) \\left( \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) P(s&#39; \\mid s, a) \\right), \\quad \\forall s&#39; \\in \\mathcal{S}. \\tag{2.18} \\end{equation}\\] If we denote \\[\\begin{equation} P^\\pi(s&#39; \\mid s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) P(s&#39; \\mid s,a), \\tag{2.19} \\end{equation}\\] as the \\(\\pi\\)-induced state-only transition dynamics, then condition (2.18) is equivalent to \\[\\begin{equation} \\mu^\\pi(s&#39;) = \\sum_{s \\in \\mathcal{S}} \\mu^\\pi(s) P^\\pi (s&#39; \\mid s), \\quad \\forall s&#39; \\in \\mathcal{S}. \\tag{2.20} \\end{equation}\\] See (2.42) for a generalization to continuous MDP. Bellman Operator. For any \\(V:\\mathcal S\\to\\mathbb R\\), define the Bellman operator associated with \\(\\pi\\) as \\(T^\\pi V:\\mathcal S\\to\\mathbb R\\) by \\[\\begin{equation} (T^\\pi V)(s)\\;=\\;\\sum_{a\\in\\mathcal A}\\pi(a\\mid s)\\sum_{s&#39;\\in\\mathcal S} P(s&#39;\\mid s,a)\\,\\Big( R(s,a)+\\gamma\\,V(s&#39;)\\Big). \\tag{2.21} \\end{equation}\\] We know that the operator \\(T^\\pi\\) is a \\(\\gamma\\)-contraction in \\(\\|\\cdot\\|_\\infty\\). Hence it has a unique fixed point \\(V^\\pi\\) satisfying \\(V^\\pi=T^\\pi V^\\pi\\). The following theorem states the almost sure convergence of TD learning iterates to the true value function. Theorem 2.1 (TD(0) Convergence (Tabular)) Under the tabular MDP setup and assumptions above, the TD(0) iterates \\(V_t\\) generated by (2.17) converge almost surely to \\(V^\\pi\\). To prove this theorem, we need the following two lemmas. Lemma 2.1 (Robbins-Siegmund Lemma) Let \\((X_t)_{t\\ge 0}\\) be nonnegative and adapted to \\((\\mathcal F_t)\\). Suppose there exist nonnegative \\((\\beta_t),(\\gamma_t),(\\xi_t)\\) with \\(\\sum_t \\gamma_t&lt;\\infty\\) and \\(\\sum_t \\xi_t&lt;\\infty\\) such that \\[ \\mathbb E[X_{t+1}\\mid \\mathcal F_t]\\;\\le\\;(1+\\gamma_t)X_t\\;-\\;\\beta_t\\;+\\;\\xi_t\\qquad\\text{almost surely} \\] Then \\(X_t\\) converges almost surely to a finite random variable and \\(\\sum_t \\beta_t&lt;\\infty\\) almost surely. This lemma is from (Robbins and Siegmund 1971). Lemma 2.2 Let \\(\\mu^\\pi\\) be the stationary distribution in (2.20), \\(D=\\mathrm{diag}(\\mu^\\pi)\\), and \\(w:=V-V^\\pi\\). Then \\[ \\langle w,\\,D\\,(T^\\pi V - V)\\rangle\\;\\le\\;-(1-\\gamma)\\,\\|w\\|_D^2, \\] where \\(\\langle x,y\\rangle=x^\\top y\\) and \\(\\|w\\|_D^2=\\sum_s \\mu^\\pi(s)\\,w(s)^2\\). Proof. First, for any two value functions \\(V, U \\in \\mathbb{R}^{|\\mathcal{S}|}\\), we have \\[ (T^\\pi V)(s) - (T^\\pi U)(s) = \\gamma \\sum_{a} \\pi(a \\mid s) \\sum_{s&#39;} P(s&#39; \\mid s,a) (V(s&#39;) - U(s)&#39;). \\] Therefore, \\[ T^\\pi V - T^\\pi U = \\gamma \\widetilde P (V - U), \\] with \\[\\begin{equation} (\\widetilde P u)(s):=\\sum_a\\pi(a\\mid s)\\sum_{s&#39;}P(s&#39;\\mid s,a)\\,u(s&#39;). \\tag{2.22} \\end{equation}\\] With this, we can write \\[\\begin{equation} \\begin{split} T^\\pi V - V &amp; = T^\\pi V - V^\\pi + V^\\pi - V \\\\ &amp; = T^\\pi V - T^\\pi V^\\pi - ( V - V^\\pi ) \\\\ &amp; = \\gamma \\widetilde P (V - V^\\pi) - ( V - V^\\pi ) \\\\ &amp; = (\\gamma \\widetilde P - I) ( V - V^\\pi ) \\\\ &amp; = (\\gamma \\widetilde P - I) w. \\end{split} \\end{equation}\\] Thus, \\[\\begin{equation} \\langle w, D(T^\\pi V - V)\\rangle = -\\,w^\\top D\\,(I-\\gamma \\widetilde P)\\,w = -\\|w\\|_D^2 + \\gamma\\,\\langle w, D\\,\\widetilde P w\\rangle. \\tag{2.23} \\end{equation}\\] Next, we prove \\(\\langle w, D\\widetilde P w\\rangle\\le \\|w\\|_D^2\\). First, we show \\(\\|\\widetilde P w\\|_D\\le \\|w\\|_D\\). For any state \\(s \\in \\mathcal{S}\\), from (2.22), we have \\[ (\\widetilde P w)(s) = \\sum_{s&#39;} P^\\pi (s&#39; \\mid s) w(s&#39;), \\] where \\(P^\\pi(s&#39; \\mid s)\\) is the \\(\\pi\\)-induced state-only transition in (2.19). Since \\(P^\\pi(\\cdot \\mid s)\\) is a probability distribution, and \\(x \\mapsto x^2\\) is convex, we have \\[ ((\\widetilde P w)(s))^2 = \\left( \\sum_{s&#39;} P^\\pi (s&#39; \\mid s) w(s&#39;) \\right)^2 \\leq \\sum_{s&#39;} P^\\pi (s&#39; \\mid s) w^2(s&#39;). \\] Therefore, we have \\[\\begin{equation} \\begin{split} \\Vert \\widetilde P w \\Vert_D^2 &amp; = \\sum_s \\mu^{\\pi}(s) ((\\widetilde P w)(s))^2 \\\\ &amp; \\leq \\sum_s \\mu^{\\pi}(s) \\left( \\sum_{s&#39;} P^\\pi (s&#39; \\mid s) w^2(s&#39;) \\right) \\\\ &amp; = \\sum_{s&#39;} \\left( \\sum_{s} \\mu^\\pi(s) P^\\pi(s&#39; \\mid s) \\right) w^2(s&#39;) \\\\ &amp; = \\sum_{s&#39;} \\mu^\\pi (s&#39;) w^2 (s&#39;) = \\Vert w \\Vert_D^2. \\end{split} \\end{equation}\\] where the second-from-last equality holds because \\(\\mu^\\pi\\) is the stationary distribution and satisfies (2.20). Second, we write \\[ \\langle w, D\\widetilde P w\\rangle = \\langle D^{0.5} w, D^{0.5} \\widetilde P w \\rangle \\leq \\Vert D^{0.5} w \\Vert \\cdot \\Vert D^{0.5} \\widetilde P w \\Vert = \\Vert w \\Vert_D \\cdot \\Vert \\widetilde P w \\Vert_D \\leq \\Vert w \\Vert_D^2. \\] Plugging this back to (2.23), we obtain \\[ \\langle w, D(T^\\pi V - V) \\rangle \\leq - \\Vert w \\Vert_D^2 + \\gamma \\Vert w \\Vert_D^2, \\] proving the desired result in the lemma. We are now ready to prove Theorem 2.1. Proof. Step 1 (TD as stochastic approximation). For the TD error \\[ \\delta_t=r_{t}+\\gamma V_t(s_{t+1})-V_t(s_t), \\] we have the conditional expectation \\[ \\mathbb E[\\delta_t\\mid \\mathcal F_t, s_t] =\\sum_{a}\\pi(a\\mid s_t)\\sum_{s&#39;}P(s&#39;\\mid s_t,a)\\Big(R(s_t,a)+\\gamma V_t(s&#39;)\\Big)-V_t(s_t) =\\big(T^\\pi V_t - V_t\\big)(s_t). \\] Define the “noise”: \\[ \\eta_{t+1}:=\\delta_t-\\mathbb E[\\delta_t\\mid \\mathcal F_t,s_t]. \\] Then \\(\\mathbb E[\\eta_{t+1}\\mid \\mathcal F_t,s_t]=0\\) and the TD update is equivalent to \\[\\begin{equation} V_{t+1}(s_t)=V_t(s_t)+\\alpha_t(s_t)\\Big( \\big(T^\\pi V_t - V_t\\big)(s_t) + \\eta_{t+1}\\Big), \\tag{2.24} \\end{equation}\\] while learving all other coordinates unchanged. Because rewards are uniformly bounded, we know that \\(V_t\\) remains bounded. Hence, \\(\\mathbb E[\\eta_{t+1}^2\\mid \\mathcal F_t, s_t]\\) is uniformly bounded. Equation (2.24) shows that the TD update can be seen as a stochastic approximation to the Bellman operator (2.21). Step 2 (Lyapunov drift). Let \\(D=\\mathrm{diag}(\\mu^\\pi)\\), a diagonal matrix whose diagonal entries are the probabilities in \\(\\mu^\\pi\\). Define the Lyapunov function \\[ \\mathcal L(V)=\\frac{1}{2} \\|V-V^\\pi\\|_D^2=\\frac{1}{2} \\sum_s \\mu^\\pi(s)\\,\\big(V(s)-V^\\pi(s)\\big)^2. \\] Let \\(w_t:=V_t - V^\\pi\\). Since only the \\(s_t\\)-coordinate changes at time \\(t\\), we have \\[\\begin{equation} \\begin{split} \\mathcal L(V_{t+1})-\\mathcal L(V_t) &amp;=\\frac{1}{2} \\mu^\\pi(s_t)\\Big(( \\underbrace{V_t(s_t)+\\alpha_t\\delta_t}_{V_{t+1}(s_t)} - V^\\pi(s_t) )^2 -(V_t(s_t)-V^\\pi(s_t))^2\\Big)\\\\ &amp;= \\mu^\\pi(s_t)\\,\\alpha_t\\,\\delta_t\\,w_t(s_t)\\;+\\;\\frac{1}{2} \\mu^\\pi(s_t)\\,\\alpha_t^2\\,\\delta_t^{\\,2}. \\end{split} \\tag{2.25} \\end{equation}\\] Define \\(g_t := T^\\pi V_t - V_t\\). Taking conditional expectation given \\(\\mathcal F_t\\) and i.i.d. \\(s_t\\sim \\mu^\\pi\\), \\[\\begin{equation} \\begin{split} \\mathbb E\\!\\left[\\mathcal L(V_{t+1})-\\mathcal L(V_t)\\mid \\mathcal F_t\\right] &amp;= \\alpha_t\\,\\mathbb E\\!\\left[\\mu^\\pi(s_t)\\,w_t(s_t)\\,g_t(s_t)\\mid \\mathcal F_t\\right] + \\frac{1}{2} \\alpha_t^2\\,\\mathbb E\\!\\left[\\mu^\\pi(s_t)\\,\\delta_t^{\\,2}\\mid \\mathcal F_t\\right]\\\\ &amp;= \\alpha_t\\,\\sum_s \\mu^\\pi(s)\\,w_t(s)\\,g_t(s) \\;+\\; \\frac{1}{2} \\alpha_t^2\\,C_t, \\end{split} \\tag{2.26} \\end{equation}\\] where \\(C_t:=\\mathbb E\\!\\left[\\mu^\\pi(s_t)\\,\\delta_t^{\\,2}\\mid \\mathcal F_t\\right]\\) is finite because rewards are bounded and \\(V_t\\) stays bounded. Assume \\(C_t \\leq C\\). At the same time, by Lemma 2.2 \\[ \\sum_s \\mu^\\pi(s)\\,w_t(s)\\,g_t(s) =\\langle w_t, D g_t\\rangle \\le -(1-\\gamma)\\,\\|w_t\\|_D^2. \\] Plugging into (2.26) yields \\[\\begin{equation} \\mathbb E\\!\\left[\\mathcal L(V_{t+1})\\mid \\mathcal F_t\\right] \\;\\le\\; \\mathcal L(V_t)\\;-\\;\\alpha_t\\,(1-\\gamma)\\,\\|w_t\\|_D^2\\;+\\;\\frac{1}{2} \\alpha_t^2\\,C. \\tag{2.27} \\end{equation}\\] This is in Robbins–Siegmund form with \\[ X_t:=\\mathcal L(V_t),\\qquad \\beta_t:=(1-\\gamma)\\,\\alpha_t\\,\\|w_t\\|_D^2,\\qquad \\gamma_t:=0,\\qquad \\xi_t:=\\frac{1}{2} C\\,\\alpha_t^2. \\] We have \\(\\sum_t \\xi_t&lt;\\infty\\) by \\(\\sum_t \\alpha_t^2&lt;\\infty\\). Therefore \\(X_t\\) converges a.s. and \\(\\sum_t \\beta_t&lt;\\infty\\) a.s., which implies \\(\\sum_t \\alpha_t \\|w_t\\|_D^2&lt;\\infty\\). Since \\(\\sum_t \\alpha_t=\\infty\\), it must be that \\(\\liminf_t \\|w_t\\|_D=0\\). Finally, using (2.27) again and the continuity of the drift, one shows that any subsequential limit of \\(V_t\\) must satisfy \\(T^\\pi V - V=0\\); by uniqueness of the fixed point, the only possible limit is \\(V^\\pi\\). Hence \\(V_t\\to V^\\pi\\) almost surely. 2.1.3 On-Policy Control Monte Carlo (MC) estimation and the TD family evaluate policies directly from interaction—no model required. We now turn evaluation into control via generalized policy iteration (GPI): repeatedly (i) evaluate the current policy from data and (ii) improve it by acting greedily with respect to the new estimates. We first cover on-policy control methods, which estimate and improve the same (typically \\(\\varepsilon\\)-greedy) policy, and then off-policy methods, which learn about a target policy while behaving with a different one. 2.1.3.1 Monte Carlo Control High-level Intuition. Goal. Learn an (approximately) optimal policy by alternating policy evaluation and policy improvement using only sampled episodes. Why action-values? Estimating \\(Q^\\pi(s,a)\\) lets us improve the policy without a model by choosing “\\(\\arg\\max_a Q(s,a)\\)”. Exploration. Pure greedy improvement can get stuck. MC control keeps the policy \\(\\varepsilon\\)-soft (e.g., \\(\\varepsilon\\)-greedy) so that every action has nonzero probability and all state-action pairs continue to be sampled. An \\(\\varepsilon\\)-soft policy is one that never rules out any action: in every state \\(s\\), each action \\(a\\) gets at least a small fraction of probability. Formally, in the tabular setup, we have that a policy \\(\\pi\\) is \\(\\varepsilon\\)-soft if and only if \\[\\begin{equation} \\forall s, \\forall a: \\quad \\pi(a \\mid s) \\geq \\frac{\\varepsilon}{|\\mathcal{A}(s)|}, \\quad \\varepsilon \\in (0,1], \\tag{2.28} \\end{equation}\\] where \\(\\mathcal{A}(s)\\) denotes the set of actions the agent can select at state \\(s\\). Coverage mechanisms. Classic guarantees use either: Exploring starts (ES): start each episode from a randomly chosen \\((s,a)\\) with nonzero probability; or \\(\\varepsilon\\)-soft / GLIE (Greedy in the Limit with Infinite Exploration): use \\(\\varepsilon\\)-greedy behavior with \\(\\varepsilon_t \\downarrow 0\\) so every \\((s,a)\\) is visited infinitely often while the policy becomes greedy in the limit. Algorithmic Form. We maintain tabular action-value estimates \\(Q(s,a)\\) and an \\(\\varepsilon\\)-soft policy \\(\\pi\\) (\\(\\varepsilon\\)-greedy w.r.t. \\(Q\\)). After each episode we update \\(Q\\) from empirical returns and then improve \\(\\pi\\). Return from time \\(t\\): \\[ g_t = r_t + \\gamma r_{t+1} + \\dots + \\gamma^{T-t} r_T = \\sum_{k=0}^{T-t} \\gamma^{k} r_{t+k}. \\] First-visit MC update (common choice): \\[\\begin{equation} Q(s_t,a_t) \\;\\leftarrow\\; Q(s_t,a_t) + \\alpha_{N(s_t,a_t)}\\!\\left(g_t - Q(s_t,a_t)\\right), \\tag{2.29} \\end{equation}\\] applied only on the first occurrence of \\((s_t,a_t)\\) in the episode. Sample-average learning uses \\(\\alpha_n = 1/n\\) per pair; more generally, use diminishing stepsizes. Policy improvement (\\(\\varepsilon\\)-greedy): \\[\\begin{equation} \\pi(a|s) \\;=\\; \\begin{cases} 1-\\varepsilon + \\dfrac{\\varepsilon}{|\\mathcal{A}(s)|}, &amp; a \\in \\arg\\max_{a&#39;} Q(s,a&#39;), \\\\ \\dfrac{\\varepsilon}{|\\mathcal{A}(s)|}, &amp; \\text{otherwise}. \\end{cases} \\tag{2.30} \\end{equation}\\] Theoretical Guarantees. Assume a tabular episodic MDP and \\(\\gamma \\in [0,1)\\). Convergence with Exploring Starts. If every state–action pair has nonzero probability of being the first pair of an episode (using ES), and each \\(Q(s,a)\\) is updated toward the true mean return from \\((s,a)\\) (e.g., via sample averages), then repeated policy evaluation and greedy improvement converge with probability 1 to an optimal deterministic policy. (If one uses an \\(\\varepsilon\\)-greedy improvement, then it converges to an optimal \\(\\varepsilon\\)-soft policy.) Convergence with \\(\\varepsilon\\)-soft GLIE behavior. If the behavior policy is GLIE—every \\((s,a)\\) is visited infinitely often and \\(\\epsilon_t \\to 0\\)—and the stepsizes for each \\((s,a)\\) satisfy the Robbins–Monro conditions \\(\\sum_{t} \\alpha_t(s,a) = \\infty,\\sum_{t} \\alpha_t(s,a)^2 &lt; \\infty\\), then \\(Q(s,a)\\) converges to \\(Q^\\star(s,a)\\) for all pairs visited infinitely often, and the \\(\\varepsilon\\)-greedy policy converges almost surely to an optimal policy. Remark. Unbiased but high-variance. MC targets \\(g_t\\) are unbiased estimates of action values under the current policy, but can have high variance—especially for long horizons—so convergence can be slower than TD methods. Keeping \\(\\varepsilon&gt;0\\) ensures exploration but limits asymptotic optimality to the best \\(\\varepsilon\\)-soft policy; hence \\(\\varepsilon_t \\downarrow 0\\) (GLIE) is recommended for optimality. 2.1.3.2 SARSA (On-Policy TD Control) High-level Intuition. Goal. Turn evaluation into control by updating action values online and improving the same policy that generates data. Key idea. Replace Monte Carlo returns with a bootstrapped target. After taking action \\(a_t\\) in state \\(s_t\\) and observing \\(r_{t}, s_{t+1}\\), sample the next action \\(a_{t+1}\\) from the current policy and update toward \\(r_{t} + \\gamma Q(s_{t+1}, a_{t+1})\\). On-policy nature. SARSA evaluates the behavior policy itself, typically an \\(\\varepsilon\\)-greedy policy w.r.t. \\(Q\\). Exploration. Use \\(\\varepsilon\\)-soft behavior so every action keeps nonzero probability. For optimality, let \\(\\varepsilon_t \\downarrow 0\\) to obtain GLIE (Greedy in the Limit with Infinite Exploration). Algorithmic Form. Let \\(Q\\) be a tabular action-value function and \\(\\pi_t\\) be \\(\\varepsilon_t\\)-greedy w.r.t. \\(Q_t\\). TD target and error: \\[\\begin{equation} y_t = r_{t} + \\gamma Q(s_{t+1}, a_{t+1}), \\qquad \\delta_t = y_t - Q(s_t, a_t). \\tag{2.31} \\end{equation}\\] SARSA update (one-step): \\[\\begin{equation} Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha_t(s_t,a_t)\\, \\delta_t. \\tag{2.32} \\end{equation}\\] \\(\\varepsilon\\)-greedy policy improvement: \\[\\begin{equation} \\pi_{t+1}(a\\mid s) = \\begin{cases} 1-\\varepsilon_{t+1} + \\dfrac{\\varepsilon_{t+1}}{|\\mathcal A(s)|}, &amp; a \\in \\arg\\max_{a&#39;} Q_{t+1}(s,a&#39;),\\\\ \\dfrac{\\varepsilon_{t+1}}{|\\mathcal A(s)|}, &amp; \\text{otherwise.} \\end{cases} \\tag{2.33} \\end{equation}\\] Variants. Expected SARSA replaces the sampled \\(a_{t+1}\\) by its expectation under \\(\\pi_t\\) for lower variance: \\[\\begin{equation} y_t = r_{t} + \\gamma \\sum_a \\pi_t(a\\mid s_{t+1}) Q(s_{t+1}, a). \\tag{2.34} \\end{equation}\\] \\(n\\)-step SARSA and SARSA(\\(\\lambda\\)) blend multi-step targets; these trade bias and variance similarly to MC vs TD. Convergence Guarantees. Assume a finite MDP, \\(\\gamma \\in [0,1)\\), asynchronous updates, and that each state–action pair is visited infinitely often. GLIE convergence to optimal policy. If the behavior is GLIE, i.e., \\(\\varepsilon_t \\downarrow 0\\) while ensuring infinite exploration, and stepsizes satisfy the Robbins–Monro conditions, then \\(Q_t \\to Q^\\star\\) almost surely and the \\(\\varepsilon_t\\)-greedy behavior becomes greedy in the limit, yielding an optimal policy almost surely. 2.1.4 Off-Policy Control Off-policy methods learn about a target policy \\(\\pi\\) while following a (potentially different) behavior policy \\(b\\) to gather data. This decoupling is useful when: you want to reuse logged data collected by some \\(b\\) (e.g., a rule-based controller or a past system), you need safer exploration by restricting behavior \\(b\\) while aiming to evaluate or improve a different \\(\\pi\\), you want to learn about the greedy policy without executing it, which motivates algorithms like Q-learning. In this section we first cover off-policy policy evaluation with importance sampling, then show how it can be used to construct an off-policy Monte Carlo control scheme in the tabular case. Finally, we present Q-learning. 2.1.4.1 Importance Sampling for Policy Evaluation Motivation. Suppose we have episodes generated by a behavior policy \\(b\\), but we want the value of a different target policy \\(\\pi\\). For a state value this is \\(V^\\pi(s) = \\mathbb{E}_\\pi[g_t \\mid s_t=s]\\), and for action values \\(Q^\\pi(s,a) = \\mathbb{E}_\\pi[g_t \\mid s_t=s, a_t=a]\\), where \\[ g_t = \\sum_{k=0}^{T-t} \\gamma^{k} r_{t+k}. \\] Because the data come from \\(b\\), the naive sample average is biased. Importance sampling (IS) reweights returns so that expectations under \\(b\\) equal those under \\(\\pi\\). A basic support condition is required: \\[\\begin{equation} \\text{If } \\pi(a\\mid s) &gt; 0 \\text{ then } b(a\\mid s) &gt; 0 \\quad \\text{for all visited } (s,a). \\tag{2.35} \\end{equation}\\] This ensures that \\(\\pi\\) is absolutely continuous with respect to \\(b\\) on the experienced trajectories. Importance Sampling (episode-wise). Consider a trajectory starting at time \\(t\\): \\[ \\tau_t = (s_t, a_t, r_t, s_{t+1}, a_{t+1}, \\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T). \\] The probability of observing this trajectory conditioned on \\(s_t = s\\), under policy \\(\\pi\\), is \\[ \\mathbb{P}_{\\pi}[\\tau_t \\mid s_t = s] = \\pi(a_t \\mid s_t) P(s_{t+1} \\mid s_t, a_t) \\pi(a_{t+1} \\mid s_{t+1}) \\cdots \\pi(a_{T-1} \\mid s_{T-1}) P(s_T \\mid s_{T-1}, a_{T-1}). \\] The probability of observing the same trajectory conditioned on \\(s_t = s\\), under policy \\(b\\), is \\[ \\mathbb{P}_{b}[\\tau_t \\mid s_t = s] = b(a_t \\mid s_t) P(s_{t+1} \\mid s_t, a_t) b(a_{t+1} \\mid s_{t+1}) \\cdots b(a_{T-1} \\mid s_{T-1}) P(s_T \\mid s_{T-1}, a_{T-1}). \\] Since the return \\(g_t\\) is a deterministic function of \\(\\tau_t\\), i.e., applying the reward function \\(R\\) to state-action pairs, we have that \\[\\begin{equation} \\begin{split} V^\\pi (s) &amp; = \\mathbb{E}_{\\pi}[g_t \\mid s_t = s] = \\sum_{\\tau_t} g_t \\mathbb{P}_\\pi [\\tau_t \\mid s_t = s] \\\\ &amp; = \\sum_{\\tau_t} g_t \\mathbb{P}_b[\\tau_t \\mid s_t = s] \\left(\\frac{\\mathbb{P}_\\pi [\\tau_t \\mid s_t = s]}{\\mathbb{P}_b[\\tau_t \\mid s_t = s]} \\right) \\\\ &amp; = \\sum_{\\tau_t} \\left( \\frac{\\pi(a_t \\mid s_t) \\pi(a_{t+1} \\mid s_{t+1}) \\cdots \\pi(a_{T-1} \\mid s_{T-1}) }{b(a_t \\mid s_t) b(a_{t+1} \\mid s_{t+1}) \\cdots b(a_{T-1} \\mid s_{T-1})} \\right) g_t \\mathbb{P}_b [\\tau_t \\mid s_t = s] \\end{split} \\tag{2.36} \\end{equation}\\] Therefore, define the likelihood ratio \\[\\begin{equation} \\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)}, \\tag{2.37} \\end{equation}\\] we have \\[\\begin{equation} V^\\pi(s) = \\mathbb{E}_b\\left[\\rho_{t:T-1} g_t \\mid s_t=s\\right]. \\tag{2.38} \\end{equation}\\] Similarly, we have \\[\\begin{equation} Q^\\pi(s,a) = \\mathbb{E}_b\\!\\left[\\rho_{t:T-1} g_t \\mid s_t=s, a_t=a\\right]. \\tag{2.39} \\end{equation}\\] Given \\(n\\) episodes, the ordinary IS estimator for \\(Q^\\pi\\) at the first visit of \\((s,a)\\) is \\[ \\hat Q_n^{\\text{IS}}(s,a) = \\frac{1}{N_n(s,a)} \\sum_{i=1}^n \\mathbf{1}\\{(s,a)\\text{ visited}\\}\\, \\rho_{t_i:T_i-1}^{(i)}\\, g_{t_i}^{(i)}, \\] where \\(N_n(s,a)\\) counts the number of first visits of \\((s,a)\\). In words, to estimate the \\(Q\\) value of the target policy \\(\\pi\\) using trajectories of the behavior policy \\(b\\), we need to reweight the return \\(g_t\\) by the likelihood ratio \\(\\rho_{t:T-1}\\). Note that the likelihood ratio does not require knowledge about the transition dynamics. Algorithmic Form: Off-policy Monte Carlo Policy Evaluation. Input: behavior \\(b\\), target \\(\\pi\\), episodes from \\(b\\) For each episode \\((s_0,a_0,r_0,s_1,\\dots,s_{T-1},a_{T-1},r_{T-1},s_T)\\): For \\(t=T-1,\\dots,0\\) compute episode-wise likelihood ratio \\(\\rho_{t:T-1}\\) and return \\(g_t\\), For first visits of \\((s_t,a_t)\\), update \\[ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha_{N(s_t,a_t)}\\big(\\rho_{t:T-1} g_t - Q(s_t,a_t)\\big). \\] Use sample averages \\(\\alpha_n=1/n\\) or Robbins-Monro stepsizes. Guarantees. Under the support condition and finite variance assumptions, ordinary IS is unbiased and converges almost surely to \\(Q^\\pi\\). 2.1.4.2 Off-Policy Monte Carlo Control High-level Intuition. We wish to improve a target policy \\(\\pi\\) toward optimality while behaving with a different exploratory policy \\(b\\). We evaluate \\(Q^\\pi\\) off-policy using IS on data from \\(b\\), then set \\(\\pi\\) greedy with respect to the updated \\(Q\\). Keep \\(b\\) sufficiently exploratory (for coverage), for example \\(\\varepsilon\\)-greedy with a fixed \\(\\varepsilon&gt;0\\) or a GLIE schedule. Algorithmic Form. Initialize \\(Q(s,a)\\) arbitrarily. Set target \\(\\pi\\) to be greedy w.r.t. \\(Q\\). Choose an exploratory behavior \\(b\\) that ensures coverage, e.g., \\(\\varepsilon\\)-greedy w.r.t. \\(Q\\) with \\(\\varepsilon&gt;0\\). Loop over iterations \\(i=0,1,2,\\dots\\): Data collection under \\(b\\): generate a batch of episodes using \\(b\\). Off-policy evaluation of \\(\\pi\\): for each episode, compute IS targets for first visits of \\((s_t,a_t)\\) and update \\(Q\\) using either ordinary IS Policy improvement: set for all states \\[ \\pi_{i+1}(s) \\in \\arg\\max_{a} Q(s,a). \\] Optionally update \\(b\\) to remain exploratory, for example \\(b\\) \\(\\leftarrow\\) \\(\\varepsilon\\)-greedy w.r.t. \\(Q\\) with a chosen \\(\\varepsilon\\) or a GLIE decay. Convergence Guarantees. Evaluation step: With the support condition and appropriate stepsizes, off-policy MC prediction converges almost surely to \\(Q^\\pi\\) when using ordinary IS. Control in the batch GPI limit: If each evaluation step produces estimates that converge to the exact \\(Q^{\\pi_i}\\) before improvement, then by the policy improvement theorem the sequence of greedy target policies \\(\\pi_i\\) converges to an optimal policy in finite MDPs. Remark. Choice of \\(b\\). A common and simple choice is an \\(\\varepsilon\\)-greedy behavior \\(b\\) w.r.t. current \\(Q\\) that maintains \\(\\varepsilon&gt;0\\) for coverage or uses GLIE so that \\(\\varepsilon_t \\downarrow 0\\) while all pairs are still visited infinitely often. 2.1.4.3 Q-Learning High-Level Intuition. What it learns. Q-Learning seeks the fixed point of the Bellman optimality operator \\[ (\\mathcal T^\\star Q)(s,a) = \\mathbb E\\big[ r_{t} + \\gamma \\max_{a&#39;} Q(s_{t+1}, a&#39;) \\mid s_t=s, a_t=a \\big], \\] whose unique fixed point is \\(Q^\\star\\). Because \\(\\mathcal T^\\star\\) is a \\(\\gamma\\)-contraction in \\(\\|\\cdot\\|_\\infty\\), repeatedly applying it converges to \\(Q^\\star\\) in the tabular case. Why off-policy. We can behave with any sufficiently exploratory policy \\(b\\) (e.g., \\(\\varepsilon\\)-greedy w.r.t. current \\(Q\\)) but learn from the greedy target \\(\\max_{a&#39;} Q(s&#39;,a&#39;)\\). No importance sampling is needed. Algorithmic Form. Let \\(Q\\) be a tabular action-value function. At each step observe a transition \\((s_t, a_t, r_{t}, s_{t+1})\\) generated by a behavior policy \\(b_t\\) (typically \\(\\varepsilon_t\\)-greedy w.r.t. \\(Q_t\\)). Target and TD error \\[ y_t = r_{t} + \\gamma \\max_{a&#39;} Q(s_{t+1}, a&#39;), \\qquad \\delta_t = y_t - Q(s_t, a_t). \\] Update \\[ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha_t(s_t,a_t)\\, \\delta_t. \\] Behavior (exploration) Use \\(\\varepsilon_t\\)-greedy with \\(\\varepsilon_t\\) decaying (GLIE) or any scheme that ensures each \\((s,a)\\) is updated infinitely often. Convergence. In a finite MDP with \\(\\gamma \\in [0,1)\\), if each \\((s,a)\\) is updated infinitely often (sufficient exploration) and stepsizes satisfy Robbins-Monro conditions, then Q-Learning converges to \\(Q^\\star\\) with probability 1. 2.1.4.4 Double Q-Learning Motivation. Max operators tend to be optimistically biased when action values are noisy. Consider an example where in state \\(s\\) one can take two actions \\(1\\) and \\(2\\). The estimated Q function \\(\\hat{Q}(s, \\cdot)\\) has two values \\(+1\\) and \\(-1\\) with equal probability. In this case we have \\(Q(s,1) = Q(s,2) = \\mathbb{E}[\\hat{Q}(s,\\cdot)] = 0\\). Therefore, \\(\\max Q(s,a) = 0\\). However, the noisy estimated \\(\\hat{Q}(s,\\cdot)\\) has four outcomes with equal probabilities: \\[ (+1,-1), (+1,+1), (-1, +1), (-1,-1). \\] Therefore, we have \\[ \\mathbb{E}[\\max_a \\hat{Q}(s,a)] = \\frac{1}{4} (1 + 1 + 1 -1) = 1/2 &gt; \\max_a Q(s,a), \\] which overestimates the max \\(Q\\) value. In general, we have \\[ \\mathbb{E}[\\max_a \\hat{Q}(s,a)] \\geq \\max_a \\mathbb{E} [\\hat{Q}(s,a)] = \\max_a Q(s,a), \\] where the estimates \\(\\hat{Q}\\) are noisy (try to prove this on your own). In Q-Learning the target \\[ y_t = r_{t} + \\gamma \\max_{a&#39;} Q(s_{t+1}, a&#39;) \\] can therefore overestimate action values and slow learning or push policies toward risky actions. Double Q-Learning reduces this bias by decoupling selection from evaluation: maintain two independent estimators \\(Q^A\\) and \\(Q^B\\). Use one to select the greedy action and the other to evaluate it, and alternate which table you update. This weakens the statistical coupling that creates overestimation. Algorithmic Form. Keep two tables \\(Q^A, Q^B\\). Use an \\(\\varepsilon\\)-greedy behavior policy with respect to a combined estimate, e.g., \\(Q^{\\text{avg}}= \\tfrac12(Q^A+Q^B)\\) or \\(Q^A+Q^B\\). At each step observe \\((s_t, a_t, r_{t}, s_{t+1})\\). With probability \\(1/2\\) update \\(Q^A\\), else update \\(Q^B\\). Update \\(Q^A\\): \\[ a^\\star = \\arg\\max_{a&#39;} Q^A(s_{t+1}, a&#39;),\\qquad y_t = r_{t} + \\gamma\\, Q^B(s_{t+1}, a^\\star), \\] \\[ Q^A(s_t, a_t) \\leftarrow Q^A(s_t, a_t) + \\alpha_t(s_t,a_t)\\big[y_t - Q^A(s_t, a_t)\\big]. \\] Update \\(Q^B\\): \\[ a^\\star = \\arg\\max_{a&#39;} Q^B(s_{t+1}, a&#39;),\\qquad y_t = r_{t} + \\gamma\\, Q^A(s_{t+1}, a^\\star), \\] \\[ Q^B(s_t, a_t) \\leftarrow Q^B(s_t, a_t) + \\alpha_t(s_t,a_t)\\big[y_t - Q^B(s_t, a_t)\\big]. \\] Behavior policy (\\(\\varepsilon\\)-greedy): choose \\(a_t \\sim \\varepsilon\\)-greedy with respect to \\(Q^{\\text{avg}}(s_t,\\cdot)\\). A GLIE schedule \\(\\varepsilon_t \\downarrow 0\\) is standard. Acting and planning: for greedy actions or plotting a single estimate, use \\(Q^{\\text{avg}} = \\tfrac12(Q^A+Q^B)\\). Convergence. Tabular setting. In a finite MDP with \\(\\gamma \\in [0,1)\\), bounded rewards, sufficient exploration so that every \\((s,a)\\) is updated infinitely often, and Robbins–Monro stepsizes for each pair. Double Q-Learning converges with probability 1 to \\(Q^\\star\\). Example 2.2 (Value-based RL for Grid World) Consider the following \\(5 \\times 5\\) grid with \\((0,4)\\) being the goal and the terminal state. At every state, the agent can take four actions: left, right, up, and down. There is a wall in the gray area shown in Fig. 2.3. Upon hitting the wall, the agent stays in the original cell. Every action incurs a reward of \\(-1\\). Once the agent arrives at the goal state, reward stays at 0. Figure 2.3: Grid World We run Generalized Policy Iteration (GPI) with Monte Carlo (on-policy), SARSA, Expected SARSA, Q-Learning, and Double Q-Learning on this problem with diminishing learning rates. Fig. 2.4 plots the error between the estimated Q values (of different algorithms) and the ground-truth optimal Q value (obtained from value iteration with known transition dynamics). Except Monte Carlo control which converges slowly, the other methods converge fast. Figure 2.4: Convergence of Estimated Q Values. From the final estimated Q value, we can extract a greedy policy, visualized below. You can play with the code here. MC Control: &gt; &gt; &gt; &gt; G ^ # ^ ^ ^ v # ^ ^ ^ v # &gt; ^ ^ &gt; &gt; &gt; &gt; ^ SARSA: &gt; &gt; &gt; &gt; G ^ # &gt; &gt; ^ ^ # ^ ^ ^ ^ # ^ ^ ^ &gt; &gt; ^ ^ ^ Expected SARSA: &gt; &gt; &gt; &gt; G ^ # &gt; &gt; ^ ^ # ^ ^ ^ ^ # &gt; &gt; ^ &gt; &gt; ^ ^ ^ Q-Learning: &gt; &gt; &gt; &gt; G ^ # ^ ^ ^ ^ # ^ ^ ^ ^ # ^ ^ ^ &gt; &gt; ^ ^ ^ Double Q-Learning: &gt; &gt; &gt; &gt; G ^ # &gt; ^ ^ ^ # &gt; ^ ^ ^ # ^ ^ ^ &gt; &gt; &gt; &gt; ^ 2.2 Function Approximation Many reinforcement learning problems have continuous state spaces–think of mechanical systems like robot arms, legged locomotion, drones, and autonomous vehicles. In these domains the state \\(s\\) (e.g., joint angles/velocities, poses) lives in \\(\\mathbb{R}^n\\), which makes a tabular representation of the value functions impossible. In this case, we must approximate values with parameterized functions. 2.2.1 Basics of Continuous MDP In a continuous MDP, at least one of the state space or the action space is a continuous space. Suppose \\(\\mathcal{S} \\subseteq \\mathbb{R}^n\\) and \\(\\mathcal{A} \\subseteq \\mathbb{R}^{m}\\) are both continuous spaces. The environment kernel \\(P(\\cdot \\mid s, a)\\) is a Markov kernel from \\(\\mathcal{S} \\times \\mathcal{A}\\) to \\(\\mathcal{S}\\): for each state-action pair \\((s,a)\\), \\(P(\\cdot \\mid s,a)\\) is a probability measure on \\(\\mathcal{S}\\). For each Borel set \\(B \\subseteq \\mathcal{S}\\), the map \\((s,a) \\mapsto P(B \\mid s, a)\\) is measurable. For example, \\(P(\\mathcal{S} \\mid s, a) = 1\\) for any \\((s,a)\\). The policy kernel \\(\\pi(\\cdot \\mid s)\\) is a stochastic kernel from \\(\\mathcal{S}\\) to \\(\\mathcal{A}\\): for each \\(s\\), \\(\\pi(\\cdot \\mid s)\\) is a probability measure on \\(\\mathcal{A}\\). Induced State-Transition Kernel. For notational convenience, given a policy and the environment kernel \\(P\\), we define a state-only Markov kernel \\[\\begin{equation} P^\\pi(B \\mid s) := \\int_{\\mathcal{A}} P(B \\mid s, a) \\pi(da \\mid s), \\quad B \\subseteq \\mathcal{S}. \\tag{2.40} \\end{equation}\\] In words, \\(P^\\pi(B \\mid s)\\) measures the probability of landing at a set \\(B\\) starting from state \\(s\\), under all actions possible for the policy \\(\\pi\\). If densities exist, i.e., \\(P(ds&#39; \\mid s, a) = p(s&#39; \\mid s, a) ds&#39;\\) and \\(\\pi(da \\mid s) = \\pi(a \\mid s) da\\), then, \\[\\begin{equation} p^\\pi(s&#39; \\mid s) := \\int_{\\mathcal{A}} p(s&#39; \\mid s, a) \\pi(a \\mid s) da \\quad\\text{and}\\quad P^{\\pi}(d s&#39; \\mid s) = p^\\pi(s&#39; \\mid s) ds&#39;. \\tag{2.41} \\end{equation}\\] Stationary State Distribution. A probability measure \\(\\mu^\\pi\\) on \\(\\mathcal{S}\\) is called stationary for the state-transition kernel \\(P^\\pi\\) if and only if \\[\\begin{equation} \\mu^{\\pi}(B) = \\int_{\\mathcal{S}} P^\\pi(B \\mid s) \\mu^{\\pi}(ds), \\quad \\forall B \\subseteq \\mathcal{S}. \\tag{2.42} \\end{equation}\\] If a density \\(\\mu^\\pi(s)\\) exists, then the above equation is the followng condition \\[\\begin{equation} \\mu^{\\pi}(s&#39;) = \\int_{\\mathcal{S}} p^\\pi(s&#39; \\mid s) \\mu^{\\pi}(s) ds. \\tag{2.43} \\end{equation}\\] In words, the state distribution \\(\\mu^\\pi\\) does not change under the state-transition kernel \\(P^\\pi\\) (e.g., if a state \\(A\\) has probability \\(0.1\\) of being visited at time \\(t\\), the probability of visiting \\(A\\) in the next time step remains \\(0.1\\), under policy \\(\\pi\\)). Under standard ergodicity assumptions, this stationary state distribution \\(\\mu^\\pi\\) exists and is unique (after sufficient steps, the initial state distribution does not matter and the state distribution follows \\(\\mu^\\pi\\)). Moreover, the empirical state distribution converge to \\(\\mu^\\pi\\). 2.2.2 Policy Evaluation For simplicity, let us first relax the state space to be a continuous space \\(\\mathcal{S} \\subseteq \\mathbb{R}^n\\). We assume the action space \\(\\mathcal{A}\\) is still finite with \\(|\\mathcal{A}|\\) elements. We first consider the problem of policy evaluation, i.e., estimate the value functions associated with a policy \\(\\pi\\) from interaction data with the environment. Bellman Consistency. Given a policy \\(\\pi: \\mathcal{S} \\mapsto \\Delta(\\mathcal{A})\\), its associated state-value function \\(V^\\pi\\) must satisfy the following Bellman Consistency equation \\[\\begin{equation} V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\int_{\\mathcal{S}} V(s&#39;) P(d s&#39; \\mid s, a) \\right]. \\tag{2.44} \\end{equation}\\] Notice that since \\(\\mathcal{S}\\) is a continuous space, we need to replace “\\(\\sum_{s&#39; \\in \\mathcal{S}}\\)” with “\\(\\int_{\\mathcal{S}}\\)”. If \\(P(d s&#39; \\mid s, a)\\) has a density \\(p(s&#39; \\mid s, a)\\), the above Bellman consistency equation also reads \\[\\begin{equation} V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left[ R(s, a) + \\gamma \\int_{\\mathcal{S}} V(s&#39;) p(s&#39; \\mid s, a) ds&#39; \\right]. \\tag{2.45} \\end{equation}\\] Bellman Operator. Define the Bellman operator \\(T^\\pi\\) acting on any bounded measurable function \\(V:\\mathcal{S}\\to\\mathbb{R}\\) by \\[\\begin{equation} (T^\\pi V)(s) = \\sum_{a\\in\\mathcal{A}} \\pi(a\\mid s)\\left[ R(s,a) + \\gamma \\int_{\\mathcal{S}} V(s&#39;) P(ds&#39;\\mid s,a)\\right]. \\tag{2.46} \\end{equation}\\] Then \\(V^\\pi\\) is the unique fixed point of \\(T^\\pi\\), i.e., \\(V^\\pi = T^\\pi V^\\pi\\). Moreover, when rewards are uniformly bounded and \\(\\gamma\\in[0,1)\\), \\(T^\\pi\\) is a \\(\\gamma\\)-contraction under the sup-norm and is monotone. Approximate Value Function. In large/continuous state spaces we restrict attention to a parametric family \\({V(\\cdot;\\theta): \\theta\\in\\mathbb{R}^d}\\) and learn \\(\\theta\\) from data. We use \\(\\nabla_\\theta V(s;\\theta) \\in \\mathbb{R}^d\\) to denote the gradient of \\(V\\) with respect to \\(\\theta\\) at state \\(s\\). A special and very important case is linear function approximation \\[\\begin{equation} V(s;\\theta) = \\theta^\\top \\phi(s), \\tag{2.47} \\end{equation}\\] where \\(\\phi(s) = [\\phi_1(s),\\ldots,\\phi_d(s)]^\\top\\) are fixed basis functions (e.g., neural network last-layer features). When \\(V(s;\\theta) = \\theta^{\\top} \\phi(s)\\), we have \\[ \\nabla_\\theta V(s;\\theta) = \\phi(s). \\] When we restrict the value function to a function class (e.g., linear features or a neural network), it is generally not guaranteed that the unique fixed point of the Bellman operator (2.46), namely \\(V^\\pi\\), belongs to that class. This misspecification (or realizability gap) means we typically cannot recover \\(V^\\pi\\) exactly; instead, we seek its best approximation according to a chosen criterion. 2.2.2.1 Monte Carlo Estimation Given an episode \\((s_t,a_t,r_t,\\dots,s_T)\\) collected by policy \\(\\pi\\), its discounted return \\[ g_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t} r_T \\] is an unbiased estimate of the value at \\(s_t\\), i.e., \\(V^{\\pi}(s_t)\\). Therefore, Monte Carlo estimation follows the intuitive idea to make the approximate value function \\(V(\\cdot, \\theta)\\) fit the returns from these episodes as close as possible: \\[\\begin{equation} \\min_{\\theta} \\frac{1}{\\mathcal{D}} \\sum_{t \\in \\mathcal{D}} \\frac{1}{2} (g_t - V(s_t, \\theta))^2, \\tag{2.48} \\end{equation}\\] where \\(\\mathcal{D}\\) denotes the dataset of episodes collected under policy \\(\\pi\\). The formulation (2.48) is a formulation in the sense that it waits until all episodes are collected before performing the optimization. In an online formulation, we can optimize after every episode the objective function \\[ \\min_{\\theta} \\frac{1}{2} (g_t - V(s_t, \\theta))^2, \\] which leads to one step of gradient descent: \\[\\begin{equation} \\theta \\ \\ \\leftarrow \\ \\ \\theta + \\alpha_t (g_t - V(s_t;\\theta)) \\nabla_\\theta V(s_t; \\theta). \\tag{2.49} \\end{equation}\\] To connect the above update back to the MC update (2.4) in the tabular case, we see that the term \\(g_t - V(s_t;\\theta\\) is similar as before the difference between the target and the current estimate. However, in the case of function approximation, the error is multiplied by the gradient \\(\\nabla_\\theta V (s_t; \\theta)\\). It is worth noting that when using function approximation, the update on \\(\\theta\\) caused by one episode \\((s_t,\\dots)\\) will affect the values at all other states even if the policy only visited state \\(s_t\\). Convergence Guarantees. Assume on-policy sampling under \\(\\pi\\), bounded rewards, and step sizes \\(\\alpha_t\\) satisfying Robbins–Monro conditions. For linear \\(V(s;\\theta)=\\theta^\\top\\phi(s)\\) with full-rank features, i.e., \\[ \\mathbb{E}_{s \\sim \\mu^\\pi} \\left[ \\phi(s) \\phi(s)^\\top \\right] \\succ 0, \\] and \\(\\mathbb{E}_{s \\sim \\mu^\\pi}\\|\\phi(s)\\|^2&lt;\\infty\\), the iterates converge almost surely to the unique global minimizer of the convex objective \\[\\begin{equation} \\theta_{\\text{MC}}^\\star \\in \\arg\\min_\\theta \\; \\frac{1}{2}\\mathbb{E}_{s_t \\sim \\mu^\\pi}\\!\\left[ \\big(V(s_t;\\theta)-V^\\pi(s_t)\\big)^2 \\right], \\tag{2.50} \\end{equation}\\] where the expectation is with respect to stationary state distribution \\(\\mu^\\pi\\) under \\(\\pi\\). For nonlinear differentiable function classes with bounded gradients, the iterates converge almost surely to a stationary point of the same objective. Correction: Since Monte Carlo Estimation can be seen as performing Stochastic Gradient Descent on the objective in (2.50), to guarantee convergence to a first-order stationary point, we need some technical conditions: (a) diminishing step sizes satisfying the Robbins-Monro condition; (b) bounded second-order moment of the stochastic gradient; and (c) \\(L\\)-smoothness of the objective. 2.2.2.2 Semi-Gradient TD(0) We know from previous discussion that MC uses the full return \\(g_t\\) as the target and thus can have high variance. A straightforward idea is to replace the MC target \\(g_t\\) in the update (2.49) by the one-step bootstrap target \\[ r_t + \\gamma V(s_{t+1};\\theta), \\] which yields the semi-gradient TD(0) update \\[\\begin{equation} \\theta \\ \\leftarrow\\ \\theta \\;+\\; \\alpha_t \\,\\big(r_t + \\gamma V(s_{t+1};\\theta) - V(s_t;\\theta)\\big)\\, \\nabla_\\theta V(s_t;\\theta). \\tag{2.51} \\end{equation}\\] (At terminal \\(s_{t+1}\\), use \\(V(s_{t+1};\\theta)=0\\) or equivalently set \\(\\gamma=0\\) for that step.) Why call it “semi-gradient”? Let the TD error be \\[ \\delta_t(\\theta) \\;:=\\; r_t + \\gamma V(s_{t+1};\\theta) - V(s_t;\\theta). \\] Consider the per-sample squared TD error objective \\[ \\min_{\\theta}\\; \\frac{1}{2} \\,\\delta_t(\\theta)^2. \\] Its true gradient (a.k.a. the residual gradient) is \\[ \\nabla_\\theta \\frac{1}{2} \\delta_t(\\theta)^2 \\;=\\; \\delta_t(\\theta)\\,\\big(\\gamma \\nabla_\\theta V(s_{t+1};\\theta) - \\nabla_\\theta V(s_t;\\theta)\\big). \\] Thus a true-gradient (residual-gradient) TD(0) step would be \\[\\begin{equation} \\theta \\ \\leftarrow\\ \\theta \\;-\\; \\alpha_t \\,\\delta_t(\\theta)\\,\\big(\\gamma \\nabla_\\theta V(s_{t+1};\\theta) - \\nabla_\\theta V(s_t;\\theta)\\big). \\tag{2.52} \\end{equation}\\] By contrast, the semi-gradient TD(0) step in (2.51) ignores the dependence of the target on \\(\\theta\\) (i.e., it drops the \\(\\gamma \\nabla_\\theta V(s_{t+1};\\theta)\\) term) and treats the target \\(r_t+\\gamma V(s_{t+1};\\theta)\\) as a constant when differentiating. Concretely, \\[ \\nabla_\\theta \\frac{1}{2} \\big( \\text{target} - V(s_t;\\theta)\\big)^2 \\;\\approx\\; -\\big(\\text{target} - V(s_t;\\theta)\\big)\\,\\nabla_\\theta V(s_t;\\theta). \\] This approximation yields the simpler update (2.51). Convergence Guarantees. When using linear approximation, the Monte Carlo estimator converges to \\(\\theta^\\star_{\\text{MC}}\\) in (2.50). We now study what the semi-gradient TD(0) updates (2.51) converge to. Projected Bellman Operator. Fix a weighting/visitation distribution \\(\\mu\\) on \\(\\mathcal S\\) (e.g., the stationary distribution \\(\\mu^\\pi\\)) and the associated inner product \\[ \\langle f,g\\rangle_\\mu := \\mathbb{E}_{s\\sim \\mu}[\\,f(s)g(s)\\,], \\qquad \\|f\\|_\\mu := \\sqrt{\\langle f,f\\rangle_\\mu}. \\] Let \\(\\mathcal V := \\{V(s;\\theta)=\\theta^\\top\\phi(s)\\;:\\;\\theta\\in\\mathbb{R}^d\\}\\) be the linear function class spanned by features \\(\\phi:\\mathcal S\\to\\mathbb{R}^d\\). The \\(\\mu\\)-orthogonal projection \\(\\Pi_\\mu:\\mathcal{F}\\to\\mathcal V\\) is \\[ \\Pi_\\mu f \\;:=\\; \\arg\\min_{V\\in\\mathcal V}\\, \\| V - f\\|_\\mu . \\] In words, given any function \\(f \\in \\mathcal{F}: \\mathcal{S} \\mapsto \\mathbb{R}\\), \\(\\Pi_\\mu f\\) returns the closest function \\(V\\) to \\(f\\) that belongs to the subset of linearly representable functions \\(\\mathcal{V}\\), where the “closest” is defined by the weighting distribution \\(\\mu\\). The Projected Bellman Operator is the composition \\[\\begin{equation} \\mathcal{T}^\\pi_{\\!\\text{proj}} \\;:=\\; \\Pi_\\mu \\, T^\\pi, \\qquad\\text{i.e.,}\\qquad \\big(\\mathcal{T}^\\pi_{\\!\\text{proj}} V\\big)(\\cdot) \\;=\\; \\Pi_\\mu \\!\\left[\\, T^\\pi V \\,\\right](\\cdot). \\tag{2.53} \\end{equation}\\] \\(T^\\pi\\) is the Bellman operator defined in (2.46). \\(\\Pi_\\mu\\) projects any function onto \\(\\mathcal V\\) using the \\(\\mu\\)-weighted \\(L^2\\) norm. In discrete \\(\\mathcal S\\), write \\(\\Phi\\in\\mathbb{R}^{|\\mathcal S|\\times d}\\) with rows \\(\\phi(s)^\\top\\) and \\(D=\\mathrm{diag}(\\mu(s))\\). Then \\[ \\Pi_\\mu f \\;=\\; \\Phi\\,(\\Phi^\\top D \\Phi)^{-1}\\Phi^\\top D f. \\] \\(T^\\pi\\) is a \\(\\gamma\\)-contraction under \\(\\|\\cdot\\|_\\mu\\), and \\(\\Pi_\\mu\\) is nonexpansive under \\(\\|\\cdot\\|_\\mu\\), hence \\(\\mathcal{T}^\\pi_{\\!\\text{proj}}\\) is a \\(\\gamma\\)-contraction: \\[ \\|\\Pi_\\mu T^\\pi V - \\Pi_\\mu T^\\pi U\\|_\\mu \\;\\le\\; \\|T^\\pi V - T^\\pi U\\|_\\mu \\;\\le\\; \\gamma \\|V-U\\|_\\mu. \\] Therefore, (2.53), the projected Bellman equation (PBE), has a unique fixed point \\(V_{\\text{TD}}^\\star\\in\\mathcal V\\) satisfying \\[\\begin{equation} V_{\\text{TD}}^\\star \\;=\\; \\Pi_\\mu\\, T^\\pi V_{\\text{TD}}^\\star. \\tag{2.54} \\end{equation}\\] Semi-gradient TD(0) Converges to the PBE Fixed Point (linear case). Assume on-policy sampling under an ergodic chain, bounded second moments, Robbins–Monro stepsizes, and full-rank features under \\(\\mu = \\mu^\\pi\\). In the linear case \\(V(s;\\theta)=\\theta^\\top\\phi(s)\\), define \\[ \\delta_t(\\theta) := r_t + \\gamma \\theta^\\top \\phi(s_{t+1}) - \\theta^\\top \\phi(s_t). \\] The semi-gradient TD(0) update (2.51) becomes \\[ \\theta \\leftarrow \\theta + \\alpha_t\\, \\delta_t(\\theta)\\,\\phi(s_t). \\] Taking conditional expectation w.r.t. the stationary visitation (and using the Markov property) yields the mean update: \\[ \\mathbb{E}\\!\\left[\\delta_t(\\theta)\\,\\phi(s_t)\\right] \\;=\\; b \\;-\\; A\\theta, \\] with the standard TD system \\[\\begin{equation} A \\;:=\\; \\mathbb{E}_{\\mu}\\!\\big[\\phi(s_t)\\big(\\phi(s_t)-\\gamma \\phi(s_{t+1})\\big)^\\top\\big], \\qquad b \\;:=\\; \\mathbb{E}_{\\mu}\\!\\big[r_{t}\\,\\phi(s_t)\\big]. \\tag{2.55} \\end{equation}\\] Thus, in expectation, TD(0) performs a stochastic approximation to the ODE \\[ \\dot\\theta \\;=\\; b - A\\theta, \\] whose unique globally asymptotically stable equilibrium is \\[ \\theta^\\star_{\\text{TD}} \\;=\\; A^{-1} b, \\] provided the symmetric part of \\(A\\) is positive definite (guaranteed on-policy with full-rank features). Standard stochastic approximation theory then gives \\[ \\theta_t \\xrightarrow{\\text{a.s.}} \\theta^\\star_{\\text{TD}}. \\] Finally, one can show the equivalence with the PBE: \\(V(\\cdot; \\theta) \\in\\mathcal V\\) satisfies \\(V(\\cdot; \\theta)=\\Pi_\\mu T^\\pi V(\\cdot; \\theta)\\) if and only if \\(A\\theta \\;=\\; b\\) (see a proof below). Hence the almost-sure limit \\(V(\\cdot; \\theta^\\star_{\\text{TD}})\\) is exactly the fixed point (2.54). Proof. The projected Bellman equation reads \\[ V(\\cdot;\\theta) = \\Pi_\\mu T^\\pi V(\\cdot; \\theta). \\] Since \\(V(\\cdot; \\theta) = \\theta^\\top \\phi(\\cdot)\\) is the orthogonal projection of \\(T^\\pi V(\\cdot; \\theta)\\) onto \\(\\mathcal V\\) weighted by \\(\\mu\\), we have that \\[ \\mathbb{E}_\\mu \\left[ \\phi(s_t) (T^\\pi V(s_t;\\theta) - V(s_t;\\theta)) \\right] = 0 = \\mathbb{E}_\\mu \\left[ \\phi(s_t) (r_t + \\gamma \\theta^\\top \\phi(s_{t+1}) - \\theta^\\top \\phi(s_t) )\\right], \\] which reduces to \\(A \\theta = b\\) with \\(A\\) and \\(b\\) defined in (2.55). What does convergence to the PBE fixed point imply? Best fixed point in the feature subspace (good). \\(V_{\\text{TD}}^\\star\\) is the unique function in \\(\\mathcal V\\) whose Bellman update \\(T^\\pi V\\) projects back to itself under \\(\\Pi_\\mu\\). If \\(V^\\pi\\in\\mathcal V\\) (realisable case), then \\(V_{\\text{TD}}^\\star=V^\\pi\\). Different target than least squares (mixed). TD(0) solves the Projected Bellman Equation (2.53); Monte Carlo least-squares solves \\[ \\min_{V\\in\\mathcal V} \\frac{1}{2} \\|V - V^\\pi\\|_\\mu^2. \\] When \\(V^\\pi\\notin\\mathcal V\\), these solutions generally differ. Either can have lower \\(\\mu\\)-weighted prediction error depending on features and dynamics; in practice TD often wins due to lower variance and online bootstrapping. 2.2.3 On-Policy Control 2.2.3.1 Semi-Gradient SARSA(0) High-level Intuition. Semi-gradient SARSA(0) is an on-policy value-based control method. It learns an action-value function \\(Q(s,a;\\theta)\\) by bootstrapping one step ahead and using the next action actually selected by the current behavior policy (e.g., \\(\\varepsilon\\)-greedy). Because the target uses \\(Q(s_{t+1},a_{t+1};\\theta)\\), SARSA trades some bias for substantially lower variance than Monte Carlo, updates online from each transition, and naturally couples policy evaluation (of the current policy) with policy improvement (by making the policy greedy/soft-greedy w.r.t. the current \\(Q\\)). Algorithmic Form (On-policy, Finite \\(\\mathcal A\\)). Let the behavior policy at time \\(t\\) be \\(\\pi_t(\\cdot\\mid s)\\) (e.g., \\(\\varepsilon_t\\)-greedy w.r.t. \\(Q(\\cdot,\\cdot;\\theta_t)\\)). For each step: Given \\(s_t\\), pick \\(a_t \\sim \\pi_t(\\cdot \\mid s_t)\\); observe \\(r_t\\) and \\(s_{t+1}\\). Pick the next action \\(a_{t+1} \\sim \\pi_t(\\cdot\\mid s_{t+1})\\). Form the TD error \\[\\begin{equation} \\delta_t \\;=\\; r_t \\;+\\; \\gamma\\, Q(s_{t+1},a_{t+1};\\theta) \\;-\\; Q(s_t,a_t;\\theta). \\tag{2.56} \\end{equation}\\] Update parameters with a semi-gradient step \\[\\begin{equation} \\theta \\;\\leftarrow\\; \\theta \\;+\\; \\alpha_t\\, \\delta_t \\,\\nabla_\\theta Q(s_t,a_t;\\theta). \\tag{2.57} \\end{equation}\\] For terminal \\(s_{t+1}\\), use \\(Q(s_{t+1},a_{t+1};\\theta)=0\\) (equivalently, set \\(\\gamma=0\\) on terminal transitions). Linear special case. If \\(Q(s,a;\\theta)=\\theta^\\top \\phi(s,a)\\), then \\(\\nabla_\\theta Q(s_t,a_t;\\theta)=\\phi(s_t,a_t)\\) and the update becomes \\[ \\theta \\leftarrow \\theta + \\alpha_t\\, \\delta_t\\, \\phi(s_t,a_t). \\] Expected SARSA (variance reduction). Replace the sample bootstrap by its expectation under \\(\\pi_t\\): \\[\\begin{equation} \\delta_t^{\\text{exp}} \\;=\\; r_t \\;+\\; \\gamma \\sum_{a&#39;\\in\\mathcal A} \\pi_t(a&#39;\\mid s_{t+1})\\, Q(s_{t+1},a&#39;;\\theta) \\;-\\; Q(s_t,a_t;\\theta), \\tag{2.58} \\end{equation}\\] then update \\(\\theta \\leftarrow \\theta + \\alpha_t\\, \\delta_t^{\\text{exp}}\\, \\nabla_\\theta Q(s_t,a_t;\\theta)\\). Update the next policy to be \\(\\varepsilon_{t+1}\\)-greedy w.r.t. the new Q value \\(Q(\\cdot, \\cdot; \\theta_{t+1})\\). (\\(\\varepsilon_t\\) follows GLIE.) Example 2.3 (Semi-Gradient SARSA for Mountain Car) Consider the Mountain Car problem from Gym illustrated in Fig. 2.5. The state space \\(\\mathcal{S} \\subset \\mathbb{R}^2\\) is continuous and contains the position of the car along the \\(x\\)-axis as well as the car’s velocity. The action space \\(\\mathcal{A}\\) is discrete and contains three elements: “\\(0\\): Accelerate to the left”, “\\(1\\): Don’t accelerate”, and “\\(2\\): Accelerate to the right”. The transition dynamics of the mountain car is: \\[\\begin{equation} \\begin{split} v_{t+1} &amp;= v_t + (a_t - 1) F - \\cos (3 p_t) g \\\\ p_{t+1} &amp;= p_t + v_{t+1} \\end{split} \\end{equation}\\] where \\((p_t, v_t)\\) denotes the state at time \\(t\\) with position and velocity, \\(a_t\\) denotes the action at time \\(t\\), \\(F=0.001\\) is the force and \\(g = 0.0025\\) is the gravitational constant. The goal is for the mountain car to reach the flag placed on top of the right hill as quickly as possible. Therefore, the agent is penalised with a reward of \\(-1\\) for each timestep. The position of the car is assigned a uniform random value in \\([-0.6 , -0.4]\\). The starting velocity of the car is always assigned to 0. In every episode, the agent is allowed a maximum of \\(200\\) steps (therefore, the worst per-episode return is \\(-200\\)). Figure 2.5: Mountain Car from Gym Naive Semi-Gradient SARSA. We first apply the semi-gradient SARSA algorithm introduced above to the mountain car problem. We parameterize the action value \\(Q\\) as a 2-layer multi-layer perceptron (MLP). Fig. 2.6 shows the average return per episode as training progreses. Clearly, the return stagnates at \\(-200\\) and the algorithm failed to learn. The rollout in Fig. 2.7 confirms that the final policy is not able to achieve the goal. You can find code for the naive semi-gradient SARSA algorithm here. Figure 2.6: Average return w.r.t. episode (Semi-Gradient SARSA) Figure 2.7: Example rollout (Semi-Gradient SARSA) Semi-Gradient SARSA with Experience Replay. Inspired by the technique of experience replay (ER) popularized by DQN (see 2.2.4.2), we incorporated ER into semi-gradient SARSA, which breaks its on-policy nature. Fig. 2.8 displays the learning curve, which shows steady increase of the per-episode return. Applying the final learned policy to the mountain car yields a successful trajectory to the top of the mountain. You can find code for the semi-gradient SARSA with experience replay algorithm here. Figure 2.8: Average return w.r.t. episode (Semi-Gradient SARSA + Experience Replay) Figure 2.9: Example rollout (Semi-Gradient SARSA + Experience Replay) 2.2.4 Off-Policy Control Off-policy control seeks to learn the optimal action–value function while collecting data under a different behavior policy (e.g., an \\(\\varepsilon\\)-soft policy). As in the tabular setting, Q-learning is the canonical off-policy control method. With function approximation, however, off-policy control becomes substantially harder than in the tabular case. To illustrate why, we first present off-policy semi-gradient TD(0) for policy evaluation and use Baird’s counterexample (Baird et al. 1995) to highlight the deadly triad: bootstrapping + function approximation + off-policy sampling can cause divergence. We then turn to the Deep Q-Network (DQN) (Mnih et al. 2015), which stabilizes Q-learning with two key mechanisms—experience replay and a target network—leading to landmark Atari results. Finally, we connect DQN to fitted Q-iteration (FQI) (Riedmiller 2005), a batch method with theoretical guarantees, to clarify why these stabilizations work (Fan et al. 2020). 2.2.4.1 Off-Policy Semi-Gradient TD(0) Setup. We aim to estimate the state-value function of a target policy \\(\\pi\\) using a different behavior policy \\(b\\). Since the state space is continuous, we employ function approximation to represent the value function as \\(V(s;\\theta)\\) with \\(\\theta \\in \\mathbb{R}^d\\). In the case of linear approximation, we have \\(V(s;\\theta) = \\theta^\\top \\phi(s)\\) where \\(\\phi(s)\\) is a function that featurizes the state. Semi-Gradient TD(0). Given a transition \\((s_t,a_t,r_t,s_{t+1})\\) collected under the behavior policy \\(b\\), form the TD error \\[ \\delta_t = r_t + \\gamma V(s_{t+1}; \\theta) - V(s_t;\\theta). \\] The off-policy Semi-Gradient TD(0) update reads \\[\\begin{equation} \\theta \\quad \\leftarrow \\quad \\theta + \\alpha_t \\rho_t \\delta_t \\nabla_\\theta V(s_t; \\theta), \\tag{2.59} \\end{equation}\\] where \\(\\rho_t\\) is the likelihood ratio as in (2.37): \\[ \\rho_t = \\frac{\\pi(a_t \\mid s_t)}{b(a_t \\mid s_t)}. \\] This off-policy semi-gradient TD(0) update (2.59) looks perfectly reasonable. However, the following Baird’s counterexample illustrates the instability of the algorithm. Baird’s Counterexample. Consider an MDP with 7 states containing 6 upper states and 1 lower state, as shown in Fig. 2.10 (the figure comes from (Sutton and Barto 1998)). There are two actions, one called “solid” and the other called “dashed”. If the agent picks “solid” at any state, then the system transitons to the lower state with probability 1. If the agent picks “dashed” at any state, then the system transitions to any one of the upper states with equal probability. All rewards are zero, and the discount factor is \\(\\gamma=0.99\\). The target policy \\(\\pi\\) always picks “solid”, while the behavior policy \\(b\\) picks “solid” with probability \\(1/7\\) and “dashed” with probability \\(6/7\\). Figure 2.10: Baird Counterexample Consider the case of linear function approximation where \\(V(s;w) = w^\\top \\phi(s)\\) where \\(w \\in \\mathbb{R}^8\\). For the upper states, the feature \\(\\phi(s)\\) leads to \\(V(s;w) = 2 w_1 + w_8\\), and for the lower state, the feature \\(\\phi(s)\\) leads to \\(V(s;w) = w_7 + 2w_8\\). This Python script implements the off-policy semi-gradient TD(0) algorithm with importance sampling for policy evaluation. Fig. 2.11 plots \\(\\Vert w \\Vert_2\\), the magnitude of \\(w\\), with respect to iterations. Clearly, we see the parameter \\(w\\) diverges under the off-policy semi-gradient TD(0) algorithm. Figure 2.11: Baird Counterexample: divergence The Deadly Triad. Three ingredients were used together in Baird’s example: Off-policy: a different behavior policy \\(b\\) is used to collect data for the evaluation of the target policy \\(\\pi\\); Function approximation: the value function employs function approximation; Bootstrapping: the TD error uses the bootstrapped target “\\(r_t + \\gamma V(s_{t+1}; \\theta)\\)” instead of the full return as in Monte Carlo. The “deadly triad” is used to illustrate that using all three ingredients together will lead to the potential divergence of policy evaluation. Why? Recall that, using linear approximation, the on-policy Semi-Gradient TD(0) algorithm guarantees convergence to the unique fixed point of the projected Bellman equation (PBE) (2.54), restated here \\[ V^\\star_{\\text{TD}} = \\Pi_{\\mu} T^\\pi V^\\star_{\\text{TD}}, \\] where \\(\\mu\\) is the stationary distribution induced by the policy \\(\\pi\\). A central reason for the guaranteed convergence is that the projected Bellman operator \\[ \\Pi_{\\mu} T^\\pi \\] is a \\(\\gamma\\)-contraction and has a unique fixed point. Therefore, the on-policy Semi-Gradient TD(0) algorithm—can be seen as a stochastic approximation of the projected Bellman operator—enjoys convergence guarantees. However, in the off-policy case, the orthogonal projection \\(\\Pi_\\mu\\) needs to be modified as \\(\\Pi_{\\nu}\\), where \\(\\nu\\) is the stationary distribution induced by the behavior policy \\(b\\). The new operator \\[ \\Pi_{\\nu} T^\\pi \\] is not guaranteed to be a \\(\\gamma\\)-contraction, due to the mismatch between \\(\\nu\\)—induced by \\(b\\)—and the target policy \\(\\pi\\). Therefore, divergence can potentially happen. How to Fix? Multiple algorithms have been proposed to fix the deadly triad. Notable examples include the gradient TD (GTD) family of algorithms (Sutton, Szepesvári, and Maei 2008), (Sutton et al. 2009), and the Emphatic TD (ETD) learning algorithm (Mahmood et al. 2015). They are influential and widely cited, but they are not (yet) mainstream in deep RL practice. Their main appeal is theoretical—they provide off-policy evaluation algorithms with convergence guarantees under linear function approximation. Moreover, GTD/TDC require two-time-scale step-sizes and ETD’s emphatic weights can have high variance, making them less attractive for large-scale control with neural networks. I encourage you to read the papers to understand the algorithms. However, in the next, I will explain the deep Q network (DQN) approach that is more popular in practice. 2.2.4.2 Deep Q Network We consider continuous state spaces with a finite action set, and a parametric action–value function \\(Q(s,a; \\theta)\\). Naive (semi-gradient) Q-Learning with Function Approximation. The goal is to learn \\(Q^\\star\\) and act \\(\\varepsilon\\)-greedily w.r.t. \\(Q(\\cdot,\\cdot; \\theta)\\). The update uses a bootstrapped optimality target built from the current network. \\[\\begin{equation} \\begin{split} y_t &amp; = r_t + \\gamma \\max_{a&#39;} Q(s_{t+1}, a&#39;;\\theta) \\\\ \\theta &amp; \\leftarrow \\theta + \\alpha \\,\\big(y_t - Q(s_t,a_t;\\theta)\\big)\\,\\nabla_\\theta Q(s_t,a_t; \\theta). \\end{split} \\tag{2.60} \\end{equation}\\] The transitions \\((s_t,a_t,r_t,s_{t+1})\\) are generated using a \\(\\varepsilon\\)-greedy policy with respect to \\(Q(s,a;\\theta)\\). This naive variant is off-policy + bootstrapping + function approximation (i.e., the deadly triad) and thus can be unstable. Deep Q Network (DQN) with Experience Replay (ER) and Target Network (TN). DQN augments the above naive Q learning with two stabilizers: Experience Replay (ER): store transitions in a buffer \\(\\mathcal{D}\\); train on i.i.d.-like mini-batches to decorrelate updates and reuse data. Target Network (TN): maintain a delayed copy \\(Q(\\cdot,\\cdot; \\theta^-)\\) to compute targets \\[ y_t = r_t + \\gamma \\max_{a&#39;} Q(s_{t+1},a&#39;;\\theta^-), \\] keeping the target fixed for many gradient steps. The full DQN algorithm is presented below. Initialize replay buffer \\(\\mathcal{D}\\) with capacity \\(N\\) Initialize approximate Q value \\(Q(s,a;\\theta)\\) Initialize target \\(Q_T(s,a;\\theta^-)\\) with \\(\\theta^- = \\theta\\) For episode \\(=1,\\dots,M\\) do: Initialize \\(s_0\\) For \\(t=0,\\dots,T\\) do: \\(\\varepsilon\\)-greedy policy: With probability \\(\\varepsilon\\) select a random action \\(a_t \\in \\mathcal{A}\\) Otherwise select \\(a_t = \\arg\\max_a Q(s_t,a_t;\\theta)\\) Observe transition \\(\\tau_t = (s_t, a_t, r_t, s_{t+1})\\) Put \\(\\tau_t\\) inside replay buffer \\(\\mathcal{D}\\) Sample a random minibatch of transitions \\(\\{(s_i, a_i, r_i, s_{i+1})\\}_{i \\in \\mathcal{I}}\\) from \\(\\mathcal{D}\\) For \\(i \\in \\mathcal{I}\\) do: Set target \\(y_i = r_i + \\gamma \\max_a Q_T(s_{i+1}, a; \\theta^-)\\) using the target network Update \\(\\theta \\leftarrow \\theta + \\alpha (y_i - Q(s_i,a_i;\\theta)) \\nabla_\\theta Q(s_i, a_i; \\theta)\\) Every \\(C\\) steps synchronize the target network with the \\(Q\\) net: \\(\\theta^- = \\theta\\) Although the naive Q-leanring with function approximation can be unstable, DQN has achieved great success and is also very efficient (Mnih et al. 2015). Fitted Q Iteration (FQI). To understand why DQN can achieve stabilized training compared to naive Q-learning with function approximation. It is insightful to look at the fitted Q iteration (FQI) algorithm, presented below. Initialize \\(Q^{(0)} = Q(s,a;\\theta_0)\\) For \\(k=0,1,2,\\dots,K-1\\) do: Sample i.i.d. transitions \\(\\{ (s_i,a_i,r_i,s_{i+1}) \\}_{i=1}^N\\) with \\((s_i, a_i)\\) drawn from a distribution \\(\\mu\\) Compute targets \\(y_i = r_i + \\gamma \\max_a Q^{(k)}(s_{t+1},a;\\theta_k),i=1,\\dots,N\\) Update the action-value function: \\[ Q^{(k+1)} = Q(s,a;\\theta_{k+1}), \\quad \\theta_{k+1} \\in \\arg\\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^N (y_i - Q(s_i,a_i;\\theta))^2 \\] The FQI algorithm samples trajectories from a fixed distribution \\(\\mu\\) and optimizes the parameter using targets generated from those trajectories. Under reasonable coverage and approximation assumptions, it converges and admits finite-sample error bounds (Antos, Szepesvári, and Munos 2007), (Munos and Szepesvári 2008). Connection to DQN. DQN is similar to FQI in the following aspects. Frozen targets: DQN’s target network \\(Q(\\cdot,\\cdot; \\theta^-)\\) plays the role of \\(Q^{(k-1)}\\) in FQI. Supervised fit: DQN’s mini-batch loss minimizes \\(\\sum (Q(s_i,a_i; \\theta) - y_i)^2\\), just like FQI’s regression step. Data usage: FQI trains on a fixed dataset; DQN’s replay buffer approximates training on an (ever-growing) quasi-fixed dataset by repeatedly sampling past transitions. Iteration vs. updates: FQI alternates full regressions and target recomputation; DQN alternates many SGD steps with periodic target updates. In the limit of many SGD steps per target update and a large replay buffer, DQN \\(\\approx\\) online, incremental FQI. This perspective explains why ER + TN make DQN far more stable than naive Q-learning with function approximation: they make the optimization behave like a sequence of supervised fits to fixed targets drawn from a nearly stationary dataset. Example 2.4 (DQN for Mountain Car) Consider again the Mountain car problem from Example 2.3. Naive Q-Learning with Function Approximation. As shown in Fig. 2.12 and Fig. 2.13, naive Q-leanring with function approximation fails to learn a good policy. Figure 2.12: Average return w.r.t. episode (Naive Q Learning) Figure 2.13: Example rollout (Naive Q Learning) DQN with Experience Replay and Target Network. Adding ER and TN to Q-learning leads to steady learning (Fig. 2.14) and a successful final policy (Fig. 2.15). You can find code for these experiments here. Figure 2.14: Average return w.r.t. episode (DQN) Figure 2.15: Example rollout (DQN) References Antos, András, Csaba Szepesvári, and Rémi Munos. 2007. “Fitted q-Iteration in Continuous Action-Space MDPs.” Advances in Neural Information Processing Systems 20. Baird, Leemon et al. 1995. “Residual Algorithms: Reinforcement Learning with Function Approximation.” In Proceedings of the Twelfth International Conference on Machine Learning, 30–37. Fan, Jianqing, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. 2020. “A Theoretical Analysis of Deep q-Learning.” In Learning for Dynamics and Control, 486–89. PMLR. Kearns, Michael J, and Satinder Singh. 2000. “Bias-Variance Error Bounds for Temporal Difference Updates.” In COLT, 142–47. Mahmood, A Rupam, Huizhen Yu, Martha White, and Richard S Sutton. 2015. “Emphatic Temporal-Difference Learning.” arXiv Preprint arXiv:1507.01569. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. Munos, Rémi, and Csaba Szepesvári. 2008. “Finite-Time Bounds for Fitted Value Iteration.” Journal of Machine Learning Research 9 (5). Riedmiller, Martin. 2005. “Neural Fitted q Iteration–First Experiences with a Data Efficient Neural Reinforcement Learning Method.” In European Conference on Machine Learning, 317–28. Springer. Robbins, Herbert, and David Siegmund. 1971. “A Convergence Theorem for Non Negative Almost Supermartingales and Some Applications.” In Optimizing Methods in Statistics, 233–57. Elsevier. Sutton, Richard S, and Andrew G Barto. 1998. Reinforcement Learning: An Introduction. Vol. 1. 1. MIT press Cambridge. Sutton, Richard S, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári, and Eric Wiewiora. 2009. “Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation.” In Proceedings of the 26th Annual International Conference on Machine Learning, 993–1000. Sutton, Richard S, Csaba Szepesvári, and Hamid Reza Maei. 2008. “A Convergent o(n) Algorithm for Off-Policy Temporal-Difference Learning with Linear Function Approximation.” Advances in Neural Information Processing Systems 21 (21): 1609–16. "],["policy-gradient.html", "Chapter 3 Policy Gradient Methods 3.1 Gradient-based Optimization 3.2 Policy Gradients 3.3 Actor–Critic Methods", " Chapter 3 Policy Gradient Methods In Chapter 2, we relaxed two key assumptions of the MDP introduced in Chapter 1: Unknown dynamics: the transition function \\(P\\) was no longer assumed to be known. Continuous states: the state space \\(\\mathcal{S}\\) was extended from finite to continuous. When only the dynamics are unknown but the MDP remains tabular, we introduced generalized versions of policy iteration (e.g., SARSA) and value iteration (e.g., Q-learning). These algorithms can recover near-optimal value functions with strong convergence guarantees. When both the dynamics are unknown and the state space is continuous, tabular methods become infeasible. In this setting, we employed function approximation to represent value functions, and generalized SARSA and Q-learning accordingly. We also introduced stabilization techniques such as experience replay and target networks to ensure more reliable learning. In this chapter, we relax a third assumption: the action space \\(\\mathcal{A}\\) is also continuous. This setting captures many important real-world systems, such as autonomous vehicles and robots. Handling continuous actions requires a departure from the value-based methods of Chapter 2. The key difficulty is that even if we had access to a near-optimal action-value function \\(Q(s,a)\\), selecting the control action requires solving \\[ \\max_a Q(s,a), \\] which is often computationally expensive and can lead to suboptimal solutions. To address this challenge, we introduce a new paradigm: policy gradient methods. Rather than learning value functions to derive policies indirectly, we directly optimize parameterized policies using gradient-based methods. We begin this chapter by reviewing the fundamentals of gradient-based optimization, and then build upon them to develop algorithms for searching optimal policies via policy gradients. 3.1 Gradient-based Optimization Gradient-based optimization is the workhorse behind most modern machine learning algorithms, including policy gradient methods. The central idea is to iteratively update the parameters of a model in the direction that most improves an objective function. 3.1.1 Basic Setup Suppose we have a differentiable objective function \\(J(\\theta)\\), where \\(\\theta \\in \\mathbb{R}^d\\) represents the parameter vector. The goal is to find \\[ \\theta^\\star \\in \\arg\\max_\\theta J(\\theta). \\] The gradient of the objective with respect to the parameters, \\[ \\nabla_\\theta J(\\theta) = \\begin{bmatrix} \\frac{\\partial J}{\\partial \\theta_1} &amp; \\frac{\\partial J}{\\partial \\theta_2} &amp; \\cdots &amp; \\frac{\\partial J}{\\partial \\theta_d} \\end{bmatrix}^\\top, \\] provides the local direction of steepest ascent. Gradient-based optimization uses this direction to iteratively update the parameters. Note that modern machine learning software tools such as PyTorch allow the user to conveniently query the gradient of any function \\(J\\) defined by neural networks. 3.1.2 Gradient Ascent and Descent The simplest method is gradient ascent (for maximization): \\[ \\theta_{k+1} = \\theta_k + \\alpha \\nabla_\\theta J(\\theta_k), \\] where \\(\\alpha &gt; 0\\) is the learning rate. For minimization, the update rule uses gradient descent: \\[ \\theta_{k+1} = \\theta_k - \\alpha \\nabla_\\theta J(\\theta_k). \\] The choice of learning rate \\(\\alpha\\) is critical: Too large \\(\\alpha\\) can cause divergence. Too small \\(\\alpha\\) leads to slow convergence. 3.1.2.1 Convergence Guarantees For convex functions \\(J(\\theta)\\), gradient descent (or ascent) can be shown to converge to the global optimum under appropriate conditions on the learning rate. For non-convex functions—which are common in reinforcement learning—gradient methods may only find so-called first-order stationary points, i.e., points \\(\\theta\\) at which the gradient \\(\\nabla_\\theta J(\\theta) = 0\\). Nevertheless, they remain effective in practice. TODO: graph different stationary points We now formalize the convergence speed of Gradient Descent (GD) for minimizing a smooth convex function. We switch to the minimization convention and write the objective as \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\) (to avoid sign confusions with \\(J\\) used for maximization). We assume exact gradients \\(\\nabla f(\\theta)\\) are available. Setup and Assumptions. (Convexity) For all \\(\\theta,\\vartheta\\in\\mathbb{R}^d\\), \\[\\begin{equation} f(\\vartheta) \\;\\ge\\; f(\\theta) + \\nabla f(\\theta)^\\top(\\vartheta-\\theta). \\tag{3.1} \\end{equation}\\] (\\(L\\)-smoothness) The gradient is \\(L\\)-Lipschitz: for all \\(\\theta,\\vartheta\\), \\[\\begin{equation} \\|\\nabla f(\\vartheta)-\\nabla f(\\theta)\\| \\;\\le\\; L\\|\\vartheta-\\theta\\|. \\tag{3.2} \\end{equation}\\] Equivalently (the descent lemma), for all \\(\\theta,\\Delta\\), \\[\\begin{equation} f(\\theta+\\Delta) \\;\\le\\; f(\\theta) + \\nabla f(\\theta)^\\top \\Delta + \\frac{L}{2}\\|\\Delta\\|^2. \\tag{3.3} \\end{equation}\\] Consider Gradient Descent with a constant stepsize \\(\\alpha&gt;0\\): \\[ \\theta_{k+1} \\;=\\; \\theta_k \\;-\\; \\alpha\\, \\nabla f(\\theta_k). \\] Theorem 3.1 (GD on smooth convex function) Let \\(f\\) be convex and \\(L\\)-smooth with a minimizer \\[ \\theta^\\star\\in\\arg\\min_\\theta f(\\theta). \\] and the global minimum \\(f^\\star = f(\\theta^\\star)\\). If \\(0&lt;\\alpha\\le \\frac{2}{L}\\), then the GD iterates satisfy for all \\(k\\ge 0\\): \\[\\begin{equation} f(\\theta_k) - f^\\star \\leq \\frac{2 (f(\\theta_0) - f^\\star) \\Vert \\theta_0 - \\theta^\\star \\Vert^2 }{2 \\Vert \\theta_0 - \\theta^\\star \\Vert^2 + k\\alpha ( 2 - L \\alpha) (f(\\theta_0) - f^\\star)} \\tag{3.4} \\end{equation}\\] In particular, choosing \\(\\alpha=\\frac{1}{L}\\) yields the canonical \\(O(1/k)\\) convergence rate in suboptimality: \\[\\begin{equation} f(\\theta_k) - f^\\star \\leq \\frac{2L \\Vert \\theta_0 - \\theta^\\star \\Vert^2}{k+4} \\tag{3.5} \\end{equation}\\] Proof. See Theorem 2.1.14 and Corollary 2.1.2 in (Nesterov 2018). Strongly Convex Case (Linear Rate). If, in addition, \\(f\\) is \\(\\mu\\)-strongly convex (\\(\\mu&gt;0\\)), i.e., for all \\(\\theta,\\vartheta\\in\\mathbb{R}^d\\), \\[\\begin{equation} f(\\vartheta)\\;\\ge\\; f(\\theta) + \\nabla f(\\theta)^\\top(\\vartheta-\\theta) \\;+\\; \\frac{\\mu}{2}\\,\\|\\vartheta-\\theta\\|^2. \\tag{3.6} \\end{equation}\\] Then, GD with \\(0&lt;\\alpha\\le \\frac{2}{\\mu + L}\\) enjoys a linear (geometric) rate: Theorem 3.2 (GD on smooth strongly convex function) If \\(f\\) is \\(L\\)-smooth and \\(\\mu\\)-strongly convex, then for \\(0&lt;\\alpha\\le \\frac{2}{\\mu + L}\\), \\[\\begin{equation} \\Vert \\theta_k - \\theta^\\star \\Vert^2 \\leq \\left( 1 - \\frac{2\\alpha \\mu L}{\\mu + L} \\right)^k \\Vert \\theta_0 - \\theta^\\star \\Vert^2. \\tag{3.7} \\end{equation}\\] If \\(\\alpha = \\frac{2}{\\mu + L}\\), then \\[\\begin{equation} \\begin{split} \\Vert \\theta_k - \\theta^\\star \\Vert &amp; \\leq \\left( \\frac{Q_f - 1}{Q_f + 1} \\right)^k \\Vert \\theta_0 - \\theta^\\star \\Vert \\\\ f(\\theta_k) - f^\\star &amp; \\leq \\frac{L}{2} \\left( \\frac{Q_f - 1}{Q_f + 1} \\right)^{2k} \\Vert \\theta_0 - \\theta^\\star \\Vert^2, \\end{split} \\tag{3.8} \\end{equation}\\] where \\(Q_f = L/\\mu\\). Proof. See Theorem 2.1.15 in (Nesterov 2018). Practical Notes. The step size \\(\\alpha=\\frac{1}{L}\\) is optimal among fixed stepsizes for the above worst-case bounds on smooth convex \\(f\\). In practice, backtracking line search or adaptive schedules can approach similar behavior without knowing \\(L\\). For policy gradients (which maximize \\(J\\)), apply the results to \\(f=-J\\) and flip the update sign (gradient ascent). The smooth/convex assumptions rarely hold globally in RL, but these results calibrate expectations about step sizes and motivate variance reduction and curvature-aware methods used later. 3.1.3 Stochastic Gradients In reinforcement learning and other large-scale machine learning problems, computing the exact gradient \\(\\nabla_\\theta J(\\theta)\\) is often infeasible. Instead, we use an unbiased estimator \\(\\hat{\\nabla}_\\theta J(\\theta)\\) computed from a subset of data (or trajectories in RL). The update becomes \\[ \\theta_{k+1} = \\theta_k + \\alpha \\hat{\\nabla}_\\theta J(\\theta_k). \\] This approach, known as stochastic gradient ascent/descent (SGD), trades off exactness for computational efficiency. Variance in the gradient estimates plays an important role in convergence speed and stability. 3.1.3.1 Convergence Guarantees We now turn to the convergence guarantees of stochastic gradient methods, which replace exact gradients with unbiased noisy estimates. Throughout this section we consider the minimization problem \\(\\min_\\theta f(\\theta)\\) and assume \\(\\nabla f\\) is available only through a stochastic oracle. Setup and Assumptions. Let \\(f:\\mathbb{R}^d\\!\\to\\!\\mathbb{R}\\) be differentiable. At iterate \\(\\theta_k\\), we observe a random vector \\(g_k\\) such that \\[ \\mathbb{E}[\\,g_k \\mid \\theta_k\\,] = \\nabla f(\\theta_k) \\quad\\text{and}\\quad \\mathbb{E}\\!\\left[\\|g_k-\\nabla f(\\theta_k)\\|^2 \\mid \\theta_k\\right] \\le \\sigma^2. \\] We will also use one of the following standard regularity conditions: (Convex + \\(L\\)-smooth) \\(f\\) is convex and the gradient is \\(L\\)-Lipschitz. (Strongly convex + \\(L\\)-smooth) \\(f\\) is \\(\\mu\\)-strongly convex and \\(L\\)-smooth. We consider the SGD update \\[ \\theta_{k+1} \\;=\\; \\theta_k - \\alpha_k\\, g_k, \\] and define the averaged iterate \\[ \\bar\\theta_K := \\frac{1}{K+1}\\sum_{k=0}^{K}\\theta_k. \\] Theorem 3.3 (SGD on smooth convex function) Assume \\(f\\) is convex and \\(L\\)-smooth. Suppose there exists \\(G\\!&gt;\\!0\\) with \\(\\mathbb{E}\\|g_k\\|^2 \\le G^2\\) for all \\(k\\). Choose a constant stepsize \\(\\alpha_k = \\alpha &gt; 0\\). Then for all \\(K \\ge 1\\), \\[\\begin{equation} \\mathbb{E}\\big[f(\\bar\\theta_K)\\big] - f^\\star \\leq \\frac{\\Vert \\theta_0 - \\theta^\\star \\Vert^2}{2 \\alpha (K+1)} + \\frac{\\alpha G^2}{2}. \\tag{3.9} \\end{equation}\\] Choose a diminishing step size \\(\\alpha_k = \\frac{\\Vert \\theta_0 - \\theta^\\star \\Vert}{G \\sqrt{k+1}}\\), then \\[\\begin{equation} \\mathbb{E}\\big[f(\\bar\\theta_K)\\big] - f^\\star \\leq \\frac{\\Vert \\theta_0 - \\theta^\\star \\Vert G}{\\sqrt{K+1}} = \\mathcal{O}\\left( \\frac{1}{\\sqrt{K}} \\right). \\tag{3.10} \\end{equation}\\] Proof. See this lecture note and (Garrigos and Gower 2023). Remarks. The bound is on the averaged iterate \\(\\bar\\theta_K\\) (the last iterate may be worse by constants without further assumptions). Replacing the second-moment bound by a variance bound \\(\\sigma^2\\) yields the same rate with \\(G^2\\) replaced by \\(\\sigma^2 + \\sup_k\\|\\nabla f(\\theta_k)\\|^2\\). With a constant stepsize, SGD converges \\(\\mathcal{O}(1/k)\\) up to a neighborhood set by the gradient noise. The next theorem states the convergence rate of SGD for minimizing strongly convex functions. Theorem 3.4 (SGD on smooth strongly convex function) Assume \\(f\\) is \\(\\mu\\)-strongly convex and \\(L\\)-smooth, and \\(\\mathbb{E}\\!\\left[\\|g_k\\|^2 \\right]\\le G^2\\). With stepsize \\(\\alpha_k = \\frac{1}{\\mu(k+1)}\\), the SGD iterates satisfy for all \\(K\\!\\ge\\!1\\), \\[\\begin{equation} \\begin{split} \\mathbb{E}[f(\\bar\\theta_K)] - f^\\star &amp; \\leq \\frac{G^2}{2 \\mu (K+1)} (1 + \\log(K+1)), \\\\ \\mathbb{E} \\Vert \\bar\\theta_K - \\theta^\\star \\Vert^2 &amp; \\leq \\frac{Q}{K+1}, \\ \\ Q = \\max \\left( \\frac{G^2}{\\mu^2}, \\Vert \\theta_0 - \\theta^\\star \\Vert^2 \\right). \\end{split} \\tag{3.11} \\end{equation}\\] Proof. See this lecture note and (Garrigos and Gower 2023). Practical Takeaways for Policy Gradients. Use diminishing stepsizes for theoretical convergence (\\(\\alpha_k \\propto 1/\\sqrt{k}\\) for general convex, \\(\\alpha_k \\propto 1/k\\) for strongly convex surrogates). With constant stepsizes, expect fast initial progress down to a variance-limited plateau; lowering variance (e.g., via baselines/advantage estimation) is as important as tuning \\(\\alpha\\). TODO: graph the different trajectories between minimizing a convex function using GD and SGD. 3.1.4 Beyond Vanilla Gradient Methods Several refinements to basic gradient updates are widely used: Momentum methods: incorporate past gradients to smooth updates and accelerate convergence. Adaptive learning rates (Adam, RMSProp, AdaGrad): adjust the learning rate per parameter based on historical gradient magnitudes. Second-order methods: approximate or use curvature information (the Hessian) for more informed updates, though often impractical in high dimensions. 3.2 Policy Gradients Policy gradients optimize a parameterized stochastic policy directly, without requiring an explicit action-value maximization step. They are applicable to both finite and continuous action spaces and are especially useful when actions are continuous or when “\\(\\arg\\max\\)” over \\(Q(s,a)\\) is costly or ill-posed. 3.2.1 Setup We consider a Markov decision process (MDP) with (possibly continuous) state space \\(\\mathcal{S}\\), action space \\(\\mathcal{A}\\), unknown dynamics \\(P\\), reward function \\(R(s,a)\\), and discount factor \\(\\gamma\\in[0,1)\\). Let \\(\\pi_\\theta(a\\mid s)\\) be a differentiable stochastic policy with parameters \\(\\theta\\in\\mathbb{R}^d\\). Trajectory. A state-action trajectory is \\(\\tau=(s_0,a_0,s_1,a_1,\\dots,s_{T})\\) with probability density/mass \\[\\begin{equation} p_\\theta(\\tau) = \\rho(s_0)\\prod_{t=0}^{T-1} \\pi_\\theta(a_t\\mid s_t)\\,P(s_{t+1}\\mid s_t,a_t), \\tag{3.12} \\end{equation}\\] where \\(\\rho\\) is the initial state distribution and \\(T\\) is the (random or fixed) episode length. Return. Define the (discounted) return \\[\\begin{equation} R(\\tau) \\;=\\; \\sum_{t=0}^{T-1}\\gamma^t R(s_t,a_t), \\tag{3.13} \\end{equation}\\] and the return-to-go \\[\\begin{equation} g_t \\;=\\; \\sum_{t&#39;=t}^{T-1}\\gamma^{t&#39;-t} R(s_{t&#39;},a_{t&#39;}). \\tag{3.14} \\end{equation}\\] Optimization objective. The goal is to maximize the expected return \\[\\begin{equation} J(\\theta) \\;\\equiv\\; \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[R(\\tau)\\right] \\;=\\; \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t R(s_t,a_t)\\right], \\tag{3.15} \\end{equation}\\] where the expectation is taken over the randomness in (i) the initial state \\(s_0 \\sim \\rho\\), (ii) the policy \\(\\pi_\\theta\\), and (iii) the transition dynamics \\(P\\). 3.2.1.1 Policy models Finite action spaces (\\(\\mathcal{A}\\) discrete). A common choice is a softmax (categorical) policy over a score (logit) function \\(f_\\theta(s,a)\\): \\[\\begin{equation} \\pi_\\theta(a\\mid s) \\;=\\; \\frac{\\exp\\{f_\\theta(s,a)\\}}{\\sum_{a&#39;\\in\\mathcal{A}}\\exp\\{f_\\theta(s,a&#39;)\\}}. \\tag{3.16} \\end{equation}\\] Here we use \\(\\exp\\{f_\\theta(s,a)\\} = e^{f_\\theta(s,a)}\\) for pretty formatting. Typically \\(f_\\theta\\) is a neural network or a linear function over features. Continuous action spaces (\\(\\mathcal{A}\\subseteq\\mathbb{R}^m\\)). A standard choice is a Gaussian policy: \\[\\begin{equation} \\pi_\\theta(a\\mid s) \\;=\\; \\mathcal{N}\\!\\big(a;\\;\\mu_\\theta(s),\\,\\Sigma_\\theta(s)\\big), \\tag{3.17} \\end{equation}\\] where \\(\\mu_\\theta(s)\\) and (often diagonal) covariance \\(\\Sigma_\\theta(s)\\) are differentiable functions (e.g., neural networks) parameterized by \\(\\theta\\). The policy \\(\\pi_\\theta(a \\mid s)\\) samples actions from the Gaussian parameterized by \\(\\mu_\\theta(s)\\) and \\(\\Sigma_\\theta(s)\\). Other choices include squashed Gaussians (e.g., \\(\\tanh\\)) or Beta distributions for bounded actions. 3.2.2 The Policy Gradient Lemma With the gradient-based optimization machinery from Section 3.1, a natural strategy for the policy optimization problem in (3.15) is gradient ascent on the objective \\(J(\\theta)\\). Consequently, the central task is to characterize the ascent direction, i.e., to compute \\(\\nabla_\\theta J(\\theta)\\). The policy gradient lemma, stated below, provides exactly this characterization. Crucially, it expresses \\(\\nabla_\\theta J(\\theta)\\) in terms of the policy’s score function \\(\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\) and returns, without differentiating through the environment dynamics. This likelihood-ratio form makes policy optimization feasible even when the transition model is unknown or non-differentiable. Theorem 3.5 (Policy Gradient Lemma) Let \\(J(\\theta)=\\mathbb{E}_{\\tau \\sim p_\\theta}[R(\\tau)]\\) as defined in (3.15) Then: \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}_{\\tau\\sim p_\\theta} \\Big[R(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau)\\Big] \\;=\\; \\mathbb{E}_{\\tau\\sim p_\\theta} \\Bigg[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\;R(\\tau)\\Bigg]. \\tag{3.18} \\end{equation}\\] By causality (future action does not affect past reward), the full return can be replaced by return-to-go: \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}_{\\tau\\sim p_\\theta} \\Bigg[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\;g_t\\Bigg]. \\tag{3.19} \\end{equation}\\] Equivalently, using value functions, \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\frac{1}{1-\\gamma} \\mathbb{E}_{s\\sim d_\\theta,\\;a\\sim\\pi_\\theta} \\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,Q^{\\pi_\\theta}(s,a)\\Big], \\tag{3.20} \\end{equation}\\] where \\(d_\\theta\\) is the (discounted) on-policy state visitation distribution for infinite-horizon MDPs: \\[\\begin{equation} d_\\theta(s) = (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t \\Pr_\\theta(s_t=s). \\tag{3.21} \\end{equation}\\] Proof. We prove the three equivalent forms step by step. Throughout, we assume \\(\\theta\\) parameterizes only the policy \\(\\pi_\\theta\\) (not the dynamics \\(P\\) nor the initial distribution \\(\\rho\\)), and that interchanging \\(\\nabla_\\theta\\) with the trajectory integral/sum is justified (e.g., bounded rewards and finite horizon or standard dominated-convergence conditions). Let the return-to-go \\(g_t\\) be defined as in (3.14). Step 1 (Log-derivative trick). Write the objective as an expectation over trajectories: \\[ J(\\theta) \\;=\\; \\int R(\\tau)\\, p_\\theta(\\tau)\\, d\\tau. \\] Differentiate under the integral and use \\[\\begin{equation} \\nabla_\\theta p_\\theta(\\tau)=p_\\theta(\\tau)\\nabla_\\theta\\log p_\\theta(\\tau) \\tag{3.22} \\end{equation}\\] we can write: \\[ \\nabla_\\theta J(\\theta) = \\int R(\\tau)\\,\\nabla_\\theta p_\\theta(\\tau)\\, d\\tau = \\int R(\\tau)\\, p_\\theta(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau)\\, d\\tau = \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\big[R(\\tau)\\,\\nabla_\\theta \\log p_\\theta(\\tau)\\big], \\] which is (3.18) up to expanding \\(\\log p_\\theta(\\tau)\\). To see why (3.22) is true, write \\[ \\nabla_\\theta \\log p_\\theta(\\tau) = \\frac{1}{p_\\theta(\\tau)} \\nabla_\\theta p_\\theta(\\tau), \\] using the chain rule. Step 2 (Policy-only dependence). Factor the trajectory likelihood/mass: \\[ p_\\theta(\\tau) = \\rho(s_0)\\,\\prod_{t=0}^{T-1}\\pi_\\theta(a_t\\mid s_t)\\,P(s_{t+1}\\mid s_t,a_t). \\] Since \\(\\rho\\) and \\(P\\) do not depend on \\(\\theta\\), \\[ \\log p_\\theta(\\tau) = \\text{const} \\;+\\; \\sum_{t=0}^{T-1}\\log \\pi_\\theta(a_t\\mid s_t) \\quad\\Rightarrow\\quad \\nabla_\\theta \\log p_\\theta(\\tau) \\;=\\; \\sum_{t=0}^{T-1}\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t). \\] Substitute into Step 1 to obtain the second equality in (3.18): \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\Bigg[\\sum_{t=0}^{T-1}\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,R(\\tau)\\Bigg]. \\] Step 3 (Causality \\(\\Rightarrow\\) return-to-go). Expand \\(R(\\tau)=\\sum_{t=0}^{T-1}\\gamma^{t} r_{t}\\) (with \\(r_{t}:=R(s_{t},a_{t})\\)) and swap sums: \\[ \\mathbb{E} \\Bigg[\\sum_{t=0}^{T-1}\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,R(\\tau)\\Bigg] = \\sum_{t=0}^{T-1}\\sum_{t&#39;=0}^{T-1}\\mathbb{E} \\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\gamma^{t&#39;} r_{t&#39;}\\big]. \\] For \\(t&#39;&lt;t\\), the factor \\(\\gamma^{t&#39;} r_{t&#39;}\\) is measurable w.r.t. the history \\(\\mathcal{F}_t=\\sigma(s_0,a_0,\\dots,s_t)\\), while \\[ \\mathbb{E} \\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big|\\,\\mathcal{F}_t\\big] = \\sum_{a} \\pi_\\theta(a\\mid s_t)\\,\\nabla_\\theta \\log \\pi_\\theta(a\\mid s_t) = \\nabla_\\theta \\sum_{a}\\pi_\\theta(a\\mid s_t) = \\nabla_\\theta 1 = 0, \\] (and analogously with integrals for continuous \\(\\mathcal{A}\\)). Hence by the tower property, \\[ \\mathbb{E} \\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\gamma^{t&#39;} r_{t&#39;}\\big]=0\\quad\\text{for all }t&#39;&lt;t. \\] Therefore only the terms with \\(t&#39;\\ge t\\) survive, and \\[ \\nabla_\\theta J(\\theta) = \\sum_{t=0}^{T-1}\\mathbb{E} \\Big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\sum_{t&#39;=t}^{T-1}\\gamma^{t&#39;} r_{t&#39;}\\Big] = \\mathbb{E} \\Bigg[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,g_t\\Bigg], \\] which is (3.19). Step 4 (Value-function form). Condition on \\((s_t,a_t)\\) and use the definition of the action-value function: \\[ Q^{\\pi_\\theta}(s_t,a_t) \\;\\equiv\\; \\mathbb{E}\\!\\left[g_t \\,\\middle|\\, s_t,a_t\\right]. \\] Taking expectations then yields \\[ \\mathbb{E} \\big[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,g_t\\big] = \\mathbb{E} \\big[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,Q^{\\pi_\\theta}(s_t,a_t)\\big]. \\] Summing over \\(t\\) and collecting terms with the (discounted) on-policy state visitation distribution \\(d_\\theta\\) (for the infinite-horizon case, e.g., \\(d_\\theta(s)=(1-\\gamma)\\sum_{t=0}^\\infty \\gamma^t\\,\\Pr_\\theta(s_t=s)\\); for finite \\(T\\), use the corresponding finite-horizon weighting), we obtain \\[ \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}_{s\\sim d_\\theta,\\;a\\sim \\pi_\\theta} \\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,Q^{\\pi_\\theta}(s,a)\\Big], \\] which is (3.20). Conclusion. Combining Steps 1–4 proves all three stated forms of the policy gradient. 3.2.3 REINFORCE The policy gradient lemma immediately gives us an algorithm. Specifically, the gradient receipe in (3.18) tells us that if we generate one trajectory \\(\\tau\\) by following the policy \\(\\pi\\), then \\[\\begin{equation} \\widehat{\\nabla_\\theta J} = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) R(\\tau) \\tag{3.23} \\end{equation}\\] is an unbiased estimator of the true gradient. With this sample gradient estimator, we obtain the classical REINFORCE algorithm. Single-Trajectory (Naive) REINFORCE Initialize \\(\\theta_0\\) for the initial policy \\(\\pi_{\\theta_0}(a \\mid s)\\) For \\(k=0,1,\\dots,\\) do: Obtain a trajectory \\(\\tau \\sim p_{\\theta_k}\\) Compute the stochastic gradient \\(g_k\\) as in (3.23) Update \\(\\theta_{k+1} = \\theta_k + \\alpha_k g_k\\) To reduce variance of the gradient estimator, we can use a minibatch of trajectories. For example, given a batch of \\(N\\) trajectories \\(\\{\\tau^{(i)}\\}_{i=1}^N\\) collected by \\(\\pi_\\theta\\), define for each timestep the return-to-go \\[ g_t^{(i)} = \\sum_{t&#39;=t}^{T^{(i)}-1} \\gamma^{t&#39;-t} R\\!\\left(s_{t&#39;}^{(i)},a_{t&#39;}^{(i)}\\right). \\] An unbiased gradient estimator, from (3.19) is \\[\\begin{equation} \\widehat{\\nabla_\\theta J} \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T^{(i)}-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta \\big(a_t^{(i)}\\mid s_t^{(i)}\\big) g_t^{(i)}. \\tag{3.24} \\end{equation}\\] This leads to the following minibatch REINFORCE algorithm. Minibatch REINFORCE Initialize \\(\\theta_0\\) for the initial policy \\(\\pi_{\\theta_0}(a \\mid s)\\) For \\(k=0,1,\\dots,\\) do: Obtain N trajectories \\(\\{ \\tau^{(i)} \\}_{i=1}^N \\sim p_{\\theta_k}\\) Compute the stochastic gradient \\(g_k\\) as in (3.24) Update \\(\\theta_{k+1} = \\theta_k + \\alpha_k g_k\\) We apply both the single-trajectory (naive) REINFORCE and a minibatch variant to the CartPole-v1 balancing task. The results show that variance reduction via minibatching is crucial for stable learning and for obtaining strong policies with policy-gradient methods. Example 3.1 (REINFORCE for Cart-Pole Balancing) Consider the cart-pole balancing task illustrated in Fig. 3.1. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart. Figure 3.1: Cart Pole balance. State Space. The state of the cart-pole system is denoted by \\(s \\in \\mathcal{S} \\subset \\mathbb{R}^4\\), containing the position and velocity of the cart, as well as the angle and angular velocity of the pole. Action Space. The action space \\(\\mathcal{A}\\) is discrete and contains two elements: pushing to the left and pushing to the right. The dynamics of the MDP is provided by the Gym simulator and is described in the original paper (Barto, Sutton, and Anderson 2012). At the beginning of the episode, all state variables are randomly initialized in \\([-0.05,0.05]\\) and the goal for the agent is to apply the actions to balance the cart-pole for as long as possible—the agent gets a reward of \\(+1\\) every step if (1) the pole angle remains between \\(-12^\\circ\\) and \\(+12^\\circ\\) and (2) the cart position remains between \\(-2.4\\) and \\(2.4\\). The maximum episode length is \\(500\\). We design a policy network in the form of (3.16) since the action space is finite. REINFORCE. We first apply the naive REINFORCE algorithm where the gradient estimator is computed from a single trajectory as in (3.23). Fig. 3.2 shows the learning curve, which indicates that the REINFORCE algorithm was not able to learn a good policy after 2000 episodes. Figure 3.2: Learning curve (Naive REINFORCE). Minibatch REINFORCE. We then apply the minibatch REINFORCE algorithm where the gradient estimator is computed from multiple (\\(20\\) in our case) trajectories as in (3.24). Fig. 3.3 shows the learning curve, which shows steady increase in the per-episode return that eventually gets close to the maximum per-episode return \\(500\\). Fig. 3.4 shows a rollout video of applying the policy training from minibatch REINFORCE. We can see the policy nicely balances the cart-pole system. You can play with the code here. Figure 3.3: Learning curve (Minibatch REINFORCE). Figure 3.4: Policy rollout (Minibatch REINFORCE). 3.2.4 Baselines and Variance Reduction From the REINFORCE experiments above, we have seen firsthand that variance reduction is critical for stable policy-gradient learning. A natural question is: what framework can we use to systematically reduce the variance of the gradient estimator while preserving unbiasedness? 3.2.4.1 Baseline A key device is a baseline \\(b:\\mathcal{S}\\to\\mathbb{R}\\) added at each timestep: \\[\\begin{equation} \\widehat{g} \\;=\\; \\sum_{t=0}^{T-1} \\gamma^t\\,\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t - b(s_t)\\big). \\tag{3.25} \\end{equation}\\] The only difference between (3.25) and the original gradient estimator (3.19) is that the baseline \\(b(s_t)\\) is subtracted from the return-to-go \\(g_t\\). The next theorem states that any state-only baseline does not change the expectation of the gradient estimator. Theorem 3.6 (Baseline Invariance) Let \\(b:\\mathcal{S}\\to\\mathbb{R}\\) be any function independent of the action \\(a_t\\). Then \\[ \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right]=0, \\] and thus \\[\\begin{equation} \\nabla_\\theta J(\\theta) \\;=\\; \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t - b(s_t)\\big)\\right]. \\tag{3.26} \\end{equation}\\] Equivalently, using action-values, \\[\\begin{equation} \\nabla_\\theta J(\\theta) = \\frac{1}{1-\\gamma}\\, \\mathbb{E}_{s\\sim d_\\theta,\\;a\\sim\\pi_\\theta} \\!\\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,\\big(Q^{\\pi_\\theta}(s,a)-b(s)\\big)\\Big]. \\tag{3.27} \\end{equation}\\] Proof. We prove (i) the baseline term has zero expectation, (ii) the baseline-subtracted estimator in (3.26) is unbiased, and (iii) the equivalent \\(Q\\)-value form (3.27). Throughout we assume standard conditions ensuring interchange of expectation and differentiation (e.g., bounded rewards with finite horizon or discounted infinite horizon, and a differentiable policy). Step 1 (Score-function expectation is zero). Fix a state \\(s\\in\\mathcal{S}\\). The score function integrates/sums to zero under the policy: \\[\\begin{equation} \\begin{split} \\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot\\mid s)}\\!\\big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\big] &amp; = \\sum_{a\\in\\mathcal{A}} \\pi_\\theta(a\\mid s)\\,\\nabla_\\theta \\log \\pi_\\theta(a\\mid s) = \\sum_{a\\in\\mathcal{A}} \\nabla_\\theta \\pi_\\theta(a\\mid s) \\\\ &amp; = \\nabla_\\theta \\sum_{a\\in\\mathcal{A}} \\pi_\\theta(a\\mid s) = \\nabla_\\theta 1 = 0, \\end{split} \\end{equation}\\] with the obvious replacement of sums by integrals for continuous \\(\\mathcal{A}\\). This identity is the standard “score has zero mean” property. Step 2 (Baseline term has zero expectation). Let \\(\\mathcal{F}_t := \\sigma(s_0,a_0,\\ldots,s_t)\\) be the history up to time \\(t\\) and recall that \\(b(s_t)\\) is independent of \\(a_t\\). Using iterated expectations: \\[ \\mathbb{E}\\!\\left[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right] = \\mathbb{E}\\!\\left[ \\gamma^t\\, b(s_t)\\, \\underbrace{\\mathbb{E}\\!\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\middle|\\, s_t\\right]}_{=\\,0~\\text{by Step 1}} \\right] = 0. \\] Summing over \\(t\\) yields \\[ \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right]=0. \\] Step 3 (Unbiasedness of the baseline-subtracted estimator). By the policy gradient lemma (likelihood-ratio form with return-to-go; see (3.19)), \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,g_t\\right]. \\] Subtract and add the baseline term inside the expectation: \\[ \\begin{split} \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,g_t\\right] &amp; = \\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t-b(s_t)\\big)\\right] \\;+\\; \\\\ &amp; \\quad \\quad \\underbrace{\\mathbb{E}\\!\\left[\\sum_{t=0}^{T-1} \\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right]}_{=\\,0~\\text{by Step 2}}. \\end{split} \\] Therefore (3.26) holds, proving that any state-only baseline preserves unbiasedness. Step 4 (Equivalent \\(Q\\)-value form). Condition on \\((s_t,a_t)\\) and use the definition \\(Q^{\\pi_\\theta}(s_t,a_t):=\\mathbb{E}[g_t\\mid s_t,a_t]\\): \\[ \\mathbb{E}\\!\\big[\\gamma^t \\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t-b(s_t)\\big)\\big] = \\mathbb{E}\\!\\Big[ \\gamma^t\\, \\mathbb{E}\\!\\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t-b(s_t)\\big)\\mid s_t\\big] \\Big]. \\] Inside the inner expectation (over \\(a_t\\sim \\pi_\\theta(\\cdot\\mid s_t)\\)) and using \\(b(s_t)\\)’s independence from \\(a_t\\), \\[ \\mathbb{E}\\!\\big[\\nabla_\\theta \\log \\pi_\\theta(a_t\\mid s_t)\\,\\big(g_t-b(s_t)\\big)\\mid s_t\\big] = \\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot\\mid s_t)}\\!\\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s_t)\\,\\big(Q^{\\pi_\\theta}(s_t,a)-b(s_t)\\big)\\Big]. \\] Summing over \\(t\\) with discount \\(\\gamma^t\\) and collecting terms with the (discounted) on-policy state-visitation distribution \\(d_\\theta\\) (cf. (3.21)) yields the infinite-horizon identity \\[ \\nabla_\\theta J(\\theta) = \\frac{1}{1-\\gamma}\\, \\mathbb{E}_{s\\sim d_\\theta,\\;a\\sim \\pi_\\theta}\\! \\Big[\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,\\big(Q^{\\pi_\\theta}(s,a)-b(s)\\big)\\Big], \\] which is (3.27). 3.2.4.2 Optimal Baseline and Advantage Among all state-only baselines \\(b(s)\\), which one minimizes the variance of the gradient estimator? Theorem 3.7 (Variance-Minimizing Baseline (per-state)) For the estimator \\[ g(s,a)=\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big), \\] the \\(b(s)\\) minimizing \\(\\operatorname{Var}[g\\mid s]\\) is \\[ b^\\star(s)= \\frac{\\mathbb{E}_{a\\sim \\pi_\\theta}\\!\\left[\\| \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\, Q^\\pi(s,a)\\right]} {\\mathbb{E}_{a\\sim \\pi_\\theta}\\!\\left[\\| \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\right]}. \\tag{3.28} \\] Assuming that the norm factor \\(\\Vert \\nabla_\\theta \\log \\pi_\\theta(a\\mid s) \\Vert^2\\) varies slowly with \\(a\\), then \\[ b^\\star(s) \\approx V^\\pi(s). \\] Proof. Let \\(s\\in\\mathcal{S}\\) be fixed and write \\[ u(a\\mid s) \\;\\equiv\\; \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\in\\mathbb{R}^d, \\qquad w(a\\mid s) \\;\\equiv\\; \\|u(a\\mid s)\\|^2 \\;\\ge 0. \\] Consider the vector-valued random variable \\[ g(s,a) \\;=\\; u(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big), \\] where the randomness is over \\(a\\sim \\pi_\\theta(\\cdot\\mid s)\\). We aim to choose \\(b(s)\\in\\mathbb{R}\\) to minimize the conditional variance \\[ \\operatorname{Var}[g\\mid s] \\;=\\; \\mathbb{E}\\!\\left[\\|g(s,a)-\\mathbb{E}[g\\mid s]\\|^2 \\,\\middle|\\, s\\right]. \\] Using the identity \\(\\operatorname{Var}[X]=\\mathbb{E}\\|X\\|^2-\\|\\mathbb{E}X\\|^2\\) (for vector \\(X\\) with Euclidean norm), we have \\[ \\operatorname{Var}[g\\mid s] \\;=\\; \\underbrace{\\mathbb{E}\\!\\left[\\|g(s,a)\\|^2 \\mid s\\right]}_{\\text{depends on } b(s)} \\;-\\; \\underbrace{\\big\\|\\mathbb{E}[g\\mid s]\\big\\|^2}_{\\text{independent of } b(s)}. \\] We first show that the mean term is independent of \\(b(s)\\). Indeed, \\[ \\mathbb{E}[g\\mid s] = \\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot\\mid s)}\\!\\big[u(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big)\\big] = \\mathbb{E}\\!\\big[u(a\\mid s)\\,Q^\\pi(s,a)\\big] \\;-\\; b(s)\\,\\underbrace{\\mathbb{E}\\!\\big[u(a\\mid s)\\big]}_{=\\,0}, \\] where \\(\\mathbb{E}[u(a\\mid s)]=\\sum_a \\pi_\\theta(a\\mid s)\\nabla_\\theta\\log\\pi_\\theta(a\\mid s)=\\nabla_\\theta \\sum_a \\pi_\\theta(a\\mid s)=\\nabla_\\theta 1=0\\) (replace sums by integrals in the continuous case). Therefore \\(\\mathbb{E}[g\\mid s]\\) does not depend on \\(b(s)\\). Consequently, minimizing \\(\\operatorname{Var}[g\\mid s]\\) is equivalent to minimizing the conditional second moment \\[ \\mathbb{E}\\!\\left[\\|g(s,a)\\|^2 \\mid s\\right] = \\mathbb{E}\\!\\left[\\|u(a\\mid s)\\|^2 \\,\\big(Q^\\pi(s,a)-b(s)\\big)^2 \\,\\middle|\\, s\\right] = \\mathbb{E}\\!\\left[w(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big)^2 \\,\\middle|\\, s\\right]. \\] The right-hand side is a convex quadratic in the scalar \\(b(s)\\). Differentiate w.r.t. \\(b(s)\\) and set to zero: \\[ \\frac{\\partial}{\\partial b(s)} \\mathbb{E}\\!\\left[w(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big)^2 \\,\\middle|\\, s\\right] = -2\\,\\mathbb{E}\\!\\left[w(a\\mid s)\\,\\big(Q^\\pi(s,a)-b(s)\\big)\\,\\middle|\\, s\\right] = 0. \\] Hence, \\[ \\mathbb{E}\\!\\left[w(a\\mid s)\\,Q^\\pi(s,a)\\,\\middle|\\, s\\right] = b(s)\\,\\mathbb{E}\\!\\left[w(a\\mid s)\\,\\middle|\\, s\\right], \\] and provided \\(\\mathbb{E}[w(a\\mid s)\\mid s]&gt;0\\) (i.e., the Fisher information at \\(s\\) is non-degenerate), the unique minimizer is \\[ b^\\star(s) = \\frac{\\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot\\mid s)}\\!\\left[\\| \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\, Q^\\pi(s,a)\\right]} {\\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot\\mid s)}\\!\\left[\\| \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\right]}, \\] which is (3.28). If \\(\\mathbb{E}[w(a\\mid s)\\mid s]=0\\) (e.g., a locally deterministic policy), then \\(g\\equiv 0\\) almost surely and any \\(b(s)\\) attains the minimum. Finally, when the weight \\(w(a\\mid s)=\\|\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\|^2\\) varies slowly with \\(a\\) (or is approximately constant) for a fixed \\(s\\), the ratio simplifies to \\[ b^\\star(s)\\;\\approx\\;\\frac{\\mathbb{E}[c(s)\\,Q^\\pi(s,a)\\mid s]}{\\mathbb{E}[c(s)\\mid s]} \\;=\\; \\mathbb{E}_{a\\sim \\pi_\\theta(\\cdot\\mid s)}\\!\\big[Q^\\pi(s,a)\\big] \\;=\\; V^\\pi(s), \\] so that the baseline-subtracted target becomes the advantage \\(A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)\\). When using \\(V^\\pi(s)\\) as the baseline, the baseline-subtracted target is called the advantage function \\[\\begin{equation} A^{\\pi_\\theta}(s,a)\\;=\\;Q^{\\pi_\\theta}(s,a)-V^{\\pi_\\theta}(s). \\tag{3.29} \\end{equation}\\] The corresponding minibatch gradient estimator becomes \\[\\begin{equation} \\widehat{\\nabla_\\theta J} \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T^{(i)}-1} \\gamma^t\\, \\nabla_\\theta \\log \\pi_\\theta \\big(a_t^{(i)}\\mid s_t^{(i)}\\big)\\, \\widehat{A}_t^{(i)}, \\quad \\widehat{A}_t^{(i)} \\approx g_t^{(i)} - V_\\phi \\big(s_t^{(i)}\\big), \\tag{3.30} \\end{equation}\\] where \\(V_\\phi\\) is a learned approximation to \\(V^{\\pi_\\theta}\\). 3.2.4.3 Intuition for the Advantage The advantage \\[ A^\\pi(s,a) \\;=\\; Q^\\pi(s,a) - V^\\pi(s) \\] measures how much better or worse action \\(a\\) is at state \\(s\\) relative to the policy’s average action quality \\(V^\\pi(s)=\\mathbb{E}_{a\\sim\\pi}[Q^\\pi(s,a)\\mid s]\\). Hence \\(\\mathbb{E}_{a\\sim\\pi}[A^\\pi(s,a)\\mid s]=0\\): it is a relative score. With a value baseline, the policy-gradient update is \\[ \\nabla_\\theta J(\\theta) \\;=\\; \\frac{1}{1-\\gamma}\\, \\mathbb{E}_{s\\sim d_\\pi,\\;a\\sim\\pi}\\!\\big[ \\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,A^\\pi(s,a) \\big]. \\] If \\(A^\\pi(s,a) &gt; 0\\): the term \\(\\nabla_\\theta \\log \\pi_\\theta(a\\mid s)\\,A^\\pi(s,a)\\) increases \\(\\log \\pi_\\theta(a\\mid s)\\) (and thus \\(\\pi_\\theta(a\\mid s)\\))—the policy puts more probability mass on actions that outperformed its average at \\(s\\). If \\(A^\\pi(s,a) &lt; 0\\): it decreases \\(\\log \\pi_\\theta(a\\mid s)\\)—the policy puts less probability mass on actions that underperformed at \\(s\\). If \\(A^\\pi(s,a) \\approx 0\\): the action performed about as expected; the update at that \\((s,a)\\) is negligible. Subtracting \\(V^\\pi(s)\\) centers returns per state, so the update depends only on relative goodness. This: preserves unbiasedness (baseline invariance), reduces variance (no large, shared offset), focuses learning on which actions at \\(s\\) should get more/less probability. 3.2.4.4 REINFORCE with a Learned Value Baseline Recall that in Section 2.2, we have introduced multiple algorithms that can learn an approximate value function for policy evaluation. For example, we can use Monte Carlo estimation. We now combine REINFORCE with a learned baseline \\(V_\\phi(s)\\approx V^{\\pi_\\theta}(s)\\), yielding a lower-variance update while keeping the estimator unbiased. Minibatch REINFORCE with a Learned Value Baseline Inputs: policy \\(\\pi_\\theta(a\\mid s)\\), value \\(V_\\phi(s)\\), discount \\(\\gamma\\in[0,1)\\), stepsizes \\(\\alpha_\\theta,\\alpha_\\phi&gt;0\\), batch size \\(N\\). Convergence controls: tolerance \\(\\varepsilon&gt;0\\), maximum inner steps \\(K_{\\max}\\) (value-fit loop), optional patience \\(P\\). Collect trajectories. Roll out \\(N\\) on-policy trajectories \\(\\{\\tau^{(i)}\\}_{i=1}^N\\) using \\(\\pi_\\theta\\). For each trajectory \\(i\\) and timestep \\(t\\), record \\((s_t^{(i)},a_t^{(i)},r_t^{(i)})\\). Compute returns-to-go. For each \\(i,t\\), \\[ g_t^{(i)} \\;=\\; \\sum_{t&#39;=t}^{T^{(i)}-1} \\gamma^{\\,t&#39;-t}\\, r_{t&#39;}^{(i)}. \\] Fit the value to convergence (critic inner loop). Define the batch regression loss \\[ \\mathcal{L}_V(\\phi) \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T^{(i)}-1} \\big(g_t^{(i)} - V_\\phi(s_t^{(i)})\\big)^2. \\] Perform gradient steps on \\(\\phi\\) until convergence on this fixed batch: \\[ \\phi \\leftarrow \\phi - \\alpha_\\phi \\,\\nabla_\\phi \\mathcal{L}_V(\\phi). \\] Repeat for \\(k=1,\\dots,K_{\\max}\\) or until \\[ \\frac{\\mathcal{L}_V^{(k-1)}-\\mathcal{L}_V^{(k)}}{\\max\\{1,|\\mathcal{L}_V^{(k-1)}|\\}} &lt; \\varepsilon \\] for \\(M\\) consecutive checks. Denote the (approximately) converged parameters by \\(\\phi^\\star\\). Form (optionally standardized) advantages using the converged value. \\[ \\widehat{A}_t^{(i)} \\;=\\; g_t^{(i)} - V_{\\phi^\\star}\\!\\big(s_t^{(i)}\\big), \\qquad \\tilde{A}_t^{(i)} \\;=\\; \\frac{\\widehat{A}_t^{(i)} - \\mu_A}{\\sigma_A+\\delta}\\ \\ (\\text{optional, batch-wise}), \\] where \\(\\mu_A,\\sigma_A\\) are the mean and std of \\(\\{\\widehat{A}_t^{(i)}\\}\\) over the whole batch, and \\(\\delta&gt;0\\) is a small constant. Single policy (actor) update. Using the converged baseline, take one ascent step: \\[ \\theta \\;\\leftarrow\\; \\theta \\;+\\; \\alpha_\\theta \\cdot \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T^{(i)}-1} \\gamma^t\\,\\nabla_\\theta \\log \\pi_\\theta \\big(a_t^{(i)}\\mid s_t^{(i)}\\big)\\,\\tilde{A}_t^{(i)}. \\] (If not standardizing, use \\(\\widehat{A}_t^{(i)}\\) in place of \\(\\tilde{A}_t^{(i)}\\).) Repeat from Step 1 with the updated policy. Notes. By baseline invariance, subtracting \\(V_{\\phi^\\star}(s)\\) keeps the policy-gradient unbiased while reducing variance. Converging the critic on each fixed batch (Steps 3–4) approximates the variance-minimizing baseline for that batch before a single actor step, often stabilizing learning in high-variance settings. Example 3.2 (REINFORCE with a Learned Value Baseline for Cart-Pole) Consider the same cart-pole balancing task in Example 3.1. We use minibatch REINFORCE with a learned value baseline (batch size \\(50\\)), the algorithm described above. Fig. 3.5 shows the learning curve. The algorithm is able to steadily increase the per-episode returns. Fig. 3.6 shows a rollout of the system trajectory under the learned policy. You can play with the code here. Figure 3.5: Learning curve (Minibatch REINFORCE with a Learned Value Baseline). Figure 3.6: Policy rollout (Minibatch REINFORCE with a Learned Value Baseline). 3.3 Actor–Critic Methods Actor–critic (AC) algorithms marry policy gradients (the actor) with value function learning (the critic). The critic reduces variance by supplying low-noise estimates of action quality (values or advantages), while the actor updates the policy using these estimates. In contrast to pure Monte Carlo baselines, actor–critic bootstraps from its own predictions, enabling online, incremental, and often more sample-efficient learning. 3.3.1 Anatomy of an Actor–Critic Actor (policy): a differentiable policy \\(\\pi_\\theta(a\\mid s)\\). Critic (value): an approximator for \\(V_\\phi(s)\\), \\(Q_\\psi(s,a)\\), or directly the advantage \\(A_\\eta(s,a)\\). Update coupling: the actor ascends a baseline-subtracted log-likelihood objective using advantage-like targets supplied by the critic. 3.3.2 On-Policy Actor–Critic with TD(0) We first learn a state value function \\(V_\\phi(s)\\) with a one-step bootstrapped TD(0) target: \\[ \\delta_t \\;\\equiv\\; r_t + \\gamma\\,V_\\phi(s_{t+1}) - V_\\phi(s_t), \\qquad \\mathcal{L}_V(\\phi) \\;=\\; \\frac12\\,\\delta_t^{\\,2}. \\tag{3.31} \\] If \\(V_\\phi \\approx V^\\pi\\), then \\(\\mathbb{E}[\\delta_t\\mid s_t,a_t]\\approx A^\\pi(s_t,a_t)\\), so \\(\\delta_t\\) serves as a low-variance advantage target for the actor: \\[ \\widehat{\\nabla_\\theta J} \\;=\\; \\frac{1}{|\\mathcal{B}|} \\sum_{(s_t,a_t)\\in \\mathcal{B}} \\nabla_\\theta \\log\\pi_\\theta(a_t\\mid s_t)\\,\\underbrace{\\delta_t}_{\\text{advantage target}}. \\tag{3.32} \\] (Practical: normalize \\(\\{\\delta_t\\}_{\\mathcal{B}}\\) to mean \\(0\\) and unit variance within a batch; clip gradients for stability.) On-Policy Actor–Critic with One-Step Bootstrap (TD(0)) Inputs: policy \\(\\pi_\\theta(a\\mid s)\\), value \\(V_\\phi(s)\\), discount \\(\\gamma\\in[0,1)\\), stepsizes \\(\\alpha_\\theta,\\alpha_\\phi&gt;0\\), rollout length \\(K\\), minibatch size \\(|\\mathcal{B}|\\). For iterations \\(k=0,1,2,\\dots\\): Collect on-policy rollouts. Run \\(\\pi_\\theta\\) for \\(K\\) steps (optionally across parallel envs), storing transitions \\(\\{(s_t,a_t,r_t,s_{t+1}\\}\\). Compute TD errors. For each transition, compute the TD error \\[ \\delta_t \\leftarrow r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t). \\] Critic update (value). Minimize \\(\\sum_{t\\in\\mathcal{B}} \\frac12\\,\\delta_t^{\\,2}\\): perform multiple steps of \\[ \\phi \\leftarrow \\phi - \\alpha_\\phi \\,\\nabla_\\phi \\Big(\\frac{1}{|\\mathcal{B}|}\\sum_{t\\in\\mathcal{B}}\\frac12\\,\\delta_t^{\\,2}\\Big). \\] Actor advantages. Set \\(\\widehat{A}_t \\leftarrow \\delta_t\\) (optionally normalize over \\(\\mathcal{B}\\)). Actor update (policy gradient). \\[ \\theta \\leftarrow \\theta + \\alpha_\\theta \\,\\frac{1}{|\\mathcal{B}|}\\sum_{t\\in\\mathcal{B}} \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,\\widehat{A}_t. \\] Repeat from step 1. We apply the on-policy actor-critic algorithm to the cart-pole balancing task. Example 3.3 (Actor--Critic with One-Step Bootstrap for Cart-Pole) Consider the same cart-pole balancing control task as before, and this time apply the on-policy actor-critic with one-step bootstrap. Fig. 3.7 shows the learning curve. Fig. 3.8 shows an example rollout of the policy. You can play with the code here. Figure 3.7: Learning curve (Actor–Critic with One-Step Bootstrap). Figure 3.8: Policy rollout (Actor–Critic with One-Step Bootstrap). References Barto, Andrew G, Richard S Sutton, and Charles W Anderson. 2012. “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.” IEEE Transactions on Systems, Man, and Cybernetics, no. 5: 834–46. Garrigos, Guillaume, and Robert M Gower. 2023. “Handbook of Convergence Theorems for (Stochastic) Gradient Methods.” arXiv Preprint arXiv:2301.11235. Nesterov, Yurii. 2018. Lectures on Convex Optimization. Vol. 137. Springer. "],["appconvex.html", "A Convex Analysis and Optimization A.1 Theory A.2 Practice", " A Convex Analysis and Optimization A.1 Theory A.1.1 Sets Convex set is one of the most important concepts in convex optimization. Checking convexity of sets is crucial to determining whether a problem is a convex problem. Here we will present some definitions of some set notations in convex optimization. Definition A.1 (Affine set) A set \\(C\\subset \\mathbb{R}^n\\) is affine if the line through any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in \\mathbb{R}\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition A.2 (Convex set) A set \\(C\\subset \\mathbb{R}^n\\) is convex if the line segment between any two distinct points in \\(C\\) lies in \\(C\\), i.e., if for any \\(x_1,x_2 \\in C\\) and any \\(\\theta \\in [0,1]\\), we have \\(\\theta x_1 + (1-\\theta)x_2 \\in C\\). Definition A.3 (Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a cone if for any \\(x\\in C\\) and any \\(\\theta\\geq 0\\), we have \\(\\theta x \\in C\\). Definition A.4 (Convex Cone) A set \\(C\\subset \\mathbb{R}^n\\) is a convex cone if \\(C\\) is convex and a cone. Below are some important examples of convex sets: Definition A.5 (Hyperplane) A hyperplane is a set of the form \\[\\{x|a^Tx = b\\}\\] Definition A.6 (Halfspaces) A (closed) halfspace is a set of the form \\[\\{x|a^Tx \\leq b\\}\\] Definition A.7 (Balls) A ball is a set of the form \\[B(x,r) = \\{y|\\|y-x\\|_2 \\leq r\\} = \\{x+ru|\\|u\\|_2\\leq 1\\}\\] where \\(r &gt;0\\). Definition A.8 (Ellipsoids) A ellipsoid is a set of the form \\[\\mathcal{E} = \\{y|(y-x)^TP^{-1}(y-x)\\leq 1\\}\\] where \\(P\\) is symmetric and positive definite. Definition A.9 (Polyhedra) A polyhedra is defined as the solution set of a finite number of linear equalities and inequalities: \\[\\mathcal{P} = \\{x|a_j^Tx\\leq b_j, j=1,...,m, c_k^Tx=d_k,k=1,...,p\\}\\] Definition A.10 (Norm ball) A norm ball \\(B\\) of radius \\(r\\) and a center \\(x_c\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[B = \\{x|\\|x-x_c\\|\\leq r\\}\\] Definition A.11 (Norm cone) A norm cone \\(C\\) associated with the norm \\(\\|\\cdot\\|\\) is defined as: \\[C = \\{(x,t)|\\|x\\|\\leq t\\}\\subset \\mathbb{R}^{n+1}\\] Simplexes are important family of polyhedra. Suppose the \\(k+1\\) points \\(v_0,...,v_k\\in \\mathbb{R}^n\\) are affinely independent, which means \\(v_1-v_0,...,v_k-v_0\\) are linearly independent. Definition A.12 (Simplex) A simplex \\(C\\) defined by points \\(v_0,...,v_k\\) is: \\[C = \\textbf{conv}\\{v_0,...,v_k\\} = \\{\\theta_0v_0 + ... \\theta_kv_k|\\theta \\succeq 0, \\textbf{1}^T\\theta = 1\\}\\] Extremely important examples of convex sets are positive semidefinite cones: Definition A.13 (Symmetric,positive semidefinite,positive definite matrices) Symmetric matrices: \\(\\textbf{S}^n = \\{X\\in\\mathbb{R}^{n\\times n}| X=X^T\\}\\) Symmetric Positive Semidefinite matrices: \\(\\textbf{S}_+^n = \\{X\\in\\textbf{S}^n| X\\succeq0\\}\\) Symmetric Positive definite matrices: \\(\\textbf{S}_{++}^n = \\{X\\in\\textbf{S}^n| X\\succ0\\}\\) In most scenarios, the set we encounter is more complicated. In general it is extermely hard to determine whether a set in convex or not. But if the set is ‘generated’ by some convex sets, we can easily determine its convexity. So let’s focus on operations that preserve convexity: Proposition A.1 Assume \\(S\\) is convex, \\(S_\\alpha,\\alpha\\in\\mathcal{A}\\) is a family of convex sets. Following operations on convex sets will preserve convexity: Intersection: \\(\\bigcap_{\\alpha\\in\\mathcal{A}}S_\\alpha\\) is convex. Image under affine function: A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is affine if it has the form \\(f(x) = Ax+b\\). The image of \\(S\\) under affine function \\(f\\) is convex. I.e. \\(f(S) = \\{f(x)|x\\in S\\}\\) is convex Image under perspective function: We define the perspective function \\(P:\\mathbb{R}^{n+1}\\), with domain \\(\\textbf{dom}P = \\mathbb{R}^n\\times \\mathbb{R}_{++}\\)(where \\(\\mathbb{R}_{++}=\\{x\\in \\mathbb{R}|x&gt;0\\}\\)) as \\(P(z,t) = z/t\\). The image of \\(S\\) under perspective function is convex. Image under linear-fractional function: We define linear fractional function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) as:\\(f(x) = (Ax+b)/(c^Tx+d)\\) with \\(\\textbf{dom}f = \\{x|c^Tx+d&gt;0\\|\\). The image of \\(S\\) under linear fractional functions is convex. In some cases, the restrictions of interior is too strict. For example, imagine a plane in \\(\\mathbb{R}^3\\). The interior of the plane is \\(\\emptyset\\). But intuitively many property should be extended to this kind of situation. Because the points in the plane also lies ‘inside’ the convex set. Thus, we will define relative interior. First we will define affine hull. Definition A.14 (Affine hull) The affine hull of a set \\(S\\) is the smallest affine set that contains \\(S\\), which can be written as: \\[\\text{aff}(S) = \\{\\sum_{i=1}^k\\alpha_ix_i|k&gt;0,x_i\\in S,\\alpha_i\\in\\mathbb{R},\\sum_{i=1}^k\\alpha_i=1\\}\\] Definition A.15 (Relative Interior) The relative interior of a set \\(S\\) (denoted \\(\\text{relint}(S)\\)) is defined as its interior within the affine hull of \\(S\\). I.e. \\[\\text{relint}(S):=\\{x\\in S: \\text{there exists } \\epsilon&gt;0 \\text{ such that }N_\\epsilon \\cap \\text{aff}(S)\\subset S\\}\\] where \\(N_\\epsilon(x)\\) is a ball of radius \\(\\epsilon\\) centered on \\(x\\). A.1.2 Convex function In this section, let’s define convex functions: Definition A.16 (Convex function) A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if \\(\\textbf{dom}\\ f\\) is convex and \\(\\forall x,y\\in \\textbf{dom}\\ f\\) and with \\(\\theta \\in [0,1]\\), we have:\\[f(\\theta x +(1-\\theta)y)\\leq \\theta f(x) + (1-\\theta)f(y)\\] The function is strictly convex if the inequality holds whenever \\(x\\neq y\\) and \\(\\theta\\in (0,1)\\). If a function is differentiable, it will be easier for us to check its convexity: Proposition A.2 (Conditions for Convex function) 1.(First order condition) Suppose \\(f\\) is differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x,y\\in \\textbf{dom} f\\), \\[f(y)\\geq f(x) +\\nabla f(x)^T(y-x)\\] 2.(Second order conditions) Suppose \\(f\\) is twice differentiable, then \\(f\\) is convex if and only if \\(\\textbf{dom} f\\) is convex and \\(\\forall x\\in \\textbf{dom} f\\), \\[\\nabla^2 f(x) \\succeq \\textbf{0}\\] For the same purpose, some operations that preserve the convexity of the convex functions are presented here: Proposition A.3 (Operations that preserve convexity) Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be a convex function and \\(g_1,...,g_n\\) be convex functions. The following operations will preserve convexity of the function: 1.(Nonnegative weighted sum): A nonnegative weighted sum of convex functions: \\[f = \\omega_1f_1 + ... +\\omega_mf_m\\] 2.(Composition with an affine mapping) Suppose \\(A\\in \\mathbb{R}^{n\\times m}\\) and \\(b\\in \\mathbb{R}^n\\), then \\(g(x) = f(Ax+b)\\) is convex. 3.(Pointwise maximum and supremum) \\(g(x) = \\max\\{g_1(x),...,g_n(x)\\}\\) is convex. If \\(h(x,y)\\) is convex in \\(x\\) for each \\(y\\in\\mathcal{A}\\), then \\(\\sup_{y\\in\\mathcal{A}} h(x,y)\\) is also convex in \\(x\\). 4.(Minimization) If \\(h(x,y)\\) is convex in \\((x,y)\\), and \\(C\\) is a convex nonempty set, then \\(\\inf_{x\\in C} h(x,y)\\) is convex in \\(x\\). 5.(Perspective of a function) The perspective of \\(f\\) is the function \\(h:\\mathbb{R}^{n+1}\\to\\mathbb{R}\\) defined by: \\(h(x,t) = tf(x/t)\\) with domain \\(\\textbf{dom}\\ h=\\{(x,t)|x/t\\in\\textbf{dom} f,t&gt;0\\}\\). And \\(h\\) is convex. A.1.3 Lagrange dual We consider an optimization problem in the standard form (without assuming convexity of anything): \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; h_i(x) = 0\\quad i=1,...,p \\\\ \\end{aligned} \\end{equation}\\] Definition A.17 (Lagrange dual function) The Lagrangian related to the problem above is defined as: \\[L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_if_i(x)+\\sum_{i=1}^p\\nu_ih_i(x)\\] The Lagrange dual function is defined as: \\[g(\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}}L(x,\\lambda,\\nu)\\] When the Lagrangian is unbounded below in \\(x\\), the dual function takes on the value \\(-\\infty\\). Note that since the Lagrange dual function is a pointwise infimum of a family of affine functions of \\((\\lambda,\\nu)\\), so it’s concave. The Lagrange dual function will give us lower bounds of the optimal value of the original problem: \\[g(\\lambda,\\nu)\\leq p^*\\]. We can see that, the dual function can give a nontrivial lower bound only when \\(\\lambda\\succeq 0\\). Thus we can solve the following dual problem to get the best lower bound. Definition A.18 (Lagrange dual problem) The lagrangian dual problem is defined as follows: \\[\\begin{equation} \\begin{aligned} d^* = \\quad \\max_{\\lambda,\\nu} \\quad &amp; g(\\lambda,\\nu)\\\\ \\textrm{s.t.} \\quad &amp; \\lambda\\succeq 0 \\end{aligned} \\end{equation}\\] This is a convex optimization problem. We can easily see that \\[d^*\\leq p^*\\] always hold. This property is called weak duality. If \\[d^*=p^*\\], it’s called strong duality. Strong duality does not hold in general, but it usually holfs for convex problems. We can find conditions that guarantee strong duality in convex problems, which are called constrained qualifications. Slater’s constraint qualification is a useful one. Theorem A.1 (Slater's constraint qualification) Strong duality holds for a convex problem \\[\\begin{equation} \\begin{aligned} p^* = \\quad \\min_{x} \\quad &amp; f_0(x)\\\\ \\textrm{s.t.} \\quad &amp; f_i(x)\\leq 0\\quad i=1...,m\\\\ &amp; Ax=b \\\\ \\end{aligned} \\end{equation}\\] if it is strictly feasible, i.e. \\[\\exists x\\in\\textbf{relint}\\mathcal{D}:\\quad f_i(x)&lt;0,\\quad i=1...m,\\quad Ax=b\\] And the linear inequalities do not need to hold with strict inequality. A.1.4 KKT condition Note that if strong duality holds, denote \\(x^*\\) to be primal optimal, and \\((\\lambda^*,\\nu^*)\\) to be dual optimal. Then: \\[\\begin{equation} \\begin{aligned} f_0(x^*) = g(\\lambda^*,\\nu^*) = &amp; \\inf_x(f_0(x)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x))\\\\ \\leq &amp; f_0(x^*)+\\sum_{i=1}^m\\lambda_i^*f_i(x)+\\sum_{i=1}^p\\nu_i^*h_i(x)\\\\ \\leq &amp; f_0(x^*)\\\\ \\end{aligned} \\end{equation}\\] from this, combining \\(\\lambda^*\\geq 0\\) and \\(f_i(x^*)\\leq 0\\), we can know that: \\(\\lambda_i^*f_i(x^*)=0\\quad i=1\\cdots m\\). This means for \\(\\lambda_i^*\\) and \\(f_i(x^*)\\), one of them must be zero, which is known as complementary slackness). Thus we arrived at the following four conditions, which are called KKT conditions. Theorem A.2 (Karush-Kuhn-Tucker(KKT) Conditions) The following four conditions are called KKT conditions (for a problem with differentiable \\(f_i,h_i\\)) Primal feasible: \\(f_i(x) \\leq 0,i,\\cdots ,m,\\ h_i(x) = 0,i=1,\\cdots ,p\\) Dual feasible: \\(\\lambda\\succeq0\\) Complementary slackness: \\(\\lambda_if_i(x)=0,i=1,\\cdots,m\\) Gradient of Lagrangian with respect to \\(x\\) vanishes:\\(\\nabla f_0(x)+\\sum_{i=1}^m\\lambda_i\\nabla f_i(x)+\\sum_{i=1}^p\\nu_i\\nabla h_i(x) = 0\\) From the discussion above, we know that if strong duality holds and \\(x,\\lambda,\\nu\\) are optimal, then they must satisfy the KKT conditions. Also if \\(x,\\lambda,\\nu\\) satisfy KKT for a convex problem, then they are optimal. However, the converse is not generally true, since KKT condition implies strong duality. If Slater’s condition is satisfied, then \\(x\\) is optimal if and only if there exist \\(\\lambda,\\nu\\) that satisfy KKT conditions. Sometimes, by solving the KKT system, we can derive the closed-form solution of a optimization directly. Also, sometimes we will use the residual of the KKT system as the termination condition. In general, \\(f_i,h_i\\) may not be differentiable. There are also KKT conditions for them, which will include knowledge of subdifferential and will not be included here. A.2 Practice A.2.1 CVX Introduction In the last section, we have learned basic concepts and theorems in convex optimization. In this section, on the other hand, we will introduce you how to model basic convex optimization problems with CVX, an easy-to-use MATLAB package. To install CVX, please refer to this page. Note that every time you what to use the CVX package, you should add it to your MATLAB path. For example, if I install CVX package in the parent directory of my current directory with default directory name cvx, the following line should be added before your CVX codes: addpath(genpath(&quot;../cvx/&quot;)); With CVX, it is incredibly easy for us to define and solve a convex optimization problem. You just need to: define the variables. define the objective function you want to minimize or maximize. define the constraints. After running your codes, the optimal objective value is stored in the variable cvx_optval, and the problem status is stored in the variable cvx_status (when your problem is well-defined, this variable’s value will be Solved). The optimal solutions will be stored in the variables you define. Throughout this section, we will study five types of convex optimization problems: linear programming (LP), quadratic programming (QP), (convex) quadratically constrained quadratic programming (QCQP), second-order cone programming (SOCP), and semidefinite programming (SDP). Given two types of optimization problems \\(A\\) and \\(B\\), we say \\(A &lt; B\\) if \\(A\\) can always be converted to \\(B\\) while the inverse is not true. Under this notation, we have \\[\\begin{equation*} \\text{LP} &lt; \\text{QP} &lt; \\text{QCQP} &lt; \\text{SOCP} &lt; \\text{SDP} \\end{equation*}\\] A.2.2 Linear Programming (LP) Definition. An LP has the following form: \\[\\begin{equation} \\tag{A.1} \\begin{aligned} \\min_{x \\in \\mathbb{R}^n} &amp; \\ c^T x \\\\ \\text{subject to } &amp; A x \\le b \\end{aligned} \\end{equation}\\] where \\(x\\) is the variable, \\(A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^m\\), and \\(c \\in \\mathbb{R}^n\\) are the parameters. Note that the constraint \\(A x \\le b\\) already incorporates linear equality constraints. To see this, consider the constraint \\(A&#39; x = b&#39;\\), we can reformulate it as \\(A x \\le b\\) by \\[\\begin{equation*} \\begin{bmatrix} A&#39; \\\\ -A&#39; \\end{bmatrix} x \\le \\begin{bmatrix} b&#39; \\\\ -b&#39; \\end{bmatrix} \\end{equation*}\\] Example. Consider the problem of minimizing a linear function \\(c_1 x_1 + c_2 x_2\\) over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). We can convert it to the standard LP form in (A.1) by simply setting \\(c\\) as \\([c_1, \\ c_2]^T\\) and the linear inequality constraint as \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the LP example setting c1 = 2; c2 = -5; l1 = 3; l2 = 7; % parameters: c, A, b c = [c1; c2]; A = [1, 0; -1, 0; 0, 1; 0, -1]; b = [l1; l1; l2; l2]; %% solve LP cvx_begin variable x(2); % define variables [x1, x2] minimize(c&#39; * x); % define the objective subject to A * x &lt;= b; % define the linear constraint cvx_end A.2.3 Quadratic Programming (QP) Definition. A QP has the following form: \\[\\begin{align} \\tag{A.2} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P x + q^T x \\\\ \\text{subject to } &amp; Gx \\le h \\\\ &amp; Ax = b \\end{align}\\] where \\(P \\in \\mathcal{S}_+^n, q\\in \\mathbb{R}^n, G \\in \\mathbb{R}^{m \\times n}, h\\in \\mathbb{R}^m, A \\in \\mathbb{R}^{p \\times n}, b \\in \\mathbb{R}^p\\). Here \\(\\mathcal{S}_+^n\\) denotes the set of positive semidefinite matrices of size \\(n\\times n\\). Obviously, if we set \\(P\\) as zero, QP will degenerate to LP. Example. Consider the problem of minimizing a quadratic function \\[\\begin{equation*} f(x_1, x_2) = p_1 x_1^2 + 2p_2 x_1 x_2 + p_3 x_2^2 + q_1 x_1 + q_2 x_2 \\end{equation*}\\] over a rectangle \\([-l_1, l_1] \\times [-l_2, l_2]\\). Since \\(P = 2 \\begin{bmatrix} p_1 &amp; p_2 \\\\ p_2 &amp; p_3 \\end{bmatrix} \\succeq 0\\), the following two conditions must hold: \\[\\begin{equation*} \\begin{cases} p_1 \\ge 0 \\\\ p_1 p_3 - 4 p_2^2 \\ge 0 \\end{cases} \\end{equation*}\\] Same as in the LP example, \\(G\\) and \\(h\\) can be expressed as: \\[\\begin{equation*} \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\le \\begin{bmatrix} l_1 \\\\ l_1 \\\\ l_2 \\\\ l_2 \\end{bmatrix} \\end{equation*}\\] Corresponding CVX codes are shown below: %% Define the QP example setting p1 = 2; p2 = 0.5; p3 = 4; q1 = -3; q2 = -6.5; l1 = 2; l2 = 2.5; % check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0); tmp2 = (p1*p3 - 4*p2^2 &gt;= 0); if ~(tmp1 &amp;&amp; tmp2) error(&quot;P is not positve semidefinite!&quot;); end % parameters: P, q, G, h P = 2 * [p1, p2; p2, p3]; q = [q1; q2]; G = [1, 0; -1, 0; 0, 1; 0, -1]; h = [l1; l1; l2; l2]; %% Solve the QP problem cvx_begin variable x(2); % define variables [x1; x2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P) + q&#39; * x; minimize(obj); subject to G * x &lt;= h; % define the linear constraint cvx_end A.2.4 Quadratically Constrained Quadratic Programming (QCQP) Definition. An (convex) QCQP has the following form: \\[\\begin{align} \\tag{A.3} \\min_{x \\in \\mathbb{R}^n} \\ &amp; \\frac{1}{2} x^T P_0 x + q_0^T x \\\\ \\text{subject to } &amp; \\frac{1}{2} x^T P_i x + q_i^T x + r_i \\le 0, \\ i = 1 \\dots m \\\\ &amp; Ax = b \\end{align}\\] where \\(P_i \\in \\mathcal{S}_+^n, i = 0 \\dots m\\), \\(q_i \\in \\mathbb{R}^n, i = 0 \\dots m\\), \\(A \\in \\mathbb{R}^{p \\times n}\\), and \\(b \\in \\mathbb{R}^p\\). Note that in other literature, you may find a more general form of QCQP: they don’t require \\(P_i\\)’s to be positive semidefinite. Yet in this case, the problem is non-convex and beyond our scope. Example. We study the problem of getting the minimum distance between two ellipses. By convention, when the ellipses overlap, we set the minimum distance as \\(0\\). This problem can be exactly solved by (convex) QCQP. Consider two ellipses of the following form: \\[\\begin{equation*} \\begin{cases} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 \\le 0 \\\\ \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 \\le 0 \\\\ \\end{cases} \\end{equation*}\\] where \\([y_1, z_1]^T\\) and \\([y_2, z_2]^T\\) are arbitrary points inside the two ellipses respectively. Also, two ensure the ellipses are well defined, we should enforce the following properties in \\((K_i, k_i, c_i), i = 1, 2\\): (1) \\(K_i \\succ 0\\); (2) Let \\(K_i = L_i L_i^T\\) be the Cholesky decomposition of \\(K_i\\). Then, ellipse \\(i\\) can be rewritten as: \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^T \\begin{bmatrix} y_i \\\\ z_i \\end{bmatrix} - L_i^{-1} k_i \\parallel^2 \\le \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i \\end{equation*}\\] Thus, \\[\\begin{equation*} \\frac{1}{2} \\parallel L_i^{-1} k_i \\parallel^2 - c_i &gt; 0 \\end{equation*}\\] With these two assumptions, we want to minimize: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 \\end{equation*}\\] Now, we construct \\(P, q, r\\)’s in QCQP with the above parameters. Define the variable \\(x\\) as \\([y_1, z_1, y_2, z_2]\\). \\(P_0\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} (y_1 - y_2)^2 + (z_1 - z_2)^2 = \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix}^T \\begin{bmatrix} 1 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ z_1 \\\\ y_2 \\\\ z_2 \\end{bmatrix} \\end{equation*}\\] \\(P_1, q_1, r_1\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix}^T K_1 \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + k_1^T \\begin{bmatrix} y_1 \\\\ z_1 \\end{bmatrix} + c_1 = \\frac{1}{2} x^T \\begin{bmatrix} K_1 &amp; O \\\\ O &amp; O \\end{bmatrix} + \\begin{bmatrix} k_1 \\\\ O \\end{bmatrix}^T x + c_1 \\le 0 \\end{equation*}\\] \\(P_2, q_2, r_2\\) can be obtained from: \\[\\begin{equation*} \\frac{1}{2} \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix}^T K_2 \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + k_2^T \\begin{bmatrix} y_2 \\\\ z_2 \\end{bmatrix} + c_2 = \\frac{1}{2} x^T \\begin{bmatrix} O &amp; O \\\\ O &amp; K_2 \\end{bmatrix} + \\begin{bmatrix} O \\\\ k_2 \\end{bmatrix}^T x + c_2 \\le 0 \\end{equation*}\\] The corresponding codes are shown below. In this example, we test the minimum distance between a circle \\(y_1^2 + z_1^2 \\le 1\\) and another circle \\((y_2 - 2)^2 + (z_2 - 2)^2 \\le 1\\). You can check whether the result from QCQP aligns with your manual calculation. %% Define the QCQP example setting K1 = eye(2); k1 = zeros(2, 1); c1 = -0.5; K2 = eye(2); k2 = [2; 2]; c2 = 3.5; if ~(if_ellipse(K1, k1, c1) &amp;&amp; if_ellipse(K2, k2, c2)) error(&quot;The example setting is not correct&quot;); end % define parameters P0, P1, P2, q1, q2, r1, r2 P0 = [1,0,-1,0; 0,1,0,-1; -1,0,1,0; 0,-1,0,1]; P1 = zeros(4, 4); P1(1:2, 1:2) = K1; P2 = zeros(4, 4); P2(3:4, 3:4) = K2; q1 = [k1; zeros(2, 1)]; q2 = [zeros(2, 1); k2]; r1 = c1; r2 = c2; %% Solve the QCQP problem cvx_begin variable x(4); % define variables [y1; z1; y2; z2] % define the objective, where quad_form(x, P) = x&#39;*P*x obj = 0.5 * quad_form(x, P0); minimize(obj); subject to 0.5 * quad_form(x, P1) + q1&#39; * x + r1 &lt;= 0; 0.5 * quad_form(x, P2) + q2&#39; * x + r2 &lt;= 0; cvx_end %% detect whether (K, k, c) generates a ellipse function flag = if_ellipse(K, k, c) L = chol(K); radius_square = 0.5 * norm(L \\ k)^2 - c; % L \\ k = inv(L) * k flag = (radius_square &gt; 0); end A.2.5 Second-Order Cone Programming (SOCP) Definition. An SOCP has the following form: \\[\\begin{align} \\tag{A.4} \\min_{x \\in \\mathbb{R}^n} \\ &amp; f^T x \\\\ \\text{subject to } &amp; || A_i x + b_i ||_2 \\le c_i^T x + d_i, \\ i = 1 \\dots m \\\\ &amp; Fx = g \\end{align}\\] where \\(f \\in \\mathbb{R}^n, A_i \\in \\mathbb{R}^{n_i \\times n}, b_i \\in \\mathbb{R}^{n_i}, c_i \\in \\mathbb{R}^n, d_i \\in \\mathbb{R}, F \\in \\mathbb{R}^{p \\times n}\\), and \\(g \\in \\mathbb{R}^p\\). Example. We consider the problem of stochastic linear programming: \\[\\begin{align} \\min_x \\ &amp; c^T x \\\\ \\text{subject to } &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p, \\ i = 1 \\dots m \\\\ &amp; a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i), \\ i = 1 \\dots m \\end{align}\\] Here \\(p\\) should be more than \\(0.5\\). We show that this problem can be converted to a SOCP: Since \\(a_i \\sim \\mathcal{N}(\\bar{a}_i, \\Sigma_i)\\), then \\((a_i^T x - b_i) \\sim \\mathcal{N}(\\bar{a}_i^T x - b_i, x^T \\Sigma_i x)\\). Standardize it: \\[\\begin{equation*} t := ||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1} \\left\\{ (a_i^T x - b_i) - (\\bar{a}_i^T x - b_i) \\right\\} \\sim \\mathcal{N}(0, 1) \\end{equation*}\\] Then, \\[\\begin{align} \\mathbb{P}(a_i^T x \\le b_i) &amp; = \\mathbb{P}(a_i^T x - b_i \\le 0) \\\\ &amp; = \\mathbb{P}(t \\le -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\\\ &amp; = \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\end{align}\\] Here \\(\\Phi(\\cdot)\\) is the cumulative distribution function of the standard normal distribution: \\[\\begin{equation*} \\Phi(\\xi) = \\int_{-\\infty}^{\\xi} e^{-\\frac{1}{2} t^2} \\ dt \\end{equation*}\\] Thus, \\[\\begin{align} &amp; \\mathbb{P}(a_i^T x \\le b_i) \\ge p \\\\ \\Longleftrightarrow &amp; \\Phi(-||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i)) \\ge p \\\\ \\Longleftrightarrow &amp; -||\\Sigma_i^{\\frac{1}{2}} x||_2^{-1}(\\bar{a}_i^T x - b_i) \\ge \\Phi^{-1}(p) \\\\ \\Longleftrightarrow &amp; \\Phi^{-1}(p) ||\\Sigma_i^{\\frac{1}{2}} x||_2 \\le b_i - \\bar{a}_i^T x \\end{align}\\] which is exactly the same as inequality constraints in SOCP formulation. (You can see why we enforce \\(p &gt; 0.5\\) here: otherwise \\(\\Phi^{-1}(p)\\) will be negative and the constraint with not be an second-order cone.) In the following code example, we set up four inequality constraints and let \\(\\bar{a}_i^T x \\le b_i, \\ i = 1 \\dots 4\\) form an square located at the origin of size \\(2\\). Then, for convenience, we set \\(\\Sigma_i \\equiv \\sigma^2 I\\). %% Define the SOCP example setting bar_a1 = [1; 0]; b1 = 1; bar_a2 = [0; 1]; b2 = 1; bar_a3 = [-1; 0]; b3 = 1; bar_a4 = [0; -1]; b4 = 1; sigma = 0.1; c = [2; 3]; p = 0.9; % p should be more than 0.5 Phi_inv = norminv(p); % get Phi^{-1}(p) %% Solve the SOCP problem cvx_begin variable x(2); % define variables [x1; x2] minimize(c&#39; * x); subject to sigma*Phi_inv * norm(x) &lt;= b1 - bar_a1&#39; * x; sigma*Phi_inv * norm(x) &lt;= b2 - bar_a2&#39; * x; sigma*Phi_inv * norm(x) &lt;= b3 - bar_a3&#39; * x; sigma*Phi_inv * norm(x) &lt;= b4 - bar_a4&#39; * x; cvx_end A.2.6 Semidefinite Programming (SDP) Definition. An SDP has the following form: \\[\\begin{align} \\tag{A.5} \\min_{X_i, x_i} \\ &amp; \\sum_{i=1}^{n_s} C_i \\cdot X_i + \\sum_{i=1}^{n_u} c_i \\cdot x_i \\\\ \\text{subject to } &amp; \\sum_{i=1}^{n_s} A_{i,j} \\cdot X_i + \\sum_{i=1}^{n_u} a_{i,j} \\cdot x_i = b_j, \\quad j = 1 \\dots m \\\\ &amp; X_i \\in \\mathcal{S}_+^{D_i}, \\quad i = 1 \\dots n_s \\\\ &amp; x_i \\in \\mathbb{R}^{d_i}, \\quad i = 1 \\dots n_u \\end{align}\\] where \\(C_i, A_{i, j} \\in \\mathbb{R}^{D_i \\times D_i}\\), \\(c_i, a_{i, j} \\in \\mathbb{R}^{d_i}\\), and \\(\\cdot\\) means element-wise product. For two square matrices \\(A, B\\), the dot product \\(A \\cdot B\\) is equal to \\(\\text{tr}(A B)\\); for two vectors \\(a, b\\), the dot product \\(a \\cdot b\\) is the same as inner product \\(a^T b\\). Note that actually there are many “standard” forms of SDP. For example, in the convex optimization theory part, you may find an SDP that looks like: \\[\\begin{align} \\min_X \\ &amp; C \\cdot X \\\\ \\text{subject to } &amp; A \\cdot X = b \\\\ &amp; X \\succeq 0 \\end{align}\\] It is convenient for us to analyze the theoretical properties of SDP with this form. Also, in SDP solvers’ User Guide, you may see more complex SDP forms which involve more general convex cones. For example, see MOSEK’s MATLAB API docs. Here we turn to use the form of (A.5) for two reasons: (1) it is general enough: our SDP example below can be converted to this form (also, SDPs from sum-of-squares programming in this book are exactly of the form (A.5)); (2) it is more readable than more complex forms. Example. We consider the problem of finding the minimum eigenvalue for a positive semidefinite matrix \\(S\\). We will show that this problem can be converted to (A.5). Since \\(S\\) is positive semidefinite, the finding procedure can be cast as \\[\\begin{align} \\max_\\lambda &amp; \\ \\lambda \\\\ \\text{subject to } &amp; S - \\lambda I \\succeq 0 \\end{align}\\] Now define an auxiliary matrix \\(X := S - \\lambda I\\). We have \\[\\begin{align} \\min_{\\lambda, X} &amp; \\ -\\lambda \\\\ \\text{subject to } &amp; X + \\lambda I = S \\\\ &amp; X \\succeq 0 \\end{align}\\] It is obvious that the linear matrix equality constraint \\(X + \\lambda I = S\\) can be divided into several linear scalar equality constraints in (A.5). For example, we consider \\(S \\in \\mathbb{S}_+^3\\). Thereby \\(X + \\lambda I = S\\) will lead to \\(6\\) linear equality constraints (We don’t consider \\(X\\) is a symmetric matrix here, since most solvers will implicitly consider this. Thus, only the upper-triangular part of \\(X\\) and \\(S\\) are actually used in the equality construction.): \\[\\begin{align} &amp; \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[0, 0], \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[0, 2] \\\\ &amp; \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X + \\lambda = S[1, 1], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\cdot X = S[1, 2], \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\cdot X + \\lambda = S[2, 2] \\end{align}\\] Seems tedious? Fortunately, CVX provides a high-level API to handle these linear equality constraints: you just need to write down X + lam * eye(3) == S; % linear equality constraints: X + lam *I = S CVX will autometically convert this high-level constraint to (A.5) and pass them to the underlying solver. To generate a ramdom \\(S \\in \\mathcal{S}_+^3\\), you just need to assign three nonnegative eigenvalues to the program. After that, an random \\(S\\) will be generated by \\(S = Q \\ \\text{diag}(\\lambda_1, \\lambda_2, \\lambda_3) \\ Q^T\\), where \\(Q\\) is random orthonormal matrix. %% Define the SDP example setting lam_list = [0.7; 2.4; 3.7]; S = generate_random_PD_matrix(lam_list); % get a PD matrix S %% Solve the SDP problem cvx_begin variable X(3, 3) symmetric; variable lam; maximize(lam); subject to % here &quot;==&quot; should be read as &quot;is in&quot; X == semidefinite(3); X + lam * eye(3) == S; cvx_end % this function help to generate PD matrix of size 3*3 % if you provide the eigenvalues [lam_1, lam_2, lam_3] function S = generate_random_PD_matrix(lam_list) if ~all(lam_list &gt;= 0) % all eigenvalues &gt;= 0 error(&quot;All eigenvalues must be nonnegative.&quot;); end D = diag(lam_list); % use QR factorization to generate a random orthonormal matrix Q [Q, ~] = qr(rand(3, 3)); S = Q * D * Q&#39;; end A.2.7 CVXPY Introduction and Examples Apart from CVX MATLAB, we also have a Python package called CVXPY, which functions almost the same as CVX MATLAB. To define and solve a convex optimization problem CVXPY, basically, there are three steps (apart from importing necessary packages): Step 1: Define parameters and variables in a certain type of convex problem. Here variables are what you are trying to optimize or “learn”. Parameters are the “coefficients” of variables in the objective and constraints. Step 2: Define the objective function and constraints. Step 3: Solve the problem and get the results. Here we provide the CVXPY codes for the above five convex optimization examples. A.2.7.1 LP import cvxpy as cp import numpy as np ## Define the LP example setting c1 = 2 c2 = -5 l1 = 3 l2 = 7 ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: c, A, b c = np.array([c1, c2]) A = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) b = np.array([l1, l1, l2, l2]) ## Step 2: define objective and constraints obj = cp.Minimize(c.T @ x) constraints = [A @ x &lt;= b] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.2 QP import cvxpy as cp import numpy as np ## Define the LP example setting p1 = 2 p2 = 0.5 p3 = 4 q1 = -3 q2 = -6.5 l1 = 2 l2 = 2.5 # check if the generated P is positive semidefinite tmp1 = (p1 &gt;= 0) tmp2 = (p1*p3 - 4*p2**2 &gt;= 0) assert(tmp1 and tmp2, &quot;P is not positve semidefinite!&quot;) ## Step 1: define variables and parameters x = cp.Variable(2) # variable: x = [x1, x2]^T # parameters: P, q, G, h P = 2*np.array([[p1, p2], [p2, p3]]) q = np.array([q1, q2]) G = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) h = np.array([l1, l1, l2, l2]) ## Step 2: define the objective and constraints fx = 0.5 * cp.quad_form(x, P) + q.T @ x obj = cp.Minimize(fx) constraints = [G @ x &lt;= h] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve the problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.3 QCQP import cvxpy as cp import numpy as np from numpy.linalg import cholesky, inv, norm ## Define the QCQP example setting def if_ellipse(K, k, c): # examine whether 0.5*x^T K x + k^T x + c &lt;= 0 is a ellipse # if K is not positive semidefinite, Cholesky will raise an error L = cholesky(K) radius_square = 0.5 * norm(inv(L) @ k)**2 - c return radius_square &gt; 0 K1 = np.eye(2) k1 = np.zeros(2) c1 = -0.5 K2 = np.array([[1, 0], [0, 1]]) k2 = np.array([2, 2]) c2 = 3.5 if not (if_ellipse(K1, k1, c1) and if_ellipse(K2, k2, c2)): raise ValueError(&quot;The example setting is not correct&quot;) ## Step 1: define variables and parameters P0 = np.array([[1,0,-1,0], [0,1,0,-1], [-1,0,1,0], [0,-1,0,1]]) P1 = np.zeros((4,4)) P1[:2, :2] = K1 P2 = np.zeros((4,4)) P2[2:, 2:] = K2 q1 = np.concatenate([k1, np.zeros(2)]) q2 = np.concatenate([np.zeros(2), k2]) r1 = c1 r2 = c2 ## Step 2: define objective and constraints x = cp.Variable(4) # variable: x = [y1, z1, y2, z2]^T fx = 0.5 * cp.quad_form(x, P0) obj = cp.Minimize(fx) con1 = (0.5 * cp.quad_form(x, P1) + q1.T @ x + r1 &lt;= 0) # ellipse 1 con2 = (0.5 * cp.quad_form(x, P2) + q2.T @ x + r2 &lt;= 0) # ellipse 2 constraints = [con1, con2] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.4 SOCP import cvxpy as cp import numpy as np from scipy.stats import norm ## Define the SOCP example setting # define bar_ai, bi (i = 1, 2, 3, 4) bar_a1 = np.array([1, 0]) b1 = 1 bar_a2 = np.array([0, 1]) b2 = 1 bar_a3 = np.array([-1, 0]) b3 = 1 bar_a4 = np.array([0, -1]) b4 = 1 sigma = 0.1 c = np.array([2, 3]) p = 0.9 # p should be more than 0.5 ## Step 1: define variables and parameters Phi_inv = norm.ppf(p) # get Phi^{-1}(p) ## Step 2: define objective and constraints x = cp.Variable(2) # variable: x = [x1, x2]^T obj = cp.Minimize(c.T @ x) # use cp.SOC(t, x) to create the SOC constraint ||x||_2 &lt;= t constraints = [ cp.SOC(b1 - bar_a1.T @ x, sigma*Phi_inv*x), cp.SOC(b2 - bar_a2.T @ x, sigma*Phi_inv*x), cp.SOC(b3 - bar_a3.T @ x, sigma*Phi_inv*x), cp.SOC(b4 - bar_a4.T @ x, sigma*Phi_inv*x), ] prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, x.value) # optimal x A.2.7.5 SDP import cvxpy as cp import numpy as np from scipy.stats import ortho_group ## Define the SDP example setting # this function help to generate PD matrix of size 3*3 # if you provide the eigenvalues [lam_1, lam_2, lam_3] def generate_random_PD_matrix(lam_list): assert np.all(lam_list &gt;= 0) # all eigenvalues &gt;= 0 # S = Q @ D @ Q.T D = np.diag(lam_list) Q = ortho_group.rvs(3) return Q @ D @ Q.T lam_list = np.array([0.5, 2.4, 3.7]) S = generate_random_PD_matrix(lam_list) # get a PD matrix S ## Step 1: define variables and parameters # get coefficients for equality constraints A_00 = np.array([[1, 0, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_00 @ X) + lam = S_00 A_01 = np.array([[0, 1, 0], [0, 0, 0], [0, 0, 0]]) # tr(A_01 @ X) = S_01 A_02 = np.array([[0, 0, 1], [0, 0, 0], [0, 0, 0]]) # tr(A_02 @ X) = S_02 A_11 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]) # tr(A_11 @ X) + lam = S_11 A_12 = np.array([[0, 0, 0], [0, 0, 1], [0, 0, 0]]) # tr(A_12 @ X) = S_12 A_22 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1]]) # tr(A_22 @ X) + lam = S_22 ## Step 2: define objective and constraints # define a PD matrix variable X of size 3*3 X = cp.Variable((3, 3), symmetric=True) constraints = [X &gt;&gt; 0] # the operator &gt;&gt; denotes matrix inequality lam = cp.Variable(1) constraints += [ cp.trace(A_00 @ X) + lam == S[0,0], cp.trace(A_01 @ X) == S[0,1], cp.trace(A_02 @ X) == S[0,2], cp.trace(A_11 @ X) + lam == S[1,1], cp.trace(A_12 @ X) == S[1,2], cp.trace(A_22 @ X) + lam == S[2,2], ] obj = cp.Minimize(-lam) prob = cp.Problem(obj, constraints) # form the problem ## Step 3: solve problem and get results prob.solve() print(&quot;status: &quot;, prob.status) # check whether the status is &quot;optimal&quot; print(&quot;optimal value: &quot;, prob.value) # optimal objective print(&quot;optimal solution: &quot;, lam.value) # optimal lam "],["app-lti-system-theory.html", "B Linear System Theory B.1 Stability B.2 Controllability and Observability B.3 Stabilizability And Detectability", " B Linear System Theory Thanks to Shucheng Kang for writing this Appendix. B.1 Stability B.1.1 Continuous-Time Stability Consider the continuous-time linear time-invariant (LTI) system \\[\\begin{equation} \\dot{x} = A x. \\tag{B.1} \\end{equation}\\] the system is said to be “diagonalizable” if \\(A\\) is diagonalizable. Definition B.1 (Asymptotic and Marginal Stability) The diagonalizable, LTI system (B.1) is “asymptotically stable” if \\(x(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “marginally stable” if \\(x(t) \\nrightarrow 0\\) but remains bounded as \\(t \\rightarrow \\infty\\) for every initial condition \\(x_0\\) “stable” if it is either asymptotically or marginally stable “unstable” if it is not stable One can show that \\(A\\)’s eigenvalues determine the LTI system’s stability, as the following Theorem states: Theorem B.1 (Stability of Continuous-Time LTI System) The diagonalizable1, LTI system (B.1) is asymptotically stable if \\(\\text{Re} (\\lambda_i) &lt; 0\\) for all \\(i\\) marginally stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(\\text{Re} (\\lambda_i) = 0\\) stable if \\(\\text{Re} (\\lambda_i) \\le 0\\) for all \\(i\\) unstable if \\(\\text{Re} (\\lambda_i) &gt; 0\\) for at least one \\(i\\) Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(\\dot{x} = Ax\\) can be restated as \\(\\dot{z} = \\Lambda z\\). Consider the \\(i\\)’s component of \\(z\\): \\[\\begin{equation*} \\dot{z}_i = \\lambda_i z_i \\Longrightarrow z_i(t) = e^{\\lambda_i t} z_i(0) \\end{equation*}\\] Since \\(\\text{Re}(\\lambda_i) &lt; 0\\), \\(z_i(t)\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_i(0)\\). B.1.2 Discrete-Time Stability Now consider the diagonalizable, discrete-time linear time-invariant (LTI) system \\[\\begin{equation} x_{t+1} = A x_t. \\tag{B.2} \\end{equation}\\] Theorem B.2 (Stability of Discrete-Time LTI System) The diagonalizable, discrete-time LTI system (B.2) is asymptotically stable if \\(|\\lambda_i| &lt; 1\\) for all \\(i\\) marginally stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) and there exists at least one \\(i\\) for which \\(|\\lambda_i| = 1\\) stable if \\(|\\lambda_i| \\leq 1\\) for all \\(i\\) unstable if \\(|\\lambda_i| &gt; 1\\) for at least one \\(i\\). Note that \\(|\\lambda_i| &lt; 1\\) means the eigenvalue lies strictly inside the unit circle in the complex plane. Proof. Here we only represent the proof of (1). Similar procedure can be adopted for the proof of (2) - (4). Since \\(A\\) is diagonalizable, there exists an similarity transformation matrix \\(T\\), s.t. \\(A = T \\Lambda T^{-1}\\), where \\(\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\). Then, under the coordinate transformation \\(z = T^{-1} x\\), \\(x_{t+1} = Ax\\) can be restated as \\(z_{t+1} = \\Lambda z_t\\). Expanding the recursion, we have \\[\\begin{equation*} z_{t} = \\Lambda^{t-1} z_0 \\Longrightarrow z_{t,i} = \\lambda_i^{t-1} z_{0,i} \\end{equation*}\\] Since \\(|\\lambda_i| &lt; 1\\), \\(z_{t,i}\\) will go to \\(0\\) as \\(t \\rightarrow 0\\) regardless how we choose \\(z_{0,i}\\). B.1.3 Lyapunov Analysis Theorem B.3 (Lyapunov Equation) The following is equivalent for a linear time-invariant system \\(\\dot{x} = A x\\) The system is globally asymptotically stable, i.e., \\(A\\) is Hurwitz and \\(\\lim_{t \\rightarrow \\infty} x(t) = 0\\) regardless of the initial condition; For any positive definite matrix \\(Q\\), the unique solution \\(P\\) to the Lyapunov equation \\[\\begin{equation} A^T P + P A = -Q \\tag{B.3} \\end{equation}\\] is positive definite. Proof. (a): \\(2 \\Rightarrow 1\\). Suppose we are given two positive definite matrices \\(P, Q \\succ 0\\) that satisfies the Lyapunov equation (B.3). Define a scalar function \\[ V(x) = x^T P x. \\] It is clear that \\(V &gt; 0\\) for any \\(x \\neq 0\\) and \\(V(x) = 0\\) (i.e., \\(V(x)\\) is positive definite). We also see \\(V(x)\\) is radially unbounded because: \\[ V(x) \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2 \\Rightarrow \\lim_{x \\rightarrow \\infty} V(x) \\rightarrow \\infty. \\] The time derivative of \\(V\\) reads \\[ \\dot{V} = 2 x^T P \\dot{x} = x^T (A^T P + P A) x = - x^T Q x. \\] Clearly, \\(\\dot{V} &lt; 0\\) for any \\(x \\neq 0\\) and \\(\\dot{V}(0) = 0\\). According to Lyapunov’s global stability theorem ??, we conclude the linear system \\(\\dot{x} = Ax\\) is globally asymptotically stable at \\(x = 0\\). (b): \\(1 \\Rightarrow 2\\). Suppose \\(A\\) is Hurwitz, we want to show that, for any \\(Q \\succ 0\\), there exists a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (B.3). In fact, consider the matrix \\[ P = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt. \\] Because \\(A\\) is Hurwitz, the integral exists, and clearly \\(P \\succ 0\\) due to \\(Q \\succ 0\\). To show this choice of \\(P\\) satisfies the Lyapunov equation, we write \\[\\begin{align} A^T P + P A &amp;= \\int_{t=0}^{\\infty} \\left( A^T e^{A^T t} Q e^{At} + e^{A^T t} Q e^{At} A \\right) dt \\\\ &amp;=\\int_{t=0}^{\\infty} d \\left( e^{A^T t} Q e^{At} \\right) \\\\ &amp; = e^{A^T t} Q e^{At}\\vert_{t = \\infty} - e^{A^T t} Q e^{At}\\vert_{t = 0} = - Q, \\end{align}\\] where the last equality holds because \\(e^{A \\infty} = 0\\) (recall \\(A\\) is Hurwitz). To show the uniqueness of \\(P\\), we assume that there exists another matrix \\(P&#39;\\) that also satisfies the Lyapunov equation. Therefore, \\[\\begin{align} P&#39; &amp;= e^{A^T t} P&#39; e^{At} \\vert_{t=0} - e^{A^T t} P&#39; e^{At} \\vert_{t=\\infty} \\\\ &amp;= - \\int_{t=0}^{\\infty} d \\left( e^{A^T t} P&#39; e^{At} \\right) \\\\ &amp;= - \\int_{t=0}^{\\infty} e^{A^T t} \\left( A^T P&#39; + P&#39; A \\right) e^{At} dt \\\\ &amp; = \\int_{t=0}^{\\infty} e^{A^T t} Q e^{At} dt = P, \\end{align}\\] leading to \\(P&#39; = P\\). Hence, the solution is unique. Convergence rate estimation. We now show that Theorem B.3 can allow us to quantify the convergence rate of a (stable) linear system towards zero. For a Hurwitz linear system \\(\\dot{x} = Ax\\), let us pick a positive definite matrix \\(Q\\). Theorem B.3 tells us we can find a unique \\(P \\succ 0\\) satisfying the Lyapunov equation (B.3). In this case, we can upper bound the scalar function \\(V = x^T P x\\) as \\[ V \\leq \\lambda_{\\max}(P) \\Vert x \\Vert^2. \\] The time derivative of \\(V\\) is \\(\\dot{V} = - x^T Q x\\), which can be upper bounded by \\[\\begin{align} \\dot{V} &amp; \\leq - \\lambda_{\\min} (Q) \\Vert x \\Vert^2 \\\\ &amp; = - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} \\underbrace{ \\left( \\lambda_{\\max} (P) \\Vert x \\Vert^2 \\right)}_{\\geq V} \\\\ &amp; \\leq - \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max} (P)} V. \\end{align}\\] Denoting \\(\\gamma(Q) = \\frac{\\lambda_{\\min} (Q)}{\\lambda_{\\max}(P)}\\), the above inequality implies \\[ V(0) e^{-\\gamma(Q) t} \\geq V(t) = x^T P x \\geq \\lambda_{\\min}(P) \\Vert x \\Vert^2. \\] As a result, \\(\\Vert x \\Vert^2\\) converges to zero exponentially with a rate at least \\(\\gamma(Q)\\), and \\(\\Vert x \\Vert\\) converges to zero exponentially with a rate at least \\(\\gamma(Q) / 2\\). Best convergence rate estimation. I have used \\(\\gamma (Q)\\) to make it explict that the rate \\(\\gamma\\) depends on the choice of \\(Q\\), because \\(P\\) is computed from the Lyapunov equation as an implicit function of \\(Q\\). Naturally, choosing different \\(Q\\) will lead to different \\(\\gamma (Q)\\). So what is the choice of \\(Q\\) that maximizes the convergence rate estimation? Corollary B.1 (Maximum Convergence Rate Estimation) \\(Q = I\\) maximizes the convergence rate estimation. Proof. let us denote \\(P_0\\) as the solution to the Lyapunov equation with \\(Q = I\\) \\[ A^T P_0 + P_0 A = - I. \\] Let \\(P\\) be the solution corresponding to a different choice of \\(Q\\) \\[ A^T P + P A = - Q. \\] Without loss of generality, we can assume \\(\\lambda_{\\min}(Q) = 1\\), because rescaling \\(Q\\) will recale \\(P\\) by the same factor, which does not affect \\(\\gamma(Q)\\). Subtracting the two Lyapunov equations above we get \\[ A^T (P - P_0) + (P - P_0) A = - (Q - I). \\] Since \\(Q - I \\succeq 0\\) (due to \\(\\lambda_{\\min}(Q) = 1\\)), we know \\(P - P_0 \\succeq 0\\) and \\(\\lambda_{\\max} (P) \\geq \\lambda_{\\max} (P_0)\\). As a result, \\[ \\gamma(Q) = \\frac{\\lambda_{\\min}(Q)}{\\lambda_{\\max}(P)} = \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P)} \\leq \\frac{\\lambda_{\\min}(I)}{\\lambda_{\\max}(P_0)} = \\gamma(I), \\] and \\(Q = I\\) maximizes the convergence rate estimation. B.2 Controllability and Observability Consider the following linear time-invariant (LTI) system \\[\\begin{equation} \\tag{B.4} \\begin{split} \\dot{x} = A x + B u \\\\ y = C x + D u \\end{split} \\end{equation}\\] where \\(x \\in \\mathbb{R}^n\\) the state, \\(u \\in \\mathbb{R}^m\\) the control input, \\(y \\in \\mathbb{R}^p\\) the output, and \\(A,B,C,D\\) are constant matrices with proper sizes. If we know the initial state \\(x(0)\\) and the control inputs \\(u(t)\\) over a period of time \\(t \\in [0, t_1]\\), the system trajectory \\((x(t), y(t))\\) can be determined as \\[\\begin{equation} \\tag{B.5} \\begin{split} x(t) &amp; = e^{At} x(0) + \\int_{0}^{t} e^{A(t-\\tau)} B u(\\tau) d\\tau \\\\ y(t) &amp; = C x(t) + D u(t) \\end{split} \\end{equation}\\] To study the internal structure of linear systems, two important properties should be considered: controllability and observability. In the following analysis, we will see that they are actually dual concepts. Their definitions (Chen 1984) are given below. Definition B.2 (Controllability) The LTI system (B.4), or the pair \\((A, B)\\), is controllable, if for any initial state \\(x(0) = x_0\\) and final state \\(x_f\\), there exists a sequence of control inputs that transfer the system from \\(x_0\\) to \\(x_f\\) in finite time. Definition B.3 (Observability) The LTI system (B.4), or the pair \\((C, A)\\), is observable, if for any unknown initial state \\(x(0)\\), there exists a finite time \\(t_1 &gt; 0\\), such that knowing \\(y\\) and \\(u\\) over \\([0, t_1]\\) suffices to determine \\(x(0)\\). Sometimes it will become more convenient for us to analyze the system (B.4) under another coordinate basis, i.e., \\(z = T x\\), where the coordinate transformation \\(T\\) is nonsingular (i.e., full-rank). Define \\(A&#39; = TAT^{-1}, B&#39; = PB, C&#39; = CT^{-1}, D&#39; = D\\), we get \\[\\begin{equation*} \\begin{split} \\dot{z} = A&#39; z + B&#39; u \\\\ y = C&#39; z + D&#39; u \\end{split} \\end{equation*}\\] Since the coordinate transformation only changes the system’s coordinate basis, physical properties like controllability and observability will not change. B.2.1 Cayley-Hamilton Theorem In the analysis of controllability and observability, Cayley Hamilton Theorem lays the foundation. The statement of the theory and its (elegant) proof are given blow. Some useful corollaries are also presented. Theorem B.4 (Cayley-Hamilton) Let \\(A \\in \\mathbb{C}^{n \\times n}\\) and denote the characteristic polynomial of \\(A\\) as \\[ \\text{det}(\\lambda I - A) = \\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n \\in \\mathbb{C}[\\lambda], \\] which is a polynomial in a single variable \\(\\lambda\\) with coefficients \\(a_1,\\dots,a_n\\). Then \\[ A^n + a_1 A^{n-1} + \\dots + a_n I = 0 \\] Proof. Define the adjugate of \\(\\lambda I - A\\) as \\[ B = \\text{adj}(\\lambda I - A) \\] From \\(B\\)’s definition, we have \\[\\begin{equation} (\\lambda I - A) B = \\text{det}(\\lambda I - A) I = (\\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_n) I \\tag{B.6} \\end{equation}\\] Also, \\(B\\) is a polynomial matrix over \\(\\lambda\\), whose maximum degree is no more than \\(n - 1\\). Therefore, we write \\(B\\) as follows: \\[ B = \\sum_{i=0}^{n-1} \\lambda^i B_i \\] where \\(B_i\\)’s are constant matrices. In this way, we unfold \\((\\lambda I - A)B\\): \\[\\begin{equation} \\tag{B.7} \\begin{split} (\\lambda I - A) B &amp; = (\\lambda I - A) \\sum_{i=0}^{n-1} \\lambda^i B_i \\\\ &amp; = \\lambda^n B_{n-1} + \\sum_{i=1}^{n-1} \\lambda^i (-A B_i + B_{i-1}) - A B_0 \\end{split} \\end{equation}\\] Since \\(\\lambda\\) can be arbitrarily set, matching the coefficients of (B.6) and (B.7), we have \\[\\begin{equation*} \\begin{split} B_{n-1} &amp; = I \\\\ -A B_i + B_{i-1} &amp; = a_{n-i} I, \\quad i = 1 \\dots n - 1 \\\\ -A B_0 &amp; = a_n I \\end{split} \\end{equation*}\\] Thus, we have \\[\\begin{equation*} \\begin{split} &amp; B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I \\\\ = &amp; I \\cdot A^n + \\sum_{i=1}^{n-1} (a_{n-i} I) \\cdot A^i + (a_n I) \\cdot I \\\\ = &amp; A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I \\end{split} \\end{equation*}\\] On the other hand, one can easily check that \\[ B_{n-1} \\cdot A^n + \\sum_{i=1}^{n-1} (-A B_i + B_{i-1}) \\cdot A^i + (-A B_0) \\cdot I = 0 \\] since each term offsets completely. Therefore, \\[ A^n + a_1 A^{n-1} + a_2 A^{n-2} + \\dots + a_n I = 0, \\] concluding the proof. Here are some corollaries of the Cayley-Hamilton Theorem. Corollary B.2 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k \\ge n\\), \\(A^k B\\) is a linear combination of \\(B, AB, A^2B, \\dots, A^{n-1}B\\). Proof. Directly from Cayley Hamilton Theorem, \\(A^n\\) can be expressed as a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). By recursion, it is easy to show that for all \\(m &gt; n\\), \\(A^m\\) is also a linear combination of \\(I, A, A^2, \\dots, A^{n-1}\\). Post-multiply both sides with \\(B\\), we get what we want. Corollary B.3 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}, k &gt; n\\), the following equality always holds: \\[ \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix}) = \\text{rank}(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix}) \\] Proof. First prove LHS \\(\\le\\) RHS. \\(\\forall v \\in \\mathbb{C}^n\\) such that \\[ v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B &amp; \\dots A^{k-1}B \\end{bmatrix} = 0 \\] \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) must hold. Second prove LHS \\(\\ge\\) RHS. For any \\(v \\in \\mathbb{C}^n\\) such that \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\) and any \\(k &gt; n\\), by Corollary B.2, there exists a sequence \\(c_i, i = 0 \\dots n-1\\) satisfy the following: \\[ v^* A^k B = v^* \\sum_{i=0}^{n-1} c_i A^i B = 0 \\] Therefore, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{k-1} B \\end{bmatrix} = 0\\). Corollary B.4 For any \\(A \\in \\mathbb{C}^{n \\times n}, B \\in \\mathbb{C}^{n \\times m}\\), define \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] If \\(\\text{rank}(\\mathcal{C}) = k_1 &lt; n\\), there exist a similarity transformation \\(T\\) such that \\[ T A T^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, T B = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}, \\bar{B}_c \\in \\mathbb{C}^{k_1 \\times m}\\). Moreover, the matrix \\[\\begin{equation*} \\bar{\\mathcal{C}} := \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c \\end{bmatrix} \\end{equation*}\\] has full row rank. Proof. Since \\(\\mathcal{C}\\) is not full row rank, we pick \\(k_1\\) linearly independent columns from \\(\\mathcal{C}\\). Denote them as \\(q_1\\dots q_{k_1}\\), \\(q_i \\in \\mathbb{C}^n\\). Then, we arbitrarily set other \\(n-k_1\\) vectors \\(q_{k_1+1} \\dots q_{n}\\) as long as \\[ Q = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\] is invertible. Define the similarity transformation matrix by \\(T = Q^{-1}\\). Note that \\(A q_i\\) can be seen as a column picked from \\(A^{k} B, k \\in \\left\\{1 \\dots n\\right\\}\\), which is guaranteed to be a linear combination of \\(B, AB, \\dots, A^{n-1}B\\) from Cayley Hamilton Theorem. Thus, \\(A q_i\\) is bound to be a linear transformation of columns from \\(\\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = \\mathcal{C}\\). Since \\(q_1\\dots q_{k_1}\\) is the largest linearly independent column vector set from \\(\\mathcal{C}\\), this implies \\(A q_i\\) can be expressed as a linear combination of \\(q_1\\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} A Q &amp; = A T^{-1} = A \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} = T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} \\end{split} \\end{equation*}\\] Similarly, \\(B\\) itself is part of \\(\\mathcal{C}\\). Therefore, each column of \\(B\\) is naturally a linear combination of \\(q_1 \\dots q_{k_1}\\): \\[\\begin{equation*} \\begin{split} B = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_{k_1} &amp; q_{k_1+1} &amp; \\dots &amp; q_{n} \\end{bmatrix} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{split} = T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] To see \\(\\bar{\\mathcal{C}}\\) has full row rank, note that \\(\\text{rank} \\mathcal{C} = k_1\\) and \\[\\begin{equation*} \\mathcal{C} = T^{-1} \\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\dots &amp; 0 \\end{bmatrix} \\end{equation*}\\] Thus, \\[\\text{rank}\\begin{bmatrix} \\bar{B}_c &amp; \\bar{A}_c \\bar{B}_c &amp; \\bar{A}_c^2 \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{k_1 - 1} \\bar{B}_c &amp; \\dots &amp; \\bar{A}_c^{n - 1} \\bar{B}_c \\end{bmatrix} = k_1. \\] By Corollary B.3, \\(\\text{rank}\\bar{\\mathcal{C}} = k_1\\). The following Corollary is especially useful in the study of pole assignment in the single-input-multiple-output (SIMO) LTI system. Corollary B.5 For any \\(A \\in \\mathbb{C}^{n \\times n}, b \\in \\mathbb{C}^{n}\\), if \\[\\begin{equation*} \\mathcal{C} = \\begin{bmatrix} b &amp; Ab &amp; \\dots &amp; A^{n-1}b \\end{bmatrix} \\in \\mathbb{C}^{n \\times n} \\end{equation*}\\] has full rank, then there exists a similarity transformation \\(T\\) such that \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(a_1, \\dots, a_n\\) are the coefficients of \\(A\\)’s characteristic polynomial: \\[\\begin{equation*} \\det(A - \\lambda I) = \\lambda^{n} + a_1 \\lambda^{n-1} + \\dots + a_n \\lambda \\end{equation*}\\] Proof. Since \\(\\mathcal{C}\\) is invertible, define its inverse \\[\\begin{equation*} \\mathcal{C}^{-1} = \\begin{bmatrix} M_1 \\\\ M_2 \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] where \\(M_i \\in \\mathbb{C}^{1 \\times n}\\). Then, \\[\\begin{equation*} I = \\mathcal{C}^{-1} \\mathcal{C} = \\begin{bmatrix} M_1 b &amp; M_1 Ab &amp; \\dots &amp; M_1 A^{n-1}b \\\\ M_2 b &amp; M_2 Ab &amp; \\dots &amp; M_2 A^{n-1}b \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} \\Longrightarrow \\begin{cases} M_n A^{n-1}b = 1 \\\\ M_n A^ib = 0, \\ i = 0,\\dots, n-2 \\end{cases} \\end{equation*}\\] Now we claim that the transformation matrix \\(T\\) can be constructed as follows: \\[\\begin{equation*} T = \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\dots \\\\ M_n \\end{bmatrix} \\end{equation*}\\] We first show \\(T\\) is invertible by calculating \\(T \\mathcal{C}\\): \\[\\begin{equation*} T \\mathcal{C} = \\begin{bmatrix} M_n A^{n-1}b &amp; \\star &amp; \\dots &amp; \\star \\\\ M_n A^{n-2}b &amp; M_n A^{n-1}b &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ M_n b &amp; M_n Ab &amp; \\dots &amp; M_n A^{n-1}b \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\star &amp; \\dots &amp; \\star \\\\ 0 &amp; 1 &amp; \\dots &amp; \\star \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} \\end{equation*}\\] Then we calculate \\(Tb\\) and \\(TA\\): \\[\\begin{equation*} \\begin{split} Tb &amp; = \\begin{bmatrix} M_n A^{n-1}b \\\\ M_n A^{n-2}b \\\\ \\vdots \\\\ M_n b \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\\\ T A &amp; = \\begin{bmatrix} M_n A^n \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} = \\begin{bmatrix} -M_n \\cdot \\sum_{i=0}^{n-1} a_{n-i} A^i \\\\ M_n A^{n-1} \\\\ \\vdots \\\\ M_n A \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} M_n A^{n-1} \\\\ M_n A^{n-2} \\\\ \\vdots \\\\ M_n A \\\\ M_n \\end{bmatrix} = A_1 T \\end{split} \\end{equation*}\\] where the penultimate equality uses Cayley Hamilton Theorem. B.2.2 Equivalent Statements for Controllability There are a few equivalent statements to express an LTI system’s controllability that one should be familiar with: Theorem B.5 (Equivalent Statements for Controllability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((A, B)\\) is controllable. The matrix \\[ W_c(t) := \\int_{0}^{t} e^{A\\tau} B B^* e^{A^* \\tau} d\\tau \\] is positive definite for any \\(t &gt; 0\\). The controllability matrix \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; A^2 B &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} \\] has full row rank. The matrix \\([A - \\lambda I, B]\\) has full row rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(x\\) be any eigenvalue and any corresponding left eigenvector \\(A\\), i.e., \\(x^* A = x^* \\lambda\\), then \\(x^* B \\ne 0\\). The eigenvalues of \\(A+BF\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(F\\). If, in addition, all eigenvalues of \\(A\\) have negative real parts, then the unique solution of \\[ A W_c + W_c A^* = -B B^* \\] is positive definite. The solution is called the controllability Gramian and can be expressed as \\[ W_c = \\int_{0}^{\\infty} e^{A \\tau} B B^* e^{A^* \\tau} d\\tau \\] Proof. (\\(1. \\Rightarrow 2.\\)) Prove by contradiction. Assume that \\((A, B)\\) is controllable but \\(W_c(t_1)\\) is singular for some \\(t_1 &gt; 0\\). This implies there exists a real vector \\(v \\ne 0 \\in \\mathbb{R}^n\\), s.t. \\[ v^* W_c(t_1) v = v^* (\\int_{0}^{t_1} e^{At} B B^* e^{A^*t} dt) v = \\int_{0}^{t_1} v^* (e^{At} B B^* e^{A^*t}) v \\ dt = 0 \\] Since \\(e^{At} BB^* e^{A^*t} \\succeq 0\\) for all \\(t\\), we must have \\[\\begin{equation*} \\begin{split} &amp; v^* (e^{At} B B^* e^{A^*t}) v = \\parallel v^* B e^{At} \\parallel^2 = 0, \\quad \\forall t \\in [0, t_1] \\\\ \\Longrightarrow &amp; v^* B e^{At} = 0, \\quad \\forall t \\in [0, t_1] \\end{split} \\end{equation*}\\] Setting \\(x(t_1) = 0\\), from (B.5), we have \\[ 0 = e^{A t_1} x(0) + \\int_{0}^{t_1} e^{A (t_1 - \\tau)} B u(\\tau) d\\tau = 0 \\] Pre-multiply the above equation by \\(v^*\\), then \\[ 0 = v^* e^{A t_1} x(0) \\] Since \\(x(0)\\) can be chosen arbitrarily, we set \\(x(0) = v e^{-A t_1}\\), which results in \\(v = 0\\). Contradiction! (\\(2. \\Rightarrow 1.\\)) For any \\(x(0) = x_0, t_1 &gt; 0, x(t_1) = x_1\\), since \\(W_c(t_1) \\succ 0\\), we set the control inputs as \\[ u(t) = -B^* e^{A^*(t_1 - t)} W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\] We claim that the picked \\(u(t)\\) satisfies (B.5) by \\[\\begin{equation*} \\begin{split} &amp; e^{At} x_0 + \\int_{0}^{t_1} e^{A(t_1-t)} B u(t) dt \\\\ &amp; = e^{At} x_0 - \\int_{0}^{t_1} e^{A(t_1-t)} B B^* e^{A^*(t_1-t)} dt \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; \\overset{\\tau = t_1-t}{=} e^{At} x_0 - \\underbrace{\\int_{0}^{t_1} e^{A\\tau} BB^* e^{A^*\\tau} d\\tau}_{W_c(t_1)} \\cdot W_c^{-1}(t_1) [e^{At_1} x_0 - x_1] \\\\ &amp; = e^{At} x_0 - [e^{At_1} x_0 - x_1] = x_1 \\end{split} \\end{equation*}\\] (\\(2. \\Rightarrow 3.\\)) Prove by contradiction. Suppose \\(W_c(t) \\succ 0, \\forall t &gt; 0\\) but \\(\\mathcal{C}\\) is not of full row rank. Then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[ v^* A^k B = 0, \\quad k = 0 \\dots n - 1 \\] By Corollary B.2, we have \\[ v^* A^k B = 0, \\ \\forall k \\in \\mathbb{N} \\Longrightarrow v^* e^{At} B = 0, \\ \\forall t &gt; 0 \\] which implies \\[ v^* W_c(t) v = v^* (\\int_{0}^{t} e^{A\\tau} B B^* e^{A^*\\tau} d\\tau) v = 0, \\quad \\forall t &gt; 0 \\] Contradiction! (\\(3. \\Rightarrow 2.\\)) Prove by contradiction. Suppose \\(\\mathcal{C}\\) has full row rank but \\(W_c(t_1)\\) is singular at some \\(t_1 &gt; 0\\). Then, similar to the proof in (\\(1. \\Rightarrow 2.\\)), there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(F(t) := v^* e^{At} B \\equiv 0, \\forall t \\in [0, t_1]\\). Since \\(F(t)\\) is infinitely differentiable, we get its \\(i\\)’s derivative at \\(t=0\\), where \\(i = 0, 1, \\dots n-1\\). This results in \\[\\begin{equation*} \\left. \\frac{d^i F}{dt^i} \\right|_{t=0} = \\left. v^* A^{i} e^{At} B \\right|_{t=0} = v^* A^i B = 0, \\quad i = 0 \\dots n-1 \\end{equation*}\\] Thus, \\(v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1} B \\end{bmatrix} = 0\\). Contradiction! (\\(3. \\Rightarrow 4.\\)) Proof by contradiction. Suppose \\([A - \\lambda I, B]\\) does not have full row rank for some \\(\\lambda \\in \\mathbb{C}\\). Then, there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\(v^* [A - \\lambda I, B] = 0\\). This implies \\(v^* A = v^* \\lambda\\) and \\(v^* B = 0\\). On the other hand, \\[\\begin{equation*} v^* \\begin{bmatrix} B &amp; AB &amp; \\dots &amp; A^{n-1}B \\end{bmatrix} = v^* \\begin{bmatrix} B &amp; \\lambda B &amp; \\dots &amp; \\lambda^{n-1} B \\end{bmatrix} = 0 \\end{equation*}\\] Contradiction! (\\(4. \\Rightarrow 5.\\)) Proof by contradiction. If there exists a left eigenvector and eigenvalue pair \\((x, \\lambda)\\), s.t. \\(x^* A = \\lambda x^*\\) while \\(x^*B = 0\\), then \\(x^* [A - \\lambda I, B] = 0\\). Contradiction! (\\(5. \\Rightarrow 3.\\)) Proof by contradiction. If the controllability matrix \\(\\mathcal{C}\\) does not have full row rank, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then, from Corollary B.4, there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{R}^{k \\times k}, \\bar{A}_{\\bar{c}} \\in \\mathbb{R}^{(n-k) \\times (n-k)}\\). Now arbitrarily pick one of \\(\\bar{A}_{\\bar{c}}\\)’s left eigenvector \\(x_{\\bar{c}}\\) and its corresponding eigenvalue \\(\\lambda_1\\). Define the vector \\(x = \\begin{bmatrix} 0 \\\\ x_{\\bar{c}} \\end{bmatrix}\\). Then, \\[\\begin{equation*} \\begin{split} x^* (TAT^{-1}) = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\end{bmatrix} \\begin{bmatrix} \\bar{A}_{c} &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{c}}^* \\bar{A}_{\\bar{c}} \\end{bmatrix} = \\begin{bmatrix} 0 &amp; \\lambda_1 x_{\\bar{c}}^* \\end{bmatrix} = \\lambda_1 x^* \\\\ x^* (TB) &amp; = \\begin{bmatrix} 0 &amp; x_{\\bar{x}} \\end{bmatrix} \\begin{bmatrix} B_{\\bar{c}} \\\\ 0 \\end{bmatrix} = 0 \\end{split} \\end{equation*}\\] which implies \\((TAT^{-1}, TB)\\) is not controllable. However, similarity transformation does not change controllability. Contradiction! (\\(6. \\Rightarrow 1.\\)) Prove by contradiction. If \\((A, B)\\) is not controllable, i.e., \\(\\text{rank}(\\mathcal{C}) = k &lt; n\\). Then from Corollary B.4, there exists a similarity transformation \\(T\\) s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] Now arbitrarily pick \\(F \\in \\mathbb{R}^{m\\times n}\\) and define \\(FT^{-1} = [F_1, F_2]\\), where \\(F_1 \\in \\mathbb{R}^{m\\times k}, F_2 \\in \\mathbb{R}^{m\\times (n-k)}\\). Thus, \\[\\begin{equation*} \\begin{split} \\text{det}(A+BF-\\lambda I) &amp; = \\text{det}\\left( T^{-1} \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} T + T^{-1} \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} F - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det}\\left( T^{-1} \\left\\{ \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} FT^{-1} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right\\} T \\right) \\\\ &amp; = \\text{det}\\left( \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix} + \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\begin{bmatrix} F_1 &amp; F_2 \\end{bmatrix} - \\lambda \\begin{bmatrix} I_1 &amp; 0 \\\\ 0 &amp; I_2 \\end{bmatrix} \\right) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1 &amp; \\bar{A}_{12} + \\bar{B}_c F_2 \\\\ 0 &amp; \\bar{A}_{\\bar{c}} - \\lambda I_2 \\end{bmatrix} \\\\ &amp; = \\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1) \\cdot \\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2) \\end{split} \\end{equation*}\\] where \\(I_1\\) is the identity matrix of size \\(k\\). Similarly, \\(I_2\\) of size \\(n-k\\). Thus, at least \\(n-k\\) eigenvalues of \\(A+BF\\) cannot be freely assigned by choosing \\(F\\). Contradiction! (\\(1. \\Rightarrow 6.\\)) Here we only represent the SIMO case. For the MIMO case, the proof is far more complex. Interesting readers can refer to (Davison and Wonham 1968) (the shortest proof I can find). Since there is only one input, the matrix \\(B\\) degenerate to vector \\(b\\). From Corollary B.5, there exist a similarity transformation matrix \\(T\\), s.t. \\[\\begin{equation*} T A T^{-1} = A_1 := \\begin{bmatrix} -a_1 &amp; -a_2 &amp; \\dots &amp; -a_{n-1} &amp; -a_n \\\\ 1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1 &amp; 0 \\end{bmatrix}, \\quad T b = b_1 := \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation*}\\] For any \\(F \\in \\mathbb{C}^{1 \\times n}\\), denote \\(FT^{-1}\\) as \\([f_1, f_2, \\dots, f_n]\\). Calculating the characteristic polynomial of \\(A + bF\\): \\[\\begin{equation*} \\begin{split} \\text{det}(\\lambda I - A - bF) &amp; = \\text{det}(\\lambda I - T^{-1}A_1 T - T^{-1} b_1 F) \\\\ &amp; = \\text{det}(\\lambda I - A_1 - b_1 F T^{-1}) \\\\ &amp; = \\text{det} \\begin{bmatrix} \\lambda + a_1 - f_1 &amp; \\lambda + a_2 - f_2 &amp; \\dots &amp; \\lambda + a_{n-1} - f_{n-1} &amp; \\lambda + a_n - f_n \\\\ -1 &amp; \\lambda &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; -1 &amp; \\lambda \\end{bmatrix} \\\\ &amp; = \\lambda^n + (a_1 - f_1) \\lambda^{n-1} + \\dots + (a_n - f_n) \\end{split} \\end{equation*}\\] By choosing \\([f_1, f_2, \\dots, f_n]\\), \\(A+bF\\)’s eigenvalues can be arbitrarily set. (\\(7. \\Rightarrow 1.\\)) Prove by contradiction. Assume that \\((A, B)\\) is not controllable. Then from 2., there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\) and \\(t_1 &gt; 0\\), \\[\\begin{equation*} F(t) = v^* e^{At} B = 0, \\quad \\forall t \\in [0, t_1] \\end{equation*}\\] Now consider \\(F(z) = v^* e^{Az} B, z\\in \\mathcal{C}\\), which is a vector of analytic function in complex analysis. For a arbitrary \\(t_2 \\in (0, t_1)\\), we have \\(F^{(i)}(t_2) = 0, \\forall i \\in \\mathbb{N}\\). Then, by invoking the fact from complex analysis: “Let \\(G\\) a connected open set and \\(f: G \\rightarrow \\mathbb{C}\\) be analytic, then \\(f \\equiv 0\\) on \\(G\\), if and only if there is a point \\(a \\in G\\) such that \\(f^{(i)}(a) = 0, \\forall n \\in \\mathbb{N}\\)”, we have \\(f(z) \\equiv 0, \\forall z \\in \\mathbb{C}\\). On the other hand, however, \\(W_c \\succ 0\\) implies there exists \\(t_3 &gt; 0\\), such that for the above \\(v\\), we have \\(v^* e^{At_3} B \\ne 0\\). Contradiction! (\\(1. \\Rightarrow 7.\\)) Since \\((A, B)\\) is controllable, from 2., \\(W_c(t) \\succ 0, \\forall t\\). Therefore, \\(W_c \\succ 0\\). The existence and uniqueness of the solution for \\(AW_c + W_cA^* = -BB^*\\) can be obtained directly from the proof of Theorem B.3, by setting \\(Q\\) there to be positive semidefinite. B.2.3 Duality Although controllability and observability seemingly have no direct connections from their definitions B.2 and B.3, the following theorem (Chen 1984) states their tight relations. Theorem B.6 (Theorem of Duality) The pair \\((C,A)\\) is observable if and only if \\((A^*,C^*)\\) is controllable. Proof. We first show that \\((C,A)\\) is observable if and only if the \\(n \\times n\\) matrix \\(W_o(t) = \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau}\\) is positive definite (nonsingular) for any \\(t&gt;0\\): “\\(\\Longleftarrow\\)”: From (B.5), given initial state \\(x(0)\\) and the inputs \\(u(t)\\), \\(y(t)\\) can be expressed as \\[\\begin{equation*} y(t) = Ce^{At} x(0) + C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau + Du(t) \\end{equation*}\\] Define a known function \\(\\bar{y}(t)\\) as \\(y(t) - C \\int_{0}^{t} e^{A(t-\\tau)} Bu(\\tau) d\\tau - Du(t)\\) and we will get \\[\\begin{equation*} Ce^{At} x(0) = \\bar{y}(t) \\end{equation*}\\] Pre-multiply the above equation by \\(e^{A^*t}C^*\\) and integrate it over \\([0,t_1]\\) to yield \\[\\begin{equation*} (\\int_{0}^{t_1} e^{A^*t} C^*C e^{At} dt) x(0) = W_o(t_1) x(0) = \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] Since \\(W_o(t_1) \\succ 0\\), \\[\\begin{equation*} x(0) = W_o(t_1)^{-1} \\int_{0}^{t_1} e^{A^*t} C^* \\bar{y}(t) dt \\end{equation*}\\] can be observed. “\\(\\Longrightarrow\\)”: Prove by contradiction. Suppose \\((C,A)\\) is observable but there exists \\(t_1 &gt;0\\), s.t. \\(W_o(t_1)\\) is singular. This implies there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* W_o(t_1) v = 0 \\Longrightarrow Ce^{At} v \\equiv 0, \\ \\forall t \\in [0,t_1] \\end{equation*}\\] Similar to the proof of Theorem B.5 (\\(7. \\Rightarrow 1.\\)), we can use conclusions from complex analysis to claim that \\(Ce^{At} v \\equiv 0, \\forall t &gt;0\\). On the other hand, we set \\(u(t) \\equiv 0\\), which results in \\(y(t) = Ce^{At}x(0)\\). In this case \\(x(0) = 0\\) and \\(x(0) = v \\ne 0\\) will lead to the same output responses \\(y(t)\\) over \\(t&gt;0\\), which implies \\((C,A)\\) is not observable. Contradiction! Next we show the duality of controllability and observability: From (1) we know \\((C,A)\\) is controllable if and only of \\[\\begin{equation*} \\int_{0}^{t} e^{A^*\\tau} C^*C e^{A\\tau} d\\tau = \\int_{0}^{t} e^{(A^*)\\tau} (C^*)^* (C^*) e^{(A^*)^*\\tau} d\\tau \\end{equation*}\\] is nonsingular for all \\(t &gt;0\\). The latter is exactly the definition of \\((A^*, C^*)\\)’s controllability Gramian \\(W_c(t)\\). B.2.4 Equivalent Statements for Observability With the Theorem of Duality B.6, we can directly write down the equivalent statements of observability without any additional proofs: Theorem B.7 (Equivalent Statements for Observability) The following statements are equivalent (Chen 1984), (Zhou, Doyle, and Glover 1996): \\((C, A)\\) is observable. The matrix \\[\\begin{equation*} W_o(t) := \\int_{0}^{t} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] is positive definite for any \\(t&gt;0\\). The observability matrix \\[\\begin{equation*} \\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^2 \\\\ \\dots \\\\ CA^{n-1} \\end{bmatrix} \\end{equation*}\\] has full column rank. The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full column rank for all \\(\\lambda \\in \\mathbb{C}\\). Let \\(\\lambda\\) and \\(y\\) be any eigenvalue and any corresponding right eigenvector of \\(A\\), i.e., \\(Ay = \\lambda y\\), then \\(Cy \\ne 0\\). The eigenvalues of \\(A+LC\\) can be freely assigned (with the restriction that complex eigenvalues are in conjugate pairs) by a suitable choice of \\(L\\). \\((A^*, C^*)\\) is controllable. If, in addition, all eigenvalues of \\(A\\) have negative parts, then the unique solution of \\[\\begin{equation*} A^* W_o + W_o A = -C^* C \\end{equation*}\\] is positive definite. The solution is called the observability Gramian and can be expressed as \\[\\begin{equation*} W_o = \\int_{0}^{\\infty} e^{A^*\\tau} C^* C e^{A\\tau} d\\tau \\end{equation*}\\] B.3 Stabilizability And Detectability To define stabilizability and detectability of an LTI system, we first introduce the concept of system mode, which can be naturally derived from the fifth definition of controllability B.5 (observability B.7). Definition B.4 (System Mode) \\(\\lambda\\) is a mode of an LTI system, if it is an eigenvalue of \\(A\\). The mode \\(\\lambda\\) is said to be: stable, if \\(\\text{Re}\\lambda &lt; 0\\), controllable, if \\(x^* B \\ne 0\\) for all left eigenvectors of \\(A\\) associated with \\(\\lambda\\), observable, if \\(C x \\ne 0\\) for all right eigenvectors of \\(A\\) associated with \\(\\lambda\\). Otherwise, the mode is said to be uncontrollable (unobservable). With the concept of system mode, the fifth definition of controllability B.5 (observability B.7) can be restated as An LTI system is controllable (observable) if and only if all modes are controllable (observable). Stabilizability (detectability) is defined similarly via loosening part of controllability (observability) conditions. Definition B.5 (Stabilizability) An LTI system is said to be stabilizable if all of its unstable modes are controllable. Definition B.6 (Detectability) An LTI system is said to be detectable if all of its unstable modes are observable. Like in the case of controllability and observability, duality also holds in stabilizability and detectability. Moreover, similarity transformation will not influence an LTI system’s stabilizability and detectability. B.3.1 Equivalent Statements for Stabilizability Theorem B.8 (Equivalent Statements for Stabilizability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((A,B)\\) is stabilizable. For all \\(\\lambda\\) and \\(x\\) such that \\(x^* A = \\lambda x^*\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(x^* B \\ne 0\\). The matrix \\([A-\\lambda I, B]\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(F\\) such that \\(A+BF\\) are Hurwitz. Proof. (\\(1. \\Leftrightarrow 2.\\)) Directly from stabilizability’s definition. (\\(2. \\Leftrightarrow 3.\\)) If 2. holds but 3. not hold, then there exists \\(v \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} v^* [A-\\lambda I, B] = 0 \\Leftrightarrow v^* A = \\lambda v^*, v^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Contradiction! Vice versa. (\\(4. \\Rightarrow 2.\\)) Prove by contradiction. Suppose there \\(x \\ne 0 \\in \\mathbb{C}^n\\), s.t. \\[\\begin{equation*} x^* [A-\\lambda I, B] = 0 \\Leftrightarrow x^* A = \\lambda x^*, x^* B = 0, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] Thus, for any \\(F\\), \\[\\begin{equation*} x^* (A+BF) = \\lambda x^*, \\text{Re} \\lambda \\ge 0 \\end{equation*}\\] On the other hand, suppose \\(A+BF\\) has \\(I\\) Jordon blocks, with each equipped with an eigenvalue \\(\\eta_i, i = 1\\dots I\\) (note that \\(\\eta_\\alpha\\) may be equal to \\(\\eta_\\beta\\), i.e., they are equivalent eigenvalues with different Jordon blocks). Since \\(A+BF\\)’s eigenvalues all have negative real parts, \\(\\text{Re} (\\eta_i) &lt; 0, i = 1\\dots I\\). For each \\(\\eta_i,i \\in \\left\\{1\\dots i\\right\\}\\), denote its \\(K_i\\) generalized left eigenvectors as \\(v_{i,1}, v_{i,2}, \\dots v_{i,K_i}\\). By definition, \\(\\sum_{i=1}^{I} K_i = n\\) and \\[\\begin{equation*} \\begin{split} v_{i,1}^* (A+BF) &amp; = v_{i,1}^* \\cdot \\eta_i \\\\ v_{i,2}^* (A+BF) &amp; = v_{i,1}^* + v_{i,2}^* \\cdot \\eta_i \\\\ &amp; \\vdots \\\\ v_{i,K_i}^* (A+BF) &amp; = v_{i,K_i-1}^* + v_{i,K_i}^* \\cdot \\eta_i \\end{split} \\end{equation*}\\] for all \\(i \\in \\left\\{1\\dots i\\right\\}\\). Also, \\(v_{i,k},i=1\\dots I, k=1\\dots K_i\\) are linearly independent and spans \\(\\mathbb{C}^n\\). Therefore, \\[\\begin{equation*} x^* = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\end{equation*}\\] which leads to \\[\\begin{equation*} \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* (A+BF) = \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot \\lambda \\cdot v_{i,k}^* \\end{equation*}\\] Since \\(v_{i,k}\\)’s are \\(A+BF\\)’s generalized eigenvectors, we have \\[\\begin{equation*} \\begin{split} &amp; \\sum_{i=1}^{I} \\sum_{k=1}^{K_i} \\xi_{i,k} \\cdot v_{i,k}^* \\cdot (A+BF) \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\xi_{i,1} \\cdot \\eta_i \\cdot v_{i,1}^* + \\sum_{k=2}^{K_i} \\xi_{i,k} (v_{i,k-1}^* + \\eta_i \\cdot v_{i,k}^* ) \\right\\} \\\\ = &amp; \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} (\\xi_{i,k}\\cdot \\eta_i + \\xi_{i,k+1}) v_{i,k}^* + \\xi_{i,K_i} \\cdot \\eta_i \\cdot v_{i,K_i}^* \\right\\} \\end{split} \\end{equation*}\\] Combining the above two equations: \\[\\begin{equation*} \\sum_{i=1}^{I} \\left\\{ \\sum_{k=1}^{K_i - 1} \\left[ \\xi_{i,k}\\cdot (\\eta_i - \\lambda) + \\xi_{i,k+1} \\right] v_{i,k}^* + \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) \\cdot v_{i,K_i}^* = 0 \\right\\} \\end{equation*}\\] Since \\(v_{i,k}\\)’s are linearly independent, for any \\(i \\in \\left\\{i\\dots I\\right\\}\\): \\[\\begin{equation*} \\begin{split} \\xi_{i,1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,2} &amp; = 0 \\Rightarrow \\xi_{i,2} = (-1) \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda) \\\\ \\xi_{i,2} \\cdot (\\eta_i - \\lambda) + \\xi_{i,3} &amp; = 0 \\Rightarrow \\xi_{i,3} = (-1)^2 \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^2 \\\\ &amp; \\vdots \\\\ \\xi_{i,K_i-1} \\cdot (\\eta_i - \\lambda) + \\xi_{i,K_i} &amp; = 0 \\Rightarrow \\xi_{i,K_i} = (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i-1} \\\\ \\xi_{i,K_i} \\cdot (\\eta_i - \\lambda) &amp; = 0 \\end{split} \\end{equation*}\\] Thus, \\[\\begin{equation*} (-1)^{K_i-1} \\cdot \\xi_{i,1} \\cdot (\\eta_i - \\lambda)^{K_i} = 0 \\end{equation*}\\] Denote \\(\\xi_{i,1}\\) as \\(r_1 e^{\\theta_1}\\), \\((\\eta_i - \\lambda)\\) as \\(r_2 e^{\\theta_2}\\). Since \\(\\text{Re} \\lambda \\ge 0, \\text{Re}(\\eta_i) &lt; 0\\), \\(r_2 &gt; 0\\). On the other hand, the following equation suggests \\[\\begin{equation*} r_1 r_2^{K_i-1} e^{j[\\theta_1 + \\theta_2 (K_i-1)]} = 0 \\end{equation*}\\] Thus, \\(r_1\\) has to be \\(0\\), which implies \\(\\xi_{i,1} = 0\\). By recursion, \\(\\xi_{i,k} = 0, \\forall k = 1\\dots K_i\\). Contradiction! (\\(1. \\Rightarrow 4.\\)) If \\((A,B)\\) is controllable, then from Theorem (thm:lticontrollable)’s sixth definition, we can freely assign the poles of \\(A+BF\\) via choosing \\(F\\) properly. Otherwise, if \\((A,B)\\) is uncontrollable, then from Corollary B.4 and proof of Theorem B.5 (\\(6. \\Rightarrow 1.\\)), there exists a similarity transformation \\(T\\), s.t. \\[\\begin{equation*} TAT^{-1} = \\begin{bmatrix} \\bar{A}_c &amp; \\bar{A}_{12} \\\\ 0 &amp; \\bar{A}_{\\bar{c}} \\end{bmatrix}, \\quad TB = \\begin{bmatrix} \\bar{B}_c \\\\ 0 \\end{bmatrix} \\end{equation*}\\] and \\[\\begin{equation*} \\text{det}(A+BF-\\lambda I) = \\underbrace{\\text{det}(\\bar{A}_c + \\bar{B}_c F_1 - \\lambda I_1)}_{\\chi_c(\\lambda)} \\cdot \\underbrace{\\text{det}(\\bar{A}_{\\bar{c}} - \\lambda I_2)}_{\\chi_{\\bar{c}}(\\lambda)} \\end{equation*}\\] where \\(\\bar{A}_c \\in \\mathbb{C}^{k_1 \\times k_1}\\), \\(I_1\\) identity matrix of size \\(k_1\\), \\([F_1,F_2] = FT^{-1}\\), and \\(k_1 = \\text{rank} \\mathcal{C}\\). Additionally, \\((\\bar{A}_c, \\bar{B}_c)\\) is controllable. Thus, \\(\\chi_c(\\lambda)\\)’s zeros can be freely assigned by choosing proper \\(F\\), i.e., system modes with \\(\\chi_c(\\lambda)\\) is controllable, regardless of its stability. On the other hand, system modes with \\(\\chi_{\\bar{c}}(\\lambda)\\) must be stable. Otherwise, we cannot affect it by assigning \\(F\\), which is a contradiction to statement (1). Therefore, \\((TAT^{-1}, TB)\\) is stabilizable. Since similarity transformation does not change stabilizability, \\((A,B)\\) is stabilizable. B.3.2 Equivalent Statements for Detectability Thanks to duality, we can directly write down the equivalent statements of observability without any additional proofs: Theorem B.9 (Equivalent Statements for Detectability) The following statements are equivalent (Zhou, Doyle, and Glover 1996): \\((C,A)\\) is detectable. For all \\(\\lambda\\) and \\(x\\) such that \\(A x = \\lambda x\\) and \\(\\text{Re} \\lambda \\ge 0\\), \\(C x \\ne 0\\). The matrix \\(\\begin{bmatrix} A - \\lambda I \\\\ C \\end{bmatrix}\\) has full rank for all \\(\\text{Re} \\lambda \\ge 0\\). There exists a matrix \\(L\\) such that \\(A+LC\\) are Hurwitz. \\((A^*, C^*)\\) is stabilizable. References Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. when \\(A\\) is not diagonalizable, similar results can be derived via Jordan decomposition.↩︎ "],["references.html", "References", " References Antos, András, Csaba Szepesvári, and Rémi Munos. 2007. “Fitted q-Iteration in Continuous Action-Space MDPs.” Advances in Neural Information Processing Systems 20. Baird, Leemon et al. 1995. “Residual Algorithms: Reinforcement Learning with Function Approximation.” In Proceedings of the Twelfth International Conference on Machine Learning, 30–37. Barto, Andrew G, Richard S Sutton, and Charles W Anderson. 2012. “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems.” IEEE Transactions on Systems, Man, and Cybernetics, no. 5: 834–46. Chen, Chi-Tsong. 1984. Linear System Theory and Design. Saunders college publishing. Davison, E., and W. Wonham. 1968. “On Pole Assignment in Multivariable Linear Systems.” IEEE Transactions on Automatic Control 13 (6): 747–48. https://doi.org/10.1109/TAC.1968.1099056. Fan, Jianqing, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. 2020. “A Theoretical Analysis of Deep q-Learning.” In Learning for Dynamics and Control, 486–89. PMLR. Garrigos, Guillaume, and Robert M Gower. 2023. “Handbook of Convergence Theorems for (Stochastic) Gradient Methods.” arXiv Preprint arXiv:2301.11235. Kearns, Michael J, and Satinder Singh. 2000. “Bias-Variance Error Bounds for Temporal Difference Updates.” In COLT, 142–47. Mahmood, A Rupam, Huizhen Yu, Martha White, and Richard S Sutton. 2015. “Emphatic Temporal-Difference Learning.” arXiv Preprint arXiv:1507.01569. Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. Munos, Rémi, and Csaba Szepesvári. 2008. “Finite-Time Bounds for Fitted Value Iteration.” Journal of Machine Learning Research 9 (5). Nesterov, Yurii. 2018. Lectures on Convex Optimization. Vol. 137. Springer. Riedmiller, Martin. 2005. “Neural Fitted q Iteration–First Experiences with a Data Efficient Neural Reinforcement Learning Method.” In European Conference on Machine Learning, 317–28. Springer. Robbins, Herbert, and David Siegmund. 1971. “A Convergence Theorem for Non Negative Almost Supermartingales and Some Applications.” In Optimizing Methods in Statistics, 233–57. Elsevier. Sutton, Richard S, and Andrew G Barto. 1998. Reinforcement Learning: An Introduction. Vol. 1. 1. MIT press Cambridge. Sutton, Richard S, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári, and Eric Wiewiora. 2009. “Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation.” In Proceedings of the 26th Annual International Conference on Machine Learning, 993–1000. Sutton, Richard S, Csaba Szepesvári, and Hamid Reza Maei. 2008. “A Convergent o(n) Algorithm for Off-Policy Temporal-Difference Learning with Linear Function Approximation.” Advances in Neural Information Processing Systems 21 (21): 1609–16. Zhou, Kemin, JC Doyle, and Keither Glover. 1996. “Robust and Optimal Control.” Control Engineering Practice 4 (8): 1189–90. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
