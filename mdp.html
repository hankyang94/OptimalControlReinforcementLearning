<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-09-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="appconvex.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.2</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.3" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.3</b> Dynamic Programming</a></li>
<li class="chapter" data-level="1.4" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.4</b> Infinite-Horizon MDP</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mdp" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Markov Decision Process<a href="mdp.html#mdp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Optimal control (OC) and reinforcement learning (RL) address the problem of making <strong>optimal decisions</strong> in the presence of a <strong>dynamic environment</strong>.</p>
<ul>
<li>In <strong>optimal control</strong>, this dynamic environment is often referred to as a <em>plant</em> or a <em>dynamical system</em>.<br />
</li>
<li>In <strong>reinforcement learning</strong>, it is modeled as a <em>Markov decision process</em> (MDP).</li>
</ul>
<p>The goal in both fields is to evaluate and design decision-making strategies that optimize long-term performance:</p>
<ul>
<li><strong>RL</strong> typically frames this as maximizing a long-term <em>reward</em>.<br />
</li>
<li><strong>OC</strong> often formulates it as minimizing a long-term <em>cost</em>.</li>
</ul>
<p>The emphasis on <strong>long-term</strong> evaluation is crucial. Because the environment evolves over time, decisions that appear beneficial in the short term may lead to poor long-term outcomes and thus be suboptimal.</p>
<hr />
<p>With this motivation, we now formalize the framework of Markov Decision Processes (MDPs), which are discrete-time stochastic dynamical systems.</p>
<div id="FiniteHorizonMDP" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Finite-Horizon MDP<a href="mdp.html#FiniteHorizonMDP" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We begin with finite-horizon MDPs and introduce infinite-horizon MDPs in the following section. An abstract definition of the finite-horizon case will be presented first, followed by illustrative examples.</p>
<p>A finite-horizon MDP is given by the following tuple:
<span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, T),
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span>: state space (set of all possible states)</li>
<li><span class="math inline">\(\mathcal{A}\)</span>: action space (set of all possible actions)</li>
<li><span class="math inline">\(P(s&#39; \mid s, a)\)</span>: probability of transitioning to state <span class="math inline">\(s&#39;\)</span> from state <span class="math inline">\(s\)</span> under action <span class="math inline">\(a\)</span> (i.e., dynamics)</li>
<li><span class="math inline">\(R(s,a)\)</span>: reward of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(T\)</span>: horizon, a positive integer</li>
</ul>
<p>For now, let us assume both the state space and the action space are discrete and have a finite number of elements. In particular, denote the number of elements in <span class="math inline">\(\mathcal{S}\)</span> as <span class="math inline">\(|\mathcal{S}|\)</span>, and the number of elements in <span class="math inline">\(\mathcal{A}\)</span> as <span class="math inline">\(|\mathcal{A}|\)</span>. This is also referred to as a <em>tabular MDP</em>.</p>
<p><strong>Policy</strong>. Decision-making in MDPs is represented by policies. A policy is a function that, given any state, outputs a distribution of actions: <span class="math inline">\(\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})\)</span>. That is, <span class="math inline">\(\pi(a \mid s)\)</span> returns the probability of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>. In finite-horizon MDPs, we consider a tuple of policies:
<span class="math display" id="eq:policy-tuple">\[\begin{equation}
\pi = (\pi_0, \dots, \pi_t, \dots, \pi_{T-1}),
\tag{1.1}
\end{equation}\]</span>
where each <span class="math inline">\(\pi_t\)</span> denotes the policy at step <span class="math inline">\(t \in [0,T-1]\)</span>.</p>
<p><strong>Trajectory and Return</strong>. Given an initial state <span class="math inline">\(s_0 \in \mathcal{S}\)</span> and a policy <span class="math inline">\(\pi\)</span>, the MDP will evolve as</p>
<ol style="list-style-type: decimal">
<li>Start at state <span class="math inline">\(s_0\)</span></li>
<li>Take action <span class="math inline">\(a_0 \sim \pi_0(a \mid s_0)\)</span> following policy <span class="math inline">\(\pi_0\)</span></li>
<li>Collect reward <span class="math inline">\(r_0 = R(s_0, a_0)\)</span> (assume <span class="math inline">\(R\)</span> is deterministic)</li>
<li>Transition to state <span class="math inline">\(s_1 \sim P(s&#39; \mid s_0, a_0)\)</span> following the dynamics</li>
<li>Go to step 2 and continue until reaching state <span class="math inline">\(s_T\)</span></li>
</ol>
<p>This evolution generates a trajectory of states, actions, and rewards:
<span class="math display">\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T).
\]</span>
The cumulative reward of this trajectory is <span class="math inline">\(g_0 = \sum_{t=0}^{T-1} r_t\)</span>, which is called the <em>return</em> of the trajectory. Clearly, <span class="math inline">\(g_0\)</span> is a random variable due to the stochasticity of both the policy and the dynamics. Similarly, if the state at time <span class="math inline">\(t\)</span> is <span class="math inline">\(s_t\)</span>, we denote:
<span class="math display">\[
g_t = r_t + \dots + r_{T-1}
\]</span>
as the return of the policy starting at <span class="math inline">\(s_t\)</span>.</p>
<div id="FiniteHorizonMDP-Value" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Value Functions<a href="mdp.html#FiniteHorizonMDP-Value" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>State-Value Function</strong>. Given a policy <span class="math inline">\(\pi\)</span> as in <a href="mdp.html#eq:policy-tuple">(1.1)</a>, which states are preferable at time <span class="math inline">\(t\)</span>? The (time-indexed) state-value function assigns to each <span class="math inline">\(s\in\mathcal{S}\)</span> the expected return from <span class="math inline">\(t\)</span> onward when starting in <span class="math inline">\(s\)</span> and following <span class="math inline">\(\pi\)</span> thereafter. Formally, define
<span class="math display" id="eq:FiniteHorizonMDP-state-value">\[\begin{equation}
V_t^\pi(s) := \mathbb{E} \left[g_t \mid s_t=s\right]
= \mathbb{E} \left[\sum_{i=t}^{T-1} R(s_i,a_i) \middle| s_t=s,a_i\sim \pi_i(\cdot\mid s_i), s_{i+1}\sim P(\cdot\mid s_i,a_i)\right].
\tag{1.2}
\end{equation}\]</span>
The expectation is over the randomness induced by both the policy and the dynamics. Thus, if <span class="math inline">\(V_t^\pi(s_1)&gt;V_t^\pi(s_2)\)</span>, then at time <span class="math inline">\(t\)</span> under policy <span class="math inline">\(\pi\)</span> it is better in expectation to be in <span class="math inline">\(s_1\)</span> than in <span class="math inline">\(s_2\)</span> because the former yields a larger expected return.</p>
<div class="highlightbox">
<p><span class="math inline">\(V^{\pi}_t(s)\)</span>: given policy <span class="math inline">\(\pi\)</span>, how good is it to start in state <span class="math inline">\(s\)</span> at time <span class="math inline">\(t\)</span>?</p>
</div>
<p><strong>Action-Value Function</strong>. Similarly, the action-value function assigns to each state-action pair <span class="math inline">\((s,a)\in\mathcal{S}\times\mathcal{A}\)</span> the expected return obtained by starting in state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span> first, and then following policy <span class="math inline">\(\pi\)</span> thereafter:
<span class="math display" id="eq:FiniteHorizonMDP-action-value">\[\begin{equation}
\begin{split}
Q_t^\pi(s,a) := &amp; \mathbb{E} \left[R(s,a) + g_{t+1} \mid s_{t+1} \sim P(\cdot \mid s,a)\right] \\
= &amp; \mathbb{E} \left[R(s,a) + \sum_{i=t+1}^{T-1} R(s_i, a_i) \middle| s_{t+1} \sim P(\cdot \mid s,a) \right].
\end{split}
\tag{1.3}
\end{equation}\]</span>
The key distinction is that the action-value function evaluates the return when the first action may deviate from policy <span class="math inline">\(\pi\)</span>, whereas the state-value function assumes strict adherence to <span class="math inline">\(\pi\)</span>. This flexibility makes the action-value function central to improving <span class="math inline">\(\pi\)</span>, since it reveals whether alternative actions can yield higher returns.</p>
<div class="highlightbox">
<p><span class="math inline">\(Q^{\pi}_t(s,a)\)</span>: At time <span class="math inline">\(t\)</span>, how good is it to take action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, then follow the policy <span class="math inline">\(\pi\)</span>?</p>
</div>
<p>It is easy to verify that the state-value function and the action-value function satisfy:
<span class="math display" id="eq:FiniteHorizonMDP-action-value-from-state-value" id="eq:FiniteHorizonMDP-state-value-from-action-value">\[\begin{align}
V_t^{\pi}(s) &amp; = \sum_{a \in \mathcal{A}} \pi_t(a \mid s) Q_t^{\pi}(s,a), \tag{1.4} \\
Q_t^{\pi}(s,a) &amp; = R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^{\pi}_{t+1} (s&#39;). \tag{1.5}
\end{align}\]</span></p>
<p>From these two equations, we can derive the Bellman Consistency equations.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:BellmanConsistency" class="proposition"><strong>Proposition 1.1  (Bellman Consistency) </strong></span>The state-value function <span class="math inline">\(V^{\pi}_t(\cdot)\)</span> in <a href="mdp.html#eq:FiniteHorizonMDP-state-value">(1.2)</a> satisfies the following recursion:
<span class="math display" id="eq:BellmanConsistency-State-Value">\[\begin{equation}
\begin{split}
V^{\pi}_t(s) &amp; = \sum_{a \in \mathcal{A}} \pi_t(a\mid s) \left( R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^{\pi}_{t+1} (s&#39;) \right) \\
    &amp; =: \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} [V^{\pi}_{t+1}(s&#39;)] \right].
\end{split}
\tag{1.6}
\end{equation}\]</span></p>
<p>Similarly, the action-value function <span class="math inline">\(Q^{\pi}_t(s,a)\)</span> in <a href="mdp.html#eq:FiniteHorizonMDP-action-value">(1.3)</a> satisfies the following recursion:
<span class="math display" id="eq:BellmanConsistency-Action-Value">\[\begin{equation}
\begin{split}
Q^{\pi}_t (s, a) &amp; = R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) \left( \sum_{a&#39; \in \mathcal{A}} \pi_{t+1}(a&#39; \mid s&#39;) Q^{\pi}_{t+1}(s&#39;, a&#39;)\right) \\
&amp; =: R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \left[\mathbb{E}_{a&#39; \sim \pi_{t+1}(\cdot \mid s&#39;)} [Q^{\pi}_{t+1}(s&#39;, a&#39;)] \right].
\end{split}
\tag{1.7}
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="policy-evaluation" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Policy Evaluation<a href="mdp.html#policy-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Bellman consistency result in Proposition <a href="mdp.html#prp:BellmanConsistency">1.1</a> is fundamental because it directly yields an algorithm for evaluating a given policy <span class="math inline">\(\pi\)</span>—that is, for computing its state-value and action-value functions—provided the transition dynamics of the MDP are known.</p>
<p>Policy evaluation for the state-value function proceeds as follows:</p>
<ul>
<li><strong>Initialization:</strong> set <span class="math inline">\(V^{\pi}_T(s) = 0\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>.<br />
</li>
<li><strong>Backward recursion:</strong> for <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>, update each <span class="math inline">\(s \in \mathcal{S}\)</span> by
<span class="math display">\[
V^{\pi}_{t}(s) = \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \big[ V^{\pi}_{t+1}(s&#39;) \big] \right].
\]</span></li>
</ul>
<p>Similarly, policy evaluation for the action-value function is given by:</p>
<ul>
<li><strong>Initialization:</strong> set <span class="math inline">\(Q^{\pi}_T(s,a) = 0\)</span> for all <span class="math inline">\(s \in \mathcal{S}, a \in \mathcal{A}\)</span>.<br />
</li>
<li><strong>Backward recursion:</strong> for <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>, update each <span class="math inline">\((s,a) \in \mathcal{S}\times\mathcal{A}\)</span> by
<span class="math display">\[
Q^{\pi}_t(s,a) = R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \left[ \mathbb{E}_{a&#39; \sim \pi_{t+1}(\cdot \mid s&#39;)} \big[ Q^{\pi}_{t+1}(s&#39;, a&#39;) \big] \right].
\]</span></li>
</ul>
<p>The essential feature of this algorithm is its backward-in-time recursion: the value functions are first set at the terminal horizon <span class="math inline">\(T\)</span>, and then propagated backward step by step through the Bellman consistency equations.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:MDPExampleGraph" class="example"><strong>Example 1.1  (MDP, Transition Graph, and Policy Evaluation) </strong></span>It is often useful to visualize small MDPs as transition graphs, where states are represented by nodes and actions are represented by directed edges connecting those nodes.</p>
<p>As a simple illustrative example, consider a robot navigating on a two-state grid. At each step, the robot can either Stay in its current state or Move to the other state. This finite-horizon MDP is fully specified by the tuple of states, actions, transition dynamics, rewards, and horizon:</p>
<ul>
<li><p>States: <span class="math inline">\(\mathcal{S} = \{\alpha, \beta \}\)</span></p></li>
<li><p>Actions: <span class="math inline">\(\mathcal{A} = \{\text{Move} , \text{Stay} \}\)</span></p></li>
<li><p>Transition dynamics: we can specify the transition dynamics in the following table</p></li>
</ul>
<table>
<colgroup>
<col width="20%" />
<col width="22%" />
<col width="34%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">State <span class="math inline">\(s\)</span></th>
<th align="center">Action <span class="math inline">\(a\)</span></th>
<th align="center">Next State <span class="math inline">\(s&#39;\)</span></th>
<th align="center">Probability <span class="math inline">\(P(s&#39; \mid s, a)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">Stay</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">Move</td>
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">Stay</td>
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">Move</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Reward: <span class="math inline">\(R(s,a)=1\)</span> if <span class="math inline">\(a = \text{Move}\)</span> and <span class="math inline">\(R(s,a)=0\)</span> if <span class="math inline">\(a = \text{Stay}\)</span></p></li>
<li><p>Horizon: <span class="math inline">\(T=2\)</span>.</p></li>
</ul>
<p>This MDP can be represented by the transition graph in Fig. <a href="mdp.html#fig:mdp-robot-transition-graph">1.1</a>. Note that for this MDP, the transition dynamics is deterministic. We will see a stochastic MDP soon.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-robot-transition-graph"></span>
<img src="images/MDP/robot-mdp-graph.png" alt="A Simple Transition Graph." width="90%" />
<p class="caption">
Figure 1.1: A Simple Transition Graph.
</p>
</div>
<p>At time <span class="math inline">\(t=0\)</span>, if the robot starts at <span class="math inline">\(s_0 = \alpha\)</span>, first chooses action <span class="math inline">\(a_0 = \text{Move}\)</span>, and then chooses action <span class="math inline">\(a_1 = \text{Stay}\)</span>, the resulting trajectory is
<span class="math display">\[
\tau = (\alpha, \text{Move}, +1, \beta, \text{Stay}, 0,  \beta).
\]</span>
The return of this trajectory is:
<span class="math display">\[
g_0 = +1 + 0 = +1.
\]</span></p>
<p><strong>Policy Evaluation</strong>. Given a policy
<span class="math display">\[\begin{equation}
\pi = (\pi_0, \pi_1), \quad \pi_0(a \mid s) = \begin{cases}
0.5 &amp; a = \text{Move} \\
0.5 &amp; a = \text{Stay}
\end{cases},
\quad
\pi_1( a \mid s) = \begin{cases}
0.8 &amp; a = \text{Move} \\
0.2 &amp; a = \text{Stay}
\end{cases}.
\end{equation}\]</span>
We can use the Bellman consistency equations to compute the state-value function. We first initialize:
<span class="math display">\[
V^{\pi}_2 = \begin{bmatrix}
0 \\ 0
\end{bmatrix},
\]</span>
where the first row contains the value at <span class="math inline">\(s = \alpha\)</span> and the second row contains the value at <span class="math inline">\(s = \beta\)</span>.
We then perform the backward recursion for <span class="math inline">\(t=1\)</span>. For <span class="math inline">\(s = \alpha\)</span>, we have
<span class="math display">\[\begin{equation}
V^{\pi}_1(\alpha) = \begin{bmatrix}
\pi_1(\text{Move} \mid \alpha) \\
\pi_1(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_2(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_2(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8
\end{equation}\]</span>
For <span class="math inline">\(s = \beta\)</span>, we have
<span class="math display">\[\begin{equation}
V^{\pi}_1(\beta) = \begin{bmatrix}
\pi_1(\text{Move} \mid \beta) \\
\pi_1(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_2(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_2(\beta)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8.
\end{equation}\]</span>
Therefore, we have
<span class="math display">\[
V^{\pi}_1 = \begin{bmatrix}
0.8 \\ 0.8
\end{bmatrix}.
\]</span>
We then proceed to the backward recursion for <span class="math inline">\(t=0\)</span>:
<span class="math display">\[\begin{align}
V_0^{\pi}(\alpha) &amp; = \begin{bmatrix}
\pi_0(\text{Move} \mid \alpha) \\
\pi_0(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_1(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_1(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3. \\
V_0^{\pi}(\beta) &amp; = \begin{bmatrix}
\pi_0(\text{Move} \mid \beta) \\
\pi_0(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_0(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_0(\beta)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3.
\end{align}\]</span>
Therefore, the state-value function at <span class="math inline">\(t=0\)</span> is
<span class="math display">\[
V^{\pi}_0 = \begin{bmatrix}
1.3 \\ 1.3
\end{bmatrix}.
\]</span>
You are encouraged to carry out the similar calculations for the action-value function.</p>
<hr />
<p>The toy example was small enough to carry out policy evaluation by hand; in realistic MDPs, we will need the help from computers.</p>
<p>Consider now an MDP whose transition graph is shown in Fig. <a href="mdp.html#fig:mdp-hangover-transition-graph">1.2</a>. This example is adapted from <a href="https://github.com/upb-lea/reinforcement_learning_course_materials/blob/master/exercises/solutions/ex02/Ex2.ipynb">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-hangover-transition-graph"></span>
<img src="images/MDP/hangover-graph.png" alt="Hangover Transition Graph." width="90%" />
<p class="caption">
Figure 1.2: Hangover Transition Graph.
</p>
</div>
<p>This MDP has six states:
<span class="math display">\[
\mathcal{S} = \{\text{Hangover}, \text{Sleep}, \text{More Sleep}, \text{Visit Lecture}, \text{Study}, \text{Pass Exam} \},
\]</span>
and two actions:
<span class="math display">\[
\mathcal{A} = \{\text{Lazy}, \text{Productive} \}.
\]</span>
The stochastic transition dynamics are labeled in the transition graph. For example, at state “Hangover”, taking action “Productive” will lead to state “Visit Lecture” with probability <span class="math inline">\(0.3\)</span> and state “Hangover” with probability <span class="math inline">\(0.7\)</span>.
The rewards of the MDP are defined as:
<span class="math display">\[
R(s,a) = \begin{cases}
+1 &amp; s = \text{Pass Exam} \\
-1 &amp; \text{otherwise}.
\end{cases}.
\]</span></p>
<p><strong>Policy Evaluation</strong>. Consider a time-invariant random policy
<span class="math display">\[
\pi = \{\pi_0,\dots,\pi_{T-1} \}, \quad \pi_t(a \mid s) = \begin{cases}
\alpha &amp; a = \text{Lazy} \\
1 - \alpha &amp; a = \text{Productive}
\end{cases},
\]</span>
that takes “Lazy” with probability <span class="math inline">\(\alpha\)</span> and “Productive” with probability <span class="math inline">\(1-\alpha\)</span>.</p>
<p>The following Python code performs policy evaluation for this MDP, with <span class="math inline">\(T=10\)</span> and <span class="math inline">\(\alpha = 0.4\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="mdp.html#cb1-1" tabindex="-1"></a><span class="co"># Finite-horizon policy evaluation for the Hangover MDP</span></span>
<span id="cb1-2"><a href="mdp.html#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="mdp.html#cb1-3" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb1-4"><a href="mdp.html#cb1-4" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List, Tuple</span>
<span id="cb1-5"><a href="mdp.html#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="mdp.html#cb1-6" tabindex="-1"></a>State <span class="op">=</span> <span class="bu">str</span></span>
<span id="cb1-7"><a href="mdp.html#cb1-7" tabindex="-1"></a>Action <span class="op">=</span> <span class="bu">str</span></span>
<span id="cb1-8"><a href="mdp.html#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="mdp.html#cb1-9" tabindex="-1"></a><span class="co"># --- MDP spec ---------------------------------------------------------------</span></span>
<span id="cb1-10"><a href="mdp.html#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="mdp.html#cb1-11" tabindex="-1"></a>S: List[State] <span class="op">=</span> [</span>
<span id="cb1-12"><a href="mdp.html#cb1-12" tabindex="-1"></a>    <span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Pass Exam&quot;</span></span>
<span id="cb1-13"><a href="mdp.html#cb1-13" tabindex="-1"></a>]</span>
<span id="cb1-14"><a href="mdp.html#cb1-14" tabindex="-1"></a>A: List[Action] <span class="op">=</span> [<span class="st">&quot;Lazy&quot;</span>, <span class="st">&quot;Productive&quot;</span>]</span>
<span id="cb1-15"><a href="mdp.html#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="mdp.html#cb1-16" tabindex="-1"></a><span class="co"># P[s, a] -&gt; list of (s_next, prob)</span></span>
<span id="cb1-17"><a href="mdp.html#cb1-17" tabindex="-1"></a>P: Dict[Tuple[State, Action], List[Tuple[State, <span class="bu">float</span>]]] <span class="op">=</span> {</span>
<span id="cb1-18"><a href="mdp.html#cb1-18" tabindex="-1"></a>    <span class="co"># Hangover</span></span>
<span id="cb1-19"><a href="mdp.html#cb1-19" tabindex="-1"></a>    (<span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-20"><a href="mdp.html#cb1-20" tabindex="-1"></a>    (<span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Visit Lecture&quot;</span>, <span class="fl">0.3</span>), (<span class="st">&quot;Hangover&quot;</span>, <span class="fl">0.7</span>)],</span>
<span id="cb1-21"><a href="mdp.html#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="mdp.html#cb1-22" tabindex="-1"></a>    <span class="co"># Sleep</span></span>
<span id="cb1-23"><a href="mdp.html#cb1-23" tabindex="-1"></a>    (<span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):          [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-24"><a href="mdp.html#cb1-24" tabindex="-1"></a>    (<span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;Productive&quot;</span>):    [(<span class="st">&quot;Visit Lecture&quot;</span>, <span class="fl">0.6</span>), (<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">0.4</span>)],</span>
<span id="cb1-25"><a href="mdp.html#cb1-25" tabindex="-1"></a></span>
<span id="cb1-26"><a href="mdp.html#cb1-26" tabindex="-1"></a>    <span class="co"># More Sleep</span></span>
<span id="cb1-27"><a href="mdp.html#cb1-27" tabindex="-1"></a>    (<span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-28"><a href="mdp.html#cb1-28" tabindex="-1"></a>    (<span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Study&quot;</span>, <span class="fl">0.5</span>), (<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">0.5</span>)],</span>
<span id="cb1-29"><a href="mdp.html#cb1-29" tabindex="-1"></a></span>
<span id="cb1-30"><a href="mdp.html#cb1-30" tabindex="-1"></a>    <span class="co"># Visit Lecture</span></span>
<span id="cb1-31"><a href="mdp.html#cb1-31" tabindex="-1"></a>    (<span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Study&quot;</span>, <span class="fl">0.8</span>), (<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">0.2</span>)],</span>
<span id="cb1-32"><a href="mdp.html#cb1-32" tabindex="-1"></a>    (<span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Study&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-33"><a href="mdp.html#cb1-33" tabindex="-1"></a></span>
<span id="cb1-34"><a href="mdp.html#cb1-34" tabindex="-1"></a>    <span class="co"># Study</span></span>
<span id="cb1-35"><a href="mdp.html#cb1-35" tabindex="-1"></a>    (<span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):         [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-36"><a href="mdp.html#cb1-36" tabindex="-1"></a>    (<span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Productive&quot;</span>):   [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">0.9</span>), (<span class="st">&quot;Study&quot;</span>, <span class="fl">0.1</span>)],</span>
<span id="cb1-37"><a href="mdp.html#cb1-37" tabindex="-1"></a></span>
<span id="cb1-38"><a href="mdp.html#cb1-38" tabindex="-1"></a>    <span class="co"># Pass Exam (absorbing)</span></span>
<span id="cb1-39"><a href="mdp.html#cb1-39" tabindex="-1"></a>    (<span class="st">&quot;Pass Exam&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-40"><a href="mdp.html#cb1-40" tabindex="-1"></a>    (<span class="st">&quot;Pass Exam&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-41"><a href="mdp.html#cb1-41" tabindex="-1"></a>}</span>
<span id="cb1-42"><a href="mdp.html#cb1-42" tabindex="-1"></a></span>
<span id="cb1-43"><a href="mdp.html#cb1-43" tabindex="-1"></a><span class="kw">def</span> R(s: State, a: Action) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-44"><a href="mdp.html#cb1-44" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot;</span></span>
<span id="cb1-45"><a href="mdp.html#cb1-45" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> s <span class="op">==</span> <span class="st">&quot;Pass Exam&quot;</span> <span class="cf">else</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb1-46"><a href="mdp.html#cb1-46" tabindex="-1"></a></span>
<span id="cb1-47"><a href="mdp.html#cb1-47" tabindex="-1"></a><span class="co"># --- Policy: time-invariant, state-independent ------------------------------</span></span>
<span id="cb1-48"><a href="mdp.html#cb1-48" tabindex="-1"></a></span>
<span id="cb1-49"><a href="mdp.html#cb1-49" tabindex="-1"></a><span class="kw">def</span> pi(a: Action, s: State, alpha: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-50"><a href="mdp.html#cb1-50" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;π(a|s): Lazy with prob α, Productive with prob 1-α.&quot;&quot;&quot;</span></span>
<span id="cb1-51"><a href="mdp.html#cb1-51" tabindex="-1"></a>    <span class="cf">return</span> alpha <span class="cf">if</span> a <span class="op">==</span> <span class="st">&quot;Lazy&quot;</span> <span class="cf">else</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha)</span>
<span id="cb1-52"><a href="mdp.html#cb1-52" tabindex="-1"></a></span>
<span id="cb1-53"><a href="mdp.html#cb1-53" tabindex="-1"></a><span class="co"># --- Policy evaluation -------------------------------------------------------</span></span>
<span id="cb1-54"><a href="mdp.html#cb1-54" tabindex="-1"></a></span>
<span id="cb1-55"><a href="mdp.html#cb1-55" tabindex="-1"></a><span class="kw">def</span> policy_evaluation(T: <span class="bu">int</span>, alpha: <span class="bu">float</span>):</span>
<span id="cb1-56"><a href="mdp.html#cb1-56" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-57"><a href="mdp.html#cb1-57" tabindex="-1"></a><span class="co">    Compute {V_t(s)} and {Q_t(s,a)} for t=0..T with terminal condition V_T = Q_T = 0.</span></span>
<span id="cb1-58"><a href="mdp.html#cb1-58" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-59"><a href="mdp.html#cb1-59" tabindex="-1"></a><span class="co">        V: Dict[int, Dict[State, float]]</span></span>
<span id="cb1-60"><a href="mdp.html#cb1-60" tabindex="-1"></a><span class="co">        Q: Dict[int, Dict[Tuple[State, Action], float]]</span></span>
<span id="cb1-61"><a href="mdp.html#cb1-61" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb1-62"><a href="mdp.html#cb1-62" tabindex="-1"></a>    <span class="cf">assert</span> T <span class="op">&gt;=</span> <span class="dv">0</span></span>
<span id="cb1-63"><a href="mdp.html#cb1-63" tabindex="-1"></a>    <span class="co"># sanity: probabilities sum to 1 for each (s,a)</span></span>
<span id="cb1-64"><a href="mdp.html#cb1-64" tabindex="-1"></a>    <span class="cf">for</span> key, rows <span class="kw">in</span> P.items():</span>
<span id="cb1-65"><a href="mdp.html#cb1-65" tabindex="-1"></a>        total <span class="op">=</span> <span class="bu">sum</span>(p <span class="cf">for</span> _, p <span class="kw">in</span> rows)</span>
<span id="cb1-66"><a href="mdp.html#cb1-66" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(total <span class="op">-</span> <span class="fl">1.0</span>) <span class="op">&gt;</span> <span class="fl">1e-9</span>:</span>
<span id="cb1-67"><a href="mdp.html#cb1-67" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f&quot;Probabilities for </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> sum to </span><span class="sc">{</span>total<span class="sc">}</span><span class="ss">, not 1.&quot;</span>)</span>
<span id="cb1-68"><a href="mdp.html#cb1-68" tabindex="-1"></a></span>
<span id="cb1-69"><a href="mdp.html#cb1-69" tabindex="-1"></a>    V: Dict[<span class="bu">int</span>, Dict[State, <span class="bu">float</span>]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb1-70"><a href="mdp.html#cb1-70" tabindex="-1"></a>    Q: Dict[<span class="bu">int</span>, Dict[Tuple[State, Action], <span class="bu">float</span>]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb1-71"><a href="mdp.html#cb1-71" tabindex="-1"></a></span>
<span id="cb1-72"><a href="mdp.html#cb1-72" tabindex="-1"></a>    <span class="co"># Terminal boundary</span></span>
<span id="cb1-73"><a href="mdp.html#cb1-73" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb1-74"><a href="mdp.html#cb1-74" tabindex="-1"></a>        V[T][s] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-75"><a href="mdp.html#cb1-75" tabindex="-1"></a>        <span class="cf">for</span> a <span class="kw">in</span> A:</span>
<span id="cb1-76"><a href="mdp.html#cb1-76" tabindex="-1"></a>            Q[T][(s, a)] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-77"><a href="mdp.html#cb1-77" tabindex="-1"></a></span>
<span id="cb1-78"><a href="mdp.html#cb1-78" tabindex="-1"></a>    <span class="co"># Backward recursion</span></span>
<span id="cb1-79"><a href="mdp.html#cb1-79" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb1-80"><a href="mdp.html#cb1-80" tabindex="-1"></a>        <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb1-81"><a href="mdp.html#cb1-81" tabindex="-1"></a>            <span class="co"># First compute Q_t(s,a)</span></span>
<span id="cb1-82"><a href="mdp.html#cb1-82" tabindex="-1"></a>            <span class="cf">for</span> a <span class="kw">in</span> A:</span>
<span id="cb1-83"><a href="mdp.html#cb1-83" tabindex="-1"></a>                exp_next <span class="op">=</span> <span class="bu">sum</span>(p <span class="op">*</span> V[t <span class="op">+</span> <span class="dv">1</span>][s_next] <span class="cf">for</span> s_next, p <span class="kw">in</span> P[(s, a)])</span>
<span id="cb1-84"><a href="mdp.html#cb1-84" tabindex="-1"></a>                Q[t][(s, a)] <span class="op">=</span> R(s, a) <span class="op">+</span> exp_next</span>
<span id="cb1-85"><a href="mdp.html#cb1-85" tabindex="-1"></a>            <span class="co"># Then V_t(s) = E_{a~π}[Q_t(s,a)]</span></span>
<span id="cb1-86"><a href="mdp.html#cb1-86" tabindex="-1"></a>            V[t][s] <span class="op">=</span> <span class="bu">sum</span>(pi(a, s, alpha) <span class="op">*</span> Q[t][(s, a)] <span class="cf">for</span> a <span class="kw">in</span> A)</span>
<span id="cb1-87"><a href="mdp.html#cb1-87" tabindex="-1"></a></span>
<span id="cb1-88"><a href="mdp.html#cb1-88" tabindex="-1"></a>    <span class="cf">return</span> V, Q</span>
<span id="cb1-89"><a href="mdp.html#cb1-89" tabindex="-1"></a></span>
<span id="cb1-90"><a href="mdp.html#cb1-90" tabindex="-1"></a><span class="co"># --- Example run -------------------------------------------------------------</span></span>
<span id="cb1-91"><a href="mdp.html#cb1-91" tabindex="-1"></a></span>
<span id="cb1-92"><a href="mdp.html#cb1-92" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb1-93"><a href="mdp.html#cb1-93" tabindex="-1"></a>    T <span class="op">=</span> <span class="dv">10</span>        <span class="co"># horizon</span></span>
<span id="cb1-94"><a href="mdp.html#cb1-94" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="fl">0.4</span>  <span class="co"># probability of choosing Lazy</span></span>
<span id="cb1-95"><a href="mdp.html#cb1-95" tabindex="-1"></a>    V, Q <span class="op">=</span> policy_evaluation(T<span class="op">=</span>T, alpha<span class="op">=</span>alpha)</span>
<span id="cb1-96"><a href="mdp.html#cb1-96" tabindex="-1"></a></span>
<span id="cb1-97"><a href="mdp.html#cb1-97" tabindex="-1"></a>    <span class="co"># Print V_0</span></span>
<span id="cb1-98"><a href="mdp.html#cb1-98" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;V_0(s) with T=</span><span class="sc">{</span>T<span class="sc">}</span><span class="ss">, alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">:&quot;</span>)</span>
<span id="cb1-99"><a href="mdp.html#cb1-99" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb1-100"><a href="mdp.html#cb1-100" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  </span><span class="sc">{</span>s<span class="sc">:13s}</span><span class="ss">: </span><span class="sc">{</span>V[<span class="dv">0</span>][s]<span class="sc">: .3f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<p>The code returns the following state values at <span class="math inline">\(t=0\)</span>:
<span class="math display" id="eq:HangoverRandomValueFunction">\[\begin{equation}
V^{\pi}_0 = \begin{bmatrix}
-3.582 \\ -2.306 \\ -2.180 \\ 1.757 \\ 2.939 \\ 10
\end{bmatrix},
\tag{1.8}
\end{equation}\]</span>
where the ordering of the states follows that defined in <span class="math inline">\(\mathcal{S}\)</span>.</p>
<p>You can find the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/hangover_policy_evaluation.py">here</a>.</p>
</div>
</div>
</div>
</div>
<div id="optimality" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Principle of Optimality<a href="mdp.html#optimality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Every policy <span class="math inline">\(\pi\)</span> induces a value function <span class="math inline">\(V_0^{\pi}\)</span> that can be evaluated by policy evaluation (assuming the transition dynamics are known). The goal of reinforcement learning is to find an optimal policy that maximizes the value function with respect to a given initial state distribution:
<span class="math display" id="eq:FiniteHorizonMDPRLProblem">\[\begin{equation}
V^\star_0 = \max_{\pi}\; \mathbb{E}_{s_0 \sim \mu(\cdot)} \big[ V_0^{\pi}(s_0) \big],
\tag{1.9}
\end{equation}\]</span>
where we have used the superscript “<span class="math inline">\(\star\)</span>” to denote the optimality of the value function. <span class="math inline">\(V^\star_0\)</span> is often known as the <em>optimal value function</em>.</p>
<p>At first glance, <a href="mdp.html#eq:FiniteHorizonMDPRLProblem">(1.9)</a> appears daunting: a naive approach would enumerate all stochastic policies <span class="math inline">\(\pi\)</span>, evaluate their value functions, and select the best. A central result in reinforcement learning and optimal control—rooted in the principle of optimality—is that the <em>optimal</em> value functions satisfy a Bellman-style recursion, analogous to Proposition <a href="mdp.html#prp:BellmanConsistency">1.1</a>. This Bellman optimality recursion enables backward computation of the optimal value functions without enumerating policies.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:FiniteHorizonMDPBellmanOptimality" class="theorem"><strong>Theorem 1.1  (Bellman Optimality (Finite Horizon, State-Value)) </strong></span>Consider a finite-horizon MDP <span class="math inline">\(\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,T)\)</span> with finite state and action sets and bounded rewards.
Define the optimal value functions <span class="math inline">\(\{V_t^\star\}_{t=0}^{T}\)</span> by the following <em>Bellman optimality</em> recursion
<span class="math display" id="eq:FiniteHorizonMDPBellmanOptimality">\[\begin{equation}
\begin{split}
V_T^\star(s)&amp; \equiv 0, \\
V_t^\star(s)&amp; = \max_{a\in\mathcal{A}}\Big\{ R(s,a)\;+\;\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^\star(s&#39;)\Big\},\ t=T-1,\ldots,0.
\end{split}
\tag{1.10}
\end{equation}\]</span>
Then, the optimal value functions are optimal in the sense of <em>statewise dominance</em>:
<span class="math display" id="eq:FiniteHorizonMDPStatewiseDominance">\[\begin{equation}
V_t^{\star}(s)\;\ge\; V_t^{\pi}(s)
\quad\text{for all policies }\pi,\; s\in\mathcal{S},\; t=0,\ldots,T.
\tag{1.11}
\end{equation}\]</span></p>
<p>Moreover, the deterministic policy
<span class="math inline">\(\pi^\star=(\pi^\star_0,\ldots,\pi^\star_{T-1})\)</span> with
<span class="math display" id="eq:FiniteHorizonMDPOptimalPolicy">\[\begin{equation}
\pi_t^\star(s)\;\in\;\arg\max_{a\in\mathcal{A}}
\Big\{ R(s,a)\;+\;\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^\star(s&#39;)\Big\}, \ \forall s\in\mathcal{S},\; t\in[T-1]
\tag{1.12}
\end{equation}\]</span>
is optimal, where ties can be broken by any fixed rule.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>We first show that the value functions defined by the Bellman optimality recursion <a href="mdp.html#eq:FiniteHorizonMDPBellmanOptimality">(1.10)</a> are <em>optimal</em> in the sense that they dominate the value functions of any other policy. The proof proceeds by backward induction.</p>
<p><strong>Base case (<span class="math inline">\(t=T\)</span>).</strong>
For every <span class="math inline">\(s\in\mathcal{S}\)</span>,
<span class="math display">\[
V^\star_T(s)\;=\;0\;=\;V_T^{\pi}(s),
\]</span>
so <span class="math inline">\(V^\star_T(s)\ge V_T^{\pi}(s)\)</span> holds trivially.</p>
<p><strong>Inductive step.</strong>
Assume <span class="math inline">\(V^\star_{t+1}(s)\ge V^{\pi}_{t+1}(s)\)</span> for all <span class="math inline">\(s\in\mathcal{S}\)</span>. Then, for any <span class="math inline">\(s\in\mathcal{S}\)</span>,
<span class="math display">\[\begin{align*}
V_t^{\pi}(s)
&amp;= \sum_{a\in\mathcal{A}} \pi_t(a\mid s)\!\left(R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^{\pi}(s&#39;)\right) \\
&amp;\le \sum_{a\in\mathcal{A}} \pi_t(a\mid s)\!\left(R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^{\star}(s&#39;)\right) \\
&amp;\le \max_{a\in\mathcal{A}} \left(R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^{\star}(s&#39;)\right)
\;=\; V_t^\star(s),
\end{align*}\]</span>
where the first inequality uses the induction hypothesis and the second uses that an expectation is bounded above by a maximum. Hence <span class="math inline">\(V_t^\star(s)\ge V_t^{\pi}(s)\)</span> for all <span class="math inline">\(s\)</span>, completing the induction. Therefore, <span class="math inline">\(\{V_t^\star\}_{t=0}^T\)</span> dominates the value functions attainable by any policy.</p>
<p>Next, we show that <span class="math inline">\(\{V_t^\star\}\)</span> is <em>attainable</em> by some policy. Since <span class="math inline">\(\mathcal{A}\)</span> is finite (tabular setting), the maximizer in the Bellman optimality operator exists for every <span class="math inline">\((t,s)\)</span>; thus we can define a (deterministic) greedy policy
<span class="math display">\[
\pi_t^\star(s)\;\in\;\arg\max_{a\in\mathcal{A}}
\Big\{ R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^\star(s&#39;) \Big\}.
\]</span>
A simple backward induction then shows <span class="math inline">\(V_t^{\pi^\star}(s)=V_t^\star(s)\)</span> for all <span class="math inline">\(t\)</span> and <span class="math inline">\(s\)</span>: at <span class="math inline">\(t=T\)</span> both are <span class="math inline">\(0\)</span>, and if <span class="math inline">\(V_{t+1}^{\pi^\star}=V_{t+1}^\star\)</span>, then by construction of <span class="math inline">\(\pi_t^\star\)</span> the Bellman equality yields <span class="math inline">\(V_t^{\pi^\star}=V_t^\star\)</span>. Consequently, the optimal value functions are achieved by the greedy (deterministic) policy <span class="math inline">\(\pi^\star\)</span>.</p>
</div>
</div>
<div class="theorembox">
<div class="corollary">
<p><span id="cor:FiniteHorizonMDPBellmanOptimalityActionValue" class="corollary"><strong>Corollary 1.1  (Bellman Optimality (Finite Horizon, Action-Value)) </strong></span>Given the optimal (state-)value functions <span class="math inline">\(V^{\star}_{t},t=0,\dots,T\)</span>, define the optimal action-value function
<span class="math display" id="eq:FiniteHorizonMDPOptimalActionValue">\[\begin{equation}
Q_t^\star(s,a)\;=\;R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^\star(s&#39;), \quad t=0,\dots,T-1.
\tag{1.13}
\end{equation}\]</span></p>
<p>Then we have
<span class="math display" id="eq:FiniteHorizonMDPStateValueActionValue">\[\begin{equation}
V_t^\star(s)=\max_{a\in\mathcal{A}} Q_t^\star(s,a),\qquad
\pi_t^\star(s)\in\arg\max_{a\in\mathcal{A}} Q_t^\star(s,a).
\tag{1.14}
\end{equation}\]</span></p>
<p>The optimal action-value functions satisfy:
<span class="math display" id="eq:FiniteHorizonMDPBellmanRecursionActionValue">\[\begin{equation}
\begin{split}
Q_T^\star(s,a) &amp; \equiv 0,\\
Q_t^\star(s,a)
&amp; = R(s,a) \;+\; \mathbb{E}_{s&#39;\sim P(\cdot\mid s,a)}
\!\left[ \max_{a&#39;\in\mathcal{A}} Q_{t+1}^\star(s&#39;,a&#39;) \right],
\quad t=T-1,\ldots,0.
\end{split}
\tag{1.15}
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="dp" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Dynamic Programming<a href="mdp.html#dp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The principle of optimality in Theorem <a href="mdp.html#thm:FiniteHorizonMDPBellmanOptimality">1.1</a> yields a constructive procedure to compute the optimal value functions and an associated deterministic optimal policy. This backward-induction procedure is the <em>dynamic programming</em> (DP) algorithm.</p>
<p><strong>Dynamic programming (finite horizon).</strong></p>
<ul>
<li><p><strong>Initialization.</strong> Set <span class="math inline">\(V_T^\star(s) = 0\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>.</p></li>
<li><p><strong>Backward recursion.</strong> For <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>:</p>
<ul>
<li><p><em>Optimal value:</em> for each <span class="math inline">\(s \in \mathcal{S}\)</span>,
<span class="math display">\[
V_t^\star(s)
= \max_{a \in \mathcal{A}}
\left\{
R(s,a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s,a)} \big[ V_{t+1}^\star(s&#39;) \big]
\right\}.
\]</span></p></li>
<li><p><em>Greedy policy (deterministic):</em> for each <span class="math inline">\(s \in \mathcal{S}\)</span>,
<span class="math display">\[
\pi_t^\star(s) \in \arg\max_{a \in \mathcal{A}}
\left\{
R(s,a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s,a)} \big[ V_{t+1}^\star(s&#39;) \big]
\right\}.
\]</span></p></li>
</ul></li>
</ul>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 1.1  </strong></span>How does dynamic programming look like when applied to the action-value function?</p>
</div>
</div>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-3" class="exercise"><strong>Exercise 1.2  </strong></span>What is the computational complexity of dynamic programming?</p>
</div>
</div>
<p>Let us try dynamic programming for the Hangover MDP presented before.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:HangoverDynamicProgramming" class="example"><strong>Example 1.2  (Dynamic Programming for Hangover MDP) </strong></span>Consider the Hangover MDP defined by the transition graph shown in Fig. <a href="mdp.html#fig:mdp-hangover-transition-graph">1.2</a>. With slight modification to the policy evaluation code, we can find the optimal value functions and optimal policies.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="mdp.html#cb2-1" tabindex="-1"></a><span class="co"># Dynamic programming (finite-horizon optimal control) for the Hangover MDP</span></span>
<span id="cb2-2"><a href="mdp.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="mdp.html#cb2-3" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb2-4"><a href="mdp.html#cb2-4" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List, Tuple</span>
<span id="cb2-5"><a href="mdp.html#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="mdp.html#cb2-6" tabindex="-1"></a>State <span class="op">=</span> <span class="bu">str</span></span>
<span id="cb2-7"><a href="mdp.html#cb2-7" tabindex="-1"></a>Action <span class="op">=</span> <span class="bu">str</span></span>
<span id="cb2-8"><a href="mdp.html#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="mdp.html#cb2-9" tabindex="-1"></a><span class="co"># --- MDP spec ---------------------------------------------------------------</span></span>
<span id="cb2-10"><a href="mdp.html#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="mdp.html#cb2-11" tabindex="-1"></a>S: List[State] <span class="op">=</span> [</span>
<span id="cb2-12"><a href="mdp.html#cb2-12" tabindex="-1"></a>    <span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Pass Exam&quot;</span></span>
<span id="cb2-13"><a href="mdp.html#cb2-13" tabindex="-1"></a>]</span>
<span id="cb2-14"><a href="mdp.html#cb2-14" tabindex="-1"></a>A: List[Action] <span class="op">=</span> [<span class="st">&quot;Lazy&quot;</span>, <span class="st">&quot;Productive&quot;</span>]</span>
<span id="cb2-15"><a href="mdp.html#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="mdp.html#cb2-16" tabindex="-1"></a><span class="co"># P[s, a] -&gt; list of (s_next, prob)</span></span>
<span id="cb2-17"><a href="mdp.html#cb2-17" tabindex="-1"></a>P: Dict[Tuple[State, Action], List[Tuple[State, <span class="bu">float</span>]]] <span class="op">=</span> {</span>
<span id="cb2-18"><a href="mdp.html#cb2-18" tabindex="-1"></a>    <span class="co"># Hangover</span></span>
<span id="cb2-19"><a href="mdp.html#cb2-19" tabindex="-1"></a>    (<span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-20"><a href="mdp.html#cb2-20" tabindex="-1"></a>    (<span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Visit Lecture&quot;</span>, <span class="fl">0.3</span>), (<span class="st">&quot;Hangover&quot;</span>, <span class="fl">0.7</span>)],</span>
<span id="cb2-21"><a href="mdp.html#cb2-21" tabindex="-1"></a></span>
<span id="cb2-22"><a href="mdp.html#cb2-22" tabindex="-1"></a>    <span class="co"># Sleep</span></span>
<span id="cb2-23"><a href="mdp.html#cb2-23" tabindex="-1"></a>    (<span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):          [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-24"><a href="mdp.html#cb2-24" tabindex="-1"></a>    (<span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;Productive&quot;</span>):    [(<span class="st">&quot;Visit Lecture&quot;</span>, <span class="fl">0.6</span>), (<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">0.4</span>)],</span>
<span id="cb2-25"><a href="mdp.html#cb2-25" tabindex="-1"></a></span>
<span id="cb2-26"><a href="mdp.html#cb2-26" tabindex="-1"></a>    <span class="co"># More Sleep</span></span>
<span id="cb2-27"><a href="mdp.html#cb2-27" tabindex="-1"></a>    (<span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-28"><a href="mdp.html#cb2-28" tabindex="-1"></a>    (<span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Study&quot;</span>, <span class="fl">0.5</span>), (<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">0.5</span>)],</span>
<span id="cb2-29"><a href="mdp.html#cb2-29" tabindex="-1"></a></span>
<span id="cb2-30"><a href="mdp.html#cb2-30" tabindex="-1"></a>    <span class="co"># Visit Lecture</span></span>
<span id="cb2-31"><a href="mdp.html#cb2-31" tabindex="-1"></a>    (<span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Study&quot;</span>, <span class="fl">0.8</span>), (<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">0.2</span>)],</span>
<span id="cb2-32"><a href="mdp.html#cb2-32" tabindex="-1"></a>    (<span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Study&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-33"><a href="mdp.html#cb2-33" tabindex="-1"></a></span>
<span id="cb2-34"><a href="mdp.html#cb2-34" tabindex="-1"></a>    <span class="co"># Study</span></span>
<span id="cb2-35"><a href="mdp.html#cb2-35" tabindex="-1"></a>    (<span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):         [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-36"><a href="mdp.html#cb2-36" tabindex="-1"></a>    (<span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Productive&quot;</span>):   [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">0.9</span>), (<span class="st">&quot;Study&quot;</span>, <span class="fl">0.1</span>)],</span>
<span id="cb2-37"><a href="mdp.html#cb2-37" tabindex="-1"></a></span>
<span id="cb2-38"><a href="mdp.html#cb2-38" tabindex="-1"></a>    <span class="co"># Pass Exam (absorbing)</span></span>
<span id="cb2-39"><a href="mdp.html#cb2-39" tabindex="-1"></a>    (<span class="st">&quot;Pass Exam&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-40"><a href="mdp.html#cb2-40" tabindex="-1"></a>    (<span class="st">&quot;Pass Exam&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-41"><a href="mdp.html#cb2-41" tabindex="-1"></a>}</span>
<span id="cb2-42"><a href="mdp.html#cb2-42" tabindex="-1"></a></span>
<span id="cb2-43"><a href="mdp.html#cb2-43" tabindex="-1"></a><span class="kw">def</span> R(s: State, a: Action) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb2-44"><a href="mdp.html#cb2-44" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot;</span></span>
<span id="cb2-45"><a href="mdp.html#cb2-45" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> s <span class="op">==</span> <span class="st">&quot;Pass Exam&quot;</span> <span class="cf">else</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb2-46"><a href="mdp.html#cb2-46" tabindex="-1"></a></span>
<span id="cb2-47"><a href="mdp.html#cb2-47" tabindex="-1"></a><span class="co"># --- Dynamic programming (Bellman optimality) -------------------------------</span></span>
<span id="cb2-48"><a href="mdp.html#cb2-48" tabindex="-1"></a></span>
<span id="cb2-49"><a href="mdp.html#cb2-49" tabindex="-1"></a><span class="kw">def</span> dynamic_programming(T: <span class="bu">int</span>):</span>
<span id="cb2-50"><a href="mdp.html#cb2-50" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-51"><a href="mdp.html#cb2-51" tabindex="-1"></a><span class="co">    Compute optimal finite-horizon tables:</span></span>
<span id="cb2-52"><a href="mdp.html#cb2-52" tabindex="-1"></a><span class="co">      - V[t][s] = V_t^*(s)</span></span>
<span id="cb2-53"><a href="mdp.html#cb2-53" tabindex="-1"></a><span class="co">      - Q[t][(s,a)] = Q_t^*(s,a)</span></span>
<span id="cb2-54"><a href="mdp.html#cb2-54" tabindex="-1"></a><span class="co">      - PI[t][s] = optimal action at (t,s)</span></span>
<span id="cb2-55"><a href="mdp.html#cb2-55" tabindex="-1"></a><span class="co">    with terminal condition V_T^* = 0.</span></span>
<span id="cb2-56"><a href="mdp.html#cb2-56" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb2-57"><a href="mdp.html#cb2-57" tabindex="-1"></a>    <span class="cf">assert</span> T <span class="op">&gt;=</span> <span class="dv">0</span></span>
<span id="cb2-58"><a href="mdp.html#cb2-58" tabindex="-1"></a></span>
<span id="cb2-59"><a href="mdp.html#cb2-59" tabindex="-1"></a>    <span class="co"># sanity: probabilities sum to 1 for each (s,a)</span></span>
<span id="cb2-60"><a href="mdp.html#cb2-60" tabindex="-1"></a>    <span class="cf">for</span> key, rows <span class="kw">in</span> P.items():</span>
<span id="cb2-61"><a href="mdp.html#cb2-61" tabindex="-1"></a>        total <span class="op">=</span> <span class="bu">sum</span>(p <span class="cf">for</span> _, p <span class="kw">in</span> rows)</span>
<span id="cb2-62"><a href="mdp.html#cb2-62" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(total <span class="op">-</span> <span class="fl">1.0</span>) <span class="op">&gt;</span> <span class="fl">1e-9</span>:</span>
<span id="cb2-63"><a href="mdp.html#cb2-63" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f&quot;Probabilities for </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> sum to </span><span class="sc">{</span>total<span class="sc">}</span><span class="ss">, not 1.&quot;</span>)</span>
<span id="cb2-64"><a href="mdp.html#cb2-64" tabindex="-1"></a></span>
<span id="cb2-65"><a href="mdp.html#cb2-65" tabindex="-1"></a>    V: Dict[<span class="bu">int</span>, Dict[State, <span class="bu">float</span>]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb2-66"><a href="mdp.html#cb2-66" tabindex="-1"></a>    Q: Dict[<span class="bu">int</span>, Dict[Tuple[State, Action], <span class="bu">float</span>]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb2-67"><a href="mdp.html#cb2-67" tabindex="-1"></a>    PI: Dict[<span class="bu">int</span>, Dict[State, Action]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb2-68"><a href="mdp.html#cb2-68" tabindex="-1"></a></span>
<span id="cb2-69"><a href="mdp.html#cb2-69" tabindex="-1"></a>    <span class="co"># Terminal boundary</span></span>
<span id="cb2-70"><a href="mdp.html#cb2-70" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-71"><a href="mdp.html#cb2-71" tabindex="-1"></a>        V[T][s] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-72"><a href="mdp.html#cb2-72" tabindex="-1"></a>        <span class="cf">for</span> a <span class="kw">in</span> A:</span>
<span id="cb2-73"><a href="mdp.html#cb2-73" tabindex="-1"></a>            Q[T][(s, a)] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-74"><a href="mdp.html#cb2-74" tabindex="-1"></a></span>
<span id="cb2-75"><a href="mdp.html#cb2-75" tabindex="-1"></a>    <span class="co"># Backward recursion (Bellman optimality)</span></span>
<span id="cb2-76"><a href="mdp.html#cb2-76" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb2-77"><a href="mdp.html#cb2-77" tabindex="-1"></a>        <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-78"><a href="mdp.html#cb2-78" tabindex="-1"></a>            <span class="co"># compute Q*_t(s,a)</span></span>
<span id="cb2-79"><a href="mdp.html#cb2-79" tabindex="-1"></a>            <span class="cf">for</span> a <span class="kw">in</span> A:</span>
<span id="cb2-80"><a href="mdp.html#cb2-80" tabindex="-1"></a>                exp_next <span class="op">=</span> <span class="bu">sum</span>(p <span class="op">*</span> V[t <span class="op">+</span> <span class="dv">1</span>][s_next] <span class="cf">for</span> s_next, p <span class="kw">in</span> P[(s, a)])</span>
<span id="cb2-81"><a href="mdp.html#cb2-81" tabindex="-1"></a>                Q[t][(s, a)] <span class="op">=</span> R(s, a) <span class="op">+</span> exp_next</span>
<span id="cb2-82"><a href="mdp.html#cb2-82" tabindex="-1"></a></span>
<span id="cb2-83"><a href="mdp.html#cb2-83" tabindex="-1"></a>            <span class="co"># greedy action and optimal value</span></span>
<span id="cb2-84"><a href="mdp.html#cb2-84" tabindex="-1"></a>            <span class="co"># tie-breaking is deterministic by the order in A</span></span>
<span id="cb2-85"><a href="mdp.html#cb2-85" tabindex="-1"></a>            best_a <span class="op">=</span> <span class="bu">max</span>(A, key<span class="op">=</span><span class="kw">lambda</span> a: Q[t][(s, a)])</span>
<span id="cb2-86"><a href="mdp.html#cb2-86" tabindex="-1"></a>            PI[t][s] <span class="op">=</span> best_a</span>
<span id="cb2-87"><a href="mdp.html#cb2-87" tabindex="-1"></a>            V[t][s] <span class="op">=</span> Q[t][(s, best_a)]</span>
<span id="cb2-88"><a href="mdp.html#cb2-88" tabindex="-1"></a></span>
<span id="cb2-89"><a href="mdp.html#cb2-89" tabindex="-1"></a>    <span class="cf">return</span> V, Q, PI</span>
<span id="cb2-90"><a href="mdp.html#cb2-90" tabindex="-1"></a></span>
<span id="cb2-91"><a href="mdp.html#cb2-91" tabindex="-1"></a><span class="co"># --- Example run -------------------------------------------------------------</span></span>
<span id="cb2-92"><a href="mdp.html#cb2-92" tabindex="-1"></a></span>
<span id="cb2-93"><a href="mdp.html#cb2-93" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb2-94"><a href="mdp.html#cb2-94" tabindex="-1"></a>    T <span class="op">=</span> <span class="dv">10</span>  <span class="co"># horizon</span></span>
<span id="cb2-95"><a href="mdp.html#cb2-95" tabindex="-1"></a>    V, Q, PI <span class="op">=</span> dynamic_programming(T<span class="op">=</span>T)</span>
<span id="cb2-96"><a href="mdp.html#cb2-96" tabindex="-1"></a></span>
<span id="cb2-97"><a href="mdp.html#cb2-97" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Optimal V_0(s) with T=</span><span class="sc">{</span>T<span class="sc">}</span><span class="ss">:&quot;</span>)</span>
<span id="cb2-98"><a href="mdp.html#cb2-98" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-99"><a href="mdp.html#cb2-99" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  </span><span class="sc">{</span>s<span class="sc">:13s}</span><span class="ss">: </span><span class="sc">{</span>V[<span class="dv">0</span>][s]<span class="sc">: .3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb2-100"><a href="mdp.html#cb2-100" tabindex="-1"></a></span>
<span id="cb2-101"><a href="mdp.html#cb2-101" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Greedy policy at t=0:&quot;</span>)</span>
<span id="cb2-102"><a href="mdp.html#cb2-102" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-103"><a href="mdp.html#cb2-103" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  </span><span class="sc">{</span>s<span class="sc">:13s}</span><span class="ss">: </span><span class="sc">{</span>PI[<span class="dv">0</span>][s]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb2-104"><a href="mdp.html#cb2-104" tabindex="-1"></a></span>
<span id="cb2-105"><a href="mdp.html#cb2-105" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Action value at t=0:&quot;</span>)</span>
<span id="cb2-106"><a href="mdp.html#cb2-106" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-107"><a href="mdp.html#cb2-107" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  </span><span class="sc">{</span>s<span class="sc">:13s}</span><span class="ss">: </span><span class="sc">{</span>Q[<span class="dv">0</span>][s, A[<span class="dv">0</span>]]<span class="sc">: .3f}</span><span class="ss">, </span><span class="sc">{</span>Q[<span class="dv">0</span>][s, A[<span class="dv">1</span>]]<span class="sc">: .3f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<p>The optimal value function at <span class="math inline">\(t=0\)</span> is:
<span class="math display" id="eq:HangoverOptimalValueFunction">\[\begin{equation}
V^\star_0 = \begin{bmatrix}
1.259 \\
3.251 \\
3.787 \\
6.222 \\
7.778 \\
10
\end{bmatrix}.
\tag{1.16}
\end{equation}\]</span>
Clearly, the optimal value function dominates the value function shown in <a href="mdp.html#eq:HangoverRandomValueFunction">(1.8)</a> of the random policy at every state.</p>
<p>The optimal actions at <span class="math inline">\(t=0\)</span> are:
<span class="math display">\[\begin{equation}
\begin{split}
\text{Hangover} &amp; : \text{Lazy} \\
\text{Sleep}        &amp; : \text{Productive} \\
\text{More Sleep}   &amp; : \text{Productive} \\
\text{Visit Lecture} &amp; : \text{Lazy} \\
\text{Study}      &amp; : \text{Productive} \\
\text{Pass Exam}    &amp; : \text{Lazy}
\end{split}.
\end{equation}\]</span></p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/hangover_dynamic_programming.py">here</a>.</p>
</div>
</div>
</div>
<div id="InfiniteHorizonMDP" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Infinite-Horizon MDP<a href="mdp.html#InfiniteHorizonMDP" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>



</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appconvex.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/01-mdp.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
