<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-11-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="value-rl.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.1.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.1.4" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.1.4</b> Dynamic Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#InfiniteHorizonMDP"><i class="fa fa-check"></i><b>1.2</b> Infinite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mdp.html"><a href="mdp.html#value-functions"><i class="fa fa-check"></i><b>1.2.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation-1"><i class="fa fa-check"></i><b>1.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="1.2.3" data-path="mdp.html"><a href="mdp.html#principle-of-optimality"><i class="fa fa-check"></i><b>1.2.3</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.2.4" data-path="mdp.html"><a href="mdp.html#policy-improvement"><i class="fa fa-check"></i><b>1.2.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="1.2.5" data-path="mdp.html"><a href="mdp.html#policy-iteration"><i class="fa fa-check"></i><b>1.2.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="1.2.6" data-path="mdp.html"><a href="mdp.html#value-iteration"><i class="fa fa-check"></i><b>1.2.6</b> Value Iteration</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="value-rl.html"><a href="value-rl.html"><i class="fa fa-check"></i><b>2</b> Value-based Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="value-rl.html"><a href="value-rl.html#tabular-methods"><i class="fa fa-check"></i><b>2.1</b> Tabular Methods</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="value-rl.html"><a href="value-rl.html#tabular-PE"><i class="fa fa-check"></i><b>2.1.1</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.1.2" data-path="value-rl.html"><a href="value-rl.html#value-rl-convergence-td"><i class="fa fa-check"></i><b>2.1.2</b> Convergence Proof of TD Learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control"><i class="fa fa-check"></i><b>2.1.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.1.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control"><i class="fa fa-check"></i><b>2.1.4</b> Off-Policy Control</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="value-rl.html"><a href="value-rl.html#function-approximation"><i class="fa fa-check"></i><b>2.2</b> Function Approximation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="value-rl.html"><a href="value-rl.html#basics-of-continuous-mdp"><i class="fa fa-check"></i><b>2.2.1</b> Basics of Continuous MDP</a></li>
<li class="chapter" data-level="2.2.2" data-path="value-rl.html"><a href="value-rl.html#policy-evaluation-2"><i class="fa fa-check"></i><b>2.2.2</b> Policy Evaluation</a></li>
<li class="chapter" data-level="2.2.3" data-path="value-rl.html"><a href="value-rl.html#on-policy-control-1"><i class="fa fa-check"></i><b>2.2.3</b> On-Policy Control</a></li>
<li class="chapter" data-level="2.2.4" data-path="value-rl.html"><a href="value-rl.html#off-policy-control-1"><i class="fa fa-check"></i><b>2.2.4</b> Off-Policy Control</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="policy-gradient.html"><a href="policy-gradient.html"><i class="fa fa-check"></i><b>3</b> Policy Gradient Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-optimization"><i class="fa fa-check"></i><b>3.1</b> Gradient-based Optimization</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="policy-gradient.html"><a href="policy-gradient.html#basic-setup"><i class="fa fa-check"></i><b>3.1.1</b> Basic Setup</a></li>
<li class="chapter" data-level="3.1.2" data-path="policy-gradient.html"><a href="policy-gradient.html#gradient-ascent-and-descent"><i class="fa fa-check"></i><b>3.1.2</b> Gradient Ascent and Descent</a></li>
<li class="chapter" data-level="3.1.3" data-path="policy-gradient.html"><a href="policy-gradient.html#stochastic-gradients"><i class="fa fa-check"></i><b>3.1.3</b> Stochastic Gradients</a></li>
<li class="chapter" data-level="3.1.4" data-path="policy-gradient.html"><a href="policy-gradient.html#beyond-vanilla-gradient-methods"><i class="fa fa-check"></i><b>3.1.4</b> Beyond Vanilla Gradient Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#policy-gradients"><i class="fa fa-check"></i><b>3.2</b> Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="policy-gradient.html"><a href="policy-gradient.html#setup"><i class="fa fa-check"></i><b>3.2.1</b> Setup</a></li>
<li class="chapter" data-level="3.2.2" data-path="policy-gradient.html"><a href="policy-gradient.html#the-policy-gradient-lemma"><i class="fa fa-check"></i><b>3.2.2</b> The Policy Gradient Lemma</a></li>
<li class="chapter" data-level="3.2.3" data-path="policy-gradient.html"><a href="policy-gradient.html#reinforce"><i class="fa fa-check"></i><b>3.2.3</b> REINFORCE</a></li>
<li class="chapter" data-level="3.2.4" data-path="policy-gradient.html"><a href="policy-gradient.html#baselines-and-variance-reduction"><i class="fa fa-check"></i><b>3.2.4</b> Baselines and Variance Reduction</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#actorcritic-methods"><i class="fa fa-check"></i><b>3.3</b> Actor–Critic Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="policy-gradient.html"><a href="policy-gradient.html#anatomy-of-an-actorcritic"><i class="fa fa-check"></i><b>3.3.1</b> Anatomy of an Actor–Critic</a></li>
<li class="chapter" data-level="3.3.2" data-path="policy-gradient.html"><a href="policy-gradient.html#ActorCriticTD"><i class="fa fa-check"></i><b>3.3.2</b> On-Policy Actor–Critic with TD(0)</a></li>
<li class="chapter" data-level="3.3.3" data-path="policy-gradient.html"><a href="policy-gradient.html#PG-GAE"><i class="fa fa-check"></i><b>3.3.3</b> Generalized Advantage Estimation (GAE)</a></li>
<li class="chapter" data-level="3.3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#off-policy-actorcritic"><i class="fa fa-check"></i><b>3.3.4</b> Off-Policy Actor–Critic</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="policy-gradient.html"><a href="policy-gradient.html#advanced-policy-gradients"><i class="fa fa-check"></i><b>3.4</b> Advanced Policy Gradients</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="policy-gradient.html"><a href="policy-gradient.html#revisiting-generalized-policy-iteration"><i class="fa fa-check"></i><b>3.4.1</b> Revisiting Generalized Policy Iteration</a></li>
<li class="chapter" data-level="3.4.2" data-path="policy-gradient.html"><a href="policy-gradient.html#performance-difference-lemma"><i class="fa fa-check"></i><b>3.4.2</b> Performance Difference Lemma</a></li>
<li class="chapter" data-level="3.4.3" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-constraint"><i class="fa fa-check"></i><b>3.4.3</b> Trust Region Constraint</a></li>
<li class="chapter" data-level="3.4.4" data-path="policy-gradient.html"><a href="policy-gradient.html#natural-policy-gradient"><i class="fa fa-check"></i><b>3.4.4</b> Natural Policy Gradient</a></li>
<li class="chapter" data-level="3.4.5" data-path="policy-gradient.html"><a href="policy-gradient.html#proof-fisher"><i class="fa fa-check"></i><b>3.4.5</b> Proof of Fisher Information</a></li>
<li class="chapter" data-level="3.4.6" data-path="policy-gradient.html"><a href="policy-gradient.html#trust-region-policy-optimization"><i class="fa fa-check"></i><b>3.4.6</b> Trust Region Policy Optimization</a></li>
<li class="chapter" data-level="3.4.7" data-path="policy-gradient.html"><a href="policy-gradient.html#proximal-policy-optimization"><i class="fa fa-check"></i><b>3.4.7</b> Proximal Policy Optimization</a></li>
<li class="chapter" data-level="3.4.8" data-path="policy-gradient.html"><a href="policy-gradient.html#soft-actorcritic"><i class="fa fa-check"></i><b>3.4.8</b> Soft Actor–Critic</a></li>
<li class="chapter" data-level="3.4.9" data-path="policy-gradient.html"><a href="policy-gradient.html#deterministic-policy-gradient"><i class="fa fa-check"></i><b>3.4.9</b> Deterministic Policy Gradient</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="policy-gradient.html"><a href="policy-gradient.html#model-based-policy-optimization"><i class="fa fa-check"></i><b>3.5</b> Model-based Policy Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html"><i class="fa fa-check"></i><b>4</b> Model-based Planning and Optimization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr"><i class="fa fa-check"></i><b>4.1</b> Linear Quadratic Regulator</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#finite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.1</b> Finite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#infinite-horizon-lqr"><i class="fa fa-check"></i><b>4.1.2</b> Infinite-Horizon LQR</a></li>
<li class="chapter" data-level="4.1.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#linear-system-basics"><i class="fa fa-check"></i><b>4.1.3</b> Linear System Basics</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#lqr-tracking"><i class="fa fa-check"></i><b>4.2</b> LQR Trajectory Tracking</a></li>
<li class="chapter" data-level="4.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#traj-opt"><i class="fa fa-check"></i><b>4.3</b> Trajectory Optimization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#traj-opt-ilqr"><i class="fa fa-check"></i><b>4.3.1</b> Iterative LQR</a></li>
<li class="chapter" data-level="4.3.2" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#ddp"><i class="fa fa-check"></i><b>4.3.2</b> Differential Dynamic Programming</a></li>
<li class="chapter" data-level="4.3.3" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#qp"><i class="fa fa-check"></i><b>4.3.3</b> Quadratic Programming</a></li>
<li class="chapter" data-level="4.3.4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#sqp"><i class="fa fa-check"></i><b>4.3.4</b> Sequential Quadratic Programming</a></li>
<li class="chapter" data-level="4.3.5" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#ipm-nlp"><i class="fa fa-check"></i><b>4.3.5</b> Interior-Point Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="model-based-plan-optimize.html"><a href="model-based-plan-optimize.html#mpc"><i class="fa fa-check"></i><b>4.4</b> Model Predictive Control</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-materials.html"><a href="advanced-materials.html"><i class="fa fa-check"></i><b>5</b> Advanced Materials</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrangian-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mdp" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Markov Decision Process<a href="mdp.html#mdp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Optimal control (OC) and reinforcement learning (RL) address the problem of making <strong>optimal decisions</strong> in the presence of a <strong>dynamic environment</strong>.</p>
<ul>
<li>In <strong>optimal control</strong>, this dynamic environment is often referred to as a <em>plant</em> or a <em>dynamical system</em>.<br />
</li>
<li>In <strong>reinforcement learning</strong>, it is modeled as a <em>Markov decision process</em> (MDP).</li>
</ul>
<p>The goal in both fields is to evaluate and design decision-making strategies that optimize long-term performance:</p>
<ul>
<li><strong>RL</strong> typically frames this as maximizing a long-term <em>reward</em>.<br />
</li>
<li><strong>OC</strong> often formulates it as minimizing a long-term <em>cost</em>.</li>
</ul>
<p>The emphasis on <strong>long-term</strong> evaluation is crucial. Because the environment evolves over time, decisions that appear beneficial in the short term may lead to poor long-term outcomes and thus be suboptimal.</p>
<hr />
<p>With this motivation, we now formalize the framework of Markov Decision Processes (MDPs), which are discrete-time stochastic dynamical systems.</p>
<div id="FiniteHorizonMDP" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Finite-Horizon MDP<a href="mdp.html#FiniteHorizonMDP" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We begin with finite-horizon MDPs and introduce infinite-horizon MDPs in the following section. An abstract definition of the finite-horizon case will be presented first, followed by illustrative examples.</p>
<p>A finite-horizon MDP is given by the following tuple:
<span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, T),
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span>: state space (set of all possible states)</li>
<li><span class="math inline">\(\mathcal{A}\)</span>: action space (set of all possible actions)</li>
<li><span class="math inline">\(P(s&#39; \mid s, a)\)</span>: probability of transitioning to state <span class="math inline">\(s&#39;\)</span> from state <span class="math inline">\(s\)</span> under action <span class="math inline">\(a\)</span> (i.e., dynamics)</li>
<li><span class="math inline">\(R(s,a)\)</span>: reward of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(T\)</span>: horizon, a positive integer</li>
</ul>
<p>For now, let us assume both the state space and the action space are discrete and have a finite number of elements. In particular, denote the number of elements in <span class="math inline">\(\mathcal{S}\)</span> as <span class="math inline">\(|\mathcal{S}|\)</span>, and the number of elements in <span class="math inline">\(\mathcal{A}\)</span> as <span class="math inline">\(|\mathcal{A}|\)</span>. This is also referred to as a <em>tabular MDP</em>.</p>
<p><strong>Policy</strong>. Decision-making in MDPs is represented by policies. A policy is a function that, given any state, outputs a distribution of actions: <span class="math inline">\(\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})\)</span>. That is, <span class="math inline">\(\pi(a \mid s)\)</span> returns the probability of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>. In finite-horizon MDPs, we consider a tuple of policies:
<span class="math display" id="eq:policy-tuple">\[\begin{equation}
\pi = (\pi_0, \dots, \pi_t, \dots, \pi_{T-1}),
\tag{1.1}
\end{equation}\]</span>
where each <span class="math inline">\(\pi_t\)</span> denotes the policy at step <span class="math inline">\(t \in [0,T-1]\)</span>.</p>
<p><strong>Trajectory and Return</strong>. Given an initial state <span class="math inline">\(s_0 \in \mathcal{S}\)</span> and a policy <span class="math inline">\(\pi\)</span>, the MDP will evolve as</p>
<ol style="list-style-type: decimal">
<li>Start at state <span class="math inline">\(s_0\)</span></li>
<li>Take action <span class="math inline">\(a_0 \sim \pi_0(a \mid s_0)\)</span> following policy <span class="math inline">\(\pi_0\)</span></li>
<li>Collect reward <span class="math inline">\(r_0 = R(s_0, a_0)\)</span> (assume <span class="math inline">\(R\)</span> is deterministic)</li>
<li>Transition to state <span class="math inline">\(s_1 \sim P(s&#39; \mid s_0, a_0)\)</span> following the dynamics</li>
<li>Go to step 2 and continue until reaching state <span class="math inline">\(s_T\)</span></li>
</ol>
<p>This evolution generates a trajectory of states, actions, and rewards:
<span class="math display">\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T).
\]</span>
The cumulative reward of this trajectory is <span class="math inline">\(g_0 = \sum_{t=0}^{T-1} r_t\)</span>, which is called the <em>return</em> of the trajectory. Clearly, <span class="math inline">\(g_0\)</span> is a random variable due to the stochasticity of both the policy and the dynamics. Similarly, if the state at time <span class="math inline">\(t\)</span> is <span class="math inline">\(s_t\)</span>, we denote:
<span class="math display">\[
g_t = r_t + \dots + r_{T-1}
\]</span>
as the return of the policy starting at <span class="math inline">\(s_t\)</span>.</p>
<div id="FiniteHorizonMDP-Value" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Value Functions<a href="mdp.html#FiniteHorizonMDP-Value" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>State-Value Function</strong>. Given a policy <span class="math inline">\(\pi\)</span> as in <a href="mdp.html#eq:policy-tuple">(1.1)</a>, which states are preferable at time <span class="math inline">\(t\)</span>? The (time-indexed) state-value function assigns to each <span class="math inline">\(s\in\mathcal{S}\)</span> the expected return from <span class="math inline">\(t\)</span> onward when starting in <span class="math inline">\(s\)</span> and following <span class="math inline">\(\pi\)</span> thereafter. Formally, define
<span class="math display" id="eq:FiniteHorizonMDP-state-value">\[\begin{equation}
V_t^\pi(s) := \mathbb{E} \left[g_t \mid s_t=s\right]
= \mathbb{E} \left[\sum_{i=t}^{T-1} R(s_i,a_i) \middle| s_t=s,a_i\sim \pi_i(\cdot\mid s_i), s_{i+1}\sim P(\cdot\mid s_i,a_i)\right].
\tag{1.2}
\end{equation}\]</span>
The expectation is over the randomness induced by both the policy and the dynamics. Thus, if <span class="math inline">\(V_t^\pi(s_1)&gt;V_t^\pi(s_2)\)</span>, then at time <span class="math inline">\(t\)</span> under policy <span class="math inline">\(\pi\)</span> it is better in expectation to be in <span class="math inline">\(s_1\)</span> than in <span class="math inline">\(s_2\)</span> because the former yields a larger expected return.</p>
<div class="highlightbox">
<p><span class="math inline">\(V^{\pi}_t(s)\)</span>: given policy <span class="math inline">\(\pi\)</span>, how good is it to start in state <span class="math inline">\(s\)</span> at time <span class="math inline">\(t\)</span>?</p>
</div>
<p><strong>Action-Value Function</strong>. Similarly, the action-value function assigns to each state-action pair <span class="math inline">\((s,a)\in\mathcal{S}\times\mathcal{A}\)</span> the expected return obtained by starting in state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span> first, and then following policy <span class="math inline">\(\pi\)</span> thereafter:
<span class="math display" id="eq:FiniteHorizonMDP-action-value">\[\begin{equation}
\begin{split}
Q_t^\pi(s,a) := &amp; \mathbb{E} \left[R(s,a) + g_{t+1} \mid s_{t+1} \sim P(\cdot \mid s,a)\right] \\
= &amp; \mathbb{E} \left[R(s,a) + \sum_{i=t+1}^{T-1} R(s_i, a_i) \middle| s_{t+1} \sim P(\cdot \mid s,a) \right].
\end{split}
\tag{1.3}
\end{equation}\]</span>
The key distinction is that the action-value function evaluates the return when the first action may deviate from policy <span class="math inline">\(\pi\)</span>, whereas the state-value function assumes strict adherence to <span class="math inline">\(\pi\)</span>. This flexibility makes the action-value function central to improving <span class="math inline">\(\pi\)</span>, since it reveals whether alternative actions can yield higher returns.</p>
<div class="highlightbox">
<p><span class="math inline">\(Q^{\pi}_t(s,a)\)</span>: At time <span class="math inline">\(t\)</span>, how good is it to take action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, then follow the policy <span class="math inline">\(\pi\)</span>?</p>
</div>
<p>It is easy to verify that the state-value function and the action-value function satisfy:
<span class="math display" id="eq:FiniteHorizonMDP-action-value-from-state-value" id="eq:FiniteHorizonMDP-state-value-from-action-value">\[\begin{align}
V_t^{\pi}(s) &amp; = \sum_{a \in \mathcal{A}} \pi_t(a \mid s) Q_t^{\pi}(s,a), \tag{1.4} \\
Q_t^{\pi}(s,a) &amp; = R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^{\pi}_{t+1} (s&#39;). \tag{1.5}
\end{align}\]</span></p>
<p>From these two equations, we can derive the Bellman Consistency equations.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:BellmanConsistency" class="proposition"><strong>Proposition 1.1  (Bellman Consistency (Finite Horizon)) </strong></span>The state-value function <span class="math inline">\(V^{\pi}_t(\cdot)\)</span> in <a href="mdp.html#eq:FiniteHorizonMDP-state-value">(1.2)</a> satisfies the following recursion:
<span class="math display" id="eq:BellmanConsistency-State-Value">\[\begin{equation}
\begin{split}
V^{\pi}_t(s) &amp; = \sum_{a \in \mathcal{A}} \pi_t(a\mid s) \left( R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^{\pi}_{t+1} (s&#39;) \right) \\
    &amp; =: \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} [V^{\pi}_{t+1}(s&#39;)] \right].
\end{split}
\tag{1.6}
\end{equation}\]</span></p>
<p>Similarly, the action-value function <span class="math inline">\(Q^{\pi}_t(s,a)\)</span> in <a href="mdp.html#eq:FiniteHorizonMDP-action-value">(1.3)</a> satisfies the following recursion:
<span class="math display" id="eq:BellmanConsistency-Action-Value">\[\begin{equation}
\begin{split}
Q^{\pi}_t (s, a) &amp; = R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) \left( \sum_{a&#39; \in \mathcal{A}} \pi_{t+1}(a&#39; \mid s&#39;) Q^{\pi}_{t+1}(s&#39;, a&#39;)\right) \\
&amp; =: R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \left[\mathbb{E}_{a&#39; \sim \pi_{t+1}(\cdot \mid s&#39;)} [Q^{\pi}_{t+1}(s&#39;, a&#39;)] \right].
\end{split}
\tag{1.7}
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="policy-evaluation" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Policy Evaluation<a href="mdp.html#policy-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Bellman consistency result in Proposition <a href="mdp.html#prp:BellmanConsistency">1.1</a> is fundamental because it directly yields an algorithm for evaluating a given policy <span class="math inline">\(\pi\)</span>—that is, for computing its state-value and action-value functions—provided the transition dynamics of the MDP are known.</p>
<p>Policy evaluation for the state-value function proceeds as follows:</p>
<ul>
<li><strong>Initialization:</strong> set <span class="math inline">\(V^{\pi}_T(s) = 0\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>.<br />
</li>
<li><strong>Backward recursion:</strong> for <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>, update each <span class="math inline">\(s \in \mathcal{S}\)</span> by
<span class="math display">\[
V^{\pi}_{t}(s) = \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \big[ V^{\pi}_{t+1}(s&#39;) \big] \right].
\]</span></li>
</ul>
<p>Similarly, policy evaluation for the action-value function is given by:</p>
<ul>
<li><strong>Initialization:</strong> set <span class="math inline">\(Q^{\pi}_T(s,a) = 0\)</span> for all <span class="math inline">\(s \in \mathcal{S}, a \in \mathcal{A}\)</span>.<br />
</li>
<li><strong>Backward recursion:</strong> for <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>, update each <span class="math inline">\((s,a) \in \mathcal{S}\times\mathcal{A}\)</span> by
<span class="math display">\[
Q^{\pi}_t(s,a) = R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \left[ \mathbb{E}_{a&#39; \sim \pi_{t+1}(\cdot \mid s&#39;)} \big[ Q^{\pi}_{t+1}(s&#39;, a&#39;) \big] \right].
\]</span></li>
</ul>
<p>The essential feature of this algorithm is its backward-in-time recursion: the value functions are first set at the terminal horizon <span class="math inline">\(T\)</span>, and then propagated backward step by step through the Bellman consistency equations.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:MDPExampleGraph" class="example"><strong>Example 1.1  (MDP, Transition Graph, and Policy Evaluation) </strong></span>It is often useful to visualize small MDPs as transition graphs, where states are represented by nodes and actions are represented by directed edges connecting those nodes.</p>
<p>As a simple illustrative example, consider a robot navigating on a two-state grid. At each step, the robot can either Stay in its current state or Move to the other state. This finite-horizon MDP is fully specified by the tuple of states, actions, transition dynamics, rewards, and horizon:</p>
<ul>
<li><p>States: <span class="math inline">\(\mathcal{S} = \{\alpha, \beta \}\)</span></p></li>
<li><p>Actions: <span class="math inline">\(\mathcal{A} = \{\text{Move} , \text{Stay} \}\)</span></p></li>
<li><p>Transition dynamics: we can specify the transition dynamics in the following table</p></li>
</ul>
<table>
<colgroup>
<col width="20%" />
<col width="22%" />
<col width="34%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">State <span class="math inline">\(s\)</span></th>
<th align="center">Action <span class="math inline">\(a\)</span></th>
<th align="center">Next State <span class="math inline">\(s&#39;\)</span></th>
<th align="center">Probability <span class="math inline">\(P(s&#39; \mid s, a)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">Stay</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">Move</td>
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">Stay</td>
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">Move</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Reward: <span class="math inline">\(R(s,a)=1\)</span> if <span class="math inline">\(a = \text{Move}\)</span> and <span class="math inline">\(R(s,a)=0\)</span> if <span class="math inline">\(a = \text{Stay}\)</span></p></li>
<li><p>Horizon: <span class="math inline">\(T=2\)</span>.</p></li>
</ul>
<p>This MDP can be represented by the transition graph in Fig. <a href="mdp.html#fig:mdp-robot-transition-graph">1.1</a>. Note that for this MDP, the transition dynamics is deterministic. We will see a stochastic MDP soon.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-robot-transition-graph"></span>
<img src="images/MDP/robot-mdp-graph.png" alt="A Simple Transition Graph." width="90%" />
<p class="caption">
Figure 1.1: A Simple Transition Graph.
</p>
</div>
<p>At time <span class="math inline">\(t=0\)</span>, if the robot starts at <span class="math inline">\(s_0 = \alpha\)</span>, first chooses action <span class="math inline">\(a_0 = \text{Move}\)</span>, and then chooses action <span class="math inline">\(a_1 = \text{Stay}\)</span>, the resulting trajectory is
<span class="math display">\[
\tau = (\alpha, \text{Move}, +1, \beta, \text{Stay}, 0,  \beta).
\]</span>
The return of this trajectory is:
<span class="math display">\[
g_0 = +1 + 0 = +1.
\]</span></p>
<p><strong>Policy Evaluation</strong>. Given a policy
<span class="math display">\[\begin{equation}
\pi = (\pi_0, \pi_1), \quad \pi_0(a \mid s) = \begin{cases}
0.5 &amp; a = \text{Move} \\
0.5 &amp; a = \text{Stay}
\end{cases},
\quad
\pi_1( a \mid s) = \begin{cases}
0.8 &amp; a = \text{Move} \\
0.2 &amp; a = \text{Stay}
\end{cases}.
\end{equation}\]</span>
We can use the Bellman consistency equations to compute the state-value function. We first initialize:
<span class="math display">\[
V^{\pi}_2 = \begin{bmatrix}
0 \\ 0
\end{bmatrix},
\]</span>
where the first row contains the value at <span class="math inline">\(s = \alpha\)</span> and the second row contains the value at <span class="math inline">\(s = \beta\)</span>.
We then perform the backward recursion for <span class="math inline">\(t=1\)</span>. For <span class="math inline">\(s = \alpha\)</span>, we have
<span class="math display">\[\begin{equation}
V^{\pi}_1(\alpha) = \begin{bmatrix}
\pi_1(\text{Move} \mid \alpha) \\
\pi_1(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_2(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_2(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8
\end{equation}\]</span>
For <span class="math inline">\(s = \beta\)</span>, we have
<span class="math display">\[\begin{equation}
V^{\pi}_1(\beta) = \begin{bmatrix}
\pi_1(\text{Move} \mid \beta) \\
\pi_1(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_2(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_2(\beta)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8.
\end{equation}\]</span>
Therefore, we have
<span class="math display">\[
V^{\pi}_1 = \begin{bmatrix}
0.8 \\ 0.8
\end{bmatrix}.
\]</span>
We then proceed to the backward recursion for <span class="math inline">\(t=0\)</span>:
<span class="math display">\[\begin{align}
V_0^{\pi}(\alpha) &amp; = \begin{bmatrix}
\pi_0(\text{Move} \mid \alpha) \\
\pi_0(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_1(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_1(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3. \\
V_0^{\pi}(\beta) &amp; = \begin{bmatrix}
\pi_0(\text{Move} \mid \beta) \\
\pi_0(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_0(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_0(\beta)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3.
\end{align}\]</span>
Therefore, the state-value function at <span class="math inline">\(t=0\)</span> is
<span class="math display">\[
V^{\pi}_0 = \begin{bmatrix}
1.3 \\ 1.3
\end{bmatrix}.
\]</span>
You are encouraged to carry out the similar calculations for the action-value function.</p>
<hr />
<p>The toy example was small enough to carry out policy evaluation by hand; in realistic MDPs, we will need the help from computers.</p>
<p>Consider now an MDP whose transition graph is shown in Fig. <a href="mdp.html#fig:mdp-hangover-transition-graph">1.2</a>. This example is adapted from <a href="https://github.com/upb-lea/reinforcement_learning_course_materials/blob/master/exercises/solutions/ex02/Ex2.ipynb">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-hangover-transition-graph"></span>
<img src="images/MDP/hangover-graph.png" alt="Hangover Transition Graph." width="90%" />
<p class="caption">
Figure 1.2: Hangover Transition Graph.
</p>
</div>
<p>This MDP has six states:
<span class="math display">\[
\mathcal{S} = \{\text{Hangover}, \text{Sleep}, \text{More Sleep}, \text{Visit Lecture}, \text{Study}, \text{Pass Exam} \},
\]</span>
and two actions:
<span class="math display">\[
\mathcal{A} = \{\text{Lazy}, \text{Productive} \}.
\]</span>
The stochastic transition dynamics are labeled in the transition graph. For example, at state “Hangover”, taking action “Productive” will lead to state “Visit Lecture” with probability <span class="math inline">\(0.3\)</span> and state “Hangover” with probability <span class="math inline">\(0.7\)</span>.
The rewards of the MDP are defined as:
<span class="math display">\[
R(s,a) = \begin{cases}
+1 &amp; s = \text{Pass Exam} \\
-1 &amp; \text{otherwise}.
\end{cases}.
\]</span></p>
<p><strong>Policy Evaluation</strong>. Consider a time-invariant random policy
<span class="math display">\[
\pi = \{\pi_0,\dots,\pi_{T-1} \}, \quad \pi_t(a \mid s) = \begin{cases}
\alpha &amp; a = \text{Lazy} \\
1 - \alpha &amp; a = \text{Productive}
\end{cases},
\]</span>
that takes “Lazy” with probability <span class="math inline">\(\alpha\)</span> and “Productive” with probability <span class="math inline">\(1-\alpha\)</span>.</p>
<p>The following Python code performs policy evaluation for this MDP, with <span class="math inline">\(T=10\)</span> and <span class="math inline">\(\alpha = 0.4\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="mdp.html#cb1-1" tabindex="-1"></a><span class="co"># Finite-horizon policy evaluation for the Hangover MDP</span></span>
<span id="cb1-2"><a href="mdp.html#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="mdp.html#cb1-3" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb1-4"><a href="mdp.html#cb1-4" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List, Tuple</span>
<span id="cb1-5"><a href="mdp.html#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="mdp.html#cb1-6" tabindex="-1"></a>State <span class="op">=</span> <span class="bu">str</span></span>
<span id="cb1-7"><a href="mdp.html#cb1-7" tabindex="-1"></a>Action <span class="op">=</span> <span class="bu">str</span></span>
<span id="cb1-8"><a href="mdp.html#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="mdp.html#cb1-9" tabindex="-1"></a><span class="co"># --- MDP spec ---------------------------------------------------------------</span></span>
<span id="cb1-10"><a href="mdp.html#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="mdp.html#cb1-11" tabindex="-1"></a>S: List[State] <span class="op">=</span> [</span>
<span id="cb1-12"><a href="mdp.html#cb1-12" tabindex="-1"></a>    <span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Pass Exam&quot;</span></span>
<span id="cb1-13"><a href="mdp.html#cb1-13" tabindex="-1"></a>]</span>
<span id="cb1-14"><a href="mdp.html#cb1-14" tabindex="-1"></a>A: List[Action] <span class="op">=</span> [<span class="st">&quot;Lazy&quot;</span>, <span class="st">&quot;Productive&quot;</span>]</span>
<span id="cb1-15"><a href="mdp.html#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="mdp.html#cb1-16" tabindex="-1"></a><span class="co"># P[s, a] -&gt; list of (s_next, prob)</span></span>
<span id="cb1-17"><a href="mdp.html#cb1-17" tabindex="-1"></a>P: Dict[Tuple[State, Action], List[Tuple[State, <span class="bu">float</span>]]] <span class="op">=</span> {</span>
<span id="cb1-18"><a href="mdp.html#cb1-18" tabindex="-1"></a>    <span class="co"># Hangover</span></span>
<span id="cb1-19"><a href="mdp.html#cb1-19" tabindex="-1"></a>    (<span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-20"><a href="mdp.html#cb1-20" tabindex="-1"></a>    (<span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Visit Lecture&quot;</span>, <span class="fl">0.3</span>), (<span class="st">&quot;Hangover&quot;</span>, <span class="fl">0.7</span>)],</span>
<span id="cb1-21"><a href="mdp.html#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="mdp.html#cb1-22" tabindex="-1"></a>    <span class="co"># Sleep</span></span>
<span id="cb1-23"><a href="mdp.html#cb1-23" tabindex="-1"></a>    (<span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):          [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-24"><a href="mdp.html#cb1-24" tabindex="-1"></a>    (<span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;Productive&quot;</span>):    [(<span class="st">&quot;Visit Lecture&quot;</span>, <span class="fl">0.6</span>), (<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">0.4</span>)],</span>
<span id="cb1-25"><a href="mdp.html#cb1-25" tabindex="-1"></a></span>
<span id="cb1-26"><a href="mdp.html#cb1-26" tabindex="-1"></a>    <span class="co"># More Sleep</span></span>
<span id="cb1-27"><a href="mdp.html#cb1-27" tabindex="-1"></a>    (<span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-28"><a href="mdp.html#cb1-28" tabindex="-1"></a>    (<span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Study&quot;</span>, <span class="fl">0.5</span>), (<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">0.5</span>)],</span>
<span id="cb1-29"><a href="mdp.html#cb1-29" tabindex="-1"></a></span>
<span id="cb1-30"><a href="mdp.html#cb1-30" tabindex="-1"></a>    <span class="co"># Visit Lecture</span></span>
<span id="cb1-31"><a href="mdp.html#cb1-31" tabindex="-1"></a>    (<span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Study&quot;</span>, <span class="fl">0.8</span>), (<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">0.2</span>)],</span>
<span id="cb1-32"><a href="mdp.html#cb1-32" tabindex="-1"></a>    (<span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Study&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-33"><a href="mdp.html#cb1-33" tabindex="-1"></a></span>
<span id="cb1-34"><a href="mdp.html#cb1-34" tabindex="-1"></a>    <span class="co"># Study</span></span>
<span id="cb1-35"><a href="mdp.html#cb1-35" tabindex="-1"></a>    (<span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):         [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-36"><a href="mdp.html#cb1-36" tabindex="-1"></a>    (<span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Productive&quot;</span>):   [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">0.9</span>), (<span class="st">&quot;Study&quot;</span>, <span class="fl">0.1</span>)],</span>
<span id="cb1-37"><a href="mdp.html#cb1-37" tabindex="-1"></a></span>
<span id="cb1-38"><a href="mdp.html#cb1-38" tabindex="-1"></a>    <span class="co"># Pass Exam (absorbing)</span></span>
<span id="cb1-39"><a href="mdp.html#cb1-39" tabindex="-1"></a>    (<span class="st">&quot;Pass Exam&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-40"><a href="mdp.html#cb1-40" tabindex="-1"></a>    (<span class="st">&quot;Pass Exam&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb1-41"><a href="mdp.html#cb1-41" tabindex="-1"></a>}</span>
<span id="cb1-42"><a href="mdp.html#cb1-42" tabindex="-1"></a></span>
<span id="cb1-43"><a href="mdp.html#cb1-43" tabindex="-1"></a><span class="kw">def</span> R(s: State, a: Action) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-44"><a href="mdp.html#cb1-44" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot;</span></span>
<span id="cb1-45"><a href="mdp.html#cb1-45" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> s <span class="op">==</span> <span class="st">&quot;Pass Exam&quot;</span> <span class="cf">else</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb1-46"><a href="mdp.html#cb1-46" tabindex="-1"></a></span>
<span id="cb1-47"><a href="mdp.html#cb1-47" tabindex="-1"></a><span class="co"># --- Policy: time-invariant, state-independent ------------------------------</span></span>
<span id="cb1-48"><a href="mdp.html#cb1-48" tabindex="-1"></a></span>
<span id="cb1-49"><a href="mdp.html#cb1-49" tabindex="-1"></a><span class="kw">def</span> pi(a: Action, s: State, alpha: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-50"><a href="mdp.html#cb1-50" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;pi(a|s): Lazy with prob alpha, Productive with prob 1-alpha.&quot;&quot;&quot;</span></span>
<span id="cb1-51"><a href="mdp.html#cb1-51" tabindex="-1"></a>    <span class="cf">return</span> alpha <span class="cf">if</span> a <span class="op">==</span> <span class="st">&quot;Lazy&quot;</span> <span class="cf">else</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha)</span>
<span id="cb1-52"><a href="mdp.html#cb1-52" tabindex="-1"></a></span>
<span id="cb1-53"><a href="mdp.html#cb1-53" tabindex="-1"></a><span class="co"># --- Policy evaluation -------------------------------------------------------</span></span>
<span id="cb1-54"><a href="mdp.html#cb1-54" tabindex="-1"></a></span>
<span id="cb1-55"><a href="mdp.html#cb1-55" tabindex="-1"></a><span class="kw">def</span> policy_evaluation(T: <span class="bu">int</span>, alpha: <span class="bu">float</span>):</span>
<span id="cb1-56"><a href="mdp.html#cb1-56" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-57"><a href="mdp.html#cb1-57" tabindex="-1"></a><span class="co">    Compute {V_t(s)} and {Q_t(s,a)} for t=0..T with terminal condition V_T = Q_T = 0.</span></span>
<span id="cb1-58"><a href="mdp.html#cb1-58" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-59"><a href="mdp.html#cb1-59" tabindex="-1"></a><span class="co">        V: Dict[int, Dict[State, float]]</span></span>
<span id="cb1-60"><a href="mdp.html#cb1-60" tabindex="-1"></a><span class="co">        Q: Dict[int, Dict[Tuple[State, Action], float]]</span></span>
<span id="cb1-61"><a href="mdp.html#cb1-61" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb1-62"><a href="mdp.html#cb1-62" tabindex="-1"></a>    <span class="cf">assert</span> T <span class="op">&gt;=</span> <span class="dv">0</span></span>
<span id="cb1-63"><a href="mdp.html#cb1-63" tabindex="-1"></a>    <span class="co"># sanity: probabilities sum to 1 for each (s,a)</span></span>
<span id="cb1-64"><a href="mdp.html#cb1-64" tabindex="-1"></a>    <span class="cf">for</span> key, rows <span class="kw">in</span> P.items():</span>
<span id="cb1-65"><a href="mdp.html#cb1-65" tabindex="-1"></a>        total <span class="op">=</span> <span class="bu">sum</span>(p <span class="cf">for</span> _, p <span class="kw">in</span> rows)</span>
<span id="cb1-66"><a href="mdp.html#cb1-66" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(total <span class="op">-</span> <span class="fl">1.0</span>) <span class="op">&gt;</span> <span class="fl">1e-9</span>:</span>
<span id="cb1-67"><a href="mdp.html#cb1-67" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f&quot;Probabilities for </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> sum to </span><span class="sc">{</span>total<span class="sc">}</span><span class="ss">, not 1.&quot;</span>)</span>
<span id="cb1-68"><a href="mdp.html#cb1-68" tabindex="-1"></a></span>
<span id="cb1-69"><a href="mdp.html#cb1-69" tabindex="-1"></a>    V: Dict[<span class="bu">int</span>, Dict[State, <span class="bu">float</span>]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb1-70"><a href="mdp.html#cb1-70" tabindex="-1"></a>    Q: Dict[<span class="bu">int</span>, Dict[Tuple[State, Action], <span class="bu">float</span>]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb1-71"><a href="mdp.html#cb1-71" tabindex="-1"></a></span>
<span id="cb1-72"><a href="mdp.html#cb1-72" tabindex="-1"></a>    <span class="co"># Terminal boundary</span></span>
<span id="cb1-73"><a href="mdp.html#cb1-73" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb1-74"><a href="mdp.html#cb1-74" tabindex="-1"></a>        V[T][s] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-75"><a href="mdp.html#cb1-75" tabindex="-1"></a>        <span class="cf">for</span> a <span class="kw">in</span> A:</span>
<span id="cb1-76"><a href="mdp.html#cb1-76" tabindex="-1"></a>            Q[T][(s, a)] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-77"><a href="mdp.html#cb1-77" tabindex="-1"></a></span>
<span id="cb1-78"><a href="mdp.html#cb1-78" tabindex="-1"></a>    <span class="co"># Backward recursion</span></span>
<span id="cb1-79"><a href="mdp.html#cb1-79" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb1-80"><a href="mdp.html#cb1-80" tabindex="-1"></a>        <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb1-81"><a href="mdp.html#cb1-81" tabindex="-1"></a>            <span class="co"># First compute Q_t(s,a)</span></span>
<span id="cb1-82"><a href="mdp.html#cb1-82" tabindex="-1"></a>            <span class="cf">for</span> a <span class="kw">in</span> A:</span>
<span id="cb1-83"><a href="mdp.html#cb1-83" tabindex="-1"></a>                exp_next <span class="op">=</span> <span class="bu">sum</span>(p <span class="op">*</span> V[t <span class="op">+</span> <span class="dv">1</span>][s_next] <span class="cf">for</span> s_next, p <span class="kw">in</span> P[(s, a)])</span>
<span id="cb1-84"><a href="mdp.html#cb1-84" tabindex="-1"></a>                Q[t][(s, a)] <span class="op">=</span> R(s, a) <span class="op">+</span> exp_next</span>
<span id="cb1-85"><a href="mdp.html#cb1-85" tabindex="-1"></a>            <span class="co"># Then V_t(s) = E_{a~pi}[Q_t(s,a)]</span></span>
<span id="cb1-86"><a href="mdp.html#cb1-86" tabindex="-1"></a>            V[t][s] <span class="op">=</span> <span class="bu">sum</span>(pi(a, s, alpha) <span class="op">*</span> Q[t][(s, a)] <span class="cf">for</span> a <span class="kw">in</span> A)</span>
<span id="cb1-87"><a href="mdp.html#cb1-87" tabindex="-1"></a></span>
<span id="cb1-88"><a href="mdp.html#cb1-88" tabindex="-1"></a>    <span class="cf">return</span> V, Q</span>
<span id="cb1-89"><a href="mdp.html#cb1-89" tabindex="-1"></a></span>
<span id="cb1-90"><a href="mdp.html#cb1-90" tabindex="-1"></a><span class="co"># --- Example run -------------------------------------------------------------</span></span>
<span id="cb1-91"><a href="mdp.html#cb1-91" tabindex="-1"></a></span>
<span id="cb1-92"><a href="mdp.html#cb1-92" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb1-93"><a href="mdp.html#cb1-93" tabindex="-1"></a>    T <span class="op">=</span> <span class="dv">10</span>        <span class="co"># horizon</span></span>
<span id="cb1-94"><a href="mdp.html#cb1-94" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="fl">0.4</span>  <span class="co"># probability of choosing Lazy</span></span>
<span id="cb1-95"><a href="mdp.html#cb1-95" tabindex="-1"></a>    V, Q <span class="op">=</span> policy_evaluation(T<span class="op">=</span>T, alpha<span class="op">=</span>alpha)</span>
<span id="cb1-96"><a href="mdp.html#cb1-96" tabindex="-1"></a></span>
<span id="cb1-97"><a href="mdp.html#cb1-97" tabindex="-1"></a>    <span class="co"># Print V_0</span></span>
<span id="cb1-98"><a href="mdp.html#cb1-98" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;V_0(s) with T=</span><span class="sc">{</span>T<span class="sc">}</span><span class="ss">, alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">:&quot;</span>)</span>
<span id="cb1-99"><a href="mdp.html#cb1-99" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb1-100"><a href="mdp.html#cb1-100" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  </span><span class="sc">{</span>s<span class="sc">:13s}</span><span class="ss">: </span><span class="sc">{</span>V[<span class="dv">0</span>][s]<span class="sc">: .3f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<p>The code returns the following state values at <span class="math inline">\(t=0\)</span>:
<span class="math display" id="eq:HangoverRandomValueFunction">\[\begin{equation}
V^{\pi}_0 = \begin{bmatrix}
-3.582 \\ -2.306 \\ -2.180 \\ 1.757 \\ 2.939 \\ 10
\end{bmatrix},
\tag{1.8}
\end{equation}\]</span>
where the ordering of the states follows that defined in <span class="math inline">\(\mathcal{S}\)</span>.</p>
<p>You can find the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/hangover_policy_evaluation.py">here</a>.</p>
</div>
</div>
</div>
<div id="optimality" class="section level3 hasAnchor" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Principle of Optimality<a href="mdp.html#optimality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Every policy <span class="math inline">\(\pi\)</span> induces a value function <span class="math inline">\(V_0^{\pi}\)</span> that can be evaluated by policy evaluation (assuming the transition dynamics are known). The goal of reinforcement learning is to find an optimal policy that maximizes the value function with respect to a given initial state distribution:
<span class="math display" id="eq:FiniteHorizonMDPRLProblem">\[\begin{equation}
V^\star_0 = \max_{\pi}\; \mathbb{E}_{s_0 \sim \mu(\cdot)} \big[ V_0^{\pi}(s_0) \big],
\tag{1.9}
\end{equation}\]</span>
where we have used the superscript “<span class="math inline">\(\star\)</span>” to denote the optimality of the value function. <span class="math inline">\(V^\star_0\)</span> is often known as the <em>optimal value function</em>.</p>
<p>At first glance, <a href="mdp.html#eq:FiniteHorizonMDPRLProblem">(1.9)</a> appears daunting: a naive approach would enumerate all stochastic policies <span class="math inline">\(\pi\)</span>, evaluate their value functions, and select the best. A central result in reinforcement learning and optimal control—rooted in the principle of optimality—is that the <em>optimal</em> value functions satisfy a Bellman-style recursion, analogous to Proposition <a href="mdp.html#prp:BellmanConsistency">1.1</a>. This Bellman optimality recursion enables backward computation of the optimal value functions without enumerating policies.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:FiniteHorizonMDPBellmanOptimality" class="theorem"><strong>Theorem 1.1  (Bellman Optimality (Finite Horizon, State-Value)) </strong></span>Consider a finite-horizon MDP <span class="math inline">\(\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,T)\)</span> with finite state and action sets and bounded rewards.
Define the optimal value functions <span class="math inline">\(\{V_t^\star\}_{t=0}^{T}\)</span> by the following <em>Bellman optimality</em> recursion
<span class="math display" id="eq:FiniteHorizonMDPBellmanOptimality">\[\begin{equation}
\begin{split}
V_T^\star(s)&amp; \equiv 0, \\
V_t^\star(s)&amp; = \max_{a\in\mathcal{A}}\Big\{ R(s,a)\;+\;\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^\star(s&#39;)\Big\},\ t=T-1,\ldots,0.
\end{split}
\tag{1.10}
\end{equation}\]</span>
Then, the optimal value functions are optimal in the sense of <em>statewise dominance</em>:
<span class="math display" id="eq:FiniteHorizonMDPStatewiseDominance">\[\begin{equation}
V_t^{\star}(s)\;\ge\; V_t^{\pi}(s)
\quad\text{for all policies }\pi,\; s\in\mathcal{S},\; t=0,\ldots,T.
\tag{1.11}
\end{equation}\]</span></p>
<p>Moreover, the deterministic policy
<span class="math inline">\(\pi^\star=(\pi^\star_0,\ldots,\pi^\star_{T-1})\)</span> with
<span class="math display" id="eq:FiniteHorizonMDPOptimalPolicy">\[\begin{equation}
\begin{split}
\pi_t^\star(s)\;\in\;\arg\max_{a\in\mathcal{A}}
\Big\{ R(s,a)\;+\;\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^\star(s&#39;)\Big\}, \\
\text{for any } s\in\mathcal{S},\; t=0,\dots,T-1
\end{split}
\tag{1.12}
\end{equation}\]</span>
is optimal, where ties can be broken by any fixed rule.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>We first show that the value functions defined by the Bellman optimality recursion <a href="mdp.html#eq:FiniteHorizonMDPBellmanOptimality">(1.10)</a> are <em>optimal</em> in the sense that they dominate the value functions of any other policy. The proof proceeds by backward induction.</p>
<p><strong>Base case (<span class="math inline">\(t=T\)</span>).</strong>
For every <span class="math inline">\(s\in\mathcal{S}\)</span>,
<span class="math display">\[
V^\star_T(s)\;=\;0\;=\;V_T^{\pi}(s),
\]</span>
so <span class="math inline">\(V^\star_T(s)\ge V_T^{\pi}(s)\)</span> holds trivially.</p>
<p><strong>Inductive step.</strong>
Assume <span class="math inline">\(V^\star_{t+1}(s)\ge V^{\pi}_{t+1}(s)\)</span> for all <span class="math inline">\(s\in\mathcal{S}\)</span>. Then, for any <span class="math inline">\(s\in\mathcal{S}\)</span>,
<span class="math display">\[\begin{align*}
V_t^{\pi}(s)
&amp;= \sum_{a\in\mathcal{A}} \pi_t(a\mid s)\!\left(R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^{\pi}(s&#39;)\right) \\
&amp;\le \sum_{a\in\mathcal{A}} \pi_t(a\mid s)\!\left(R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^{\star}(s&#39;)\right) \\
&amp;\le \max_{a\in\mathcal{A}} \left(R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^{\star}(s&#39;)\right)
\;=\; V_t^\star(s),
\end{align*}\]</span>
where the first inequality uses the induction hypothesis and the second uses that an expectation is bounded above by a maximum. Hence <span class="math inline">\(V_t^\star(s)\ge V_t^{\pi}(s)\)</span> for all <span class="math inline">\(s\)</span>, completing the induction. Therefore, <span class="math inline">\(\{V_t^\star\}_{t=0}^T\)</span> dominates the value functions attainable by any policy.</p>
<p>Next, we show that <span class="math inline">\(\{V_t^\star\}\)</span> is <em>attainable</em> by some policy. Since <span class="math inline">\(\mathcal{A}\)</span> is finite (tabular setting), the maximizer in the Bellman optimality operator exists for every <span class="math inline">\((t,s)\)</span>; thus we can define a (deterministic) greedy policy
<span class="math display">\[
\pi_t^\star(s)\;\in\;\arg\max_{a\in\mathcal{A}}
\Big\{ R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^\star(s&#39;) \Big\}.
\]</span>
A simple backward induction then shows <span class="math inline">\(V_t^{\pi^\star}(s)=V_t^\star(s)\)</span> for all <span class="math inline">\(t\)</span> and <span class="math inline">\(s\)</span>: at <span class="math inline">\(t=T\)</span> both are <span class="math inline">\(0\)</span>, and if <span class="math inline">\(V_{t+1}^{\pi^\star}=V_{t+1}^\star\)</span>, then by construction of <span class="math inline">\(\pi_t^\star\)</span> the Bellman equality yields <span class="math inline">\(V_t^{\pi^\star}=V_t^\star\)</span>. Consequently, the optimal value functions are achieved by the greedy (deterministic) policy <span class="math inline">\(\pi^\star\)</span>.</p>
</div>
</div>
<div class="theorembox">
<div class="corollary">
<p><span id="cor:FiniteHorizonMDPBellmanOptimalityActionValue" class="corollary"><strong>Corollary 1.1  (Bellman Optimality (Finite Horizon, Action-Value)) </strong></span>Given the optimal (state-)value functions <span class="math inline">\(V^{\star}_{t},t=0,\dots,T\)</span>, define the optimal action-value function
<span class="math display" id="eq:FiniteHorizonMDPOptimalActionValue">\[\begin{equation}
Q_t^\star(s,a)\;=\;R(s,a)+\sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\,V_{t+1}^\star(s&#39;), \quad t=0,\dots,T-1.
\tag{1.13}
\end{equation}\]</span></p>
<p>Then we have
<span class="math display" id="eq:FiniteHorizonMDPStateValueActionValue">\[\begin{equation}
V_t^\star(s)=\max_{a\in\mathcal{A}} Q_t^\star(s,a),\qquad
\pi_t^\star(s)\in\arg\max_{a\in\mathcal{A}} Q_t^\star(s,a).
\tag{1.14}
\end{equation}\]</span></p>
<p>The optimal action-value functions satisfy:
<span class="math display" id="eq:FiniteHorizonMDPBellmanRecursionActionValue">\[\begin{equation}
\begin{split}
Q_T^\star(s,a) &amp; \equiv 0,\\
Q_t^\star(s,a)
&amp; = R(s,a) \;+\; \mathbb{E}_{s&#39;\sim P(\cdot\mid s,a)}
\!\left[ \max_{a&#39;\in\mathcal{A}} Q_{t+1}^\star(s&#39;,a&#39;) \right],
\quad t=T-1,\ldots,0.
\end{split}
\tag{1.15}
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="dp" class="section level3 hasAnchor" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Dynamic Programming<a href="mdp.html#dp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The principle of optimality in Theorem <a href="mdp.html#thm:FiniteHorizonMDPBellmanOptimality">1.1</a> yields a constructive procedure to compute the optimal value functions and an associated deterministic optimal policy. This backward-induction procedure is the <em>dynamic programming</em> (DP) algorithm.</p>
<p><strong>Dynamic programming (finite horizon).</strong></p>
<ul>
<li><p><strong>Initialization.</strong> Set <span class="math inline">\(V_T^\star(s) = 0\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>.</p></li>
<li><p><strong>Backward recursion.</strong> For <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>:</p>
<ul>
<li><p><em>Optimal value:</em> for each <span class="math inline">\(s \in \mathcal{S}\)</span>,
<span class="math display">\[
V_t^\star(s)
= \max_{a \in \mathcal{A}}
\left\{
R(s,a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s,a)} \big[ V_{t+1}^\star(s&#39;) \big]
\right\}.
\]</span></p></li>
<li><p><em>Greedy policy (deterministic):</em> for each <span class="math inline">\(s \in \mathcal{S}\)</span>,
<span class="math display">\[
\pi_t^\star(s) \in \arg\max_{a \in \mathcal{A}}
\left\{
R(s,a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s,a)} \big[ V_{t+1}^\star(s&#39;) \big]
\right\}.
\]</span></p></li>
</ul></li>
</ul>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-2" class="exercise"><strong>Exercise 1.1  </strong></span>How does dynamic programming look like when applied to the action-value function?</p>
</div>
</div>
<div class="exercisebox">
<div class="exercise">
<p><span id="exr:unlabeled-div-3" class="exercise"><strong>Exercise 1.2  </strong></span>What is the computational complexity of dynamic programming?</p>
</div>
</div>
<p>Let us try dynamic programming for the Hangover MDP presented before.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:HangoverDynamicProgramming" class="example"><strong>Example 1.2  (Dynamic Programming for Hangover MDP) </strong></span>Consider the Hangover MDP defined by the transition graph shown in Fig. <a href="mdp.html#fig:mdp-hangover-transition-graph">1.2</a>. With slight modification to the policy evaluation code, we can find the optimal value functions and optimal policies.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="mdp.html#cb2-1" tabindex="-1"></a><span class="co"># Dynamic programming (finite-horizon optimal control) for the Hangover MDP</span></span>
<span id="cb2-2"><a href="mdp.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="mdp.html#cb2-3" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb2-4"><a href="mdp.html#cb2-4" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List, Tuple</span>
<span id="cb2-5"><a href="mdp.html#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="mdp.html#cb2-6" tabindex="-1"></a>State <span class="op">=</span> <span class="bu">str</span></span>
<span id="cb2-7"><a href="mdp.html#cb2-7" tabindex="-1"></a>Action <span class="op">=</span> <span class="bu">str</span></span>
<span id="cb2-8"><a href="mdp.html#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="mdp.html#cb2-9" tabindex="-1"></a><span class="co"># --- MDP spec ---------------------------------------------------------------</span></span>
<span id="cb2-10"><a href="mdp.html#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="mdp.html#cb2-11" tabindex="-1"></a>S: List[State] <span class="op">=</span> [</span>
<span id="cb2-12"><a href="mdp.html#cb2-12" tabindex="-1"></a>    <span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Pass Exam&quot;</span></span>
<span id="cb2-13"><a href="mdp.html#cb2-13" tabindex="-1"></a>]</span>
<span id="cb2-14"><a href="mdp.html#cb2-14" tabindex="-1"></a>A: List[Action] <span class="op">=</span> [<span class="st">&quot;Lazy&quot;</span>, <span class="st">&quot;Productive&quot;</span>]</span>
<span id="cb2-15"><a href="mdp.html#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="mdp.html#cb2-16" tabindex="-1"></a><span class="co"># P[s, a] -&gt; list of (s_next, prob)</span></span>
<span id="cb2-17"><a href="mdp.html#cb2-17" tabindex="-1"></a>P: Dict[Tuple[State, Action], List[Tuple[State, <span class="bu">float</span>]]] <span class="op">=</span> {</span>
<span id="cb2-18"><a href="mdp.html#cb2-18" tabindex="-1"></a>    <span class="co"># Hangover</span></span>
<span id="cb2-19"><a href="mdp.html#cb2-19" tabindex="-1"></a>    (<span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-20"><a href="mdp.html#cb2-20" tabindex="-1"></a>    (<span class="st">&quot;Hangover&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Visit Lecture&quot;</span>, <span class="fl">0.3</span>), (<span class="st">&quot;Hangover&quot;</span>, <span class="fl">0.7</span>)],</span>
<span id="cb2-21"><a href="mdp.html#cb2-21" tabindex="-1"></a></span>
<span id="cb2-22"><a href="mdp.html#cb2-22" tabindex="-1"></a>    <span class="co"># Sleep</span></span>
<span id="cb2-23"><a href="mdp.html#cb2-23" tabindex="-1"></a>    (<span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):          [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-24"><a href="mdp.html#cb2-24" tabindex="-1"></a>    (<span class="st">&quot;Sleep&quot;</span>, <span class="st">&quot;Productive&quot;</span>):    [(<span class="st">&quot;Visit Lecture&quot;</span>, <span class="fl">0.6</span>), (<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">0.4</span>)],</span>
<span id="cb2-25"><a href="mdp.html#cb2-25" tabindex="-1"></a></span>
<span id="cb2-26"><a href="mdp.html#cb2-26" tabindex="-1"></a>    <span class="co"># More Sleep</span></span>
<span id="cb2-27"><a href="mdp.html#cb2-27" tabindex="-1"></a>    (<span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-28"><a href="mdp.html#cb2-28" tabindex="-1"></a>    (<span class="st">&quot;More Sleep&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Study&quot;</span>, <span class="fl">0.5</span>), (<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">0.5</span>)],</span>
<span id="cb2-29"><a href="mdp.html#cb2-29" tabindex="-1"></a></span>
<span id="cb2-30"><a href="mdp.html#cb2-30" tabindex="-1"></a>    <span class="co"># Visit Lecture</span></span>
<span id="cb2-31"><a href="mdp.html#cb2-31" tabindex="-1"></a>    (<span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Study&quot;</span>, <span class="fl">0.8</span>), (<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">0.2</span>)],</span>
<span id="cb2-32"><a href="mdp.html#cb2-32" tabindex="-1"></a>    (<span class="st">&quot;Visit Lecture&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Study&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-33"><a href="mdp.html#cb2-33" tabindex="-1"></a></span>
<span id="cb2-34"><a href="mdp.html#cb2-34" tabindex="-1"></a>    <span class="co"># Study</span></span>
<span id="cb2-35"><a href="mdp.html#cb2-35" tabindex="-1"></a>    (<span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):         [(<span class="st">&quot;More Sleep&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-36"><a href="mdp.html#cb2-36" tabindex="-1"></a>    (<span class="st">&quot;Study&quot;</span>, <span class="st">&quot;Productive&quot;</span>):   [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">0.9</span>), (<span class="st">&quot;Study&quot;</span>, <span class="fl">0.1</span>)],</span>
<span id="cb2-37"><a href="mdp.html#cb2-37" tabindex="-1"></a></span>
<span id="cb2-38"><a href="mdp.html#cb2-38" tabindex="-1"></a>    <span class="co"># Pass Exam (absorbing)</span></span>
<span id="cb2-39"><a href="mdp.html#cb2-39" tabindex="-1"></a>    (<span class="st">&quot;Pass Exam&quot;</span>, <span class="st">&quot;Lazy&quot;</span>):       [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-40"><a href="mdp.html#cb2-40" tabindex="-1"></a>    (<span class="st">&quot;Pass Exam&quot;</span>, <span class="st">&quot;Productive&quot;</span>): [(<span class="st">&quot;Pass Exam&quot;</span>, <span class="fl">1.0</span>)],</span>
<span id="cb2-41"><a href="mdp.html#cb2-41" tabindex="-1"></a>}</span>
<span id="cb2-42"><a href="mdp.html#cb2-42" tabindex="-1"></a></span>
<span id="cb2-43"><a href="mdp.html#cb2-43" tabindex="-1"></a><span class="kw">def</span> R(s: State, a: Action) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb2-44"><a href="mdp.html#cb2-44" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Reward: +1 in Pass Exam, -1 otherwise.&quot;&quot;&quot;</span></span>
<span id="cb2-45"><a href="mdp.html#cb2-45" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> s <span class="op">==</span> <span class="st">&quot;Pass Exam&quot;</span> <span class="cf">else</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb2-46"><a href="mdp.html#cb2-46" tabindex="-1"></a></span>
<span id="cb2-47"><a href="mdp.html#cb2-47" tabindex="-1"></a><span class="co"># --- Dynamic programming (Bellman optimality) -------------------------------</span></span>
<span id="cb2-48"><a href="mdp.html#cb2-48" tabindex="-1"></a></span>
<span id="cb2-49"><a href="mdp.html#cb2-49" tabindex="-1"></a><span class="kw">def</span> dynamic_programming(T: <span class="bu">int</span>):</span>
<span id="cb2-50"><a href="mdp.html#cb2-50" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-51"><a href="mdp.html#cb2-51" tabindex="-1"></a><span class="co">    Compute optimal finite-horizon tables:</span></span>
<span id="cb2-52"><a href="mdp.html#cb2-52" tabindex="-1"></a><span class="co">      - V[t][s] = V_t^*(s)</span></span>
<span id="cb2-53"><a href="mdp.html#cb2-53" tabindex="-1"></a><span class="co">      - Q[t][(s,a)] = Q_t^*(s,a)</span></span>
<span id="cb2-54"><a href="mdp.html#cb2-54" tabindex="-1"></a><span class="co">      - PI[t][s] = optimal action at (t,s)</span></span>
<span id="cb2-55"><a href="mdp.html#cb2-55" tabindex="-1"></a><span class="co">    with terminal condition V_T^* = 0.</span></span>
<span id="cb2-56"><a href="mdp.html#cb2-56" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb2-57"><a href="mdp.html#cb2-57" tabindex="-1"></a>    <span class="cf">assert</span> T <span class="op">&gt;=</span> <span class="dv">0</span></span>
<span id="cb2-58"><a href="mdp.html#cb2-58" tabindex="-1"></a></span>
<span id="cb2-59"><a href="mdp.html#cb2-59" tabindex="-1"></a>    <span class="co"># sanity: probabilities sum to 1 for each (s,a)</span></span>
<span id="cb2-60"><a href="mdp.html#cb2-60" tabindex="-1"></a>    <span class="cf">for</span> key, rows <span class="kw">in</span> P.items():</span>
<span id="cb2-61"><a href="mdp.html#cb2-61" tabindex="-1"></a>        total <span class="op">=</span> <span class="bu">sum</span>(p <span class="cf">for</span> _, p <span class="kw">in</span> rows)</span>
<span id="cb2-62"><a href="mdp.html#cb2-62" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">abs</span>(total <span class="op">-</span> <span class="fl">1.0</span>) <span class="op">&gt;</span> <span class="fl">1e-9</span>:</span>
<span id="cb2-63"><a href="mdp.html#cb2-63" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f&quot;Probabilities for </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss"> sum to </span><span class="sc">{</span>total<span class="sc">}</span><span class="ss">, not 1.&quot;</span>)</span>
<span id="cb2-64"><a href="mdp.html#cb2-64" tabindex="-1"></a></span>
<span id="cb2-65"><a href="mdp.html#cb2-65" tabindex="-1"></a>    V: Dict[<span class="bu">int</span>, Dict[State, <span class="bu">float</span>]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb2-66"><a href="mdp.html#cb2-66" tabindex="-1"></a>    Q: Dict[<span class="bu">int</span>, Dict[Tuple[State, Action], <span class="bu">float</span>]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb2-67"><a href="mdp.html#cb2-67" tabindex="-1"></a>    PI: Dict[<span class="bu">int</span>, Dict[State, Action]] <span class="op">=</span> defaultdict(<span class="bu">dict</span>)</span>
<span id="cb2-68"><a href="mdp.html#cb2-68" tabindex="-1"></a></span>
<span id="cb2-69"><a href="mdp.html#cb2-69" tabindex="-1"></a>    <span class="co"># Terminal boundary</span></span>
<span id="cb2-70"><a href="mdp.html#cb2-70" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-71"><a href="mdp.html#cb2-71" tabindex="-1"></a>        V[T][s] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-72"><a href="mdp.html#cb2-72" tabindex="-1"></a>        <span class="cf">for</span> a <span class="kw">in</span> A:</span>
<span id="cb2-73"><a href="mdp.html#cb2-73" tabindex="-1"></a>            Q[T][(s, a)] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-74"><a href="mdp.html#cb2-74" tabindex="-1"></a></span>
<span id="cb2-75"><a href="mdp.html#cb2-75" tabindex="-1"></a>    <span class="co"># Backward recursion (Bellman optimality)</span></span>
<span id="cb2-76"><a href="mdp.html#cb2-76" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb2-77"><a href="mdp.html#cb2-77" tabindex="-1"></a>        <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-78"><a href="mdp.html#cb2-78" tabindex="-1"></a>            <span class="co"># compute Q*_t(s,a)</span></span>
<span id="cb2-79"><a href="mdp.html#cb2-79" tabindex="-1"></a>            <span class="cf">for</span> a <span class="kw">in</span> A:</span>
<span id="cb2-80"><a href="mdp.html#cb2-80" tabindex="-1"></a>                exp_next <span class="op">=</span> <span class="bu">sum</span>(p <span class="op">*</span> V[t <span class="op">+</span> <span class="dv">1</span>][s_next] <span class="cf">for</span> s_next, p <span class="kw">in</span> P[(s, a)])</span>
<span id="cb2-81"><a href="mdp.html#cb2-81" tabindex="-1"></a>                Q[t][(s, a)] <span class="op">=</span> R(s, a) <span class="op">+</span> exp_next</span>
<span id="cb2-82"><a href="mdp.html#cb2-82" tabindex="-1"></a></span>
<span id="cb2-83"><a href="mdp.html#cb2-83" tabindex="-1"></a>            <span class="co"># greedy action and optimal value</span></span>
<span id="cb2-84"><a href="mdp.html#cb2-84" tabindex="-1"></a>            <span class="co"># tie-breaking is deterministic by the order in A</span></span>
<span id="cb2-85"><a href="mdp.html#cb2-85" tabindex="-1"></a>            best_a <span class="op">=</span> <span class="bu">max</span>(A, key<span class="op">=</span><span class="kw">lambda</span> a: Q[t][(s, a)])</span>
<span id="cb2-86"><a href="mdp.html#cb2-86" tabindex="-1"></a>            PI[t][s] <span class="op">=</span> best_a</span>
<span id="cb2-87"><a href="mdp.html#cb2-87" tabindex="-1"></a>            V[t][s] <span class="op">=</span> Q[t][(s, best_a)]</span>
<span id="cb2-88"><a href="mdp.html#cb2-88" tabindex="-1"></a></span>
<span id="cb2-89"><a href="mdp.html#cb2-89" tabindex="-1"></a>    <span class="cf">return</span> V, Q, PI</span>
<span id="cb2-90"><a href="mdp.html#cb2-90" tabindex="-1"></a></span>
<span id="cb2-91"><a href="mdp.html#cb2-91" tabindex="-1"></a><span class="co"># --- Example run -------------------------------------------------------------</span></span>
<span id="cb2-92"><a href="mdp.html#cb2-92" tabindex="-1"></a></span>
<span id="cb2-93"><a href="mdp.html#cb2-93" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb2-94"><a href="mdp.html#cb2-94" tabindex="-1"></a>    T <span class="op">=</span> <span class="dv">10</span>  <span class="co"># horizon</span></span>
<span id="cb2-95"><a href="mdp.html#cb2-95" tabindex="-1"></a>    V, Q, PI <span class="op">=</span> dynamic_programming(T<span class="op">=</span>T)</span>
<span id="cb2-96"><a href="mdp.html#cb2-96" tabindex="-1"></a></span>
<span id="cb2-97"><a href="mdp.html#cb2-97" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Optimal V_0(s) with T=</span><span class="sc">{</span>T<span class="sc">}</span><span class="ss">:&quot;</span>)</span>
<span id="cb2-98"><a href="mdp.html#cb2-98" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-99"><a href="mdp.html#cb2-99" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  </span><span class="sc">{</span>s<span class="sc">:13s}</span><span class="ss">: </span><span class="sc">{</span>V[<span class="dv">0</span>][s]<span class="sc">: .3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb2-100"><a href="mdp.html#cb2-100" tabindex="-1"></a></span>
<span id="cb2-101"><a href="mdp.html#cb2-101" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Greedy policy at t=0:&quot;</span>)</span>
<span id="cb2-102"><a href="mdp.html#cb2-102" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-103"><a href="mdp.html#cb2-103" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  </span><span class="sc">{</span>s<span class="sc">:13s}</span><span class="ss">: </span><span class="sc">{</span>PI[<span class="dv">0</span>][s]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb2-104"><a href="mdp.html#cb2-104" tabindex="-1"></a></span>
<span id="cb2-105"><a href="mdp.html#cb2-105" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Action value at t=0:&quot;</span>)</span>
<span id="cb2-106"><a href="mdp.html#cb2-106" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> S:</span>
<span id="cb2-107"><a href="mdp.html#cb2-107" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;  </span><span class="sc">{</span>s<span class="sc">:13s}</span><span class="ss">: </span><span class="sc">{</span>Q[<span class="dv">0</span>][s, A[<span class="dv">0</span>]]<span class="sc">: .3f}</span><span class="ss">, </span><span class="sc">{</span>Q[<span class="dv">0</span>][s, A[<span class="dv">1</span>]]<span class="sc">: .3f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<p>The optimal value function at <span class="math inline">\(t=0\)</span> is:
<span class="math display" id="eq:HangoverOptimalValueFunction">\[\begin{equation}
V^\star_0 = \begin{bmatrix}
1.259 \\
3.251 \\
3.787 \\
6.222 \\
7.778 \\
10
\end{bmatrix}.
\tag{1.16}
\end{equation}\]</span>
Clearly, the optimal value function dominates the value function shown in <a href="mdp.html#eq:HangoverRandomValueFunction">(1.8)</a> of the random policy at every state.</p>
<p>The optimal actions at <span class="math inline">\(t=0\)</span> are:
<span class="math display">\[\begin{equation}
\begin{split}
\text{Hangover} &amp; : \text{Lazy} \\
\text{Sleep}        &amp; : \text{Productive} \\
\text{More Sleep}   &amp; : \text{Productive} \\
\text{Visit Lecture} &amp; : \text{Lazy} \\
\text{Study}      &amp; : \text{Productive} \\
\text{Pass Exam}    &amp; : \text{Lazy}
\end{split}.
\end{equation}\]</span></p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/hangover_dynamic_programming.py">here</a>.</p>
</div>
</div>
</div>
</div>
<div id="InfiniteHorizonMDP" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Infinite-Horizon MDP<a href="mdp.html#InfiniteHorizonMDP" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a finite-horizon MDP, the horizon <span class="math inline">\(T\)</span> must be specified in advance in order to carry out policy evaluation and dynamic programming. The finite horizon naturally provides a terminal condition, which serves as the boundary condition that allows backward recursion to proceed.</p>
<p>In many practical applications, however, the horizon <span class="math inline">\(T\)</span> is not well defined or is difficult to determine. In such cases, it is often more natural and convenient to adopt the infinite-horizon MDP formulation.</p>
<p>An infinite-horizon MDP is given by the following tuple:
<span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma),
\]</span>
where <span class="math inline">\(\mathcal{S}\)</span>, <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(P\)</span>, and <span class="math inline">\(R\)</span> are the same as defined before in a finite-horizon MDP. We still restrict ourselves to the tabular MDP setup where <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> both have a finite number of elements.</p>
<p>The key difference between the finite-horizon and infinite-horizon formulations is that the fixed horizon <span class="math inline">\(T\)</span> is replaced by a <strong>discount factor</strong> <span class="math inline">\(\gamma \in [0,1)\)</span>. This discount factor weights future rewards less heavily than immediate rewards, as we will see shortly.</p>
<p><strong>Stationary Policy.</strong> In an infinite-horizon MDP, we focus on <em>stationary</em> policies <span class="math inline">\(\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})\)</span>, where <span class="math inline">\(\pi(a \mid s)\)</span> denotes the probability of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>.</p>
<p>In contrast, in a finite-horizon MDP we considered a tuple of <span class="math inline">\(T\)</span> policies (see <a href="mdp.html#eq:policy-tuple">(1.1)</a>), where each <span class="math inline">\(\pi_t\)</span> could vary with time (i.e., policies were non-stationary).</p>
<p>Intuitively, in the infinite-horizon setting, it suffices to consider stationary policies because the decision-making problem at time <span class="math inline">\(t\)</span> is equivalent to the problem at time <span class="math inline">\(t + k\)</span> for any <span class="math inline">\(k \in \mathbb{N}\)</span>, as both face the same infinite horizon.</p>
<p><strong>Trajectory and Return</strong>. Given an initial state <span class="math inline">\(s_0 \in \mathcal{S}\)</span> and a stationary policy <span class="math inline">\(\pi\)</span>, the MDP will evolve as</p>
<ol style="list-style-type: decimal">
<li>Start at state <span class="math inline">\(s_0\)</span></li>
<li>Take action <span class="math inline">\(a_0 \sim \pi(\cdot \mid s_0)\)</span> following policy <span class="math inline">\(\pi\)</span></li>
<li>Collect reward <span class="math inline">\(r_0 = R(s_0, a_0)\)</span></li>
<li>Transition to state <span class="math inline">\(s_1 \sim P(s&#39; \mid s_0, a_0)\)</span> following the dynamics</li>
<li>Go to step 2 and continue forever</li>
</ol>
<p>This process generates a trajectory of states, actions, and rewards:
<span class="math display">\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots).
\]</span></p>
<p>The return of a trajectory is defined as
<span class="math display">\[
g_0 = r_0 + \gamma r_1 + \gamma^2 r_2 + \dots = \sum_{t=0}^{\infty} \gamma^t r_t.
\]</span></p>
<p>Here, the discount factor <span class="math inline">\(\gamma\)</span> plays a key role: it progressively reduces the weight of rewards received further in the future, making them less influential as <span class="math inline">\(t\)</span> increases.</p>
<div id="value-functions" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Value Functions<a href="mdp.html#value-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to the case of finite-horizon MDP, we can define the state-value function and the action-value function associated with a policy <span class="math inline">\(\pi\)</span>.</p>
<p><strong>State-Value Function</strong>. The value of a state <span class="math inline">\(s \in \mathcal{S}\)</span> under policy <span class="math inline">\(\pi\)</span> is the expected discounted return obtained when starting from <span class="math inline">\(s\)</span> at time <span class="math inline">\(0\)</span>:
<span class="math display" id="eq:InfiniteHorizonMDPStateValue">\[\begin{equation}
V^{\pi}(s) := \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \,\middle|\, s_0 = s, a_t \sim \pi(\cdot \mid s_t), s_{t+1} \sim P(\cdot \mid s_t, a_t) \right].
\tag{1.17}
\end{equation}\]</span></p>
<p><strong>Action-Value Function</strong>. The value of a state-action pair <span class="math inline">\((s,a) \in \mathcal{S} \times \mathcal{A}\)</span> under policy <span class="math inline">\(\pi\)</span> is the expected discounted return obtained by first taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, and then following policy <span class="math inline">\(\pi\)</span> thereafter:
<span class="math display" id="eq:InfiniteHorizonMDPActionValue">\[\begin{equation}
Q^{\pi}(s,a) := \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \,\middle|\, s_0 = s, a_0 = a, a_t \sim \pi(\cdot \mid s_t), s_{t+1} \sim P(\cdot \mid s_t, a_t) \right].
\tag{1.18}
\end{equation}\]</span></p>
<p>Note that a nice feature of having a discount factor <span class="math inline">\(\gamma \in [0,1)\)</span> is that both the state-value and the action-value functions are guaranteed to be bounded even if the horizon is unbounded (assuming the reward function is bounded).</p>
<p>We can verify the state-value function and the action value function satisfy the following relationship:
<span class="math display" id="eq:InfiniteHorizonMDPStateValueActionValueRelation-2" id="eq:InfiniteHorizonMDPStateValueActionValueRelation-1">\[\begin{align}
V^{\pi}(s) &amp;= \sum_{a \in \mathcal{A}} \pi(a \mid s) Q^{\pi}(s,a) \tag{1.19}\\
Q^{\pi}(s,a) &amp; = R(s,a) + \gamma \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^{\pi}(s&#39;). \tag{1.20}
\end{align}\]</span></p>
<p>Combining these two equations, we arrive at the Bellman consistency result for infinite-horizon MDP.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:BellmanConsistencyInfiniteHorizon" class="proposition"><strong>Proposition 1.2  (Bellman Consistency (Infinite Horizon)) </strong></span>The state-value function <span class="math inline">\(V^{\pi}\)</span> in <a href="mdp.html#eq:InfiniteHorizonMDPStateValue">(1.17)</a> satisfies the following recursion:
<span class="math display" id="eq:BellmanConsistency-InfiniteHorizon-State-Value">\[\begin{equation}
\begin{split}
V^{\pi} (s) &amp; = \sum_{a \in \mathcal{A}} \pi (a\mid s) \left( R(s,a) + \gamma \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^{\pi} (s&#39;) \right) \\
    &amp; =: \mathbb{E}_{a \sim \pi(\cdot \mid s)} \left[ R(s, a) + \gamma \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} [V^{\pi}(s&#39;)] \right].
\end{split}
\tag{1.21}
\end{equation}\]</span></p>
<p>Similarly, the action-value function <span class="math inline">\(Q^{\pi}(s,a)\)</span> in <a href="mdp.html#eq:InfiniteHorizonMDPActionValue">(1.18)</a> satisfies the following recursion:
<span class="math display" id="eq:BellmanConsistency-InfiniteHorizon-Action-Value">\[\begin{equation}
\begin{split}
Q^{\pi} (s, a) &amp; = R(s,a) + \gamma \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) \left( \sum_{a&#39; \in \mathcal{A}} \pi(a&#39; \mid s&#39;) Q^{\pi}(s&#39;, a&#39;)\right) \\
&amp; =: R(s, a) + \gamma \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \left[\mathbb{E}_{a&#39; \sim \pi(\cdot \mid s&#39;)} [Q^{\pi}(s&#39;, a&#39;)] \right].
\end{split}
\tag{1.22}
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="policy-evaluation-1" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Policy Evaluation<a href="mdp.html#policy-evaluation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a policy <span class="math inline">\(\pi\)</span>, how can we compute its associated state-value and action-value functions?</p>
<ul>
<li><p><strong>Finite-horizon case.</strong> We initialize the terminal value function <span class="math inline">\(V_T^{\pi}(s) = 0\)</span> for every <span class="math inline">\(s \in \mathcal{S}\)</span>, and then apply the Bellman Consistency result (Proposition <a href="mdp.html#prp:BellmanConsistency">1.1</a>) to perform backward recursion.</p></li>
<li><p><strong>Infinite-horizon case.</strong> The Bellman Consistency result (Proposition <a href="mdp.html#prp:BellmanConsistencyInfiniteHorizon">1.2</a>) takes a different form and does not provide the same simple recipe for backward recursion.</p></li>
</ul>
<p><strong>System of Linear Equations.</strong> A closer look at the Bellman Consistency equation <a href="mdp.html#eq:BellmanConsistency-InfiniteHorizon-State-Value">(1.21)</a> for the state-value function shows that it defines a square system of linear equations. Specifically, the value function <span class="math inline">\(V^{\pi}\)</span> can be represented as a vector with <span class="math inline">\(|\mathcal{S}|\)</span> variables, and <a href="mdp.html#eq:BellmanConsistency-InfiniteHorizon-State-Value">(1.21)</a> provides <span class="math inline">\(|\mathcal{S}|\)</span> linear equations over these variables.<br />
Thus, one way to compute the state-value function is to set up this linear system and solve it. However, doing so typically requires matrix inversion or factorization, which can be computationally expensive.</p>
<p>The same reasoning applies to the action-value function <span class="math inline">\(Q^{\pi}\)</span>, which can be represented as a vector of <span class="math inline">\(|\mathcal{S}||\mathcal{A}|\)</span> variables constrained by <span class="math inline">\(|\mathcal{S}||\mathcal{A}|\)</span> linear equations.</p>
<p>The following proposition states that, instead of solving a linear system of equations, one can use a globally convergent iterative scheme, one that is very much like the policy evaluation algorithm for the finite-horizon MDP, to evaluate the state-value function associated with a policy <span class="math inline">\(\pi\)</span>.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:PolicyEvaluationInfiniteHorizonStateValue" class="proposition"><strong>Proposition 1.3  (Policy Evaluation (Infinite Horizon, State-Value)) </strong></span>Consider an infinite-horizon MDP <span class="math inline">\(\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,\gamma)\)</span>. Fix a policy <span class="math inline">\(\pi\)</span> and consider the iterative scheme for the state-value function:
<span class="math display" id="eq:PolicyEvaluationInfiniteHorizonStateValue">\[\begin{equation}
V_{k+1}(s) \;\; \gets \;\; \sum_{a \in \mathcal{A}} \pi(a \mid s)
\left[ R(s,a) + \gamma \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s,a) V_k(s&#39;) \right],
\quad \forall s \in \mathcal{S}.
\tag{1.23}
\end{equation}\]</span></p>
<p>Then, starting from any initialization <span class="math inline">\(V_0 \in \mathbb{R}^{|\mathcal{S}|}\)</span>, the sequence <span class="math inline">\(\{V_k\}\)</span> converges to the unique fixed point <span class="math inline">\(V^{\pi}\)</span>, the state-value function associated with policy <span class="math inline">\(\pi\)</span>.</p>
</div>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>To prove the convergence of the policy evaluation algorithm, we shall introduce the notion of a Bellman operator.</p>
<p><strong>Bellman Operator</strong>. Any value function <span class="math inline">\(V(s)\)</span> can be interpreted as a vector in <span class="math inline">\(\mathbb{R}^{|\mathcal{S}|}\)</span> (recall we are in the tabular MDP case). Given any value function <span class="math inline">\(V \in \mathbb{R}^{|\mathcal{S}|}\)</span>, and a policy <span class="math inline">\(\pi\)</span>, define the Bellman operator associated with <span class="math inline">\(\pi\)</span> as <span class="math inline">\(T^{\pi}: \mathbb{R}^{|\mathcal{S}|} \mapsto \mathbb{R}^{|\mathcal{S}|}\)</span>:
<span class="math display" id="eq:BellmanOperatorPolicy">\[\begin{equation}
(T^{\pi} V)(s) := \sum_{a \in \mathcal{A}} \pi(a \mid s)
\left[ R(s,a) + \gamma \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s,a) V(s&#39;) \right].
\tag{1.24}
\end{equation}\]</span></p>
<div class="highlightbox">
<p>We claim that <span class="math inline">\(T^{\pi}\)</span> has two important properties.</p>
<ul>
<li><p><strong>Monotonicity</strong>. If <span class="math inline">\(V \leq W\)</span> (i.e., <span class="math inline">\(V(s) \leq W(s)\)</span> for any <span class="math inline">\(s \in \mathcal{S}\)</span>), then <span class="math inline">\(T^{\pi} V \leq T^{\pi}W\)</span>. To see this, observe that
<span class="math display">\[\begin{align*}
(T^{\pi}V)(s) - (T^\pi W)(s) &amp;= \sum_{a} \pi(a \mid s) \left(\gamma \sum_{s&#39;} P(s&#39; \mid s, a) (V(s&#39;) - W(s&#39;)) \right) \\
&amp; = \gamma \mathbb{E}_{a \sim \pi(\cdot \mid s), s&#39; \sim P(\cdot \mid s,a)}[V(s&#39;) - W(s&#39;)].
\end{align*}\]</span>
Therefore, if <span class="math inline">\(V(s&#39;) - W(s&#39;) \leq 0\)</span> for any <span class="math inline">\(s&#39; \in \mathcal{S}\)</span>, then <span class="math inline">\(T^{\pi}V \leq T^{\pi} W\)</span>.</p></li>
<li><p><strong><span class="math inline">\(\gamma\)</span>-Contraction</strong>. For any value function <span class="math inline">\(V \in \mathbb{R}^{|\mathcal{S}|}\)</span>, define the <span class="math inline">\(\ell_{\infty}\)</span> norm (sup norm) as
<span class="math display">\[
\Vert V \Vert_{\infty} = \max_{s \in \mathcal{S}} |V(s)|.
\]</span></p></li>
</ul>
<p>We claim that the Bellman operator <span class="math inline">\(T^{\pi}\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction in the sup norm, i.e.,
<span class="math display" id="eq:gamma-contraction-Bellman-operator">\[\begin{equation}
\Vert T^\pi V - T^\pi W \Vert_{\infty} \leq \gamma \Vert V - W \Vert_{\infty}, \quad \forall V, W \in \mathbb{R}^{|\mathcal{S}|}.
\tag{1.25}
\end{equation}\]</span>
To prove this, observe that for any <span class="math inline">\(s \in \mathcal{S}\)</span>, we have:
<span class="math display">\[\begin{align*}
|(T^\pi V)(s) - (T^\pi W)(s)|
&amp;= \left| \sum_a \pi(a|s)\,\gamma \sum_{s&#39;} P(s&#39;|s,a)\big(V(s&#39;) - W(s&#39;)\big) \right| \\
&amp;\le \gamma \sum_a \pi(a|s)\sum_{s&#39;} P(s&#39;|s,a)\,|V(s&#39;) - W(s&#39;)| \\
&amp;\le \gamma \|V - W\|_\infty \sum_a \pi(a|s)\sum_{s&#39;} P(s&#39;|s,a) \\
&amp;= \gamma \|V - W\|_\infty.
\end{align*}\]</span></p>
<p>Taking the maximum over <span class="math inline">\(s\)</span> gives
<span class="math display">\[
\|T^\pi V - T^\pi W\|_\infty \le \gamma \|V - W\|_\infty,
\]</span>
so <span class="math inline">\(T^\pi\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction in the sup norm.</p>
</div>
<p>With the Bellman operator defined, we observe that the value function of <span class="math inline">\(\pi\)</span>, denoted <span class="math inline">\(V^{\pi}\)</span> in <a href="mdp.html#eq:BellmanConsistency-InfiniteHorizon-State-Value">(1.21)</a>, is a <strong>fixed point</strong> of <span class="math inline">\(T^{\pi}\)</span>. That is to say <span class="math inline">\(V^{\pi}\)</span> satisfies:
<span class="math display">\[
T^{\pi} V^{\pi} = V^{\pi}.
\]</span>
In other words, <span class="math inline">\(V^{\pi}\)</span> is fixed (remains unchanged) under the Bellman operator.</p>
<p>Since <span class="math inline">\(T^{\pi}\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction, by the Banach Fixed-Point Theorem, we know that there exists a unique fixed point to <span class="math inline">\(T^{\pi}\)</span>, which is <span class="math inline">\(V^{\pi}\)</span>. Moreover, since
<span class="math display">\[
\Vert V_{k} - V^{\pi} \Vert_{\infty} = \Vert T^{\pi} V_{k-1} - T^{\pi} V^{\pi} \Vert_{\infty} \leq \gamma \Vert V_{k-1} - V^{\pi} \Vert_{\infty},
\]</span>
we can deduce the rate of convergence
<span class="math display">\[
\Vert V_{k} - V^{\pi} \Vert_{\infty} \leq \gamma^{k} \Vert V_0 - V^{\pi} \Vert_{\infty}.
\]</span>
Therefore, policy evaluation globally converges from any initialization <span class="math inline">\(V_0\)</span> at a linear rate of <span class="math inline">\(\gamma\)</span>.</p>
</div>
<p>We have a similar policy evaluation algorithm for the action-value function.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:PolicyEvaluationInfiniteHorizonActionValue" class="proposition"><strong>Proposition 1.4  (Policy Evaluation (Infinite Horizon, Action-Value)) </strong></span>Fix a policy <span class="math inline">\(\pi\)</span>. Consider the iterative scheme on <span class="math inline">\(Q:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\)</span>:
<span class="math display" id="eq:PolicyEvaluationInfiniteHorizonActionValue">\[\begin{equation}
\begin{split}
Q_{k+1}(s,a) \;\gets\; R(s,a)
\;+\; \gamma \sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\!\left(\sum_{a&#39;\in\mathcal{A}} \pi(a&#39;\mid s&#39;)\, Q_k(s&#39;,a&#39;)\right),
\\
\forall (s,a)\in\mathcal{S}\times\mathcal{A}.
\end{split}
\tag{1.26}
\end{equation}\]</span>
Then, for any initialization <span class="math inline">\(Q_0\)</span>, the sequence <span class="math inline">\(\{Q_k\}\)</span> converges to the unique fixed point <span class="math inline">\(Q^{\pi}\)</span>, the action-value function associated with policy <span class="math inline">\(\pi\)</span>.</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>Define the Bellman operator on action-values
<span class="math display">\[
(T^{\pi}Q)(s,a) := R(s,a) + \gamma \sum_{s&#39;} P(s&#39;\mid s,a)\Big(\sum_{a&#39;} \pi(a&#39;\mid s&#39;)\, Q(s&#39;,a&#39;)\Big).
\]</span>
<span class="math inline">\(T^{\pi}\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction in the sup-norm on <span class="math inline">\(\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\)</span>; hence by the Banach fixed-point theorem, global convergence holds regardless of initialization.</p>
</div>
</div>
<p>Let us apply policy evaluation to an infinite-horizon MDP.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:InfiniteHorizonMDPPolicyEvaluation" class="example"><strong>Example 1.3  (Policy Evaluation for Inverted Pendulum) </strong></span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-pendulum-illustration"></span>
<img src="images/MDP/pendulum-drawing.png" alt="Inverted Pendulum." width="40%" />
<p class="caption">
Figure 1.3: Inverted Pendulum.
</p>
</div>
<p>We consider the inverted pendulum with state <span class="math inline">\(s=(\theta, \dot\theta)\)</span> and action (torque) <span class="math inline">\(a = u\)</span>, as visualized in Fig. <a href="mdp.html#fig:mdp-pendulum-illustration">1.3</a>. Our goal is to swing up the pendulum from any initial state to the upright position <span class="math inline">\(s = (0,0)\)</span>.</p>
<p><strong>Continuous-Time Dynamics</strong>. The continuous-time dynamics of the inverted pendulum is
<span class="math display">\[
\ddot{\theta} \;=\; \frac{g}{l}\sin(\theta) \;+\; \frac{1}{ml^2}u \;-\; c\,\dot{\theta},
\]</span>
where <span class="math inline">\(m &gt; 0\)</span> is the mass of the pendulum, <span class="math inline">\(l &gt; 0\)</span> is the length of the pole, <span class="math inline">\(c &gt; 0\)</span> is the damping coefficient, and <span class="math inline">\(g\)</span> is the gravitational constant.</p>
<p><strong>Discretization (Euler)</strong>. With timestep <span class="math inline">\(\Delta t\)</span>, we obtain the following discrete-time dynamics:
<span class="math display" id="eq:PendulumDynamicsDiscrete">\[\begin{equation}
\begin{split}
\theta_{k+1} &amp;= \theta_k + \Delta t \, \dot{\theta}_k, \\
\dot{\theta}_{k+1} &amp;= \dot{\theta}_k + \Delta t
\Big(\tfrac{g}{l}\sin(\theta_k) + \tfrac{1}{ml^2}u_k - c\,\dot{\theta}_k\Big).
\end{split}
\tag{1.27}
\end{equation}\]</span></p>
<p>We wrap angles to <span class="math inline">\([-\pi,\pi]\)</span> via <span class="math inline">\(\operatorname{wrap}(\theta)=\mathrm{atan2}(\sin\theta,\cos\theta)\)</span>.</p>
<p><strong>Tabular MDP</strong>. We convert the discrete-time dynamics into a tabular MDP.</p>
<ul>
<li><strong>State grid.</strong> <span class="math inline">\(\theta \in [-\pi,\pi]\)</span>, <span class="math inline">\(\dot\theta \in [-\pi,\pi]\)</span> on uniform grids:
<span class="math display">\[
\mathcal{S}=\{\;(\theta_i,\dot\theta_j)\;:\; i=1,\dots,N_\theta,\; j=1, \dots, N_{\dot\theta}\;\}.
\]</span></li>
<li><strong>Action grid.</strong> <span class="math inline">\(u \in [-mgl/2, mgl/2]\)</span> on <span class="math inline">\(N_u\)</span> uniform points:
<span class="math display">\[
\mathcal{A}=\{u_\ell:\ell=1,\dots,N_u\}.
\]</span></li>
<li><strong>Stochastic transition kernel (nearest-3 interpolation).</strong> From a grid point <span class="math inline">\(s=(\theta_i,\dot\theta_j)\)</span> and an action <span class="math inline">\(u_\ell\)</span>,
compute the next continuous state <span class="math inline">\(s^+ = (\theta^+,\dot\theta^+)\)</span> via the discrete-time dynamics in <a href="mdp.html#eq:PendulumDynamicsDiscrete">(1.27)</a>.
If <span class="math inline">\(s^+\notin\mathcal{S}\)</span>, choose the three closest grid states
<span class="math inline">\(\{s^{(1)},s^{(2)},s^{(3)}\}\)</span> by Euclidean distance in <span class="math inline">\((\theta,\dot\theta)\)</span> and assign
probabilities
<span class="math display">\[
p_r \propto \frac{1}{\|s^+ - s^{(r)}\|_2 + \varepsilon},\quad r=1,2,3,
\qquad \sum_r p_r=1,
\]</span>
so nearer grid points receive higher probability (use a small <span class="math inline">\(\varepsilon&gt;0\)</span> to avoid division by zero).</li>
<li><strong>Reward.</strong> A quadratic shaping penalty around the upright equilibrium:
<span class="math display">\[
R(s,a) = -\Big(\theta^2 + 0.1\,\dot\theta^2 + 0.01\,u^2\Big).
\]</span></li>
<li><strong>Discount.</strong> <span class="math inline">\(\gamma \in [0,1)\)</span>. We obtain a discounted, infinite-horizon, <strong>tabular</strong> MDP.</li>
</ul>
<p><strong>Policy</strong>. For policy evaluation, consider <span class="math inline">\(\pi(a\mid s)\)</span> be uniform over the discretized actions, i.e., a random policy.</p>
<p><strong>Policy Evaluation</strong>. The following python script performs policy evaluation.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="mdp.html#cb3-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="mdp.html#cb3-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="mdp.html#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="mdp.html#cb3-4" tabindex="-1"></a><span class="co"># ----- Physical &amp; MDP parameters -----</span></span>
<span id="cb3-5"><a href="mdp.html#cb3-5" tabindex="-1"></a>g, l, m, c <span class="op">=</span> <span class="fl">9.81</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span></span>
<span id="cb3-6"><a href="mdp.html#cb3-6" tabindex="-1"></a>dt <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb3-7"><a href="mdp.html#cb3-7" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.97</span></span>
<span id="cb3-8"><a href="mdp.html#cb3-8" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb3-9"><a href="mdp.html#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a href="mdp.html#cb3-10" tabindex="-1"></a><span class="co"># Grids</span></span>
<span id="cb3-11"><a href="mdp.html#cb3-11" tabindex="-1"></a>N_theta <span class="op">=</span> <span class="dv">41</span></span>
<span id="cb3-12"><a href="mdp.html#cb3-12" tabindex="-1"></a>N_thetadot <span class="op">=</span> <span class="dv">41</span></span>
<span id="cb3-13"><a href="mdp.html#cb3-13" tabindex="-1"></a>N_u <span class="op">=</span> <span class="dv">21</span></span>
<span id="cb3-14"><a href="mdp.html#cb3-14" tabindex="-1"></a></span>
<span id="cb3-15"><a href="mdp.html#cb3-15" tabindex="-1"></a>theta_grid <span class="op">=</span> np.linspace(<span class="op">-</span>np.pi, np.pi, N_theta)</span>
<span id="cb3-16"><a href="mdp.html#cb3-16" tabindex="-1"></a>thetadot_grid <span class="op">=</span> np.linspace(<span class="op">-</span>np.pi, np.pi, N_thetadot)</span>
<span id="cb3-17"><a href="mdp.html#cb3-17" tabindex="-1"></a>u_max <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> m <span class="op">*</span> g <span class="op">*</span> l</span>
<span id="cb3-18"><a href="mdp.html#cb3-18" tabindex="-1"></a>u_grid <span class="op">=</span> np.linspace(<span class="op">-</span>u_max, u_max, N_u)</span>
<span id="cb3-19"><a href="mdp.html#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="mdp.html#cb3-20" tabindex="-1"></a><span class="co"># Helpers to index/unwrap</span></span>
<span id="cb3-21"><a href="mdp.html#cb3-21" tabindex="-1"></a><span class="kw">def</span> wrap_angle(x):</span>
<span id="cb3-22"><a href="mdp.html#cb3-22" tabindex="-1"></a>    <span class="cf">return</span> np.arctan2(np.sin(x), np.cos(x))</span>
<span id="cb3-23"><a href="mdp.html#cb3-23" tabindex="-1"></a></span>
<span id="cb3-24"><a href="mdp.html#cb3-24" tabindex="-1"></a><span class="kw">def</span> state_index(i, j):</span>
<span id="cb3-25"><a href="mdp.html#cb3-25" tabindex="-1"></a>    <span class="cf">return</span> i <span class="op">*</span> N_thetadot <span class="op">+</span> j</span>
<span id="cb3-26"><a href="mdp.html#cb3-26" tabindex="-1"></a></span>
<span id="cb3-27"><a href="mdp.html#cb3-27" tabindex="-1"></a><span class="kw">def</span> index_to_state(idx):</span>
<span id="cb3-28"><a href="mdp.html#cb3-28" tabindex="-1"></a>    i <span class="op">=</span> idx <span class="op">//</span> N_thetadot</span>
<span id="cb3-29"><a href="mdp.html#cb3-29" tabindex="-1"></a>    j <span class="op">=</span> idx <span class="op">%</span> N_thetadot</span>
<span id="cb3-30"><a href="mdp.html#cb3-30" tabindex="-1"></a>    <span class="cf">return</span> theta_grid[i], thetadot_grid[j]</span>
<span id="cb3-31"><a href="mdp.html#cb3-31" tabindex="-1"></a></span>
<span id="cb3-32"><a href="mdp.html#cb3-32" tabindex="-1"></a>S <span class="op">=</span> N_theta <span class="op">*</span> N_thetadot</span>
<span id="cb3-33"><a href="mdp.html#cb3-33" tabindex="-1"></a>A <span class="op">=</span> N_u</span>
<span id="cb3-34"><a href="mdp.html#cb3-34" tabindex="-1"></a></span>
<span id="cb3-35"><a href="mdp.html#cb3-35" tabindex="-1"></a><span class="co"># ----- Dynamics step (continuous -&gt; one Euler step) -----</span></span>
<span id="cb3-36"><a href="mdp.html#cb3-36" tabindex="-1"></a><span class="kw">def</span> step_euler(theta, thetadot, u):</span>
<span id="cb3-37"><a href="mdp.html#cb3-37" tabindex="-1"></a>    theta_next <span class="op">=</span> wrap_angle(theta <span class="op">+</span> dt <span class="op">*</span> thetadot)</span>
<span id="cb3-38"><a href="mdp.html#cb3-38" tabindex="-1"></a>    thetadot_next <span class="op">=</span> thetadot <span class="op">+</span> dt <span class="op">*</span> ((g<span class="op">/</span>l) <span class="op">*</span> np.sin(theta) <span class="op">+</span> (<span class="dv">1</span><span class="op">/</span>(m<span class="op">*</span>l<span class="op">*</span>l))<span class="op">*</span>u <span class="op">-</span> c<span class="op">*</span>thetadot)</span>
<span id="cb3-39"><a href="mdp.html#cb3-39" tabindex="-1"></a>    <span class="co"># clip angular velocity to grid range (bounded MDP)</span></span>
<span id="cb3-40"><a href="mdp.html#cb3-40" tabindex="-1"></a>    thetadot_next <span class="op">=</span> np.clip(thetadot_next, thetadot_grid[<span class="dv">0</span>], thetadot_grid[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb3-41"><a href="mdp.html#cb3-41" tabindex="-1"></a>    <span class="cf">return</span> theta_next, thetadot_next</span>
<span id="cb3-42"><a href="mdp.html#cb3-42" tabindex="-1"></a></span>
<span id="cb3-43"><a href="mdp.html#cb3-43" tabindex="-1"></a><span class="co"># ----- Find 3 nearest grid states and probability weights (inverse-distance) -----</span></span>
<span id="cb3-44"><a href="mdp.html#cb3-44" tabindex="-1"></a><span class="co"># Pre-compute all grid points for fast nearest neighbor search</span></span>
<span id="cb3-45"><a href="mdp.html#cb3-45" tabindex="-1"></a>grid_pts <span class="op">=</span> np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing<span class="op">=</span><span class="st">&#39;ij&#39;</span>), axis<span class="op">=-</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-46"><a href="mdp.html#cb3-46" tabindex="-1"></a></span>
<span id="cb3-47"><a href="mdp.html#cb3-47" tabindex="-1"></a><span class="kw">def</span> nearest3_probs(theta_next, thetadot_next):</span>
<span id="cb3-48"><a href="mdp.html#cb3-48" tabindex="-1"></a>    x <span class="op">=</span> np.array([theta_next, thetadot_next])</span>
<span id="cb3-49"><a href="mdp.html#cb3-49" tabindex="-1"></a>    dists <span class="op">=</span> np.linalg.norm(grid_pts <span class="op">-</span> x[<span class="va">None</span>, :], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-50"><a href="mdp.html#cb3-50" tabindex="-1"></a>    nn_idx <span class="op">=</span> np.argpartition(dists, <span class="dv">3</span>)[:<span class="dv">3</span>]  <span class="co"># three smallest (unordered)</span></span>
<span id="cb3-51"><a href="mdp.html#cb3-51" tabindex="-1"></a>    <span class="co"># sort those 3 by distance for stability</span></span>
<span id="cb3-52"><a href="mdp.html#cb3-52" tabindex="-1"></a>    nn_idx <span class="op">=</span> nn_idx[np.argsort(dists[nn_idx])]</span>
<span id="cb3-53"><a href="mdp.html#cb3-53" tabindex="-1"></a>    d <span class="op">=</span> dists[nn_idx]</span>
<span id="cb3-54"><a href="mdp.html#cb3-54" tabindex="-1"></a>    w <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (d <span class="op">+</span> eps)</span>
<span id="cb3-55"><a href="mdp.html#cb3-55" tabindex="-1"></a>    p <span class="op">=</span> w <span class="op">/</span> w.<span class="bu">sum</span>()</span>
<span id="cb3-56"><a href="mdp.html#cb3-56" tabindex="-1"></a>    <span class="cf">return</span> nn_idx.astype(<span class="bu">int</span>), p</span>
<span id="cb3-57"><a href="mdp.html#cb3-57" tabindex="-1"></a></span>
<span id="cb3-58"><a href="mdp.html#cb3-58" tabindex="-1"></a><span class="co"># ----- Reward -----</span></span>
<span id="cb3-59"><a href="mdp.html#cb3-59" tabindex="-1"></a><span class="kw">def</span> reward(theta, thetadot, u):</span>
<span id="cb3-60"><a href="mdp.html#cb3-60" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(theta<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>thetadot<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.01</span><span class="op">*</span>u<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-61"><a href="mdp.html#cb3-61" tabindex="-1"></a></span>
<span id="cb3-62"><a href="mdp.html#cb3-62" tabindex="-1"></a><span class="co"># ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] -----</span></span>
<span id="cb3-63"><a href="mdp.html#cb3-63" tabindex="-1"></a>R <span class="op">=</span> np.zeros((S, A))</span>
<span id="cb3-64"><a href="mdp.html#cb3-64" tabindex="-1"></a>NS_idx <span class="op">=</span> np.zeros((S, A, <span class="dv">3</span>), dtype<span class="op">=</span><span class="bu">int</span>)   <span class="co"># next-state indices (3 nearest)</span></span>
<span id="cb3-65"><a href="mdp.html#cb3-65" tabindex="-1"></a>NS_prob <span class="op">=</span> np.zeros((S, A, <span class="dv">3</span>))            <span class="co"># their probabilities</span></span>
<span id="cb3-66"><a href="mdp.html#cb3-66" tabindex="-1"></a></span>
<span id="cb3-67"><a href="mdp.html#cb3-67" tabindex="-1"></a><span class="cf">for</span> i, th <span class="kw">in</span> <span class="bu">enumerate</span>(theta_grid):</span>
<span id="cb3-68"><a href="mdp.html#cb3-68" tabindex="-1"></a>    <span class="cf">for</span> j, thd <span class="kw">in</span> <span class="bu">enumerate</span>(thetadot_grid):</span>
<span id="cb3-69"><a href="mdp.html#cb3-69" tabindex="-1"></a>        s <span class="op">=</span> state_index(i, j)</span>
<span id="cb3-70"><a href="mdp.html#cb3-70" tabindex="-1"></a>        <span class="cf">for</span> a, u <span class="kw">in</span> <span class="bu">enumerate</span>(u_grid):</span>
<span id="cb3-71"><a href="mdp.html#cb3-71" tabindex="-1"></a>            <span class="co"># reward at current (s,a)</span></span>
<span id="cb3-72"><a href="mdp.html#cb3-72" tabindex="-1"></a>            R[s, a] <span class="op">=</span> reward(th, thd, u)</span>
<span id="cb3-73"><a href="mdp.html#cb3-73" tabindex="-1"></a>            <span class="co"># next continuous state</span></span>
<span id="cb3-74"><a href="mdp.html#cb3-74" tabindex="-1"></a>            th_n, thd_n <span class="op">=</span> step_euler(th, thd, u)</span>
<span id="cb3-75"><a href="mdp.html#cb3-75" tabindex="-1"></a>            <span class="co"># map to 3 nearest grid states</span></span>
<span id="cb3-76"><a href="mdp.html#cb3-76" tabindex="-1"></a>            nn_idx, p <span class="op">=</span> nearest3_probs(th_n, thd_n)</span>
<span id="cb3-77"><a href="mdp.html#cb3-77" tabindex="-1"></a>            NS_idx[s, a, :] <span class="op">=</span> nn_idx</span>
<span id="cb3-78"><a href="mdp.html#cb3-78" tabindex="-1"></a>            NS_prob[s, a, :] <span class="op">=</span> p</span>
<span id="cb3-79"><a href="mdp.html#cb3-79" tabindex="-1"></a></span>
<span id="cb3-80"><a href="mdp.html#cb3-80" tabindex="-1"></a><span class="co"># ----- Fixed policy: uniform over actions -----</span></span>
<span id="cb3-81"><a href="mdp.html#cb3-81" tabindex="-1"></a>Pi <span class="op">=</span> np.full((S, A), <span class="fl">1.0</span> <span class="op">/</span> A)</span>
<span id="cb3-82"><a href="mdp.html#cb3-82" tabindex="-1"></a></span>
<span id="cb3-83"><a href="mdp.html#cb3-83" tabindex="-1"></a><span class="co"># ----- Iterative policy evaluation -----</span></span>
<span id="cb3-84"><a href="mdp.html#cb3-84" tabindex="-1"></a>V <span class="op">=</span> np.zeros(S)  <span class="co"># initialization (any vector works)</span></span>
<span id="cb3-85"><a href="mdp.html#cb3-85" tabindex="-1"></a>tol <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb3-86"><a href="mdp.html#cb3-86" tabindex="-1"></a>max_iters <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb3-87"><a href="mdp.html#cb3-87" tabindex="-1"></a></span>
<span id="cb3-88"><a href="mdp.html#cb3-88" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb3-89"><a href="mdp.html#cb3-89" tabindex="-1"></a>    V_new <span class="op">=</span> np.zeros_like(V)</span>
<span id="cb3-90"><a href="mdp.html#cb3-90" tabindex="-1"></a>    <span class="co"># Compute Bellman update: V_{k+1}(s) = sum_a Pi(s,a)[ R(s,a) + gamma * sum_j P(s,a,j) V_k(ns_j) ]</span></span>
<span id="cb3-91"><a href="mdp.html#cb3-91" tabindex="-1"></a>    <span class="co"># First, expected next V for each (s,a)</span></span>
<span id="cb3-92"><a href="mdp.html#cb3-92" tabindex="-1"></a>    EV_next <span class="op">=</span> (NS_prob <span class="op">*</span> V[NS_idx]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">2</span>)  <span class="co"># shape: (S, A)</span></span>
<span id="cb3-93"><a href="mdp.html#cb3-93" tabindex="-1"></a>    <span class="co"># Then expectation over actions under Pi</span></span>
<span id="cb3-94"><a href="mdp.html#cb3-94" tabindex="-1"></a>    V_new <span class="op">=</span> (Pi <span class="op">*</span> (R <span class="op">+</span> gamma <span class="op">*</span> EV_next)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)  <span class="co"># shape: (S,)</span></span>
<span id="cb3-95"><a href="mdp.html#cb3-95" tabindex="-1"></a>    <span class="co"># Check convergence</span></span>
<span id="cb3-96"><a href="mdp.html#cb3-96" tabindex="-1"></a>    <span class="cf">if</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(V_new <span class="op">-</span> V)) <span class="op">&lt;</span> tol:</span>
<span id="cb3-97"><a href="mdp.html#cb3-97" tabindex="-1"></a>        V <span class="op">=</span> V_new</span>
<span id="cb3-98"><a href="mdp.html#cb3-98" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Converged in </span><span class="sc">{</span>k<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations (sup-norm change &lt; </span><span class="sc">{</span>tol<span class="sc">}</span><span class="ss">).&quot;</span>)</span>
<span id="cb3-99"><a href="mdp.html#cb3-99" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb3-100"><a href="mdp.html#cb3-100" tabindex="-1"></a>    V <span class="op">=</span> V_new</span>
<span id="cb3-101"><a href="mdp.html#cb3-101" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-102"><a href="mdp.html#cb3-102" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Reached max_iters=</span><span class="sc">{</span>max_iters<span class="sc">}</span><span class="ss"> without meeting tolerance </span><span class="sc">{</span>tol<span class="sc">}</span><span class="ss">.&quot;</span>)</span>
<span id="cb3-103"><a href="mdp.html#cb3-103" tabindex="-1"></a></span>
<span id="cb3-104"><a href="mdp.html#cb3-104" tabindex="-1"></a>V_grid <span class="op">=</span> V.reshape(N_theta, N_thetadot)</span>
<span id="cb3-105"><a href="mdp.html#cb3-105" tabindex="-1"></a></span>
<span id="cb3-106"><a href="mdp.html#cb3-106" tabindex="-1"></a><span class="co"># V_grid: shape (N_theta, N_thetadot)</span></span>
<span id="cb3-107"><a href="mdp.html#cb3-107" tabindex="-1"></a><span class="co"># theta_grid, thetadot_grid already defined</span></span>
<span id="cb3-108"><a href="mdp.html#cb3-108" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>), dpi<span class="op">=</span><span class="dv">120</span>)</span>
<span id="cb3-109"><a href="mdp.html#cb3-109" tabindex="-1"></a>im <span class="op">=</span> ax.imshow(</span>
<span id="cb3-110"><a href="mdp.html#cb3-110" tabindex="-1"></a>    V_grid,</span>
<span id="cb3-111"><a href="mdp.html#cb3-111" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">&quot;lower&quot;</span>,</span>
<span id="cb3-112"><a href="mdp.html#cb3-112" tabindex="-1"></a>    extent<span class="op">=</span>[thetadot_grid.<span class="bu">min</span>(), thetadot_grid.<span class="bu">max</span>(),</span>
<span id="cb3-113"><a href="mdp.html#cb3-113" tabindex="-1"></a>            theta_grid.<span class="bu">min</span>(), theta_grid.<span class="bu">max</span>()],</span>
<span id="cb3-114"><a href="mdp.html#cb3-114" tabindex="-1"></a>    aspect<span class="op">=</span><span class="st">&quot;auto&quot;</span>,</span>
<span id="cb3-115"><a href="mdp.html#cb3-115" tabindex="-1"></a>    cmap<span class="op">=</span><span class="st">&quot;viridis&quot;</span>  <span class="co"># any matplotlib colormap, e.g., &quot;plasma&quot;, &quot;inferno&quot;</span></span>
<span id="cb3-116"><a href="mdp.html#cb3-116" tabindex="-1"></a>)</span>
<span id="cb3-117"><a href="mdp.html#cb3-117" tabindex="-1"></a>cbar <span class="op">=</span> fig.colorbar(im, ax<span class="op">=</span>ax)</span>
<span id="cb3-118"><a href="mdp.html#cb3-118" tabindex="-1"></a>cbar.set_label(<span class="vs">r&quot;$V^\pi(\theta,\dot{\theta})$&quot;</span>)</span>
<span id="cb3-119"><a href="mdp.html#cb3-119" tabindex="-1"></a></span>
<span id="cb3-120"><a href="mdp.html#cb3-120" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r&quot;$\dot{\theta}$&quot;</span>)</span>
<span id="cb3-121"><a href="mdp.html#cb3-121" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\theta$&quot;</span>)</span>
<span id="cb3-122"><a href="mdp.html#cb3-122" tabindex="-1"></a>ax.set_title(<span class="vs">r&quot;State-value $V^\pi$ (tabular policy evaluation)&quot;</span>)</span>
<span id="cb3-123"><a href="mdp.html#cb3-123" tabindex="-1"></a></span>
<span id="cb3-124"><a href="mdp.html#cb3-124" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-125"><a href="mdp.html#cb3-125" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>Running the code, it shows that policy evaluation converges in 518 iterations under tolerance <span class="math inline">\(10^{-6}\)</span>.</p>
<p>Fig. <a href="mdp.html#fig:mdp-pendulum-value-function-policy-evaluation">1.4</a> plots the value function over the state grid.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-pendulum-value-function-policy-evaluation"></span>
<img src="images/MDP/pendulum_policy_evaluation.png" alt="Value Function from Policy Evaluation." width="80%" />
<p class="caption">
Figure 1.4: Value Function from Policy Evaluation.
</p>
</div>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_policy_evaluation.py">here</a>.</p>
</div>
</div>
</div>
<div id="principle-of-optimality" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Principle of Optimality<a href="mdp.html#principle-of-optimality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In an infinite-horizon MDP, our goal is to find the optimal policy that maximizes the expected long-term discounted return:
<span class="math display">\[
V^\star := \max_{\pi} \mathbb{E}_{s \sim \mu(\cdot)} [V^\pi(s)],
\]</span>
where <span class="math inline">\(\mu\)</span> is a given initial distribution. We call <span class="math inline">\(V^\star\)</span> the optimal value function.</p>
<p>Given a policy <span class="math inline">\(\pi\)</span> and its associated value function <span class="math inline">\(V^\pi\)</span>, how do we know if the policy is already optimal?</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:BellmanOptimalityInfiniteHorizon" class="theorem"><strong>Theorem 1.2  (Bellman Optimality (Infinite Horizon)) </strong></span>For an infinite-horizon MDP with discount factor <span class="math inline">\(\gamma \in [0,1)\)</span>,
the optimal state-value function <span class="math inline">\(V^\star(s)\)</span>
satisfies the Bellman optimality equation
<span class="math display" id="eq:BellmanOptimalityInfiniteHorizonStateValue">\[\begin{equation}
V^\star(s) \;=\; \max_{a \in \mathcal{A}}
\Big[\, R(s,a) + \gamma \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s,a)\, V^\star(s&#39;) \,\Big].
\tag{1.28}
\end{equation}\]</span></p>
<p>Define the optimal action-value function as
<span class="math display" id="eq:InfiniteHorizonOptimalActionValue">\[\begin{equation}
Q^\star(s,a) = R(s,a) + \gamma \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^\star(s&#39;).
\tag{1.29}
\end{equation}\]</span>
We have that <span class="math inline">\(Q^\star(s,a)\)</span> satisfies
<span class="math display" id="eq:BellmanOptimalityInfiniteHorizonActionValue">\[\begin{equation}
Q^\star(s,a) \;=\; R(s,a) + \gamma \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s,a)\, \left[\max_{a&#39; \in \mathcal{A}} Q^\star(s&#39;,a&#39;) \right].
\tag{1.30}
\end{equation}\]</span></p>
<p>Moreover, any greedy policy with respect to <span class="math inline">\(V^\star\)</span> (equivalently, to <span class="math inline">\(Q^\star\)</span>) is optimal:
<span class="math display" id="eq:InfiniteHorizonOptimalPolicy">\[\begin{equation}
\begin{split}
\pi^\star(s) &amp; \in \arg\max_{a \in \mathcal{A}}
\Big[\, R(s,a) + \gamma \sum_{s&#39;} P(s&#39; \mid s,a)\, V^\star(s&#39;) \,\Big]
\quad\Longleftrightarrow\quad \\
\pi^\star(s) &amp; \in \arg\max_{a \in \mathcal{A}} Q^\star(s,a).
\end{split}
\tag{1.31}
\end{equation}\]</span></p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>We will first show that <span class="math inline">\(V^\star\)</span> has statewise dominance over all other policies, and then show that <span class="math inline">\(V^\star\)</span> can be attained by the greedy policy.</p>
<p><strong>Claim.</strong> For any discounted MDP with <span class="math inline">\(\gamma \in [0,1)\)</span> and any policy <span class="math inline">\(\pi\)</span>,
<span class="math display">\[
V^\star(s) \;\ge\; V^{\pi}(s)\qquad \forall s\in\mathcal{S},
\]</span>
where <span class="math inline">\(V^\star\)</span> is the unique solution of the Bellman <strong>optimality</strong> equation and <span class="math inline">\(V^\pi\)</span> solves the Bellman <strong>consistency</strong> equation for <span class="math inline">\(\pi\)</span>.</p>
<p><strong>Proof via Bellman Operators</strong>.
Define the Bellman operators
<span class="math display">\[
(T^\pi V)(s) := \sum_{a}\pi(a\mid s)\Big[ R(s,a)+\gamma \sum_{s&#39;} P(s&#39;\mid s,a)V(s&#39;) \Big],
\]</span>
<span class="math display">\[
(T^\star V)(s) := \max_{a}\Big[ R(s,a)+\gamma \sum_{s&#39;} P(s&#39;\mid s,a)V(s&#39;) \Big].
\]</span></p>
<p>Key facts:</p>
<ol style="list-style-type: decimal">
<li>(<strong>Monotonicity</strong>) If <span class="math inline">\(V \ge W\)</span> componentwise, then <span class="math inline">\(T^\pi V \ge T^\pi W\)</span> and <span class="math inline">\(T^\star V \ge T^\star W\)</span>.</li>
<li>(<strong>Dominance of <span class="math inline">\(T^*\)</span></strong>) For any <span class="math inline">\(V\)</span> and any <span class="math inline">\(\pi\)</span>,
<span class="math display">\[
T^\star V \;\ge\; T^\pi V
\]</span>
because the max over actions is at least the <span class="math inline">\(\pi\)</span>-weighted average.</li>
<li>(<strong>Fixed points</strong>) <span class="math inline">\(V^\pi = T^\pi V^\pi\)</span> and <span class="math inline">\(V^\star = T^\star V^\star\)</span>.</li>
<li>(<strong>Contraction</strong>) Each <span class="math inline">\(T^\pi\)</span> and <span class="math inline">\(T^\star\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction in the sup-norm; hence their fixed points are unique.</li>
</ol>
<p>Now start from <span class="math inline">\(V^\pi\)</span>. Using (2),
<span class="math display">\[
V^\pi = T^\pi V^\pi \;\le\; T^\star V^\pi.
\]</span>
Applying <span class="math inline">\(T^\star\)</span> repeatedly and using (1),
<span class="math display">\[
V^\pi \;\le\; T^\star V^\pi \;\le\; (T^\star)^2 V^\pi \;\le\; \cdots
\]</span>
The sequence <span class="math inline">\((T^\star)^k V^\pi\)</span> converges (by contraction) to the unique fixed point of <span class="math inline">\(T^\star\)</span>, namely <span class="math inline">\(V^\star\)</span>. Taking limits preserves the inequality, yielding <span class="math inline">\(V^\pi \le V^\star\)</span> statewise.</p>
</div>
</div>
<p>The Bellman optimality condition tells us, if a policy <span class="math inline">\(\pi\)</span> is already greedy with respect to its value function <span class="math inline">\(V^\pi\)</span>, then <span class="math inline">\(\pi\)</span> is the optimal policy and <span class="math inline">\(V^\pi\)</span> is the optimal value function.</p>
<p>In the next, we introduce two algorithms that can guarantee finding the optimal policy and the optimal value function.</p>
<p>The first algorithm, policy iteration (PI), iterates over the space of policies; while the second algorithm, value iteration (VI), iterates over the space of value functions.</p>
</div>
<div id="policy-improvement" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Policy Improvement<a href="mdp.html#policy-improvement" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The policy evaluation algorithm enables us to compute the value functions associated with a given policy <span class="math inline">\(\pi\)</span>. The next result, known as the <em>Policy Improvement Lemma</em>, shows that once we have <span class="math inline">\(V^{\pi}\)</span>, constructing a greedy policy with respect to <span class="math inline">\(V^{\pi}\)</span> guarantees performance that is at least as good as <span class="math inline">\(\pi\)</span>, and strictly better in some states unless <span class="math inline">\(\pi\)</span> is already greedy with respect to <span class="math inline">\(V^{\pi}\)</span>.</p>
<div class="theorembox">
<div class="lemma">
<p><span id="lem:InfiniteHorizonPolicyImprovement" class="lemma"><strong>Lemma 1.1  (Policy Improvement) </strong></span>Let <span class="math inline">\(\pi\)</span> be any policy and let <span class="math inline">\(V^{\pi}\)</span> be its state-value function.<br />
Define a new policy <span class="math inline">\(\pi&#39;\)</span> such that for each state <span class="math inline">\(s\)</span>,
<span class="math display">\[
\pi&#39;(s) \in \arg\max_{a \in \mathcal{A}}
\Big[ R(s,a) + \gamma \sum_{s&#39;} P(s&#39; \mid s,a) V^{\pi}(s&#39;) \Big].
\]</span></p>
<p>Then for all states <span class="math inline">\(s \in \mathcal{S}\)</span>,
<span class="math display">\[
V^{\pi&#39;}(s) \;\ge\; V^{\pi}(s).
\]</span>
Moreover, the inequality is strict for some state <span class="math inline">\(s\)</span> unless <span class="math inline">\(\pi\)</span> is already greedy with respect to <span class="math inline">\(V^\pi\)</span> (which implies optimality).</p>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(V^{\pi}\)</span> be the value function of a policy <span class="math inline">\(\pi\)</span>, and define a new (possibly stochastic) policy <span class="math inline">\(\pi&#39;\)</span> that is greedy w.r.t. <span class="math inline">\(V^{\pi}\)</span>:
<span class="math display">\[
\pi&#39;(\cdot \mid s) \in \arg\max_{\mu \in \Delta(\mathcal{A})}
\sum_{a}\mu(a)\Big[ R(s,a) + \gamma \sum_{s&#39;} P(s&#39;\mid s,a)\, V^{\pi}(s&#39;)\Big].
\]</span>
Define the Bellman operators
<span class="math display">\[\begin{align*}
(T^{\pi}V)(s) &amp; := \sum_a \pi(a\mid s)\Big[R(s,a)+\gamma\sum_{s&#39;}P(s&#39;\mid s,a)V(s&#39;)\Big],\\
(T^{\pi&#39;}V)(s) &amp; := \sum_a \pi&#39;(a\mid s)\Big[\cdots\Big].
\end{align*}\]</span></p>
<p><strong>Step 1: One-step improvement at <span class="math inline">\(V^{\pi}\)</span></strong>.
By greediness of <span class="math inline">\(\pi&#39;\)</span> w.r.t. <span class="math inline">\(V^{\pi}\)</span>,
<span class="math display">\[
(T^{\pi&#39;} V^{\pi})(s)
= \max_{\mu}\sum_a \mu(a)\Big[R(s,a)+\gamma\sum_{s&#39;}P(s&#39;\mid s,a)V^{\pi}(s&#39;)\Big]
\;\;\ge\;\; (T^{\pi} V^{\pi})(s) = V^{\pi}(s),
\]</span>
for all <span class="math inline">\(s\)</span>. Hence
<span class="math display" id="eq:ProofPolicyImprovementStepOne">\[\begin{equation}
T^{\pi&#39;} V^{\pi} \;\ge\; V^{\pi}\quad\text{(componentwise).}
\tag{1.32}
\end{equation}\]</span></p>
<p><strong>Step 2: Monotonicity + contraction yield global improvement</strong>.
The operator <span class="math inline">\(T^{\pi&#39;}\)</span> is <strong>monotone</strong> (order-preserving) and a <strong><span class="math inline">\(\gamma\)</span>-contraction</strong> in the sup-norm.<br />
Apply <span class="math inline">\(T^{\pi&#39;}\)</span> repeatedly to both sides of <a href="mdp.html#eq:ProofPolicyImprovementStepOne">(1.32)</a>:
<span class="math display">\[
(T^{\pi&#39;})^k V^{\pi} \;\ge\; (T^{\pi&#39;})^{k-1} V^{\pi} \;\ge\; \cdots \;\ge\; V^{\pi},\qquad k=1,2,\dots
\]</span>
By contraction, <span class="math inline">\((T^{\pi&#39;})^k V^{\pi} \to V^{\pi&#39;}\)</span>, the unique fixed point of <span class="math inline">\(T^{\pi&#39;}\)</span>.<br />
Taking limits preserves the inequality, so
<span class="math display">\[
V^{\pi&#39;} \;\ge\; V^{\pi}\quad\text{statewise.}
\]</span></p>
<p><strong>Strict improvement condition</strong>.
If there exists a state <span class="math inline">\(s\)</span> such that
<span class="math display">\[
(T^{\pi&#39;} V^{\pi})(s) \;&gt;\; V^{\pi}(s),
\]</span>
then by monotonicity we have a strict increase at that state after one iteration, and the limit remains strictly larger at that state (or at any state that can reach it with positive probability under <span class="math inline">\(\pi&#39;\)</span>).<br />
This happens precisely when <span class="math inline">\(\pi&#39;\)</span> selects, with positive probability, an action <span class="math inline">\(a\)</span> for which
<span class="math display">\[
Q^{\pi}(s,a)=R(s,a) + \gamma \sum_{s&#39;} P(s&#39;\mid s,a)\, V^{\pi}(s&#39;) \;&gt;\; V^{\pi}(s),
\]</span>
i.e., when <span class="math inline">\(\pi\)</span> was not already greedy (optimal) at <span class="math inline">\(s\)</span>.</p>
</div>
</div>
</div>
<div id="policy-iteration" class="section level3 hasAnchor" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> Policy Iteration<a href="mdp.html#policy-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The policy improvement lemma and the principle of optimality, combined together, leads to the first algorithm that guarantees convergence to an optimal policy. This algorithm is called policy iteration.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:PolicyIterationConvergence" class="theorem"><strong>Theorem 1.3  (Convergence of Policy Iteration) </strong></span>Consider a discounted MDP with finite state and action sets and <span class="math inline">\(\gamma\in[0,1)\)</span>. Let <span class="math inline">\(\{\pi_k\}_{k\ge0}\)</span> be the sequence produced by <em>Policy Iteration (PI)</em>:</p>
<ol style="list-style-type: decimal">
<li><strong>Policy evaluation:</strong> compute <span class="math inline">\(V^{\pi_k}\)</span> such that <span class="math inline">\(V^{\pi_k}=T^{\pi_k}V^{\pi_k}\)</span>.</li>
<li><strong>Policy improvement:</strong> choose <span class="math inline">\(\pi_{k+1}\)</span> greedy w.r.t. <span class="math inline">\(V^{\pi_k}\)</span>:
<span class="math display">\[
\pi_{k+1}(s) \in \arg\max_{a}\Big[ R(s,a)+\gamma\sum_{s&#39;}P(s&#39;|s,a)\,V^{\pi_k}(s&#39;)\Big].
\]</span></li>
</ol>
<p>Then:</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(V^{\pi_{k+1}} \ge V^{\pi_k}\)</span> componentwise, and the inequality is strict for some state unless <span class="math inline">\(\pi_{k+1}=\pi_k\)</span>.</p></li>
<li><p>If <span class="math inline">\(\pi_{k+1}=\pi_k\)</span>, then <span class="math inline">\(V^{\pi_k}\)</span> satisfies the Bellman optimality equation; hence <span class="math inline">\(\pi_k\)</span> is optimal and <span class="math inline">\(V^{\pi_k}=V^*\)</span>.</p></li>
<li><p>Because the number of stationary policies is finite, PI terminates in finitely many iterations at an optimal policy <span class="math inline">\(\pi^*\)</span> with value <span class="math inline">\(V^*\)</span>.</p></li>
<li><p><span class="math inline">\(\Vert V^{\pi_{k+1}} - V^\star \Vert_{\infty} \leq \gamma \Vert V^{\pi_k} -  V^\star \Vert_{\infty}\)</span>, for any <span class="math inline">\(k\)</span> (i.e., contraction).</p></li>
</ol>
</div>
</div>
<div class="proofbox">
<div class="proof">
<p><span id="unlabeled-div-8" class="proof"><em>Proof</em>. </span>By the policy improvement lemma, we have
<span class="math display">\[
V^{\pi_{k+1}} \geq V^{\pi_k}.
\]</span>
By monotonicity of the Bellman operator <span class="math inline">\(T^{\pi_{k+1}}\)</span>, we have
<span class="math display">\[
V^{\pi_{k+1}} = T^{\pi_{k+1}} V^{\pi_{k+1}} \geq  T^{\pi_{k+1}} V^{\pi_k}.
\]</span>
By definition of the Bellman optimality operator, we have
<span class="math display">\[
T^{\pi_{k+1}} V^{\pi_k} = T^\star V^{\pi_k}.
\]</span>
Therefore,
<span class="math display">\[
0 \geq V^{\pi_{k+1}} - V^\star \geq T^{\pi_{k+1}} V^{\pi_k} - V^\star = T^\star V^{\pi_k} - T^\star V^\star
\]</span>
As a result,
<span class="math display">\[
\Vert V^{\pi_{k+1}} - V^\star \Vert_{\infty} \leq \Vert T^\star V^{\pi_k} - T^\star V^\star \Vert_{\infty} \leq \gamma \Vert V^{\pi_k} -  V^\star \Vert_{\infty}.
\]</span>
This proves the contraction result (d).</p>
</div>
</div>
<p>Let us apply Policy Iteration to the inverted pendulum problem.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:InvertedPendulumPolicyIteration" class="example"><strong>Example 1.4  (Policy Iteration for Inverted Pendulum) </strong></span>The following code performs policy iteration for the inverted pendulum problem.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="mdp.html#cb4-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="mdp.html#cb4-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="mdp.html#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="mdp.html#cb4-4" tabindex="-1"></a><span class="co"># ----- Physical &amp; MDP parameters -----</span></span>
<span id="cb4-5"><a href="mdp.html#cb4-5" tabindex="-1"></a>g, l, m, c <span class="op">=</span> <span class="fl">9.81</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span></span>
<span id="cb4-6"><a href="mdp.html#cb4-6" tabindex="-1"></a>dt <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb4-7"><a href="mdp.html#cb4-7" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.97</span></span>
<span id="cb4-8"><a href="mdp.html#cb4-8" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb4-9"><a href="mdp.html#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="mdp.html#cb4-10" tabindex="-1"></a><span class="co"># Grids</span></span>
<span id="cb4-11"><a href="mdp.html#cb4-11" tabindex="-1"></a>N_theta <span class="op">=</span> <span class="dv">101</span></span>
<span id="cb4-12"><a href="mdp.html#cb4-12" tabindex="-1"></a>N_thetadot <span class="op">=</span> <span class="dv">101</span></span>
<span id="cb4-13"><a href="mdp.html#cb4-13" tabindex="-1"></a>N_u <span class="op">=</span> <span class="dv">51</span></span>
<span id="cb4-14"><a href="mdp.html#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a href="mdp.html#cb4-15" tabindex="-1"></a>theta_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.5</span><span class="op">*</span>np.pi, <span class="fl">1.5</span><span class="op">*</span>np.pi, N_theta)</span>
<span id="cb4-16"><a href="mdp.html#cb4-16" tabindex="-1"></a>thetadot_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.5</span><span class="op">*</span>np.pi, <span class="fl">1.5</span><span class="op">*</span>np.pi, N_thetadot)</span>
<span id="cb4-17"><a href="mdp.html#cb4-17" tabindex="-1"></a>u_max <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> m <span class="op">*</span> g <span class="op">*</span> l</span>
<span id="cb4-18"><a href="mdp.html#cb4-18" tabindex="-1"></a>u_grid <span class="op">=</span> np.linspace(<span class="op">-</span>u_max, u_max, N_u)</span>
<span id="cb4-19"><a href="mdp.html#cb4-19" tabindex="-1"></a></span>
<span id="cb4-20"><a href="mdp.html#cb4-20" tabindex="-1"></a><span class="co"># Helpers to index/unwrap</span></span>
<span id="cb4-21"><a href="mdp.html#cb4-21" tabindex="-1"></a><span class="kw">def</span> wrap_angle(x):</span>
<span id="cb4-22"><a href="mdp.html#cb4-22" tabindex="-1"></a>    <span class="cf">return</span> np.arctan2(np.sin(x), np.cos(x))</span>
<span id="cb4-23"><a href="mdp.html#cb4-23" tabindex="-1"></a></span>
<span id="cb4-24"><a href="mdp.html#cb4-24" tabindex="-1"></a><span class="kw">def</span> state_index(i, j):</span>
<span id="cb4-25"><a href="mdp.html#cb4-25" tabindex="-1"></a>    <span class="cf">return</span> i <span class="op">*</span> N_thetadot <span class="op">+</span> j</span>
<span id="cb4-26"><a href="mdp.html#cb4-26" tabindex="-1"></a></span>
<span id="cb4-27"><a href="mdp.html#cb4-27" tabindex="-1"></a><span class="kw">def</span> index_to_state(idx):</span>
<span id="cb4-28"><a href="mdp.html#cb4-28" tabindex="-1"></a>    i <span class="op">=</span> idx <span class="op">//</span> N_thetadot</span>
<span id="cb4-29"><a href="mdp.html#cb4-29" tabindex="-1"></a>    j <span class="op">=</span> idx <span class="op">%</span> N_thetadot</span>
<span id="cb4-30"><a href="mdp.html#cb4-30" tabindex="-1"></a>    <span class="cf">return</span> theta_grid[i], thetadot_grid[j]</span>
<span id="cb4-31"><a href="mdp.html#cb4-31" tabindex="-1"></a></span>
<span id="cb4-32"><a href="mdp.html#cb4-32" tabindex="-1"></a>S <span class="op">=</span> N_theta <span class="op">*</span> N_thetadot</span>
<span id="cb4-33"><a href="mdp.html#cb4-33" tabindex="-1"></a>A <span class="op">=</span> N_u</span>
<span id="cb4-34"><a href="mdp.html#cb4-34" tabindex="-1"></a></span>
<span id="cb4-35"><a href="mdp.html#cb4-35" tabindex="-1"></a><span class="co"># ----- Dynamics step (continuous -&gt; one Euler step) -----</span></span>
<span id="cb4-36"><a href="mdp.html#cb4-36" tabindex="-1"></a><span class="kw">def</span> step_euler(theta, thetadot, u):</span>
<span id="cb4-37"><a href="mdp.html#cb4-37" tabindex="-1"></a>    theta_next <span class="op">=</span> wrap_angle(theta <span class="op">+</span> dt <span class="op">*</span> thetadot)</span>
<span id="cb4-38"><a href="mdp.html#cb4-38" tabindex="-1"></a>    thetadot_next <span class="op">=</span> thetadot <span class="op">+</span> dt <span class="op">*</span> ((g<span class="op">/</span>l) <span class="op">*</span> np.sin(theta) <span class="op">+</span> (<span class="dv">1</span><span class="op">/</span>(m<span class="op">*</span>l<span class="op">*</span>l))<span class="op">*</span>u <span class="op">-</span> c<span class="op">*</span>thetadot)</span>
<span id="cb4-39"><a href="mdp.html#cb4-39" tabindex="-1"></a>    <span class="co"># clip angular velocity to grid range (bounded MDP)</span></span>
<span id="cb4-40"><a href="mdp.html#cb4-40" tabindex="-1"></a>    thetadot_next <span class="op">=</span> np.clip(thetadot_next, thetadot_grid[<span class="dv">0</span>], thetadot_grid[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb4-41"><a href="mdp.html#cb4-41" tabindex="-1"></a>    <span class="cf">return</span> theta_next, thetadot_next</span>
<span id="cb4-42"><a href="mdp.html#cb4-42" tabindex="-1"></a></span>
<span id="cb4-43"><a href="mdp.html#cb4-43" tabindex="-1"></a><span class="co"># ----- Find 3 nearest grid states and probability weights (inverse-distance) -----</span></span>
<span id="cb4-44"><a href="mdp.html#cb4-44" tabindex="-1"></a>grid_pts <span class="op">=</span> np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing<span class="op">=</span><span class="st">&#39;ij&#39;</span>), axis<span class="op">=-</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-45"><a href="mdp.html#cb4-45" tabindex="-1"></a></span>
<span id="cb4-46"><a href="mdp.html#cb4-46" tabindex="-1"></a><span class="kw">def</span> nearest3_probs(theta_next, thetadot_next):</span>
<span id="cb4-47"><a href="mdp.html#cb4-47" tabindex="-1"></a>    x <span class="op">=</span> np.array([theta_next, thetadot_next])</span>
<span id="cb4-48"><a href="mdp.html#cb4-48" tabindex="-1"></a>    dists <span class="op">=</span> np.linalg.norm(grid_pts <span class="op">-</span> x[<span class="va">None</span>, :], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-49"><a href="mdp.html#cb4-49" tabindex="-1"></a>    nn_idx <span class="op">=</span> np.argpartition(dists, <span class="dv">3</span>)[:<span class="dv">3</span>]      <span class="co"># three smallest (unordered)</span></span>
<span id="cb4-50"><a href="mdp.html#cb4-50" tabindex="-1"></a>    nn_idx <span class="op">=</span> nn_idx[np.argsort(dists[nn_idx])]  <span class="co"># sort those 3 by distance</span></span>
<span id="cb4-51"><a href="mdp.html#cb4-51" tabindex="-1"></a>    d <span class="op">=</span> dists[nn_idx]</span>
<span id="cb4-52"><a href="mdp.html#cb4-52" tabindex="-1"></a>    w <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (d <span class="op">+</span> eps)</span>
<span id="cb4-53"><a href="mdp.html#cb4-53" tabindex="-1"></a>    p <span class="op">=</span> w <span class="op">/</span> w.<span class="bu">sum</span>()</span>
<span id="cb4-54"><a href="mdp.html#cb4-54" tabindex="-1"></a>    <span class="cf">return</span> nn_idx.astype(<span class="bu">int</span>), p</span>
<span id="cb4-55"><a href="mdp.html#cb4-55" tabindex="-1"></a></span>
<span id="cb4-56"><a href="mdp.html#cb4-56" tabindex="-1"></a><span class="co"># ----- Reward -----</span></span>
<span id="cb4-57"><a href="mdp.html#cb4-57" tabindex="-1"></a><span class="kw">def</span> reward(theta, thetadot, u):</span>
<span id="cb4-58"><a href="mdp.html#cb4-58" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(theta<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>thetadot<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.01</span><span class="op">*</span>u<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-59"><a href="mdp.html#cb4-59" tabindex="-1"></a></span>
<span id="cb4-60"><a href="mdp.html#cb4-60" tabindex="-1"></a><span class="co"># ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] -----</span></span>
<span id="cb4-61"><a href="mdp.html#cb4-61" tabindex="-1"></a>R <span class="op">=</span> np.zeros((S, A))</span>
<span id="cb4-62"><a href="mdp.html#cb4-62" tabindex="-1"></a>NS_idx <span class="op">=</span> np.zeros((S, A, <span class="dv">3</span>), dtype<span class="op">=</span><span class="bu">int</span>)   <span class="co"># next-state indices (3 nearest)</span></span>
<span id="cb4-63"><a href="mdp.html#cb4-63" tabindex="-1"></a>NS_prob <span class="op">=</span> np.zeros((S, A, <span class="dv">3</span>))             <span class="co"># their probabilities</span></span>
<span id="cb4-64"><a href="mdp.html#cb4-64" tabindex="-1"></a></span>
<span id="cb4-65"><a href="mdp.html#cb4-65" tabindex="-1"></a><span class="cf">for</span> i, th <span class="kw">in</span> <span class="bu">enumerate</span>(theta_grid):</span>
<span id="cb4-66"><a href="mdp.html#cb4-66" tabindex="-1"></a>    <span class="cf">for</span> j, thd <span class="kw">in</span> <span class="bu">enumerate</span>(thetadot_grid):</span>
<span id="cb4-67"><a href="mdp.html#cb4-67" tabindex="-1"></a>        s <span class="op">=</span> state_index(i, j)</span>
<span id="cb4-68"><a href="mdp.html#cb4-68" tabindex="-1"></a>        <span class="cf">for</span> a, u <span class="kw">in</span> <span class="bu">enumerate</span>(u_grid):</span>
<span id="cb4-69"><a href="mdp.html#cb4-69" tabindex="-1"></a>            <span class="co"># reward at current (s,a)</span></span>
<span id="cb4-70"><a href="mdp.html#cb4-70" tabindex="-1"></a>            R[s, a] <span class="op">=</span> reward(th, thd, u)</span>
<span id="cb4-71"><a href="mdp.html#cb4-71" tabindex="-1"></a>            <span class="co"># next continuous state</span></span>
<span id="cb4-72"><a href="mdp.html#cb4-72" tabindex="-1"></a>            th_n, thd_n <span class="op">=</span> step_euler(th, thd, u)</span>
<span id="cb4-73"><a href="mdp.html#cb4-73" tabindex="-1"></a>            <span class="co"># map to 3 nearest grid states</span></span>
<span id="cb4-74"><a href="mdp.html#cb4-74" tabindex="-1"></a>            nn_idx, p <span class="op">=</span> nearest3_probs(th_n, thd_n)</span>
<span id="cb4-75"><a href="mdp.html#cb4-75" tabindex="-1"></a>            NS_idx[s, a, :] <span class="op">=</span> nn_idx</span>
<span id="cb4-76"><a href="mdp.html#cb4-76" tabindex="-1"></a>            NS_prob[s, a, :] <span class="op">=</span> p</span>
<span id="cb4-77"><a href="mdp.html#cb4-77" tabindex="-1"></a></span>
<span id="cb4-78"><a href="mdp.html#cb4-78" tabindex="-1"></a><span class="co"># =======================</span></span>
<span id="cb4-79"><a href="mdp.html#cb4-79" tabindex="-1"></a><span class="co">#     POLICY ITERATION</span></span>
<span id="cb4-80"><a href="mdp.html#cb4-80" tabindex="-1"></a><span class="co"># =======================</span></span>
<span id="cb4-81"><a href="mdp.html#cb4-81" tabindex="-1"></a></span>
<span id="cb4-82"><a href="mdp.html#cb4-82" tabindex="-1"></a><span class="co"># Represent policy as a deterministic action index per state: pi[s] in {0..A-1}</span></span>
<span id="cb4-83"><a href="mdp.html#cb4-83" tabindex="-1"></a><span class="co"># Start from uniform-random policy (deterministic tie-breaker: middle action)</span></span>
<span id="cb4-84"><a href="mdp.html#cb4-84" tabindex="-1"></a>pi <span class="op">=</span> np.full(S, A <span class="op">//</span> <span class="dv">2</span>, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb4-85"><a href="mdp.html#cb4-85" tabindex="-1"></a></span>
<span id="cb4-86"><a href="mdp.html#cb4-86" tabindex="-1"></a><span class="kw">def</span> policy_evaluation(pi, V_init<span class="op">=</span><span class="va">None</span>, tol<span class="op">=</span><span class="fl">1e-6</span>, max_iters<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb4-87"><a href="mdp.html#cb4-87" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Iterative policy evaluation for deterministic pi (action index per state).&quot;&quot;&quot;</span></span>
<span id="cb4-88"><a href="mdp.html#cb4-88" tabindex="-1"></a>    V <span class="op">=</span> np.zeros(S) <span class="cf">if</span> V_init <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> V_init.copy()</span>
<span id="cb4-89"><a href="mdp.html#cb4-89" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb4-90"><a href="mdp.html#cb4-90" tabindex="-1"></a>        <span class="co"># For each state s, use chosen action a = pi[s]</span></span>
<span id="cb4-91"><a href="mdp.html#cb4-91" tabindex="-1"></a>        a <span class="op">=</span> pi  <span class="co"># shape (S,)</span></span>
<span id="cb4-92"><a href="mdp.html#cb4-92" tabindex="-1"></a>        <span class="co"># Expected next value under chosen action</span></span>
<span id="cb4-93"><a href="mdp.html#cb4-93" tabindex="-1"></a>        EV_next <span class="op">=</span> (NS_prob[np.arange(S), a] <span class="op">*</span> V[NS_idx[np.arange(S), a]]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (S,)</span></span>
<span id="cb4-94"><a href="mdp.html#cb4-94" tabindex="-1"></a>        V_new <span class="op">=</span> R[np.arange(S), a] <span class="op">+</span> gamma <span class="op">*</span> EV_next</span>
<span id="cb4-95"><a href="mdp.html#cb4-95" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(V_new <span class="op">-</span> V)) <span class="op">&lt;</span> tol:</span>
<span id="cb4-96"><a href="mdp.html#cb4-96" tabindex="-1"></a>            <span class="co"># print(f&quot;Policy evaluation converged in {k+1} iterations.&quot;)</span></span>
<span id="cb4-97"><a href="mdp.html#cb4-97" tabindex="-1"></a>            <span class="cf">return</span> V_new</span>
<span id="cb4-98"><a href="mdp.html#cb4-98" tabindex="-1"></a>        V <span class="op">=</span> V_new</span>
<span id="cb4-99"><a href="mdp.html#cb4-99" tabindex="-1"></a>    <span class="co"># print(&quot;Policy evaluation reached max_iters without meeting tolerance.&quot;)</span></span>
<span id="cb4-100"><a href="mdp.html#cb4-100" tabindex="-1"></a>    <span class="cf">return</span> V</span>
<span id="cb4-101"><a href="mdp.html#cb4-101" tabindex="-1"></a></span>
<span id="cb4-102"><a href="mdp.html#cb4-102" tabindex="-1"></a><span class="kw">def</span> policy_improvement(V, pi_old<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-103"><a href="mdp.html#cb4-103" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Greedy improvement: pi&#39;(s) = argmax_a [ R(s,a) + gamma * E[V(s&#39;)] ].&quot;&quot;&quot;</span></span>
<span id="cb4-104"><a href="mdp.html#cb4-104" tabindex="-1"></a>    <span class="co"># Compute Q(s,a) = R + gamma * sum_j P(s,a,j) V(ns_j)</span></span>
<span id="cb4-105"><a href="mdp.html#cb4-105" tabindex="-1"></a>    EV_next <span class="op">=</span> (NS_prob <span class="op">*</span> V[NS_idx]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">2</span>)      <span class="co"># (S, A)</span></span>
<span id="cb4-106"><a href="mdp.html#cb4-106" tabindex="-1"></a>    Q <span class="op">=</span> R <span class="op">+</span> gamma <span class="op">*</span> EV_next                           <span class="co"># (S, A)</span></span>
<span id="cb4-107"><a href="mdp.html#cb4-107" tabindex="-1"></a>    pi_new <span class="op">=</span> np.argmax(Q, axis<span class="op">=</span><span class="dv">1</span>).astype(<span class="bu">int</span>)         <span class="co"># greedy deterministic policy</span></span>
<span id="cb4-108"><a href="mdp.html#cb4-108" tabindex="-1"></a>    stable <span class="op">=</span> (pi_old <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>) <span class="kw">and</span> np.array_equal(pi_new, pi_old)</span>
<span id="cb4-109"><a href="mdp.html#cb4-109" tabindex="-1"></a>    <span class="cf">return</span> pi_new, stable</span>
<span id="cb4-110"><a href="mdp.html#cb4-110" tabindex="-1"></a></span>
<span id="cb4-111"><a href="mdp.html#cb4-111" tabindex="-1"></a><span class="co"># Main PI loop</span></span>
<span id="cb4-112"><a href="mdp.html#cb4-112" tabindex="-1"></a>max_pi_iters <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-113"><a href="mdp.html#cb4-113" tabindex="-1"></a>V <span class="op">=</span> np.zeros(S)</span>
<span id="cb4-114"><a href="mdp.html#cb4-114" tabindex="-1"></a><span class="cf">for</span> it <span class="kw">in</span> <span class="bu">range</span>(max_pi_iters):</span>
<span id="cb4-115"><a href="mdp.html#cb4-115" tabindex="-1"></a>    <span class="co"># Policy evaluation</span></span>
<span id="cb4-116"><a href="mdp.html#cb4-116" tabindex="-1"></a>    V <span class="op">=</span> policy_evaluation(pi, V_init<span class="op">=</span>V, tol<span class="op">=</span><span class="fl">1e-6</span>, max_iters<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb4-117"><a href="mdp.html#cb4-117" tabindex="-1"></a>    <span class="co"># Policy improvement</span></span>
<span id="cb4-118"><a href="mdp.html#cb4-118" tabindex="-1"></a>    pi_new, stable <span class="op">=</span> policy_improvement(V, pi_old<span class="op">=</span>pi)</span>
<span id="cb4-119"><a href="mdp.html#cb4-119" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;[PI] Iter </span><span class="sc">{</span>it<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: policy changed = </span><span class="sc">{</span><span class="kw">not</span> stable<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb4-120"><a href="mdp.html#cb4-120" tabindex="-1"></a>    pi <span class="op">=</span> pi_new</span>
<span id="cb4-121"><a href="mdp.html#cb4-121" tabindex="-1"></a>    <span class="cf">if</span> stable:</span>
<span id="cb4-122"><a href="mdp.html#cb4-122" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;Policy iteration converged: policy stable.&quot;</span>)</span>
<span id="cb4-123"><a href="mdp.html#cb4-123" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb4-124"><a href="mdp.html#cb4-124" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb4-125"><a href="mdp.html#cb4-125" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Reached max_pi_iters without policy stability (may still be near-optimal).&quot;</span>)</span>
<span id="cb4-126"><a href="mdp.html#cb4-126" tabindex="-1"></a></span>
<span id="cb4-127"><a href="mdp.html#cb4-127" tabindex="-1"></a><span class="co"># ----- Visualization -----</span></span>
<span id="cb4-128"><a href="mdp.html#cb4-128" tabindex="-1"></a>V_grid <span class="op">=</span> V.reshape(N_theta, N_thetadot)</span>
<span id="cb4-129"><a href="mdp.html#cb4-129" tabindex="-1"></a></span>
<span id="cb4-130"><a href="mdp.html#cb4-130" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>), dpi<span class="op">=</span><span class="dv">120</span>)</span>
<span id="cb4-131"><a href="mdp.html#cb4-131" tabindex="-1"></a>im <span class="op">=</span> ax.imshow(</span>
<span id="cb4-132"><a href="mdp.html#cb4-132" tabindex="-1"></a>    V_grid,</span>
<span id="cb4-133"><a href="mdp.html#cb4-133" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">&quot;lower&quot;</span>,</span>
<span id="cb4-134"><a href="mdp.html#cb4-134" tabindex="-1"></a>    extent<span class="op">=</span>[thetadot_grid.<span class="bu">min</span>(), thetadot_grid.<span class="bu">max</span>(),</span>
<span id="cb4-135"><a href="mdp.html#cb4-135" tabindex="-1"></a>            theta_grid.<span class="bu">min</span>(), theta_grid.<span class="bu">max</span>()],</span>
<span id="cb4-136"><a href="mdp.html#cb4-136" tabindex="-1"></a>    aspect<span class="op">=</span><span class="st">&quot;auto&quot;</span>,</span>
<span id="cb4-137"><a href="mdp.html#cb4-137" tabindex="-1"></a>    cmap<span class="op">=</span><span class="st">&quot;viridis&quot;</span></span>
<span id="cb4-138"><a href="mdp.html#cb4-138" tabindex="-1"></a>)</span>
<span id="cb4-139"><a href="mdp.html#cb4-139" tabindex="-1"></a>cbar <span class="op">=</span> fig.colorbar(im, ax<span class="op">=</span>ax)</span>
<span id="cb4-140"><a href="mdp.html#cb4-140" tabindex="-1"></a>cbar.set_label(<span class="vs">r&quot;$V^{\pi}(\theta,\dot{\theta})$ (final PI)&quot;</span>)</span>
<span id="cb4-141"><a href="mdp.html#cb4-141" tabindex="-1"></a></span>
<span id="cb4-142"><a href="mdp.html#cb4-142" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r&quot;$\dot{\theta}$&quot;</span>)</span>
<span id="cb4-143"><a href="mdp.html#cb4-143" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\theta$&quot;</span>)</span>
<span id="cb4-144"><a href="mdp.html#cb4-144" tabindex="-1"></a>ax.set_title(<span class="vs">r&quot;State-value $V$ after Policy Iteration&quot;</span>)</span>
<span id="cb4-145"><a href="mdp.html#cb4-145" tabindex="-1"></a></span>
<span id="cb4-146"><a href="mdp.html#cb4-146" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-147"><a href="mdp.html#cb4-147" tabindex="-1"></a>plt.show()</span>
<span id="cb4-148"><a href="mdp.html#cb4-148" tabindex="-1"></a></span>
<span id="cb4-149"><a href="mdp.html#cb4-149" tabindex="-1"></a><span class="co"># Visualize the greedy action *value* (torque)</span></span>
<span id="cb4-150"><a href="mdp.html#cb4-150" tabindex="-1"></a>pi_grid <span class="op">=</span> pi.reshape(N_theta, N_thetadot)          <span class="co"># action indices</span></span>
<span id="cb4-151"><a href="mdp.html#cb4-151" tabindex="-1"></a>action_values <span class="op">=</span> u_grid[pi_grid]                    <span class="co"># map indices -&gt; torques</span></span>
<span id="cb4-152"><a href="mdp.html#cb4-152" tabindex="-1"></a></span>
<span id="cb4-153"><a href="mdp.html#cb4-153" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>), dpi<span class="op">=</span><span class="dv">120</span>)</span>
<span id="cb4-154"><a href="mdp.html#cb4-154" tabindex="-1"></a>im <span class="op">=</span> plt.imshow(action_values,</span>
<span id="cb4-155"><a href="mdp.html#cb4-155" tabindex="-1"></a>           origin<span class="op">=</span><span class="st">&quot;lower&quot;</span>,</span>
<span id="cb4-156"><a href="mdp.html#cb4-156" tabindex="-1"></a>           extent<span class="op">=</span>[thetadot_grid.<span class="bu">min</span>(), thetadot_grid.<span class="bu">max</span>(),</span>
<span id="cb4-157"><a href="mdp.html#cb4-157" tabindex="-1"></a>                   theta_grid.<span class="bu">min</span>(), theta_grid.<span class="bu">max</span>()],</span>
<span id="cb4-158"><a href="mdp.html#cb4-158" tabindex="-1"></a>           aspect<span class="op">=</span><span class="st">&quot;auto&quot;</span>, cmap<span class="op">=</span><span class="st">&quot;coolwarm&quot;</span>)         <span class="co"># diverging colormap good for ± torque</span></span>
<span id="cb4-159"><a href="mdp.html#cb4-159" tabindex="-1"></a>cbar <span class="op">=</span> plt.colorbar(im)</span>
<span id="cb4-160"><a href="mdp.html#cb4-160" tabindex="-1"></a>cbar.set_label(<span class="st">&quot;Greedy action value (torque)&quot;</span>)</span>
<span id="cb4-161"><a href="mdp.html#cb4-161" tabindex="-1"></a></span>
<span id="cb4-162"><a href="mdp.html#cb4-162" tabindex="-1"></a>plt.xlabel(<span class="vs">r&quot;$\dot{\theta}$&quot;</span>)</span>
<span id="cb4-163"><a href="mdp.html#cb4-163" tabindex="-1"></a>plt.ylabel(<span class="vs">r&quot;$\theta$&quot;</span>)</span>
<span id="cb4-164"><a href="mdp.html#cb4-164" tabindex="-1"></a>plt.title(<span class="st">&quot;Greedy policy (torque) after PI&quot;</span>)</span>
<span id="cb4-165"><a href="mdp.html#cb4-165" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-166"><a href="mdp.html#cb4-166" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>Running the code produces the optimal value function shown in Fig. <a href="mdp.html#fig:mdp-pendulum-PI-value">1.5</a> and the optimal policy shown in Fig. <a href="mdp.html#fig:mdp-pendulum-PI-policy">1.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-pendulum-PI-value"></span>
<img src="images/MDP/pendulum_PI_value.png" alt="Optimal Value Function after Policy Iteration" width="80%" />
<p class="caption">
Figure 1.5: Optimal Value Function after Policy Iteration
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-pendulum-PI-policy"></span>
<img src="images/MDP/pendulum_PI_policy.png" alt="Optimal Policy after Policy Iteration" width="80%" />
<p class="caption">
Figure 1.6: Optimal Policy after Policy Iteration
</p>
</div>
<p>We can apply the optimal policy to the pendulum with an initial state of <span class="math inline">\((-\pi, 0)\)</span> (i.e., the bottomright position). Fig. <a href="mdp.html#fig:mdp-pendulum-PI-rollout-trajectory">1.7</a> plots the rollout trajectory of <span class="math inline">\(\theta, \dot{\theta}, u\)</span>. We can see that the optimal policy is capable of performing “bang-bang” control to accumulate energy before swinging up.</p>
<p>Fig. <a href="mdp.html#fig:mdp-pendulum-PI-rollout-trajectory-value">1.8</a> overlays the trajectory on top of the optimal value function.</p>
<p>You can play with the code <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_policy_iteration.py">here</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-pendulum-PI-rollout-trajectory"></span>
<img src="images/MDP/pendulum_optimal_trajectory.png" alt="Optimal Trajectory of Pendulum Swing-Up" width="90%" />
<p class="caption">
Figure 1.7: Optimal Trajectory of Pendulum Swing-Up
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-pendulum-PI-rollout-trajectory-value"></span>
<img src="images/MDP/pendulum_optimal_trajectory_value.png" alt="Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function" width="90%" />
<p class="caption">
Figure 1.8: Optimal Trajectory of Pendulum Swing-Up Overlayed with Optimal Value Function
</p>
</div>
</div>
</div>
</div>
<div id="value-iteration" class="section level3 hasAnchor" number="1.2.6">
<h3><span class="header-section-number">1.2.6</span> Value Iteration<a href="mdp.html#value-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Policy iteration—as the name suggests—iterates on <em>policies</em>: it alternates between
(1) <em>policy evaluation</em> (computing <span class="math inline">\(V^{\pi}\)</span> for the current policy <span class="math inline">\(\pi\)</span>) and
(2) <em>policy improvement</em> (making <span class="math inline">\(\pi\)</span> greedy w.r.t. <span class="math inline">\(V^{\pi}\)</span>).</p>
<p>An alternative, often very effective, method is <em>value iteration</em>. Unlike policy iteration, value iteration does <em>not</em> explicitly maintain a policy during its updates; it iterates directly on the value function toward the fixed point of the Bellman optimality* operator. Once the value function has (approximately) converged, the optimal policy is obtained by a single greedy extraction step. Note that intermediate value iterates need not correspond to the value of any actual policy.</p>
<p>The value iteration (VI) algorithm works as follows:</p>
<p><strong>Initialization.</strong> Choose any <span class="math inline">\(V_0:\mathcal{S}\to\mathbb{R}\)</span> (e.g., <span class="math inline">\(V_0 \equiv 0\)</span>).<br />
<strong>Iteration.</strong> For <span class="math inline">\(k=0,1,2,\dots\)</span>,
<span class="math display">\[
V_{k+1}(s) \;\leftarrow\; \max_{a\in\mathcal{A}}
\Big[\, R(s,a) \;+\; \gamma \sum_{s&#39;\in\mathcal{S}} P(s&#39;\mid s,a)\; V_k(s&#39;) \,\Big],
\quad \forall s\in\mathcal{S}.
\]</span>
<strong>Stopping rule.</strong> Stop when <span class="math inline">\(\lVert V_{k+1}-V_k\rVert_\infty \le \varepsilon\)</span> (or any chosen tolerance).</p>
<p><strong>Policy extraction (greedy):</strong>
<span class="math display">\[
\pi_{k+1}(s) \in \arg\max_{a\in\mathcal{A}}
\Big[\, R(s,a) \;+\; \gamma \sum_{s&#39;} P(s&#39;\mid s,a)\; V_{k+1}(s&#39;) \,\Big].
\]</span></p>
<p>The following theorem states the convergence of value iteration.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:ValueIterationConvergence" class="theorem"><strong>Theorem 1.4  (Convergence of Value Iteration) </strong></span>Let <span class="math inline">\(T^\star\)</span> be the Bellman optimality operator,
<span class="math display">\[
(T^\star V)(s) := \max_{a}\Big[ R(s,a) + \gamma \sum_{s&#39;} P(s&#39;\mid s,a)\, V(s&#39;) \Big].
\]</span>
For <span class="math inline">\(\gamma\in[0,1)\)</span> and finite <span class="math inline">\(\mathcal{S},\mathcal{A}\)</span>, <span class="math inline">\(T^\star\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction in the sup-norm. Hence, for any <span class="math inline">\(V_0\)</span>,
<span class="math display">\[
V_k \;=\; (T^\star )^k V_0 \;\xrightarrow[k\to\infty]{}\; V^*,
\]</span>
the unique fixed point of <span class="math inline">\(T^\star\)</span>. Moreover, the greedy policy <span class="math inline">\(\pi_k\)</span> extracted from <span class="math inline">\(V_k\)</span> converges to an optimal policy <span class="math inline">\(\pi^\star\)</span>.</p>
<p>In addition, after <span class="math inline">\(k\)</span> iterations, we have
<span class="math display">\[
\lVert V_k - V^* \rVert_\infty \;\le\; \gamma^k \, \lVert V_0 - V^* \rVert_\infty.
\]</span></p>
</div>
</div>
<p>Finally, we apply value iteration to the inverted pendulum problem.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:InvertedPendulumValueIteration" class="example"><strong>Example 1.5  (Value Iteration for Inverted Pendulum) </strong></span>The following code performs value iteration for the inverted pendulum problem.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="mdp.html#cb5-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="mdp.html#cb5-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="mdp.html#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="mdp.html#cb5-4" tabindex="-1"></a><span class="co"># ----- Physical &amp; MDP parameters -----</span></span>
<span id="cb5-5"><a href="mdp.html#cb5-5" tabindex="-1"></a>g, l, m, c <span class="op">=</span> <span class="fl">9.81</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span></span>
<span id="cb5-6"><a href="mdp.html#cb5-6" tabindex="-1"></a>dt <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb5-7"><a href="mdp.html#cb5-7" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.97</span></span>
<span id="cb5-8"><a href="mdp.html#cb5-8" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb5-9"><a href="mdp.html#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="mdp.html#cb5-10" tabindex="-1"></a><span class="co"># Grids</span></span>
<span id="cb5-11"><a href="mdp.html#cb5-11" tabindex="-1"></a>N_theta <span class="op">=</span> <span class="dv">101</span></span>
<span id="cb5-12"><a href="mdp.html#cb5-12" tabindex="-1"></a>N_thetadot <span class="op">=</span> <span class="dv">101</span></span>
<span id="cb5-13"><a href="mdp.html#cb5-13" tabindex="-1"></a>N_u <span class="op">=</span> <span class="dv">51</span></span>
<span id="cb5-14"><a href="mdp.html#cb5-14" tabindex="-1"></a></span>
<span id="cb5-15"><a href="mdp.html#cb5-15" tabindex="-1"></a>theta_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.5</span><span class="op">*</span>np.pi, <span class="fl">1.5</span><span class="op">*</span>np.pi, N_theta)</span>
<span id="cb5-16"><a href="mdp.html#cb5-16" tabindex="-1"></a>thetadot_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.5</span><span class="op">*</span>np.pi, <span class="fl">1.5</span><span class="op">*</span>np.pi, N_thetadot)</span>
<span id="cb5-17"><a href="mdp.html#cb5-17" tabindex="-1"></a>u_max <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> m <span class="op">*</span> g <span class="op">*</span> l</span>
<span id="cb5-18"><a href="mdp.html#cb5-18" tabindex="-1"></a>u_grid <span class="op">=</span> np.linspace(<span class="op">-</span>u_max, u_max, N_u)</span>
<span id="cb5-19"><a href="mdp.html#cb5-19" tabindex="-1"></a></span>
<span id="cb5-20"><a href="mdp.html#cb5-20" tabindex="-1"></a><span class="co"># Helpers to index/unwrap</span></span>
<span id="cb5-21"><a href="mdp.html#cb5-21" tabindex="-1"></a><span class="kw">def</span> wrap_angle(x):</span>
<span id="cb5-22"><a href="mdp.html#cb5-22" tabindex="-1"></a>    <span class="cf">return</span> np.arctan2(np.sin(x), np.cos(x))</span>
<span id="cb5-23"><a href="mdp.html#cb5-23" tabindex="-1"></a></span>
<span id="cb5-24"><a href="mdp.html#cb5-24" tabindex="-1"></a><span class="kw">def</span> state_index(i, j):</span>
<span id="cb5-25"><a href="mdp.html#cb5-25" tabindex="-1"></a>    <span class="cf">return</span> i <span class="op">*</span> N_thetadot <span class="op">+</span> j</span>
<span id="cb5-26"><a href="mdp.html#cb5-26" tabindex="-1"></a></span>
<span id="cb5-27"><a href="mdp.html#cb5-27" tabindex="-1"></a><span class="kw">def</span> index_to_state(idx):</span>
<span id="cb5-28"><a href="mdp.html#cb5-28" tabindex="-1"></a>    i <span class="op">=</span> idx <span class="op">//</span> N_thetadot</span>
<span id="cb5-29"><a href="mdp.html#cb5-29" tabindex="-1"></a>    j <span class="op">=</span> idx <span class="op">%</span> N_thetadot</span>
<span id="cb5-30"><a href="mdp.html#cb5-30" tabindex="-1"></a>    <span class="cf">return</span> theta_grid[i], thetadot_grid[j]</span>
<span id="cb5-31"><a href="mdp.html#cb5-31" tabindex="-1"></a></span>
<span id="cb5-32"><a href="mdp.html#cb5-32" tabindex="-1"></a>S <span class="op">=</span> N_theta <span class="op">*</span> N_thetadot</span>
<span id="cb5-33"><a href="mdp.html#cb5-33" tabindex="-1"></a>A <span class="op">=</span> N_u</span>
<span id="cb5-34"><a href="mdp.html#cb5-34" tabindex="-1"></a></span>
<span id="cb5-35"><a href="mdp.html#cb5-35" tabindex="-1"></a><span class="co"># ----- Dynamics step (continuous -&gt; one Euler step) -----</span></span>
<span id="cb5-36"><a href="mdp.html#cb5-36" tabindex="-1"></a><span class="kw">def</span> step_euler(theta, thetadot, u):</span>
<span id="cb5-37"><a href="mdp.html#cb5-37" tabindex="-1"></a>    theta_next <span class="op">=</span> wrap_angle(theta <span class="op">+</span> dt <span class="op">*</span> thetadot)</span>
<span id="cb5-38"><a href="mdp.html#cb5-38" tabindex="-1"></a>    thetadot_next <span class="op">=</span> thetadot <span class="op">+</span> dt <span class="op">*</span> ((g<span class="op">/</span>l) <span class="op">*</span> np.sin(theta) <span class="op">+</span> (<span class="dv">1</span><span class="op">/</span>(m<span class="op">*</span>l<span class="op">*</span>l))<span class="op">*</span>u <span class="op">-</span> c<span class="op">*</span>thetadot)</span>
<span id="cb5-39"><a href="mdp.html#cb5-39" tabindex="-1"></a>    <span class="co"># clip angular velocity to grid range (bounded MDP)</span></span>
<span id="cb5-40"><a href="mdp.html#cb5-40" tabindex="-1"></a>    thetadot_next <span class="op">=</span> np.clip(thetadot_next, thetadot_grid[<span class="dv">0</span>], thetadot_grid[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb5-41"><a href="mdp.html#cb5-41" tabindex="-1"></a>    <span class="cf">return</span> theta_next, thetadot_next</span>
<span id="cb5-42"><a href="mdp.html#cb5-42" tabindex="-1"></a></span>
<span id="cb5-43"><a href="mdp.html#cb5-43" tabindex="-1"></a><span class="co"># ----- Find 3 nearest grid states and probability weights (inverse-distance) -----</span></span>
<span id="cb5-44"><a href="mdp.html#cb5-44" tabindex="-1"></a>grid_pts <span class="op">=</span> np.stack(np.meshgrid(theta_grid, thetadot_grid, indexing<span class="op">=</span><span class="st">&#39;ij&#39;</span>), axis<span class="op">=-</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb5-45"><a href="mdp.html#cb5-45" tabindex="-1"></a></span>
<span id="cb5-46"><a href="mdp.html#cb5-46" tabindex="-1"></a><span class="kw">def</span> nearest3_probs(theta_next, thetadot_next):</span>
<span id="cb5-47"><a href="mdp.html#cb5-47" tabindex="-1"></a>    x <span class="op">=</span> np.array([theta_next, thetadot_next])</span>
<span id="cb5-48"><a href="mdp.html#cb5-48" tabindex="-1"></a>    dists <span class="op">=</span> np.linalg.norm(grid_pts <span class="op">-</span> x[<span class="va">None</span>, :], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-49"><a href="mdp.html#cb5-49" tabindex="-1"></a>    nn_idx <span class="op">=</span> np.argpartition(dists, <span class="dv">3</span>)[:<span class="dv">3</span>]      <span class="co"># three smallest (unordered)</span></span>
<span id="cb5-50"><a href="mdp.html#cb5-50" tabindex="-1"></a>    nn_idx <span class="op">=</span> nn_idx[np.argsort(dists[nn_idx])]  <span class="co"># sort those 3 by distance</span></span>
<span id="cb5-51"><a href="mdp.html#cb5-51" tabindex="-1"></a>    d <span class="op">=</span> dists[nn_idx]</span>
<span id="cb5-52"><a href="mdp.html#cb5-52" tabindex="-1"></a>    w <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (d <span class="op">+</span> eps)</span>
<span id="cb5-53"><a href="mdp.html#cb5-53" tabindex="-1"></a>    p <span class="op">=</span> w <span class="op">/</span> w.<span class="bu">sum</span>()</span>
<span id="cb5-54"><a href="mdp.html#cb5-54" tabindex="-1"></a>    <span class="cf">return</span> nn_idx.astype(<span class="bu">int</span>), p</span>
<span id="cb5-55"><a href="mdp.html#cb5-55" tabindex="-1"></a></span>
<span id="cb5-56"><a href="mdp.html#cb5-56" tabindex="-1"></a><span class="co"># ----- Reward -----</span></span>
<span id="cb5-57"><a href="mdp.html#cb5-57" tabindex="-1"></a><span class="kw">def</span> reward(theta, thetadot, u):</span>
<span id="cb5-58"><a href="mdp.html#cb5-58" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(theta<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>thetadot<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.01</span><span class="op">*</span>u<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb5-59"><a href="mdp.html#cb5-59" tabindex="-1"></a></span>
<span id="cb5-60"><a href="mdp.html#cb5-60" tabindex="-1"></a><span class="co"># ----- Build tabular MDP: R[s,a] and sparse P[s,a,3] -----</span></span>
<span id="cb5-61"><a href="mdp.html#cb5-61" tabindex="-1"></a>R <span class="op">=</span> np.zeros((S, A))</span>
<span id="cb5-62"><a href="mdp.html#cb5-62" tabindex="-1"></a>NS_idx <span class="op">=</span> np.zeros((S, A, <span class="dv">3</span>), dtype<span class="op">=</span><span class="bu">int</span>)   <span class="co"># next-state indices (3 nearest)</span></span>
<span id="cb5-63"><a href="mdp.html#cb5-63" tabindex="-1"></a>NS_prob <span class="op">=</span> np.zeros((S, A, <span class="dv">3</span>))             <span class="co"># their probabilities</span></span>
<span id="cb5-64"><a href="mdp.html#cb5-64" tabindex="-1"></a></span>
<span id="cb5-65"><a href="mdp.html#cb5-65" tabindex="-1"></a><span class="cf">for</span> i, th <span class="kw">in</span> <span class="bu">enumerate</span>(theta_grid):</span>
<span id="cb5-66"><a href="mdp.html#cb5-66" tabindex="-1"></a>    <span class="cf">for</span> j, thd <span class="kw">in</span> <span class="bu">enumerate</span>(thetadot_grid):</span>
<span id="cb5-67"><a href="mdp.html#cb5-67" tabindex="-1"></a>        s <span class="op">=</span> state_index(i, j)</span>
<span id="cb5-68"><a href="mdp.html#cb5-68" tabindex="-1"></a>        <span class="cf">for</span> a, u <span class="kw">in</span> <span class="bu">enumerate</span>(u_grid):</span>
<span id="cb5-69"><a href="mdp.html#cb5-69" tabindex="-1"></a>            R[s, a] <span class="op">=</span> reward(th, thd, u)</span>
<span id="cb5-70"><a href="mdp.html#cb5-70" tabindex="-1"></a>            th_n, thd_n <span class="op">=</span> step_euler(th, thd, u)</span>
<span id="cb5-71"><a href="mdp.html#cb5-71" tabindex="-1"></a>            nn_idx, p <span class="op">=</span> nearest3_probs(th_n, thd_n)</span>
<span id="cb5-72"><a href="mdp.html#cb5-72" tabindex="-1"></a>            NS_idx[s, a, :] <span class="op">=</span> nn_idx</span>
<span id="cb5-73"><a href="mdp.html#cb5-73" tabindex="-1"></a>            NS_prob[s, a, :] <span class="op">=</span> p</span>
<span id="cb5-74"><a href="mdp.html#cb5-74" tabindex="-1"></a></span>
<span id="cb5-75"><a href="mdp.html#cb5-75" tabindex="-1"></a><span class="co"># =======================</span></span>
<span id="cb5-76"><a href="mdp.html#cb5-76" tabindex="-1"></a><span class="co">#       VALUE ITERATION</span></span>
<span id="cb5-77"><a href="mdp.html#cb5-77" tabindex="-1"></a><span class="co"># =======================</span></span>
<span id="cb5-78"><a href="mdp.html#cb5-78" tabindex="-1"></a></span>
<span id="cb5-79"><a href="mdp.html#cb5-79" tabindex="-1"></a><span class="co"># Bellman optimality update:</span></span>
<span id="cb5-80"><a href="mdp.html#cb5-80" tabindex="-1"></a><span class="co"># V_{k+1}(s) = max_a [ R(s,a) + gamma * sum_j P(s,a,j) * V_k(ns_j) ]</span></span>
<span id="cb5-81"><a href="mdp.html#cb5-81" tabindex="-1"></a>V <span class="op">=</span> np.zeros(S)</span>
<span id="cb5-82"><a href="mdp.html#cb5-82" tabindex="-1"></a>tol <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb5-83"><a href="mdp.html#cb5-83" tabindex="-1"></a>max_vi_iters <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-84"><a href="mdp.html#cb5-84" tabindex="-1"></a></span>
<span id="cb5-85"><a href="mdp.html#cb5-85" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(max_vi_iters):</span>
<span id="cb5-86"><a href="mdp.html#cb5-86" tabindex="-1"></a>    <span class="co"># Expected next V for every (s,a), given current V_k</span></span>
<span id="cb5-87"><a href="mdp.html#cb5-87" tabindex="-1"></a>    EV_next <span class="op">=</span> (NS_prob <span class="op">*</span> V[NS_idx]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">2</span>)   <span class="co"># shape (S, A)</span></span>
<span id="cb5-88"><a href="mdp.html#cb5-88" tabindex="-1"></a>    Q <span class="op">=</span> R <span class="op">+</span> gamma <span class="op">*</span> EV_next                       <span class="co"># shape (S, A)</span></span>
<span id="cb5-89"><a href="mdp.html#cb5-89" tabindex="-1"></a>    V_new <span class="op">=</span> np.<span class="bu">max</span>(Q, axis<span class="op">=</span><span class="dv">1</span>)                     <span class="co"># greedy backup over actions</span></span>
<span id="cb5-90"><a href="mdp.html#cb5-90" tabindex="-1"></a></span>
<span id="cb5-91"><a href="mdp.html#cb5-91" tabindex="-1"></a>    delta <span class="op">=</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(V_new <span class="op">-</span> V))</span>
<span id="cb5-92"><a href="mdp.html#cb5-92" tabindex="-1"></a>    <span class="co"># Optional: a stopping rule aligned with policy loss bound could scale tol</span></span>
<span id="cb5-93"><a href="mdp.html#cb5-93" tabindex="-1"></a>    <span class="co"># e.g., stop when delta &lt;= tol * (1 - gamma) / (2 * gamma)</span></span>
<span id="cb5-94"><a href="mdp.html#cb5-94" tabindex="-1"></a>    <span class="cf">if</span> delta <span class="op">&lt;</span> tol:</span>
<span id="cb5-95"><a href="mdp.html#cb5-95" tabindex="-1"></a>        V <span class="op">=</span> V_new</span>
<span id="cb5-96"><a href="mdp.html#cb5-96" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Value Iteration converged in </span><span class="sc">{</span>k<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations (sup-norm change </span><span class="sc">{</span>delta<span class="sc">:.2e}</span><span class="ss">).&quot;</span>)</span>
<span id="cb5-97"><a href="mdp.html#cb5-97" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb5-98"><a href="mdp.html#cb5-98" tabindex="-1"></a>    V <span class="op">=</span> V_new</span>
<span id="cb5-99"><a href="mdp.html#cb5-99" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb5-100"><a href="mdp.html#cb5-100" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Reached max_vi_iters=</span><span class="sc">{</span>max_vi_iters<span class="sc">}</span><span class="ss"> (last sup-norm change </span><span class="sc">{</span>delta<span class="sc">:.2e}</span><span class="ss">).&quot;</span>)</span>
<span id="cb5-101"><a href="mdp.html#cb5-101" tabindex="-1"></a></span>
<span id="cb5-102"><a href="mdp.html#cb5-102" tabindex="-1"></a><span class="co"># Greedy policy extraction from the final V</span></span>
<span id="cb5-103"><a href="mdp.html#cb5-103" tabindex="-1"></a>EV_next <span class="op">=</span> (NS_prob <span class="op">*</span> V[NS_idx]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">2</span>)   <span class="co"># recompute with final V</span></span>
<span id="cb5-104"><a href="mdp.html#cb5-104" tabindex="-1"></a>Q <span class="op">=</span> R <span class="op">+</span> gamma <span class="op">*</span> EV_next</span>
<span id="cb5-105"><a href="mdp.html#cb5-105" tabindex="-1"></a>pi <span class="op">=</span> np.argmax(Q, axis<span class="op">=</span><span class="dv">1</span>)                     <span class="co"># deterministic greedy policy (indices)</span></span>
<span id="cb5-106"><a href="mdp.html#cb5-106" tabindex="-1"></a></span>
<span id="cb5-107"><a href="mdp.html#cb5-107" tabindex="-1"></a><span class="co"># ----- Visualization: Value function -----</span></span>
<span id="cb5-108"><a href="mdp.html#cb5-108" tabindex="-1"></a>V_grid <span class="op">=</span> V.reshape(N_theta, N_thetadot)</span>
<span id="cb5-109"><a href="mdp.html#cb5-109" tabindex="-1"></a></span>
<span id="cb5-110"><a href="mdp.html#cb5-110" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>), dpi<span class="op">=</span><span class="dv">120</span>)</span>
<span id="cb5-111"><a href="mdp.html#cb5-111" tabindex="-1"></a>im <span class="op">=</span> ax.imshow(</span>
<span id="cb5-112"><a href="mdp.html#cb5-112" tabindex="-1"></a>    V_grid,</span>
<span id="cb5-113"><a href="mdp.html#cb5-113" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">&quot;lower&quot;</span>,</span>
<span id="cb5-114"><a href="mdp.html#cb5-114" tabindex="-1"></a>    extent<span class="op">=</span>[thetadot_grid.<span class="bu">min</span>(), thetadot_grid.<span class="bu">max</span>(),</span>
<span id="cb5-115"><a href="mdp.html#cb5-115" tabindex="-1"></a>            theta_grid.<span class="bu">min</span>(), theta_grid.<span class="bu">max</span>()],</span>
<span id="cb5-116"><a href="mdp.html#cb5-116" tabindex="-1"></a>    aspect<span class="op">=</span><span class="st">&quot;auto&quot;</span>,</span>
<span id="cb5-117"><a href="mdp.html#cb5-117" tabindex="-1"></a>    cmap<span class="op">=</span><span class="st">&quot;viridis&quot;</span></span>
<span id="cb5-118"><a href="mdp.html#cb5-118" tabindex="-1"></a>)</span>
<span id="cb5-119"><a href="mdp.html#cb5-119" tabindex="-1"></a>cbar <span class="op">=</span> fig.colorbar(im, ax<span class="op">=</span>ax)</span>
<span id="cb5-120"><a href="mdp.html#cb5-120" tabindex="-1"></a>cbar.set_label(<span class="vs">r&quot;$V^*(\theta,\dot{\theta})$ (Value Iteration)&quot;</span>)</span>
<span id="cb5-121"><a href="mdp.html#cb5-121" tabindex="-1"></a></span>
<span id="cb5-122"><a href="mdp.html#cb5-122" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r&quot;$\dot{\theta}$&quot;</span>)</span>
<span id="cb5-123"><a href="mdp.html#cb5-123" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r&quot;$\theta$&quot;</span>)</span>
<span id="cb5-124"><a href="mdp.html#cb5-124" tabindex="-1"></a>ax.set_title(<span class="vs">r&quot;State-value $V$ after Value Iteration&quot;</span>)</span>
<span id="cb5-125"><a href="mdp.html#cb5-125" tabindex="-1"></a></span>
<span id="cb5-126"><a href="mdp.html#cb5-126" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-127"><a href="mdp.html#cb5-127" tabindex="-1"></a>plt.show()</span>
<span id="cb5-128"><a href="mdp.html#cb5-128" tabindex="-1"></a></span>
<span id="cb5-129"><a href="mdp.html#cb5-129" tabindex="-1"></a><span class="co"># ----- Visualization: Greedy torque field -----</span></span>
<span id="cb5-130"><a href="mdp.html#cb5-130" tabindex="-1"></a>pi_grid <span class="op">=</span> pi.reshape(N_theta, N_thetadot)   <span class="co"># action indices</span></span>
<span id="cb5-131"><a href="mdp.html#cb5-131" tabindex="-1"></a>action_values <span class="op">=</span> u_grid[pi_grid]             <span class="co"># map indices -&gt; torques</span></span>
<span id="cb5-132"><a href="mdp.html#cb5-132" tabindex="-1"></a></span>
<span id="cb5-133"><a href="mdp.html#cb5-133" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>,<span class="dv">5</span>), dpi<span class="op">=</span><span class="dv">120</span>)</span>
<span id="cb5-134"><a href="mdp.html#cb5-134" tabindex="-1"></a>im <span class="op">=</span> plt.imshow(</span>
<span id="cb5-135"><a href="mdp.html#cb5-135" tabindex="-1"></a>    action_values,</span>
<span id="cb5-136"><a href="mdp.html#cb5-136" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">&quot;lower&quot;</span>,</span>
<span id="cb5-137"><a href="mdp.html#cb5-137" tabindex="-1"></a>    extent<span class="op">=</span>[thetadot_grid.<span class="bu">min</span>(), thetadot_grid.<span class="bu">max</span>(),</span>
<span id="cb5-138"><a href="mdp.html#cb5-138" tabindex="-1"></a>            theta_grid.<span class="bu">min</span>(), theta_grid.<span class="bu">max</span>()],</span>
<span id="cb5-139"><a href="mdp.html#cb5-139" tabindex="-1"></a>    aspect<span class="op">=</span><span class="st">&quot;auto&quot;</span>,</span>
<span id="cb5-140"><a href="mdp.html#cb5-140" tabindex="-1"></a>    cmap<span class="op">=</span><span class="st">&quot;coolwarm&quot;</span>   <span class="co"># good for ± torque</span></span>
<span id="cb5-141"><a href="mdp.html#cb5-141" tabindex="-1"></a>)</span>
<span id="cb5-142"><a href="mdp.html#cb5-142" tabindex="-1"></a>cbar <span class="op">=</span> plt.colorbar(im)</span>
<span id="cb5-143"><a href="mdp.html#cb5-143" tabindex="-1"></a>cbar.set_label(<span class="st">&quot;Greedy action value (torque)&quot;</span>)</span>
<span id="cb5-144"><a href="mdp.html#cb5-144" tabindex="-1"></a></span>
<span id="cb5-145"><a href="mdp.html#cb5-145" tabindex="-1"></a>plt.xlabel(<span class="vs">r&quot;$\dot{\theta}$&quot;</span>)</span>
<span id="cb5-146"><a href="mdp.html#cb5-146" tabindex="-1"></a>plt.ylabel(<span class="vs">r&quot;$\theta$&quot;</span>)</span>
<span id="cb5-147"><a href="mdp.html#cb5-147" tabindex="-1"></a>plt.title(<span class="st">&quot;Greedy policy (torque) extracted from Value Iteration&quot;</span>)</span>
<span id="cb5-148"><a href="mdp.html#cb5-148" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-149"><a href="mdp.html#cb5-149" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>Try it for yourself <a href="https://github.com/ComputationalRobotics/2025-ES-AM-158-LECTURE-CODE/blob/main/pendulum_value_iteration.py">here</a>!</p>
<p>You should obtain the same results as policy iteration.</p>
</div>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="value-rl.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/01-mdp.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
