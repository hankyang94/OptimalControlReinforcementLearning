<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning</title>
  <meta name="description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  <meta name="github-repo" content="hankyang94/OptimalControlReinforcementLearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Markov Decision Process | Optimal Control and Reinforcement Learning" />
  
  <meta name="twitter:description" content="Lecture notes for Harvard ES/AM 158 Introduction to Optimal Control and Reinforcement Learning." />
  

<meta name="author" content="Heng Yang" />


<meta name="date" content="2025-09-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="appconvex.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimal Control and Reinforcement Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#offerings"><i class="fa fa-check"></i>Offerings</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mdp.html"><a href="mdp.html"><i class="fa fa-check"></i><b>1</b> Markov Decision Process</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP"><i class="fa fa-check"></i><b>1.1</b> Finite-Horizon MDP</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="mdp.html"><a href="mdp.html#FiniteHorizonMDP-Value"><i class="fa fa-check"></i><b>1.1.1</b> Value Functions</a></li>
<li class="chapter" data-level="1.1.2" data-path="mdp.html"><a href="mdp.html#policy-evaluation"><i class="fa fa-check"></i><b>1.1.2</b> Policy Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mdp.html"><a href="mdp.html#optimality"><i class="fa fa-check"></i><b>1.2</b> Principle of Optimality</a></li>
<li class="chapter" data-level="1.3" data-path="mdp.html"><a href="mdp.html#dp"><i class="fa fa-check"></i><b>1.3</b> Dynamic Programming</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appconvex.html"><a href="appconvex.html"><i class="fa fa-check"></i><b>A</b> Convex Analysis and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory"><i class="fa fa-check"></i><b>A.1</b> Theory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appconvex.html"><a href="appconvex.html#sets"><i class="fa fa-check"></i><b>A.1.1</b> Sets</a></li>
<li class="chapter" data-level="A.1.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-convexfunction"><i class="fa fa-check"></i><b>A.1.2</b> Convex function</a></li>
<li class="chapter" data-level="A.1.3" data-path="appconvex.html"><a href="appconvex.html#lagrange-dual"><i class="fa fa-check"></i><b>A.1.3</b> Lagrange dual</a></li>
<li class="chapter" data-level="A.1.4" data-path="appconvex.html"><a href="appconvex.html#appconvex-theory-kkt"><i class="fa fa-check"></i><b>A.1.4</b> KKT condition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appconvex.html"><a href="appconvex.html#appconvex-practice"><i class="fa fa-check"></i><b>A.2</b> Practice</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appconvex.html"><a href="appconvex.html#cvx-introduction"><i class="fa fa-check"></i><b>A.2.1</b> CVX Introduction</a></li>
<li class="chapter" data-level="A.2.2" data-path="appconvex.html"><a href="appconvex.html#linear-programming-lp"><i class="fa fa-check"></i><b>A.2.2</b> Linear Programming (LP)</a></li>
<li class="chapter" data-level="A.2.3" data-path="appconvex.html"><a href="appconvex.html#quadratic-programming-qp"><i class="fa fa-check"></i><b>A.2.3</b> Quadratic Programming (QP)</a></li>
<li class="chapter" data-level="A.2.4" data-path="appconvex.html"><a href="appconvex.html#quadratically-constrained-quadratic-programming-qcqp"><i class="fa fa-check"></i><b>A.2.4</b> Quadratically Constrained Quadratic Programming (QCQP)</a></li>
<li class="chapter" data-level="A.2.5" data-path="appconvex.html"><a href="appconvex.html#second-order-cone-programming-socp"><i class="fa fa-check"></i><b>A.2.5</b> Second-Order Cone Programming (SOCP)</a></li>
<li class="chapter" data-level="A.2.6" data-path="appconvex.html"><a href="appconvex.html#semidefinite-programming-sdp"><i class="fa fa-check"></i><b>A.2.6</b> Semidefinite Programming (SDP)</a></li>
<li class="chapter" data-level="A.2.7" data-path="appconvex.html"><a href="appconvex.html#cvxpy-introduction-and-examples"><i class="fa fa-check"></i><b>A.2.7</b> CVXPY Introduction and Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html"><i class="fa fa-check"></i><b>B</b> Linear System Theory</a>
<ul>
<li class="chapter" data-level="B.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability"><i class="fa fa-check"></i><b>B.1</b> Stability</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-ct"><i class="fa fa-check"></i><b>B.1.1</b> Continuous-Time Stability</a></li>
<li class="chapter" data-level="B.1.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-stability-dt"><i class="fa fa-check"></i><b>B.1.2</b> Discrete-Time Stability</a></li>
<li class="chapter" data-level="B.1.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#lyapunov-analysis"><i class="fa fa-check"></i><b>B.1.3</b> Lyapunov Analysis</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#app-lti-controllable-observable"><i class="fa fa-check"></i><b>B.2</b> Controllability and Observability</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>B.2.1</b> Cayley-Hamilton Theorem</a></li>
<li class="chapter" data-level="B.2.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-controllability"><i class="fa fa-check"></i><b>B.2.2</b> Equivalent Statements for Controllability</a></li>
<li class="chapter" data-level="B.2.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#duality"><i class="fa fa-check"></i><b>B.2.3</b> Duality</a></li>
<li class="chapter" data-level="B.2.4" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-observability"><i class="fa fa-check"></i><b>B.2.4</b> Equivalent Statements for Observability</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#stabilizability-and-detectability"><i class="fa fa-check"></i><b>B.3</b> Stabilizability And Detectability</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-stabilizability"><i class="fa fa-check"></i><b>B.3.1</b> Equivalent Statements for Stabilizability</a></li>
<li class="chapter" data-level="B.3.2" data-path="app-lti-system-theory.html"><a href="app-lti-system-theory.html#equivalent-statements-for-detectability"><i class="fa fa-check"></i><b>B.3.2</b> Equivalent Statements for Detectability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimal Control and Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mdp" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Markov Decision Process<a href="mdp.html#mdp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Optimal control (OC) and reinforcement learning (RL) address the problem of making <strong>optimal decisions</strong> in the presence of a <strong>dynamic environment</strong>.</p>
<ul>
<li>In <strong>optimal control</strong>, this dynamic environment is often referred to as a <em>plant</em> or a <em>dynamical system</em>.<br />
</li>
<li>In <strong>reinforcement learning</strong>, it is modeled as a <em>Markov decision process</em> (MDP).</li>
</ul>
<p>The goal in both fields is to evaluate and design decision-making strategies that optimize long-term performance:</p>
<ul>
<li><strong>RL</strong> typically frames this as maximizing a long-term <em>reward</em>.<br />
</li>
<li><strong>OC</strong> often formulates it as minimizing a long-term <em>cost</em>.</li>
</ul>
<p>The emphasis on <strong>long-term</strong> evaluation is crucial. Because the environment evolves over time, decisions that appear beneficial in the short term may lead to poor long-term outcomes and thus be suboptimal.</p>
<hr />
<p>With this motivation, we now formalize the framework of Markov Decision Processes (MDPs), which are discrete-time stochastic dynamical systems.</p>
<div id="FiniteHorizonMDP" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Finite-Horizon MDP<a href="mdp.html#FiniteHorizonMDP" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We begin with finite-horizon MDPs and introduce infinite-horizon MDPs in the following section. An abstract definition of the finite-horizon case will be presented first, followed by illustrative examples.</p>
<p>A finite-horizon MDP is given by the following tuple:
<span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, T),
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span>: state space (set of all possible states)</li>
<li><span class="math inline">\(\mathcal{A}\)</span>: action space (set of all possible actions)</li>
<li><span class="math inline">\(P(s&#39; \mid s, a)\)</span>: probability of transitioning to state <span class="math inline">\(s&#39;\)</span> from state <span class="math inline">\(s\)</span> under action <span class="math inline">\(a\)</span> (i.e., dynamics)</li>
<li><span class="math inline">\(R(s,a)\)</span>: reward of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(T\)</span>: horizon, a positive integer</li>
</ul>
<p>For now, let us assume both the state space and the action space are discrete and have a finite number of elements. In particular, denote the number of elements in <span class="math inline">\(\mathcal{S}\)</span> as <span class="math inline">\(|\mathcal{S}|\)</span>, and the number of elements in <span class="math inline">\(\mathcal{A}\)</span> as <span class="math inline">\(|\mathcal{A}|\)</span>. This is also referred to as a <em>tabular MDP</em>.</p>
<p><strong>Policy</strong>. Decision-making in MDPs is represented by policies. A policy is a function that, given any state, outputs a distribution of actions: <span class="math inline">\(\pi: \mathcal{S} \mapsto \Delta(\mathcal{A})\)</span>. That is, <span class="math inline">\(\pi(a \mid s)\)</span> returns the probability of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>. In finite-horizon MDPs, we consider a tuple of policies:
<span class="math display" id="eq:policy-tuple">\[\begin{equation}
\pi = (\pi_0, \dots, \pi_t, \dots, \pi_{T-1}),
\tag{1.1}
\end{equation}\]</span>
where each <span class="math inline">\(\pi_t\)</span> denotes the policy at step <span class="math inline">\(t \in [0,T-1]\)</span>.</p>
<p><strong>Trajectory and Return</strong>. Given an initial state <span class="math inline">\(s_0 \in \mathcal{S}\)</span> and a policy <span class="math inline">\(\pi\)</span>, the MDP will evolve as</p>
<ol style="list-style-type: decimal">
<li>Start at state <span class="math inline">\(s_0\)</span></li>
<li>Take action <span class="math inline">\(a_0 \sim \pi_0(a \mid s_0)\)</span> following policy <span class="math inline">\(\pi_0\)</span></li>
<li>Collect reward <span class="math inline">\(r_0 = R(s_0, a_0)\)</span> (assume <span class="math inline">\(R\)</span> is deterministic)</li>
<li>Transition to state <span class="math inline">\(s_1 \sim P(s&#39; \mid s_0, a_0)\)</span> following the dynamics</li>
<li>Go to step 2 and continue until reaching state <span class="math inline">\(s_T\)</span></li>
</ol>
<p>This evolution generates a trajectory of states, actions, and rewards:
<span class="math display">\[
\tau = (s_0, a_0, r_0, s_1, a_1, r_1,\dots, s_{T-1}, a_{T-1}, r_{T-1}, s_T).
\]</span>
The cumulative reward of this trajectory is <span class="math inline">\(g_0 = \sum_{t=0}^{T-1} r_t\)</span>, which is called the <em>return</em> of the trajectory. Clearly, <span class="math inline">\(g_0\)</span> is a random variable due to the stochasticity of both the policy and the dynamics. Similarly, if the state at time <span class="math inline">\(t\)</span> is <span class="math inline">\(s_t\)</span>, we denote:
<span class="math display">\[
g_t = r_t + \dots + r_{T-1}
\]</span>
as the return of the policy starting at <span class="math inline">\(s_t\)</span>.</p>
<div id="FiniteHorizonMDP-Value" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Value Functions<a href="mdp.html#FiniteHorizonMDP-Value" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>State-Value Function</strong>. Given a policy <span class="math inline">\(\pi\)</span> as in <a href="mdp.html#eq:policy-tuple">(1.1)</a>, which states are preferable at time <span class="math inline">\(t\)</span>? The (time-indexed) state-value function assigns to each <span class="math inline">\(s\in\mathcal{S}\)</span> the expected return from <span class="math inline">\(t\)</span> onward when starting in <span class="math inline">\(s\)</span> and following <span class="math inline">\(\pi\)</span> thereafter. Formally, define
<span class="math display" id="eq:FiniteHorizonMDP-state-value">\[\begin{equation}
V_t^\pi(s) := \mathbb{E} \left[g_t \mid s_t=s\right]
= \mathbb{E} \left[\sum_{i=t}^{T-1} R(s_i,a_i) \middle| s_t=s,a_i\sim \pi_i(\cdot\mid s_i), s_{i+1}\sim P(\cdot\mid s_i,a_i)\right].
\tag{1.2}
\end{equation}\]</span>
The expectation is over the randomness induced by both the policy and the dynamics. Thus, if <span class="math inline">\(V_t^\pi(s_1)&gt;V_t^\pi(s_2)\)</span>, then at time <span class="math inline">\(t\)</span> under policy <span class="math inline">\(\pi\)</span> it is better in expectation to be in <span class="math inline">\(s_1\)</span> than in <span class="math inline">\(s_2\)</span> because the former yields a larger expected return.</p>
<div class="highlightbox">
<p><span class="math inline">\(V^{\pi}_t(s)\)</span>: given policy <span class="math inline">\(\pi\)</span>, how good is it to start in state <span class="math inline">\(s\)</span> at time <span class="math inline">\(t\)</span>?</p>
</div>
<p><strong>Action-Value Function</strong>. Similarly, the action-value function assigns to each state-action pair <span class="math inline">\((s,a)\in\mathcal{S}\times\mathcal{A}\)</span> the expected return obtained by starting in state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span> first, and then following policy <span class="math inline">\(\pi\)</span> thereafter:
<span class="math display" id="eq:FiniteHorizonMDP-action-value">\[\begin{equation}
\begin{split}
Q_t^\pi(s,a) := &amp; \mathbb{E} \left[R(s,a) + g_{t+1} \mid s_{t+1} \sim P(\cdot \mid s,a)\right] \\
= &amp; \mathbb{E} \left[R(s,a) + \sum_{i=t+1}^{T-1} R(s_i, a_i) \middle| s_{t+1} \sim P(\cdot \mid s,a) \right].
\end{split}
\tag{1.3}
\end{equation}\]</span>
The key distinction is that the action-value function evaluates the return when the first action may deviate from policy <span class="math inline">\(\pi\)</span>, whereas the state-value function assumes strict adherence to <span class="math inline">\(\pi\)</span>. This flexibility makes the action-value function central to improving <span class="math inline">\(\pi\)</span>, since it reveals whether alternative actions can yield higher returns.</p>
<div class="highlightbox">
<p><span class="math inline">\(Q^{\pi}_t(s,a)\)</span>: At time <span class="math inline">\(t\)</span>, how good is it to take action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, then follow the policy <span class="math inline">\(\pi\)</span>?</p>
</div>
<p>It is easy to verify that the state-value function and the action-value function satisfy:
<span class="math display" id="eq:FiniteHorizonMDP-action-value-from-state-value" id="eq:FiniteHorizonMDP-state-value-from-action-value">\[\begin{align}
V_t^{\pi}(s) &amp; = \sum_{a \in \mathcal{A}} \pi_t(a \mid s) Q_t^{\pi}(s,a), \tag{1.4} \\
Q_t^{\pi}(s,a) &amp; = R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^{\pi}_{t+1} (s&#39;). \tag{1.5}
\end{align}\]</span></p>
<p>From these two equations, we can derive the Bellman Consistency equations.</p>
<div class="theorembox">
<div class="proposition">
<p><span id="prp:BellmanConsistency" class="proposition"><strong>Proposition 1.1  (Bellman Consistency) </strong></span>The state-value function <span class="math inline">\(V^{\pi}_t(\cdot)\)</span> in <a href="mdp.html#eq:FiniteHorizonMDP-state-value">(1.2)</a> satisfies the following recursion:
<span class="math display" id="eq:BellmanConsistency-State-Value">\[\begin{equation}
\begin{split}
V^{\pi}_t(s) &amp; = \sum_{a \in \mathcal{A}} \pi_t(a\mid s) \left( R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) V^{\pi}_{t+1} (s&#39;) \right) \\
    &amp; =: \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} [V^{\pi}_{t+1}(s&#39;)] \right].
\end{split}
\tag{1.6}
\end{equation}\]</span></p>
<p>Similarly, the action-value function <span class="math inline">\(Q^{\pi}_t(s,a)\)</span> in <a href="mdp.html#eq:FiniteHorizonMDP-action-value">(1.3)</a> satisfies the following recursion:
<span class="math display" id="eq:BellmanConsistency-Action-Value">\[\begin{equation}
\begin{split}
Q^{\pi}_t (s, a) &amp; = R(s,a) + \sum_{s&#39; \in \mathcal{S}} P(s&#39; \mid s, a) \left( \sum_{a&#39; \in \mathcal{A}} \pi_{t+1}(a&#39; \mid s&#39;) Q^{\pi}_{t+1}(s&#39;, a&#39;)\right) \\
&amp; =: R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \left[\mathbb{E}_{a&#39; \sim \pi_{t+1}(\cdot \mid s&#39;)} [Q^{\pi}_{t+1}(s&#39;, a&#39;)] \right].
\end{split}
\tag{1.7}
\end{equation}\]</span></p>
</div>
</div>
</div>
<div id="policy-evaluation" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Policy Evaluation<a href="mdp.html#policy-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Bellman consistency result in Proposition <a href="mdp.html#prp:BellmanConsistency">1.1</a> is fundamental because it directly yields an algorithm for evaluating a given policy <span class="math inline">\(\pi\)</span>—that is, for computing its state-value and action-value functions—provided the transition dynamics of the MDP are known.</p>
<p>Policy evaluation for the state-value function proceeds as follows:</p>
<ul>
<li><strong>Initialization:</strong> set <span class="math inline">\(V^{\pi}_T(s) = 0\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>.<br />
</li>
<li><strong>Backward recursion:</strong> for <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>, update each <span class="math inline">\(s \in \mathcal{S}\)</span> by
<span class="math display">\[
V^{\pi}_{t}(s) = \mathbb{E}_{a \sim \pi_t(\cdot \mid s)} \left[ R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \big[ V^{\pi}_{t+1}(s&#39;) \big] \right].
\]</span></li>
</ul>
<p>Similarly, policy evaluation for the action-value function is given by:</p>
<ul>
<li><strong>Initialization:</strong> set <span class="math inline">\(Q^{\pi}_T(s,a) = 0\)</span> for all <span class="math inline">\(s \in \mathcal{S}, a \in \mathcal{A}\)</span>.<br />
</li>
<li><strong>Backward recursion:</strong> for <span class="math inline">\(t = T-1, T-2, \dots, 0\)</span>, update each <span class="math inline">\((s,a) \in \mathcal{S}\times\mathcal{A}\)</span> by
<span class="math display">\[
Q^{\pi}_t(s,a) = R(s, a) + \mathbb{E}_{s&#39; \sim P(\cdot \mid s, a)} \left[ \mathbb{E}_{a&#39; \sim \pi_{t+1}(\cdot \mid s&#39;)} \big[ Q^{\pi}_{t+1}(s&#39;, a&#39;) \big] \right].
\]</span></li>
</ul>
<p>The essential feature of this algorithm is its backward-in-time recursion: the value functions are first set at the terminal horizon <span class="math inline">\(T\)</span>, and then propagated backward step by step through the Bellman consistency equations.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:MDPExampleGraph" class="example"><strong>Example 1.1  (MDP, Transition Graph, and Policy Evaluation) </strong></span>It is often useful to visualize small MDPs as transition graphs, where states are represented by nodes and actions are represented by directed edges connecting those nodes.</p>
<p>As a simple illustrative example, consider a robot navigating on a two-state grid. At each step, the robot can either Stay in its current state or Move to the other state. This finite-horizon MDP is fully specified by the tuple of states, actions, transition dynamics, rewards, and horizon:</p>
<ul>
<li><p>States: <span class="math inline">\(\mathcal{S} = \{\alpha, \beta \}\)</span></p></li>
<li><p>Actions: <span class="math inline">\(\mathcal{A} = \{\text{Move} , \text{Stay} \}\)</span></p></li>
<li><p>Transition dynamics: we can specify the transition dynamics in the following table</p></li>
</ul>
<table>
<colgroup>
<col width="20%" />
<col width="22%" />
<col width="34%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">State <span class="math inline">\(s\)</span></th>
<th align="center">Action <span class="math inline">\(a\)</span></th>
<th align="center">Next State <span class="math inline">\(s&#39;\)</span></th>
<th align="center">Probability <span class="math inline">\(P(s&#39; \mid s, a)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">Stay</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">Move</td>
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">Stay</td>
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\beta\)</span></td>
<td align="center">Move</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Reward: <span class="math inline">\(R(s,a)=1\)</span> if <span class="math inline">\(a = \text{Move}\)</span> and <span class="math inline">\(R(s,a)=0\)</span> if <span class="math inline">\(a = \text{Stay}\)</span></p></li>
<li><p>Horizon: <span class="math inline">\(T=2\)</span>.</p></li>
</ul>
<p>This MDP can be represented by the transition graph in Fig. <a href="mdp.html#fig:mdp-robot-transition-graph">1.1</a>. Note that for this MDP, the transition dynamics is deterministic. We will see a stochastic MDP soon.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdp-robot-transition-graph"></span>
<img src="images/MDP/robot-mdp-graph.png" alt="A Simple Transition Graph." width="90%" />
<p class="caption">
Figure 1.1: A Simple Transition Graph.
</p>
</div>
<p>At time <span class="math inline">\(t=0\)</span>, if the robot starts at <span class="math inline">\(s_0 = \alpha\)</span>, first chooses action <span class="math inline">\(a_0 = \text{Move}\)</span>, and then chooses action <span class="math inline">\(a_1 = \text{Stay}\)</span>, the resulting trajectory is
<span class="math display">\[
\tau = (\alpha, \text{Move}, +1, \beta, \text{Stay}, 0,  \beta).
\]</span>
The return of this trajectory is:
<span class="math display">\[
g_0 = +1 + 0 = +1.
\]</span></p>
<p><strong>Policy Evaluation</strong>. Given a policy
<span class="math display">\[\begin{equation}
\pi = (\pi_0, \pi_1), \quad \pi_0(a \mid s) = \begin{cases}
0.5 &amp; a = \text{Move} \\
0.5 &amp; a = \text{Stay}
\end{cases},
\quad
\pi_1( a \mid s) = \begin{cases}
0.8 &amp; a = \text{Move} \\
0.2 &amp; a = \text{Stay}
\end{cases}.
\end{equation}\]</span>
We can use the Bellman consistency equations to compute the state-value function. We first initialize:
<span class="math display">\[
V^{\pi}_2 = \begin{bmatrix}
0 \\ 0
\end{bmatrix},
\]</span>
where the first row contains the value at <span class="math inline">\(s = \alpha\)</span> and the second row contains the value at <span class="math inline">\(s = \beta\)</span>.
We then perform the backward recursion for <span class="math inline">\(t=1\)</span>. For <span class="math inline">\(s = \alpha\)</span>, we have
<span class="math display">\[\begin{equation}
V^{\pi}_1(\alpha) = \begin{bmatrix}
\pi_1(\text{Move} \mid \alpha) \\
\pi_1(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_2(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_2(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8
\end{equation}\]</span>
For <span class="math inline">\(s = \beta\)</span>, we have
<span class="math display">\[\begin{equation}
V^{\pi}_1(\beta) = \begin{bmatrix}
\pi_1(\text{Move} \mid \beta) \\
\pi_1(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_2(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_2(\beta)
\end{bmatrix} = \begin{bmatrix}
0.8 \\ 0.2
\end{bmatrix}^{\top}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} = 0.8.
\end{equation}\]</span>
Therefore, we have
<span class="math display">\[
V^{\pi}_1 = \begin{bmatrix}
0.8 \\ 0.8
\end{bmatrix}.
\]</span>
We then proceed to the backward recursion for <span class="math inline">\(t=0\)</span>:
<span class="math display">\[\begin{align}
V_0^{\pi}(\alpha) &amp; = \begin{bmatrix}
\pi_0(\text{Move} \mid \alpha) \\
\pi_0(\text{Stay} \mid \alpha)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\alpha, \text{Move}) + V^{\pi}_1(\beta) \\
R(\alpha, \text{Stay}) + V^{\pi}_1(\alpha)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3. \\
V_0^{\pi}(\beta) &amp; = \begin{bmatrix}
\pi_0(\text{Move} \mid \beta) \\
\pi_0(\text{Stay} \mid \beta)
\end{bmatrix}^{\top} \begin{bmatrix}
R(\beta, \text{Move}) + V^{\pi}_0(\alpha) \\
R(\beta, \text{Stay}) + V^{\pi}_0(\beta)
\end{bmatrix} = \begin{bmatrix}
0.5 \\ 0.5
\end{bmatrix}^{\top}
\begin{bmatrix}
1.8 \\ 0.8
\end{bmatrix} = 1.3.
\end{align}\]</span>
Therefore, the state-value function at <span class="math inline">\(t=0\)</span> is
<span class="math display">\[
V^{\pi}_0 = \begin{bmatrix}
1.3 \\ 1.3
\end{bmatrix}.
\]</span>
You are encouraged to carry out the similar calculations for the action-value function.</p>
<hr />
<p>The toy example was small enough to carry out policy evaluation by hand; in realistic MDPs, we will need the help from computers.</p>
</div>
</div>
</div>
</div>
<div id="optimality" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Principle of Optimality<a href="mdp.html#optimality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="dp" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Dynamic Programming<a href="mdp.html#dp" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>



</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appconvex.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/hankyang94/OptimalControlReinforcementLearning/blob/main/01-mdp.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["optimal-control-reinforcement-learning.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
