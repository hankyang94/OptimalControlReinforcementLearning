\begin{thebibliography}{}

\bibitem[Antos et~al., 2007]{antos2007fitted}
Antos, A., Szepesv{\'a}ri, C., and Munos, R. (2007).
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock {\em Advances in neural information processing systems}, 20.

\bibitem[Arnold and Laub, 1984]{arnold84ieee-generalized}
Arnold, W.~F. and Laub, A.~J. (1984).
\newblock Generalized eigenproblem algorithms and software for algebraic
  riccati equations.
\newblock {\em Proceedings of the IEEE}, 72(12):1746--1754.

\bibitem[Baird et~al., 1995]{baird1995residual}
Baird, L. et~al. (1995).
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In {\em Proceedings of the twelfth international conference on
  machine learning}, pages 30--37.

\bibitem[Barto et~al., 2012]{barto2012neuronlike}
Barto, A.~G., Sutton, R.~S., and Anderson, C.~W. (2012).
\newblock Neuronlike adaptive elements that can solve difficult learning
  control problems.
\newblock {\em IEEE transactions on systems, man, and cybernetics},
  (5):834--846.

\bibitem[Chen, 1984]{chen1984book-linear}
Chen, C.-T. (1984).
\newblock {\em Linear system theory and design}.
\newblock Saunders college publishing.

\bibitem[Davison and Wonham, 1968]{davison1968tac-poleassign}
Davison, E. and Wonham, W. (1968).
\newblock On pole assignment in multivariable linear systems.
\newblock {\em IEEE Transactions on Automatic Control}, 13(6):747--748.

\bibitem[Fan et~al., 2020]{fan2020theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020).
\newblock A theoretical analysis of deep q-learning.
\newblock In {\em Learning for dynamics and control}, pages 486--489. PMLR.

\bibitem[Garrigos and Gower, 2023]{garrigos2023handbook}
Garrigos, G. and Gower, R.~M. (2023).
\newblock Handbook of convergence theorems for (stochastic) gradient methods.
\newblock {\em arXiv preprint arXiv:2301.11235}.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages
  1861--1870. Pmlr.

\bibitem[Janner et~al., 2019]{janner2019trust}
Janner, M., Fu, J., Zhang, M., and Levine, S. (2019).
\newblock When to trust your model: Model-based policy optimization.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Kakade, 2001]{kakade2001natural}
Kakade, S.~M. (2001).
\newblock A natural policy gradient.
\newblock {\em Advances in neural information processing systems}, 14.

\bibitem[Kang et~al., 2024]{kang2024fast}
Kang, S., Xu, X., Sarva, J., Liang, L., and Yang, H. (2024).
\newblock Fast and certifiable trajectory optimization.
\newblock In {\em International Workshop on the Algorithmic Foundations of
  Robotics}.

\bibitem[Kearns and Singh, 2000]{kearns2000bias}
Kearns, M.~J. and Singh, S. (2000).
\newblock Bias-variance error bounds for temporal difference updates.
\newblock In {\em COLT}, pages 142--147.

\bibitem[Lillicrap et~al., 2015]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D. (2015).
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}.

\bibitem[Liu and Nocedal, 1989]{liu1989limited}
Liu, D.~C. and Nocedal, J. (1989).
\newblock On the limited memory bfgs method for large scale optimization.
\newblock {\em Mathematical programming}, 45(1):503--528.

\bibitem[Mahmood et~al., 2015]{mahmood2015emphatic}
Mahmood, A.~R., Yu, H., White, M., and Sutton, R.~S. (2015).
\newblock Emphatic temporal-difference learning.
\newblock {\em arXiv preprint arXiv:1507.01569}.

\bibitem[Mnih et~al., 2015]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
  (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533.

\bibitem[Munos and Szepesv{\'a}ri, 2008]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C. (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(5).

\bibitem[Nesterov, 2018]{nesterov2018lectures}
Nesterov, Y. (2018).
\newblock {\em Lectures on convex optimization}, volume 137.
\newblock Springer.

\bibitem[Nocedal and Wright, 1999]{nocedal99book-numerical}
Nocedal, J. and Wright, S.~J. (1999).
\newblock {\em Numerical optimization}.
\newblock Springer.

\bibitem[Rawlings et~al., 2020]{rawlings2020model}
Rawlings, J.~B., Mayne, D.~Q., and Diehl, M. (2020).
\newblock {\em Model predictive control: theory, computation, and design},
  volume~2.
\newblock Nob Hill Publishing Madison, WI.

\bibitem[Riedmiller, 2005]{riedmiller2005neural}
Riedmiller, M. (2005).
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In {\em European conference on machine learning}, pages 317--328.
  Springer.

\bibitem[Robbins and Siegmund, 1971]{robbins1971convergence}
Robbins, H. and Siegmund, D. (1971).
\newblock A convergence theorem for non negative almost supermartingales and
  some applications.
\newblock In {\em Optimizing methods in statistics}, pages 233--257. Elsevier.

\bibitem[Schulman et~al., 2015a]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015a).
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages
  1889--1897. PMLR.

\bibitem[Schulman et~al., 2015b]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015b).
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Silver et~al., 2014]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
  (2014).
\newblock Deterministic policy gradient algorithms.
\newblock In {\em International conference on machine learning}, pages
  387--395. Pmlr.

\bibitem[Sutton and Barto, 1998]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G. (1998).
\newblock {\em Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge.

\bibitem[Sutton et~al., 2009]{sutton2009fast}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv{\'a}ri, C., and Wiewiora, E. (2009).
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 993--1000.

\bibitem[Sutton et~al., 2008]{sutton2008convergent}
Sutton, R.~S., Szepesv{\'a}ri, C., and Maei, H.~R. (2008).
\newblock A convergent o(n) algorithm for off-policy temporal-difference
  learning with linear function approximation.
\newblock {\em Advances in neural information processing systems},
  21(21):1609--1616.

\bibitem[W{\"a}chter and Biegler, 2006]{wachter2006implementation}
W{\"a}chter, A. and Biegler, L.~T. (2006).
\newblock On the implementation of an interior-point filter line-search
  algorithm for large-scale nonlinear programming.
\newblock {\em Mathematical programming}, 106(1):25--57.

\bibitem[Zhou et~al., 1996]{zhou1996book-robust}
Zhou, K., Doyle, J., and Glover, K. (1996).
\newblock Robust and optimal control.
\newblock {\em Control Engineering Practice}, 4(8):1189--1190.

\end{thebibliography}
