\begin{thebibliography}{}

\bibitem[Antos et~al., 2007]{antos2007fitted}
Antos, A., Szepesv{\'a}ri, C., and Munos, R. (2007).
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock {\em Advances in neural information processing systems}, 20.

\bibitem[Baird et~al., 1995]{baird1995residual}
Baird, L. et~al. (1995).
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In {\em Proceedings of the twelfth international conference on
  machine learning}, pages 30--37.

\bibitem[Barto et~al., 2012]{barto2012neuronlike}
Barto, A.~G., Sutton, R.~S., and Anderson, C.~W. (2012).
\newblock Neuronlike adaptive elements that can solve difficult learning
  control problems.
\newblock {\em IEEE transactions on systems, man, and cybernetics},
  (5):834--846.

\bibitem[Chen, 1984]{chen1984book-linear}
Chen, C.-T. (1984).
\newblock {\em Linear system theory and design}.
\newblock Saunders college publishing.

\bibitem[Davison and Wonham, 1968]{davison1968tac-poleassign}
Davison, E. and Wonham, W. (1968).
\newblock On pole assignment in multivariable linear systems.
\newblock {\em IEEE Transactions on Automatic Control}, 13(6):747--748.

\bibitem[Fan et~al., 2020]{fan2020theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020).
\newblock A theoretical analysis of deep q-learning.
\newblock In {\em Learning for dynamics and control}, pages 486--489. PMLR.

\bibitem[Garrigos and Gower, 2023]{garrigos2023handbook}
Garrigos, G. and Gower, R.~M. (2023).
\newblock Handbook of convergence theorems for (stochastic) gradient methods.
\newblock {\em arXiv preprint arXiv:2301.11235}.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages
  1861--1870. Pmlr.

\bibitem[Kakade, 2001]{kakade2001natural}
Kakade, S.~M. (2001).
\newblock A natural policy gradient.
\newblock {\em Advances in neural information processing systems}, 14.

\bibitem[Kearns and Singh, 2000]{kearns2000bias}
Kearns, M.~J. and Singh, S. (2000).
\newblock Bias-variance error bounds for temporal difference updates.
\newblock In {\em COLT}, pages 142--147.

\bibitem[Mahmood et~al., 2015]{mahmood2015emphatic}
Mahmood, A.~R., Yu, H., White, M., and Sutton, R.~S. (2015).
\newblock Emphatic temporal-difference learning.
\newblock {\em arXiv preprint arXiv:1507.01569}.

\bibitem[Mnih et~al., 2015]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
  (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533.

\bibitem[Munos and Szepesv{\'a}ri, 2008]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C. (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(5).

\bibitem[Nesterov, 2018]{nesterov2018lectures}
Nesterov, Y. (2018).
\newblock {\em Lectures on convex optimization}, volume 137.
\newblock Springer.

\bibitem[Riedmiller, 2005]{riedmiller2005neural}
Riedmiller, M. (2005).
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In {\em European conference on machine learning}, pages 317--328.
  Springer.

\bibitem[Robbins and Siegmund, 1971]{robbins1971convergence}
Robbins, H. and Siegmund, D. (1971).
\newblock A convergence theorem for non negative almost supermartingales and
  some applications.
\newblock In {\em Optimizing methods in statistics}, pages 233--257. Elsevier.

\bibitem[Schulman et~al., 2015a]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015a).
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages
  1889--1897. PMLR.

\bibitem[Schulman et~al., 2015b]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015b).
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Sutton and Barto, 1998]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G. (1998).
\newblock {\em Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge.

\bibitem[Sutton et~al., 2009]{sutton2009fast}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv{\'a}ri, C., and Wiewiora, E. (2009).
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 993--1000.

\bibitem[Sutton et~al., 2008]{sutton2008convergent}
Sutton, R.~S., Szepesv{\'a}ri, C., and Maei, H.~R. (2008).
\newblock A convergent o(n) algorithm for off-policy temporal-difference
  learning with linear function approximation.
\newblock {\em Advances in neural information processing systems},
  21(21):1609--1616.

\bibitem[Zhou et~al., 1996]{zhou1996book-robust}
Zhou, K., Doyle, J., and Glover, K. (1996).
\newblock Robust and optimal control.
\newblock {\em Control Engineering Practice}, 4(8):1189--1190.

\end{thebibliography}
