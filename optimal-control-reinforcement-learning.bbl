\begin{thebibliography}{}

\bibitem[Antos et~al., 2007]{antos2007fitted}
Antos, A., Szepesv{\'a}ri, C., and Munos, R. (2007).
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock {\em Advances in neural information processing systems}, 20.

\bibitem[Baird et~al., 1995]{baird1995residual}
Baird, L. et~al. (1995).
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In {\em Proceedings of the twelfth international conference on
  machine learning}, pages 30--37.

\bibitem[Chen, 1984]{chen1984book-linear}
Chen, C.-T. (1984).
\newblock {\em Linear system theory and design}.
\newblock Saunders college publishing.

\bibitem[Davison and Wonham, 1968]{davison1968tac-poleassign}
Davison, E. and Wonham, W. (1968).
\newblock On pole assignment in multivariable linear systems.
\newblock {\em IEEE Transactions on Automatic Control}, 13(6):747--748.

\bibitem[Fan et~al., 2020]{fan2020theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020).
\newblock A theoretical analysis of deep q-learning.
\newblock In {\em Learning for dynamics and control}, pages 486--489. PMLR.

\bibitem[Kearns and Singh, 2000]{kearns2000bias}
Kearns, M.~J. and Singh, S. (2000).
\newblock Bias-variance error bounds for temporal difference updates.
\newblock In {\em COLT}, pages 142--147.

\bibitem[Mahmood et~al., 2015]{mahmood2015emphatic}
Mahmood, A.~R., Yu, H., White, M., and Sutton, R.~S. (2015).
\newblock Emphatic temporal-difference learning.
\newblock {\em arXiv preprint arXiv:1507.01569}.

\bibitem[Mnih et~al., 2015]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
  (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533.

\bibitem[Munos and Szepesv{\'a}ri, 2008]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C. (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(5).

\bibitem[Riedmiller, 2005]{riedmiller2005neural}
Riedmiller, M. (2005).
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In {\em European conference on machine learning}, pages 317--328.
  Springer.

\bibitem[Robbins and Siegmund, 1971]{robbins1971convergence}
Robbins, H. and Siegmund, D. (1971).
\newblock A convergence theorem for non negative almost supermartingales and
  some applications.
\newblock In {\em Optimizing methods in statistics}, pages 233--257. Elsevier.

\bibitem[Sutton and Barto, 1998]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G. (1998).
\newblock {\em Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge.

\bibitem[Sutton et~al., 2009]{sutton2009fast}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv{\'a}ri, C., and Wiewiora, E. (2009).
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 993--1000.

\bibitem[Sutton et~al., 2008]{sutton2008convergent}
Sutton, R.~S., Szepesv{\'a}ri, C., and Maei, H.~R. (2008).
\newblock A convergent o(n) algorithm for off-policy temporal-difference
  learning with linear function approximation.
\newblock {\em Advances in neural information processing systems},
  21(21):1609--1616.

\bibitem[Zhou et~al., 1996]{zhou1996book-robust}
Zhou, K., Doyle, J., and Glover, K. (1996).
\newblock Robust and optimal control.
\newblock {\em Control Engineering Practice}, 4(8):1189--1190.

\end{thebibliography}
